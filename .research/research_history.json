{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "parameter-efficient tuning",
    "adapter-based fine-tuning",
    "low-rank adaptation",
    "prompt tuning",
    "hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Parameter-Efficient Fine-Tuning Design Spaces",
      "abstract": "Parameter-efficient fine-tuning aims to achieve performance comparable to\nfine-tuning, using fewer trainable parameters. Several strategies (e.g.,\nAdapters, prefix tuning, BitFit, and LoRA) have been proposed. However, their\ndesigns are hand-crafted separately, and it remains unclear whether certain\ndesign patterns exist for parameter-efficient fine-tuning. Thus, we present a\nparameter-efficient fine-tuning design paradigm and discover design patterns\nthat are applicable to different experimental settings. Instead of focusing on\ndesigning another individual tuning strategy, we introduce parameter-efficient\nfine-tuning design spaces that parameterize tuning structures and tuning\nstrategies. Specifically, any design space is characterized by four components:\nlayer grouping, trainable parameter allocation, tunable groups, and strategy\nassignment. Starting from an initial design space, we progressively refine the\nspace based on the model quality of each design choice and make greedy\nselection at each stage over these four components. We discover the following\ndesign patterns: (i) group layers in a spindle pattern; (ii) allocate the\nnumber of trainable parameters to layers uniformly; (iii) tune all the groups;\n(iv) assign proper tuning strategies to different groups. These design patterns\nresult in new parameter-efficient fine-tuning methods. We show experimentally\nthat these methods consistently and significantly outperform investigated\nparameter-efficient fine-tuning strategies across different backbone models and\ndifferent tasks in natural language processing.",
      "full_text": "PARAMETER -EFFICIENT FINE -TUNING DESIGN SPACES Jiaao Chen†∗, Aston Zhang‡, Xingjian Shi‡, Mu Li‡, Alex Smola‡, Diyi Yang⋄ †Georgia Institute of Technology,‡Amazon Web Services, ⋄Stanford University ABSTRACT Parameter-efﬁcient ﬁne-tuning aims to achieve performance comparable to ﬁne-tuning, using fewer trainable parameters. Several strategies (e.g., Adapters, preﬁx tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether cer- tain design patterns exist for parameter-efﬁcient ﬁne-tuning. Thus, we present a parameter-efﬁcient ﬁne-tuning design paradigm and discover design patterns that are applicable to different experi- mental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize tuning structures and tuning strate- gies. Speciﬁcally, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from an initial design space, we progressively reﬁne the space based on the model quality of each design choice and make greedy selection at each stage over these four components. We discover the following design patterns: (i) group layers in a spindle pattern; (ii) allocate the number of trainable parameters to layers uni- formly; (iii) tune all the groups; (iv) assign proper tuning strategies to different groups. These design patterns result in new parameter-efﬁcient ﬁne-tuning methods. We show experimentally that these methods consistently and signiﬁcantly outperform investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different tasks in natural language processing1. 1 Introduction Large pretrained models have achieved the state-of-the-art performances across a wide variety of downstream natural language processing tasks through ﬁne-tuning on task-speciﬁc labeled data [Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019, Joshi et al., 2019, Sun et al., 2019, Clark et al., 2019, Lewis et al., 2020a, Bao et al., 2020, He et al., 2020, Raffel et al., 2020, Ziems et al., 2022]. However, ﬁne-tuning all the parameters and storing them separately for different tasks is expensive in terms of computation and storage overhead (e.g., 355M parameters for RoBERTa [Liu et al., 2019] and 175B parameters for GPT- 3 [Brown et al., 2020]). This makes it difﬁcult to deploy in real-world natural language processing (NLP) systems composed of multiple tasks. To adapt general knowledge in pretrained models to speciﬁc down-stream tasks in a more parameter-efﬁcient way, various strategies have been proposed where only a small number of (extra) parameters are learned while the remaining pretrained parameters are frozen [Houlsby et al., 2019a, Pfeiffer et al., 2021, Li and Liang, 2021, Brown et al., 2020, Lester et al., 2021a, Schick and Sch ¨utze, 2021, Ziems et al., 2022]. Adapter tuning [Houlsby et al., 2019a] is among the earliest strategies to steer pretrained models with a limited number of parameters. It inserts adapters (small neural modules) to each layer of the pretrained network and only the adapters are trained at the ﬁne-tuning time. Inspired by the success of prompting methods that control pretrained language models through textual prompts [Brown et al., 2020], preﬁx tuning [Li and Liang, 2021] and prompt tuning [Lester et al., 2021b] prepend additional tunable tokens to the input or hidden layers and only train these soft prompts when ﬁne-tuning on downstream tasks. BitFit [Zaken et al., 2021] updates the bias terms in pretrained models while freezing the remaining parameters. LoRA [Hu et al., 2021] decomposes attention weight gradients into low-rank matrices to reduce the number of trainable parameters. With promising results from such research, He et al. [2022] proposed a uniﬁed view of these existing strategies and ∗Work done during an internship at Amazon Web Services. Correspondence to Jiaao Chen<jiaaochen@gatech.edu> and Aston Zhang <astonz@amazon.com>. 1Code is available at: https://github.com/amazon-science/peft-design-spaces . arXiv:2301.01821v1  [cs.CL]  4 Jan 2023P P P L P L A B L A B L… Layer Grouping P L Strategy Assignment Trainable Parameter Allocation Tunable Groups p ⇥ p Figure 1: A parameter-efﬁcient ﬁne-tuning design space. It is characterized by (i) layer grouping (how to group consecutive layers), (ii) trainable parameter allocation (how to allocate the number of trainable parameters to layers), (iii) tunable groups (which groups will be ﬁnetuned), and (iv) strategy assignment (how to assign proper strategies, such as among Adapter, Preﬁx, BitFit, and LoRA, to groups). illustrated differences and connections among them. Like its antecedents, the resulting method is stillequally assigned to different pretrained layers. Despite being effective, most parameter-efﬁcient ﬁne-tuning strategies have been developed via manual design pro- cesses, without much consideration of whether design patterns exist across these different strategies and how such patterns might apply to different backbone models and downstream tasks. Moreover, different strategies are usually applied separately; thus, it is unclear which strategy works best when and where [Mao et al., 2022], as well as how these different strategies reinforce or complement each other. In this light, our goal is to understand the parameter- efﬁcient ﬁne-tuning design in a more comprehensive view and discover design patterns that are both interpretable and applicable across different experimental settings. Instead of designing yet another individual strategy that is equally applied to different pretrained layers, we introduce parameter-efﬁcient ﬁne-tuning design spaces that parameterize both tuning structures and strategies. More con- cretely, any of these design spaces is characterized by four major components as shown in Figure 1: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained parameter-efﬁcient ﬁne-tuning design space, we progressively reﬁne the space by comparing the overall quality of models randomly sampled from design spaces enforced with different constraints (e.g., each group has the same number of layers). Throughout the experimental process, we discover several design patterns for parameter-efﬁcient ﬁne-tuning, such as group layers in a spindle pattern, allocate the number of trainable parameters to layers uniformly, tune all the groups, and assign proper tuning strategies to different groups. We fur- ther introduce new parameter-efﬁcient ﬁne-tuning methods that adopt all these discovered design patterns. Extensive experiments show that our methods consistently outperform investigated parameter-efﬁcient ﬁne-tuning strategies. Al- though we use T5 [Raffel et al., 2020] and classiﬁcation tasks as the working example, we ﬁnd that our methods with all these discovered design patters are applicable to other backbones (e.g., RoBERTa [Liu et al., 2019], BART [Lewis et al., 2020b], and XLNet [Yang et al., 2019]) and different natural language processing tasks (e.g., summarization, machine translation, and eight SuperGLUE datasets). Our contributions can be summarized as follows: (i) We introduce parameter-efﬁcient ﬁne-tuning design spaces. (ii) Based on these design spaces, we discover several design patterns in parameter-efﬁcient ﬁne-tuning via comprehen- sive experiments. (iii) Our discovered design patterns lead to parameter-efﬁcient ﬁne-tuning methods, consistently outperforming investigated parameter-efﬁcient ﬁne-tuning strategies across different backbone models and different NLP tasks. 22 Related Work Our work is closely related to and built upon the research about the network design spaces and parameter-efﬁcient ﬁne-tuning. We discuss the connections and differences below. Network Design Spaces A lot of works designed neural network models via an ad-hoc discovery of new design choices that improve performances [Radosavovic et al., 2019], such as the use of deeper architectures or residuals. Recently, there have been works [Radosavovic et al., 2020, You et al., 2020, Radosavovic et al., 2019] performing at the design space level to discover new design principles for convolutional neural networks [Radosavovic et al., 2020] and graph neural networks [You et al., 2020]. Inspired by this line of research, we focus on the design space perspective to rethink parameter-efﬁcient ﬁne-tuning, with the goal of discovering design patterns that are applicable to different experimental settings. Parameter-Efﬁcient Fine-Tuning for NLP As pretrained models grow in size, storing ﬁne-tuned models becomes exceedingly expensive, and ﬁne-tuning becomes infeasible for those without extremely high compute resources. A growing body of research has been devoted to ﬁnding parameter-efﬁcient alternatives for adapting large-scale pre- trained models with reduced memory and storage costs. Houlsby et al. [2019b] proposed to adapt large models using bottleneck layers (with skip-connections) between each layer. This idea has been extended in many domains [Stick- land and Murray, 2019, Pfeiffer et al., 2020, Rebufﬁ et al., 2017, Lin et al., 2020]. Other works have aimed to avoid introducing additional parameters by identifying and training only a subset of all model parameters [Zhao et al., 2020, Guo et al., 2020, Mallya et al., 2018, Radiya-Dixit and Wang, 2020, Sung et al., 2021, Zaken et al., 2021]. Recent works also explored the idea of rank decomposition based on parameterized hypercomplex multiplications via the Kro- necker product [Zhang et al., 2021a] and injecting trainable rank decomposition matrices into each layer [Hu et al., 2021, Karimi Mahabadi et al., 2021]. Li and Liang [2021] introduced preﬁx-tuning that prepends a set of preﬁxes to autoregressive language models or prepends preﬁxes for both encoders and decoders. The preﬁx parameters are updated while the pretrained parameters are ﬁxed. Lester et al. [2021a] proposed a similar method, but only added virtual tokens at the embedding layer of large-scale models rather than discrete prompts [Deng et al., 2022, Zhong et al., 2022]. Bari et al. [2022] proposed semi-parametric prompt tuning that converges more easily, where memory prompts are input-adaptive without the need for tuning. Recently, He et al. [2022] and Ding et al. [2022] proposed a uniﬁed view of the existing parameter-efﬁcient ﬁne-tuning strategies and illustrated the difference and connections among them. Mao et al. [2022] also introduced a uniﬁed framework to combine different methods through mixture- of-experts. In contrast to these aforementioned works that assign their individual method equally to different pretrained layers, we focus on more general design spaces of parameter-efﬁcient ﬁne-tuning. This could provide a more comprehensive view of parameter-efﬁcient ﬁne-tuning in terms of both the tuning structures and tuning strategies. Through experiments where we progressively reﬁne design spaces, we discover design patterns for parameter-efﬁcient ﬁne-tuning. 3 Components of Design Spaces When deﬁning design spaces of parameter-efﬁcient ﬁne-tuning, we aim to cover key design components and provide a representative set of choices in each design component. Note that our goal is not to enumerate all possible design spaces, but to demonstrate how the use of design spaces can help inform parameter-efﬁcient ﬁne-tuning research. Concretely, in our work, the parameter-efﬁcient ﬁne-tuning design spaces are formed by a representative set of choices in parameter-efﬁcient ﬁne-tuning, which consists of the following four components: (i) layer grouping, (ii) trainable parameter allocation, (iii) tunable groups, and (iv) strategy assignment. Following the illustrated design space exam- ple in Figure 1, we describe these four design components in detail below and will explore their design choices in Section 4. Layer Grouping Different layers in pretrained models capture different information and behave differently. For example, Jawahar et al. [2019] found that the {3, 4, 5, 6, 7, 9, 12}-th layers have the most representation power in BERT and every layer captures a different type of information ranging from the surface, syntactic, to the semantic level representation of text. For instance, the 9th layer has predictive power for semantic tasks such as checking random swapping of coordinated clausal conjuncts, while the 3rd layer performs best in surface tasks like predicting sentence length. Therefore when adapting these pretrained models to downstream tasks, how to group layers with similar behaviors together is critical to the design and application of proper parameter-efﬁcient ﬁne-tuning strategies. For this design component, we study the patterns of how to group consecutive layers in pretrained models (e.g., transformer layers in T5) during the ﬁne-tuning process. 3Trainable Parameter Allocation In parameter-efﬁcient ﬁne-tuning, the total number of trainable parameters is usually preset, such as a small portion of the total number of parameters in the pretrained models. We will study different design choices for how to allocate a predeﬁned number of trainable parameters to layers. Tunable Groups Zaken et al. [2021] found that not all the parameters need to be tuned during ﬁne-tuning on the downstream tasks. For instance, BitFit [Zaken et al., 2021] only updates the bias parameters in pretrained models while freezing the remaining parameters. Thus, we study which groups need to be learned during parameter-efﬁcient ﬁne-tuning to attain better performances. Strategy Assignment In order to improve the parameter efﬁciency, different sets of strategies [Li and Liang, 2021, Lester et al., 2021a, Houlsby et al., 2019a, Hu et al., 2021] have been proposed where only a small number of (ex- tra) parameters are tuned and the remaining parameters in these pretrained models are frozen to adapt their general knowledge to speciﬁc down-stream tasks. Inspired by effectiveness of offering architectural ﬂexibility [Zhang et al., 2021a,b], we hypothesize that different groups might beneﬁt from different proper strategies (or combinations) for capturing different types of information. More formally, given a set of individual strategies Afor assignment, for any group Gi, assign a subset Ui ⊂A to each layer in Gi. 4 Discovering Design Patterns Building on these four different design components of PEFT design spaces, we will start from a relatively uncon- strained design space and progressively discover the design patterns. 4.1 Design Space Experimental Setup We ﬁrst describe our experimental setup for discovering the design patterns. Note that our process is generic for other tasks and future pretrained backbone models. Datasets Our process for discovering design patterns of PEFT is based on the average performances on the widely- used GLUE benchmark [Wang et al., 2018]. It covers a wide range of natural language understanding tasks. First, single-sentence tasks include (i) Stanford Sentiment Treebank (SST-2) and (ii) Corpus of Linguistic Acceptability (CoLA). Second, similarity and paraphrase tasks include (i) Quora Question Pairs (QQP), (ii) Semantic Textual Sim- ilarity Benchmark (STS-B), and (iii) Microsoft Research Paraphrase Corpus (MRPC). Third, inference tasks include (i) Multi-Genre Natural Language Inference (MNLI), (ii) Question Natural Language Inference (QNLI), and (iii) Rec- ognizing Textual Entailment (RTE). To compare performances, the Matthews correlation is measured for CoLA; the Spearman correlation is used for STS-B, and accuracy is measured for the rest GLUE tasks. Pretrained Backbone Models and Model Settings We use T5-base/3b [Raffel et al., 2020] as the main pretrained backbone models for discovering design patterns via our PEFT design spaces. We use Hugging Face 2 for our imple- mentations and follow the default settings. During the exploration, we set the total number of trainable parameters (in the percentage of that in the backbone model) to 0.5% by following He et al. [2022]. 4.2 Discovering Design Patterns Using T5-base In this subsection, we describe the empirical process for discovering the design patterns using T5-base (pretrained backbone model) as the working example. Each PEFT design space (denoted as Si) consists of a set of models ( Si- models) that satisfy constraints characterizing the space with respect to layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. To discover design patterns, we start from a relatively unconstrained PEFT design space ( S0). Then we progressively reﬁne design spaces (from S0 to S1:4) by comparing overall quality of models in design spaces enforced with different constraints (e.g., each group has the same number of layers). To quantify the overall quality of models in any design space Si with a low-compute, low-epoch regime [Radosavovic et al., 2020], we randomly sample 100 models from Si, ﬁne-tune with 3 epochs 3, and compute the average of the GLUE average performances. 2https://huggingface.co/docs/transformers/index 3We set the low epoch by observing whether it is enough for models to obtain stable performances to draw consistent conclusions (See Table 7 in the Appendix). 4We emphasize that our goal is to demonstrate how the perspective of design spaces can help inform PEFT research, rather than to ﬁnd out the “best” design space or method. For computational efﬁciency, it is beyond the scope of this work to enumerate all possible constraints with respect to the design space components (Section 3). 4.2.1 The Initial S0 Design Space The initial relatively unconstrained design space S0 consists of all models without constraints on the design space components (Section 3). Individual PEFT strategies consist of Adapter, Preﬁx, BitFit, and LoRA. One can think of this S0 design space as a set of random models ( S0-models) with random design patterns. Speciﬁcally, without grouping constraints, each layer of the pretrained layer has a half chance to be tuned: if tuned, random strategies (or combinations) with a random amount of trainable parameters are assigned to that layer. Before comparing more subtle design patterns such as how to properly assign tunable strategies among Adapter, Preﬁx, BitFit, and LoRA, we begin with exploring how to group layers and how to allocate the total number of trainable parameters to layers. 4.2.2 The S1 Design Space with Additional Grouping Constraints Inspired by Radosavovic et al. [2020], we also consider 4 groups (G1, . . . , G4, in the order of forward pass) in the experiments 4. Denote by Ni the number of layers in Gi. As illustrated in Figure 2, we compare the following layer grouping patterns: (i) Increasing (Ni+1 > Ni): the number of layers in groups gradually increases; (ii) Uniform (Ni+1 = Ni): the number of layers in groups is the same; (iii) Decreasing (Ni+1 < Ni): the number of layers in groups gradually decreases; (iv) Spindle (N1 < N2 = N3 > N4): the numbers of layers in groups at both ends are smaller; and (v) Bottleneck (N1 > N2 = N3 < N4): the numbers of layers in groups at both ends are bigger. Figure 2: Layer grouping patterns, where the horizontal and vertical axes represent groups (G1, . . . , G4) and numbers of layers in groups. These layer grouping patterns lead to 5 different design spaces. Any of these 5 design spaces consists of all models in the S0 design space that satisfy one of these grouping pattern constraints. To compare the overall model qualities of different design spaces, we (i) randomly sample 100 models from the S0 design space that satisfy each grouping pattern constraint (Figure 2); (ii) ﬁne-tune with 3 epochs; and (iii) compute the average performances for each design space. We will follow this procedure as we progressively add new constraints later. The averaged performances are shown in Table 1 5. We ﬁnd that models from the design space with the spindle grouping pattern (Figure 2) consistently outperform those from the other design spaces across all the 8 GLUE tasks. This may be due to the complexities of information captured in different layers of large pretrained models, which favor information adaptation in the discovered layer grouping pattern. From now on, we will group layers in a spindle pattern. We refer to S0 with this additional design pattern as the new S1 design space. 4.2.3 The S2 Design Space with Additional Parameter Constraints We continue to explore design patterns in trainable parameter allocation to reﬁne the S1 design space. Denote by ni the number of trainable parameters for the i-th layer of the pretrained backbone model, we compare the following design patterns: (i) Increasing (ni+1 ≥ni): the number of trainable parameters in every layer gradually increases (or remains the same); (ii) Uniform (ni+1 = ni): the number of trainable parameters in every layer is the same; and (iii) Decreasing (ni+1 ≤ni): the number of trainable parameters in every layer gradually decreases (or remains the same). Following the procedure described in Section 4.2.2, we obtain 100 models for each of these 3 new design spaces. Table 2 reports the average performances of these 3 design spaces. The uniform allocation design pattern obtains the highest GLUE average performance, making this relatively simple, interpretable design pattern favorable. 4The experimental results with 8 groups are shown in the Table 16 in the Appendix. 5The training time for the step is shown in the Table 18 in the Appendix. 5Table 1: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 70.0 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 37.3 73.3 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 Table 2: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different parameter allocation constraints to the S1 design space. Param Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 87.2 77.9 79.4 78.7 71.6 77.6 81.4 32.0 73.2 Uniform 87.8 77.4 80.1 80.5 73.9 78.1 80.4 34.3 74.0 Decreasing 86.4 75.8 78.4 77.0 70.4 77.1 78.7 35.8 72.4 We will allocate the number of trainable parameters to layers uniformly. We refer to S1 with this additional design pattern as the new S2 design space. 4.2.4 The S3 Design Space with Additional Tunable Group Constraints Before digging into the strategy assignment design patterns, it is necessary to examine which groups need to be tuned. After all, it is only meaningful to study assigning strategies to different groups after we ﬁnd out which groups need to be ﬁne-tuned. As shown in Table 3, we explore various design patterns in tunable groups to further constrain the S2 design space. Based on the GLUE average performances, we ﬁnd that all the groups need to be tuned to obtain the best performances. This suggests that all the groups of pretrained layers have captured useful information that should be adapted to the downstream tasks. We will tune all the groups. We refer to S2 with this additional design pattern as the new S3 design space. 4.2.5 The S4 Design Space with Additional Strategy Constraints Finally, we study the subtle design pattern with respect to assigning proper strategies by further constraining the derived S3 design space. Speciﬁcally, each design space consists of models that assign a subset of {Adapter (A), Preﬁx (P), BitFit (B), and LoRA (L) }to all layers of any group Gi (i = 1, . . . ,4). We begin by adding different G1 strategy assignment constraints to the S3 space. Following the same pattern discovery procedure (Section 4.2.2), we discover strategy assignment patterns for G1. Then we progressively add Gi (i >1) strategy assignment constraints together with the discovered strategy assignment patterns for all Gj (j = 1, . . . , i−1) to the S3 space. Due to space limit, we present results of this process in the Appendix ( G1 in Table 8, G2 Table 9, G3 in Table 10, and G4 in Table 11), which suggests strategy assignment ofG1-(A, L) – G2-(A, P) – G3-(A, P, B) –G4-(P, B, L) for the T5-base pretrained backbone model. We will assign the discovered proper tuning strategies to groups.We refer to S3 with this additional design pattern as the new S4 design space, which consists of the ﬁnal S4-model. 4.3 Discovering Design Patterns Using T5-3b We then repeat the above process on T5-3b to examine if the design patterns we discovered using smaller models (T5- base) still apply when we use larger models. The results are shown in Table 12 (layer grouping), Table 13 (trainable parameter allocation), Table 14 (tunable groups) and Table 15 (strategy assignment) in the Appendix. We observe that the design patterns still apply when larger models like T5-3b are used: (i) grouping layers in a spindle pattern (Table 12), (ii) uniformly allocating the number of trainable parameters to layers (Table 13), (iii) tuning all the groups 6Table 3: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different tunable group constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 82.6 72.1 77.6 70.6 65.3 71.9 77.6 27.6 68.2 G2 83.3 72.8 77.5 72.8 63.6 72.8 77.5 27.5 68.4 G3 83.6 73.3 78.2 73.3 66.4 71.3 77.9 22.9 68.4 G4 83.2 73.0 77.9 73.7 63.9 72.0 77.9 27.9 68.7 G1, G2 83.5 73.2 78.0 75.4 67.7 73.2 78.0 28.0 69.6 G3, G4 87.8 74.6 78.3 76.9 68.6 74.3 78.3 28.3 70.7 G1, G2, G3 86.0 75.8 79.0 77.8 71.8 78.8 79.0 33.0 72.6 G2, G3, G4 85.2 76.6 79.1 78.6 70.1 77.6 79.1 31.9 72.2 G1,G2,G3,G4 88.3 77.4 82.1 81.5 74.9 79.4 81.4 34.3 74.9 Table 4: Performances of different tuning methods on the GLUE datasets using the T5-base (upper part) and T5-3b (lower part) pretrained backbone models, respectively. The results are averaged over 20 random runs (with standard deviations as subscripts). The S4-model and the S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 95.2 87.1 93.7 89.4 80.1 89.4 90.7 51.1 84.5 Adapter 94.6 85.5 89.8 86.7 75.3 86.7 89.1 59.2 83.3 Preﬁx 94.0 81.6 87.8 83.4 64.3 83.1 84.8 34.0 76.6 BitFit 94.4 84.5 90.6 88.3 74.3 86.6 90.1 57.7 83.3 LoRA 94.8 84.7 91.6 88.5 75.8 86.3 88.7 51.5 82.7 S4-model 95.5∗∗ 1.7 87.6∗∗ 1.0 92.7∗∗ 1.1 88.8∗∗ 1.0 80.4∗ 2.3 87.4∗ 2.0 91.2∗∗ 2.4 62.2∗ 3.2 85.7 full 97.4 91.4 96.3 89.7 91.1 90.6 92.5 67.1 89.5 Adapter 96.3 89.9 94.7 87.8 83.4 90 89.7 65.2 87.1 Preﬁx 96.3 82.8 88.9 85.5 78.3 83.5 85.4 42.7 80.4 BitFit 95.8 89.5 93.5 88.5 86.2 90.7 88.6 64.2 87.1 LoRA 96.2 90.6 94.9 89.1 91.2 91.1 91.1 67.4 88.9 S4-3b-model 97.2∗∗ 1.8 91.6∗∗ 1.2 96.6∗∗ 1.0 89.5∗∗ 1.5 91.5∗ 2.8 91.5∗ 2.5 91.9∗ 2.0 69.7∗ 3.4 89.9 (Table 14), and (iv) tuning different groups with proper strategies (Table 15). For T5-3b, the discovered proper strategy assignment is G1-(P, L) –G2-(A, L) – G3-(P, B, L) –G4-(A, P, B). We refer to the ﬁnal design space asS4-3b and the ﬁnal model in this space as S4-3b-model. 5 Evaluation The S4-model (Section 4.2.5) and S4-3b-model (Section 4.3) adopt all the design patterns that have been discovered by using T5-base and T5-3b, respectively. As a result, they are both new methods of PEFT. We will evaluate their effectiveness when applied to different pretrained backbone models and different NLP tasks. 5.1 Experimental Setup Datasets Besides the GLUE datasets [Wang et al., 2018] (Section 4.1), we further evaluate our methods on two generation tasks used by He et al. [2022]: (i) Abstractive Summarization using XSum [Narayan et al., 2018], and (ii) Machine Translation using the WMT 2016 en-ro dataset [Bojar et al., 2016]. We report ROUGE scores [Lin, 2004] on the XSum test set, and BLEU scores [Papineni et al., 2002] on the en-ro test set. Models and Model Settings We mainly compare our methods with the following baselines: (i) Full Fine-tuning (full): it ﬁne-tunes all the model parameters in the pretrained models; (ii) Adapter [Houlsby et al., 2019a]: it adds adapter modules to each transformer layer; (iii) Preﬁx [Li and Liang, 2021]: it optimizes a set of small continuous vectors prepended to transformer layers; (iv) BitFit [Zaken et al., 2021]: it only updates the bias terms in pretrained models; (v) LoRA [Hu et al., 2021]: it decomposes the attention weight into low-rank matrices to reduce the number of trainable parameters. Besides T5 [Raffel et al., 2020], we additionally apply our methods to other backbone models 7Table 5: Performances of different tuning methods on GLUE datasets using the RoBERTa-base (upper part) and RoBERTa-large (lower part) pretrained backbone models. The results are averaged over 20 random runs (with standard deviations as subscripts). Here we also include two baselines: (i) S0-model, where all the designs are randomly selected for RoBERTa as in the S0 design space; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in the S3 design space. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05(∗) or even p <0.01(∗∗). Method SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Average full 94.8 87.6 92.8 91.9 80.8 90.3 90.2 63.6 86.5 Adapter 94.2 87.1 93.1 90.2 71.5 89.7 88.5 60.8 84.4 Preﬁx 94.0 86.8 91.3 90.5 74.5 90.3 88.2 61.5 84.6 BitFit 93.7 84.8 91.3 84.5 77.8 90.8 90.0 61.8 84.3 LoRA 94.9 87.5 93.1 90.8 83.1 90.0 89.6 62.6 86.4 S0-model 94.2 95.3 90.4 90.6 75.6 89.6 88.0 60.9 85.6 S3-model 94.3 87.2 92.8 91.0 81.8 90.3 89.2 63.2 86.2 S4-model 94.81.6 87.8∗∗ 0.8 93.4∗∗ 1.3 91.6∗ 1.2 85.8∗∗ 1.8 90.4∗ 2.0 90.0∗∗ 1.8 63.2∗ 3.5 87.1 full 96.4 90.2 94.7 92.2 86.6 92.4 90.9 68.0 88.9 Adapter 96.6 90.5 94.8 91.7 80.1 92.1 90.9 67.8 88.1 Preﬁx 95.7 87.6 92.1 88.7 82.3 89.6 87.4 62.8 85.7 BitFit 96.1 88.0 93.4 90.2 86.2 90.9 92.7 64.2 87.7 LoRA 96.2 90.6 94.7 91.6 87.4 92.0 89.7 68.2 88.8 S0-model 95.5 86.5 92.3 89.8 84.6 89.2 86.3 61.2 85.6 S3-model 96.3 89.4 93.8 90.2 85.9 90.8 90.9 63.4 87.6 S4-3b-model 96.6∗∗ 1.3 90.8∗ 1.1 95.1∗∗ 0.8 92.0∗∗ 1.2 87.22.8 92.3∗ 2.2 91.8∗∗ 1.8 68.4∗ 3.2 89.3 including RoBERTa-base/large [Liu et al., 2019] and BART-base/large [Lewis et al., 2020a]. We use the default settings. We set the total number of trainable parameters (in the percentage of that in the backbone model) by following He et al. [2022]. Speciﬁcally, this value is set to 0.5% for Adapter, Preﬁx, LoRA, and our methods, and 0.1% for BitFit. For all the experiments, we followed Liu et al. [2019] to set the linear decay scheduler with a warmup ratio of 0.06 for training. The batch size was 128 for base models and 64 for large models. The maximum learning rate was 5e −5 and the maximum number of training epochs was set to be either 5 or 10. All the experiments were performed using 8 A100 GPUs. 5.2 Effectiveness on GLUE with T5 Backbones Table 6: Performances of different tuning methods on generation tasks (XSUM and en-ro) using the BART-base (upper part) and BART-large (lower part) pretrained backbone models. Method XSUM(R-1/2/L) en-ro (BLEU) full 40.5/19.2/34.8 34.5 Adapter 37.7/17.9/33.1 33.3 Preﬁx 38.2/18.4/32.4 33.8 BitFit 37.2/17.5/31.4 33.2 LoRA 38.9/18.6/33.5 33.6 PA 39.3/18.7/33.8 33.8 S4-model 40.2/19.3/34.2 34.1 full 45.1/22.3/37.2 37.9 Adapter 43.8/20.8/35.7 35.3 Preﬁx 43.4/20.4/35.5 35.6 BitFit 42.8/18.7/33.2 35.2 LoRA 42.9/19.4/34.8 35.8 PA 43.9/20.6/35.6 36.4 S4-3b-model 44.3/21.7/36.8 37.2 With our discovered design patterns, we ﬁne-tune T5-base (S4-model) and T5-3b ( S4-3b-model) on GLUE and compare them with all the baseline methods. The results are shown in Table 4, where the key measure is the GLUE average performance (last column). We ﬁnd that our S4-model and S4- 3b-model consistently outperform the investigated methods in the key measure. By tuning only 0.5% parameters, our methods even outperform the full ﬁne-tuning baseline where all the parameters are tuned, indicating the effectiveness of our discov- ered PEFT design patterns. 5.3 General Effectiveness on GLUE with RoBERTa Backbones We directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5- base and T5-3b) to ﬁne-tune the RoBERTa-base and RoBERTa-large pretrained backbone models (with no extra discovery process), respectively. We keep all the other settings the same and evaluate them on GLUE datasets. We also compare with variant methods randomly sampled from two de- 8sign spaces: (i) S0-model, where all the designs are randomly selected for RoBERTa as in S0; (ii) S3-model, where strategies are randomly assigned to different RoBERTa layer groups as in S3. Table 5 shows that (i) the design pat- terns (adopted by S4-model and S4-3b-model) discovered using T5 models are applicable to the RoBERTa backbone models and outperform the investigated methods in GLUE average performances with no extra discovery process;(ii) improved performances fromS0-models, S3-models, to S4-(3b)-models support adding more constraints in the pattern discovery process (Section 4). 5.4 General Effectiveness on Generation Tasks with BART Backbones Like in Section 5.3, we further directly apply the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the BART-base and BART-large pretrained backbone models (without additional discovery process.), respectively. We evaluate the models on two generation tasks: summarization (XSUM) and machine translation (en-ro) following He et al. [2022]. We also compare with PA (parallel adapter) using the same number of trainable parameters [He et al., 2022]. Table 6 shows that our methods, although adopting design patterns discovered from classiﬁcation tasks using T5, still outperform investigated PEFT strategies on generation tasks with different BART backbones. 6 Conclusion PEFT adapts knowledge in pretrained models to down-stream tasks in a more parameter-efﬁcient fashion. Instead of focusing on designing another strategy in the ﬁrst place, we introduced PEFT design spaces. We empirically discovered several design patterns in PEFT. These design patterns led to new PEFT methods. Experiments showed that these methods consistently outperform investigated PEFT strategies across different backbone models and different tasks in natural language processing. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InAdvances in neural information processing systems, pages 5754–5764, 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64–77, 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, transla- tion, and comprehension. SCL, 2020a. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for uniﬁed language model pre-training. arXiv preprint arXiv:2002.12804, 2020. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2020. Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, and Diyi Yang. V ALUE: Understanding dialect disparity in NLU. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 91: Long Papers) , pages 3701–3720, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.258. URL https://aclanthology.org/2022.acl-long.258. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning , vol- ume 97 of Proceedings of Machine Learning Research , pages 2790–2799. PMLR, 09–15 Jun 2019a. URL http://proceedings.mlr.press/v97/houlsby19a.html. Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 487–503, Online, April 2021. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2021.eacl-main.39. Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation, 2021. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning, 2021a. Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classiﬁcation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pages 255–269, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045–3059, Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243. Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer- based masked language-models, 2021. URL https://arxiv.org/abs/2106.10199. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. UniPELT: A uniﬁed framework for parameter-efﬁcient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 6253–6264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https: //aclanthology.org/2022.acl-long.433. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language genera- tion, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics , pages 7871–7880, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://www.aclweb.org/anthology/2020.acl-main.703. Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr Doll ´ar. On network design spaces for visual recognition, 2019. URL https://arxiv.org/abs/1905.13214. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Designing network design spaces, 2020. URL https://arxiv.org/abs/2003.13678. Jiaxuan You, Rex Ying, and Jure Leskovec. Design space for graph neural networks, 2020. URL https://arxiv. org/abs/2011.08843. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019b. Asa Cooper Stickland and Iain Murray. Bert and pals: Projected attention layers for efﬁcient adaptation in multi-task learning. In International Conference on Machine Learning, pages 5986–5995. PMLR, 2019. 10Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter- efﬁcient transfer learning. arXiv preprint arXiv:2004.03829, 2020. Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch ¨utze. Masking as an efﬁcient alternative to ﬁnetuning for pretrained language models. arXiv preprint arXiv:2004.12406, 2020. Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463, 2020. Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. Evani Radiya-Dixit and Xin Wang. How ﬁne can ﬁne-tuning be? learning efﬁcient language models. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2435–2443. PMLR, 2020. Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural networks with ﬁxed sparse masks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Pro- cessing Systems, volume 34, pages 24193–24205. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper/2021/file/cb2653f548f8709598e8b5156738cc51-Paper.pdf. Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected lay- ers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In International Conference on Learning Representations, 2021a. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022–1035, 2021. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning, 2022. URL https: //arxiv.org/abs/2205.12548. Wanjun Zhong, Yifan Gao, Ning Ding, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan. Improving task generalization via uniﬁed schema prompt, 2022. URL https://arxiv.org/abs/2208.03229. M Saiful Bari, Aston Zhang, Shuai Zheng, Xingjian Shi, Yi Zhu, Shaﬁq Joty, and Mu Li. Spt: Semi-parametric prompt tuning for multitask prompted learning. arXiv preprint arXiv:2212.10929, 2022. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Delta tuning: A comprehensive study of parameter efﬁcient methods for pre-trained language models, 2022. URL https://arxiv.org/abs/2203.06904. Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does BERT learn about the structure of language? In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy, July 2019. Association for Computational Linguistics. Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and Shuai Zhang. Self-instantiated recurrent units with dynamic soft recursion. Advances in Neural Information Processing Systems, 34:6503–6514, 2021b. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convo- lutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206. Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Ji- meno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur ´elie N ´ev´eol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Mar- cos Zampieri. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131–198, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2301. URL https://aclanthology.org/W16-2301. 11Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002. 12A More Experimental Results Table 7: Average performances (low-compute, low-epoch regime: 100 random models, tuning epochs = 1, 2, 3, 4, 20 for ﬁve different blocks) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg 1 epochs Increasing 73.2 63.3 67.8 68.8 63.8 67.2 64.1 11.0 59.9 Uniform 72.8 64.1 63.4 63.4 62.5 69.8 65.8 12.1 59.2 Decreasing 72.4 63.2 65.1 69.8 59.3 62.7 63.6 18.7 59.4 Spindle 72.6 64.8 66.8 71.1 62.1 62.3 64.8 12.3 59.6 Bottleneck 72.2 63.7 65.3 68.3 61.2 63.2 66.6 12.1 59.0 2 epochs Increasing 76.2 69.3 73.2 76.5 65.8 72.2 74.0 21.0 66.0 Uniform 74.8 70.9 74.1 75.6 66.5 73.4 71.2 22.1 66.1 Decreasing 71.4 70.1 72.1 76.8 64.3 71.7 73.6 18.7 64.8 Spindle 76.6 71.9 71.8 74.4 67.5 73.5 71.8 22.3 66.2 Bottleneck 74.2 71.1 69.6 73.3 65.2 73.3 73.6 24.1 65.5 3 epochs Increasing 85.3 74.9 77.2 77.5 66.8 76.2 76.0 33.0 70.8 Uniform 84.8 73.7 78.1 78.6 68.5 77.8 79.2 36.1 72.1 Decreasing 81.9 72.1 78.3 76.7 67.3 75.9 78.6 28.7 69.9 Spindle 86.9 75.5 79.8 79.4 69.8 78.3 80.1 47.3 74.6 Bottleneck 84.5 74.6 76.9 78.1 69.2 76.2 78.6 32.1 71.3 4 epochs Increasing 88.3 78.5 80.2 80.5 70.8 80.2 80.0 37.0 74.4 Uniform 88.8 78.9 81.9 81.5 71.5 80.8 81.4 39.1 75.4 Decreasing 87.6 74.1 80.8 81.7 79.3 78.9 79.6 38.7 75.1 Spindle 89.6 79.8 83.6 82.8 71.8 81.3 82.1 39.3 76.3 Bottleneck 86.5 77.6 82.7 81.1 70.2 70.9 81.6 36.1 73.3 20 epochs Increasing 92.3 83.3 86.2 82.5 71.8 82.2 84.0 51.0 79.1 Uniform 92.8 83.9 86.1 83.6 72.5 83.8 84.2 52.1 79.9 Decreasing 91.4 82.1 85.1 83.1 69.3 81.7 83.6 48.7 78.1 Spindle 93.6 84.8 87.8 84.4 73.5 84.3 85.8 52.3 80.8 Bottleneck 92.1 82.6 85.6 83.3 71.2 83.2 84.6 52.1 79.3 B General Effectiveness on SuperGLUE with XLNet Backbones We also directly use the S4-model and S4-3b-model (adopting design patterns discovered using T5-base and T5-3b) to ﬁne-tune the XLNet-base and XLNet-large pretrained backbone models without any extra discovery process. We keep all the other settings the same and evaluate them on SuperGLUE datasets. Table 17 reiterates the fact that our PEFT design patterns discovered from T5 models are generelizable to the XLNet backbone models and outperform the investigated methods in other tasks (SuperGLUE) with no additional discovery process. C On the Discovery Sequence In this work, we follow the discovery sequence of “grouping patterns – trainable parameter allocation – tunable groups – strategy assignment”: 1. To explore and understand the design patterns in all the layers in large pre-trained models in scale, it is necessary and more efﬁcient to study the layers in the unit of groups. So we start with the grouping patterns. 13Table 8: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G1 strategy assignment con- straints to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 89.8 83.5 84.9 80.8 72.5 80.8 78.5 37.7 76.1 G1-Preﬁx (P) 89.3 83.1 84.4 80.1 70.1 80.0 77.6 33.0 74.7 G1-BitFit (B) 89.0 82.9 84.1 81.4 72.0 81.1 77.0 30.8 74.8 G1-LoRA (L) 89.9 83.6 85.0 81.1 71.8 81.0 78.8 35.3 75.8 G1-(P, L) 89.1 82.8 85.1 81.2 71.9 81.5 79.1 35.0 75.7 G1-(A, P) 89.8 82.8 84.8 81.1 72.2 81.3 79.2 36.4 75.9 G1-(A, L) 89.6 83.8 85.6 81.3 72.9 81.7 79.5 36.8 76.4 G1-(A, P, L) 89.6 83.5 85.2 81.5 72.2 81.4 79.2 35.2 75.9 G1-(P, B, L) 89.3 83.6 85.5 81.6 72.3 81.0 78.8 35.7 76.0 G1-(A, P, B) 89.2 83.3 84.8 81.8 72.5 81.1 78.6 35.6 75.8 G1-(A, B, L) 89.8 83.4 84.8 81.1 72.6 81.6 79.4 34.8 75.9 G1-(A, P, B, L) 90.0 83.1 85.3 81.6 72.6 81.4 79.2 36.5 76.1 Table 9: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G2 strategy assignment con- straints with G1-(L, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G2-Adapter (A) 91.6 84.3 85.5 82.3 73.5 82.8 81.3 38.8 77.5 G2-Preﬁx (P) 89.6 84.0 86.5 81.5 73.3 82.5 80.5 36.2 76.7 G2-BitFit (B) 91.2 83.6 85.7 82.9 72.6 82.6 80.8 33.1 76.5 G2-LoRA (L) 91.4 84.4 86.1 82.0 72.8 81.8 81.6 39.8 77.4 G2-(P, L) 91.6 84.6 86.8 81.8 73.8 82.8 82.0 38.5 77.7 G2-(A, P) 92.2 84.2 87.1 82.2 74.4 83.0 82.5 40.8 78.3 G2-(A, L) 92.0 84.4 86.5 81.8 73.6 82.6 82.2 40.1 77.9 G2-(A, P, L) 91.8 84.8 86.8 81.8 74.1 83.0 82.1 37.9 77.7 G2-(P, B, L) 91.6 84.1 87.1 82.0 74.0 82.9 82.4 35.8 77.4 G2-(A, P, B) 91.8 84.2 86.8 82.1 73.7 83.3 82.2 41.2 78.1 G2-(A, B, L) 92.2 84.3 86.1 82.0 74.1 83.2 82.0 37.6 77.6 G2-(A, P, B, L) 92.0 84.1 87.0 81.9 74.2 83.1 81.3 42.4 78.1 2. Once ﬁguring out the optimal grouping patterns, it is then important to explore how to allocate the trainable parameters to these different groups in order to study more subtle designs with fair comparisons (e.g., this would allow comparing different patterns of strategy assignments without the impact from different trainable parameters.). 3. Next, it becomes inﬂuential to examine which groups need to be learned during ﬁne-tuning before we dig into the strategy assignment patterns. Because it is only meaningful to study assigning strategies to different groups after we ﬁgure out which groups need to be learned. 4. Finally, we study the tuning strategy assignment, which is the most subtle design. 14Table 10: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G3 strategy assignment constraints with G1-(L, A) – G2-(P, A) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G3-Adapter (A) 92.5 85.3 87.5 83.3 73.9 84.0 83.8 44.9 79.4 G3-Preﬁx (P) 91.5 84.7 86.7 82.6 74.2 83.8 82.9 40.5 78.4 G3-BitFit (B) 91.9 84.3 87.0 82.0 73.6 84.1 83.3 36.1 77.8 G3-LoRA (L) 92.8 85.4 87.8 83.5 74.7 82.4 84.0 44.0 79.3 G3-(P, L) 93.0 85.2 88.3 83.8 75.2 84.4 84.2 37.9 79.0 G3-(A, P) 92.4 85.6 88.1 83.6 75.0 84.2 84.0 41.8 79.3 G3-(A, L) 92.0 85.9 88.2 83.1 75.3 84.3 83.9 42.2 79.4 G3-(A, P, L) 92.6 86.0 87.5 83.4 75.6 84.6 83.5 43.9 79.6 G3-(P, B, L) 92.7 85.8 87.2 83.7 75.2 84.5 83.8 40.8 79.2 G3-(A, P, B) 93.3 85.8 88.6 84.0 75.5 84.9 84.1 42.1 79.8 G3-(A, B, L) 93.7 86.5 88.0 83.2 75.8 84.2 84.2 39.7 79.4 G3-(A, P, B, L) 93.3 85.6 87.7 83.8 75.2 84.3 84.4 41.6 79.4 Table 11: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different G4 strategy assignment constraints with G1-(A, L) – G2-(A, P) – G3-(A, P, B) to the S3 design space. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G4-Adapter (A) 93.8 85.8 88.6 84.8 76.3 85.8 86.0 48.5 81.2 G4-Preﬁx (P) 93.5 85.2 88.3 83.6 76.8 85.3 85.6 44.8 80.3 G4-BitFit (B) 94.1 85.3 88.9 84.4 77.1 85.4 86.2 46.1 80.9 G4-LoRA (L) 94.0 86.0 89.2 85.0 77.2 85.5 85.8 47.7 81.3 G4-(P, L) 94.3 86.2 89.3 85.8 78.0 86.0 88.2 47.2 81.8 G4-(A, P) 94.1 86.2 89.6 85.4 77.9 86.2 86.9 45.3 81.4 G4-(A, L) 94.2 85.9 89.2 85.5 77.8 86.2 88.0 46.8 81.7 G4-(A, P, L) 94.1 85.8 88.8 85.7 77.4 86.5 87.9 44.8 81.3 G4-(P, B, L) 94.6 86.4 90.4 86.1 78.2 86.8 88.5 47.2 82.3 G4-(A, P, B) 94.5 86.0 89.6 86.0 78.0 86.2 88.1 44.8 81.6 G4-(A, B, L) 94.3 86.4 89.2 85.6 78.2 86.4 88.3 46.6 81.9 G4-(A, P, B, L) 94.2 86.2 89.2 85.9 78.5 86.1 88.0 45.3 81.6 Table 12: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Grouping Patterns SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 80.3 72.1 74.7 72.8 76.9 75.2 71.0 32.2 69.4 Increasing 84.4 75.7 83.0 78.3 82.7 80.3 76.3 42.1 75.3 Uniform 86.8 77.1 82.6 76.2 83.8 81.6 77.3 48.9 76.8 Decreasing 83.2 74.3 81.8 77.3 82.8 79.9 76.5 40.8 74.5 Spindle 88.6 78.8 83.7 77.7 84.2 80.9 78.3 44.6 77.1 Bottleneck 86.3 77.0 82.2 75.6 83.3 80.2 77.1 41.5 75.4 15Table 13: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different layer parameter constraints to the S1 design space. Parameter Allocation SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg Increasing 90.3 79.3 84.9 79.3 85.2 82.8 79.2 50.1 78.9 Uniform 90.6 80.8 84.6 79.7 85.5 82.4 78.9 50.8 79.1 Decreasing 88.6 78.2 83.5 78.1 84.4 81.5 78.1 49.6 77.7 Table 14: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different tuning groups constraints to the S2 design space. Tunable Groups SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1 88.3 78.3 82.2 77.4 82.1 80.7 76.1 49.4 76.8 G2 89.1 78.8 82.1 77.2 82.3 81.2 76.4 49.6 77.1 G3 89.6 78.5 82.6 78.1 83.8 81.9 77.4 48.7 77.5 G4 89.8 79.3 82.7 77.9 83.5 81.9 77.9 48.5 77.1 G1, G2 90.1 80.2 83.4 78.5 84.3 82.4 78.5 51.1 78.5 G3, G4 90.5 80.6 83.8 78.7 84.2 83 78.2 50.3 78.6 G1, G2, G3 90.6 80.3 84.9 79.3 84.7 82.9 79.3 50.2 79.0 G2, G3, G4 90.8 80.9 84.6 79.1 85.1 83.1 79.1 49.2 78.9 G1, G2, G3, G4 91.1 81.4 85.2 80.4 85.9 83.5 80.0 51.6 79.9 16Table 15: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-3b pretrained backbone model. We compare adding different strategy assignment con- straints following the process in Section 4.2.5. Strategy Assignment SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg G1-Adapter (A) 91.1 81.4 86.1 80.5 86.7 83.3 80.1 50.8 80.0 G1-Preﬁx (P) 90.8 81.1 85.5 80.2 86.2 83.1 79.8 50.2 79.6 G1-BitFit (B) 90.2 81.3 85.1 79.6 85.8 82.8 79.6 49.5 79.2 G1-LoRA (L) 91.4 81.9 86.2 80.8 86.4 83.9 80.8 49.6 80.0 G1-(P, L) 91.8 82.9 86.8 81.3 87.1 84.2 81.6 52.3 81.0 G1-(A, P) 91.3 81.9 86.4 81.1 85.6 83.7 80.7 52.8 80.1 G1-(A, L) 91.6 82.3 86.1 81.5 85.8 84.9 81.5 51.8 80.6 G1-(A, P, L) 91.1 81.7 85.8 81.2 86.4 84.2 80.9 52.3 80.4 G1-(P, B, L) 91.5 82.8 86.3 81.4 86.1 83.6 81.2 51.5 80.5 G1-(A, P, B) 91.3 82.3 86.7 80.8 86.8 84.3 80.7 51.8 80.5 G1-(A, B, L) 91.7 82.5 86.2 81.3 86.3 84.6 81.3 51.7 80.7 G1-(A, P, B, L) 91.6 82.3 86.2 81.1 86.6 84.2 81.1 51.1 80.5 G2-Adapter (A) 92.1 82.5 86.4 81.8 87.2 84.8 81.8 53.8 81.3 G2-Preﬁx (P) 91.8 83.1 87.2 81.6 86.2 84.4 81.1 52.8 81.0 G2-BitFit (B) 91.2 82.1 86.4 81.1 86.3 84.6 80.3 53.1 80.6 G2-LoRA (L) 92.6 82.9 87.5 81.3 87.4 85.1 81.9 52.2 81.4 G2-(P, L) 91.6 82.7 87.6 81.6 87.8 85.3 82.1 52.8 81.4 G2-(A, P) 92.1 83.3 87.5 81.9 87.4 85.5 81.8 53.1 81.5 G2-(A, L) 92.5 83.7 88.1 82.2 87.4 85.7 82.9 53.6 82.1 G2-(A, P, L) 92.3 83.4 87.4 81.6 87.1 85.3 81.4 53.2 81.4 G2-(P, B, L) 91.8 83.1 87.4 81.5 87.2 85.1 82.7 53.8 81.5 G2-(A, P, B) 91.5 82.6 87.8 81.3 86.5 85.2 82.1 54.2 81.4 G2-(A, B, L) 92.6 83.5 87.2 82 87.3 86.5 82.5 52.8 81.8 G2-(A, P, B, L) 92.8 83.2 87.6 81.6 87.5 85.5 82.4 51.2 81.5 G3-Adapter (A) 92.6 84.1 88.3 81.8 87.8 85.4 82.8 55.2 82.2 G3-Preﬁx (P) 92.1 83.3 87.6 81.4 87.1 85.4 82.6 53.5 81.6 G3-BitFit (B) 92.4 83.9 88.4 82.1 87.2 85.8 82.4 53.3 81.9 G3-LoRA (L) 93.1 84.3 87.7 82.4 87.8 86.2 83.1 54.3 82.3 G3-(P, L) 92.8 84.1 88.7 82.6 88.2 86.2 83.3 54.7 82.6 G3-(A, P) 93.1 83.8 89.1 82.3 88.1 85.8 82.6 55.1 82.5 G3-(A, L) 92.7 84.5 88.4 82.8 88.2 86.1 83.5 54.6 82.6 G3-(A, P, L) 92.8 84.6 88.1 82.5 87.7 85.5 83.2 53.8 82.3 G3-(P, B, L) 93.6 84.9 89.3 83.1 88.2 86.5 83.9 55.8 83.2 G3-(A, P, B) 93.3 83.9 88.5 82.2 88.4 86.2 83.5 55.3 82.6 G3-(A, B, L) 93.4 84.2 88.9 82.6 87.8 85.8 84.2 54.9 82.7 G3-(A, P, B, L) 92.2 84.4 88.7 82.3 88.5 86.2 84.2 54.2 82.5 G4-Adapter (A) 92.8 85.2 89.1 83.5 87.8 86.5 84.2 56.3 83.2 G4-Preﬁx (P) 92.8 84.6 89.5 82.6 87.4 86.5 83.8 55.8 82.8 G4-BitFit (B) 93.8 84.9 89.5 83.3 88.7 86.8 84.4 55.2 83.3 G4-LoRA (L) 93.3 84.7 89.3 82.7 88.3 86.2 82.7 54.7 82.7 G4-(P, L) 93.8 85.3 89.6 83.6 88.6 86.8 84.6 56.3 83.5 G4-(A, P) 93.8 84.9 89.8 84.3 88.5 86.6 84.8 56.7 83.6 G4-(A, L) 93.7 85.6 89.5 84.1 88.2 86.6 85.2 55.4 83.5 G4-(A, P, L) 94.2 85.2 89.6 83.9 88.2 86.4 84.9 55.9 83.5 G4-(P, B, L) 93.8 85.9 89.8 83.6 88.6 86.9 85.2 56.3 83.7 G4-(A, P, B) 94.4 85.7 90.1 84.8 88.9 87.2 85.3 57.3 84.2 G4-(A, B, L) 93.8 85.3 89.5 84.1 88.8 86.7 85.5 56.6 83.7 G4-(A, P, B, L) 94.1 85.4 89.7 84.4 88.5 86.5 85.2 56.8 83.8 17Table 16: Average performances (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model. We compare adding different layer grouping constraints to the S0 design space. Layer grouping is based on 8 groups. Layer Grouping SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA Avg S0-models 76.9 70.1 72.5 73.3 63.6 71.7 73.8 24.3 65.7 Increasing 83.2 74 .1 76 .6 77 .1 67 .7 76.8 74.7 30.0 70.0 Uniform 83.6 73.4 78.0 77.9 68.2 76.4 78.6 34.2 71.3 Decreasing 80.3 71.6 77.4 75.5 67.0 75.3 77.2 26.4 68.9 Spindle 86.2 74.3 79.1 78.6 68.5 77.4 79.5 35.1 72.3 Bottleneck 83.2 73.1 75.8 77.6 67.9 75.3 78.2 31.4 70.3 Table 17: Performances of different tuning methods on the SuperGLUE datasets using the XLNet-base (upper part) and XLNet-large (lower part) pretrained backbone models, respectively. The results are averaged over 10 random runs. The S4-model and S4-3b-model perform signiﬁcantly better than the second-best PEFT methods in all the eight datasets at the signiﬁcance level p <0.05 (*) or even p <0.01 (**). Method BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC Average Adapter 72.8 71.3/78.0 64.0 67.0/24.5 71.0/71.8 76.2 65.0 60.8 66.2 Preﬁx 72.0 70.5/77.0 63.3 66.4/23.8 69.9/71.0 75.5 64.4 60.8 65.9 BitFit 71.8 70.0/76.2 62.8 65.8/22.6 69.4/70.6 74.5 64.8 60.6 65.2 LoRA 72.2 71.1/77.8 64.7 67.4/24.8 70.8/71.3 76.8 65.1 61.1 66.4 S4-model 73.8∗∗ 71.7/78.4∗ 65.9∗∗ 68.2/25.5∗∗ 71.1/72.0∗ 78.4∗∗ 65.8∗ 62.6∗ 67.5 Adapter 74.4 71.4/81.1 67.4 68.8/26.4 71.7/72.4 80.8 68.0 64.6 68.8 Preﬁx 72.4 70.0/78.3 66.9 68.8/25.8 70.9/71.2 78.8 66.9 64.0 67.7 BitFit 71.1 70.7/79.8 68.0 68.6/25.4 71.1/71.6 80.4 67.2 64.3 68.1 LoRA 74.1 72.1/80.9 67.9 69.1/26.8 72.0/72.8 81.0 67.8 64.4 69.0 S4-3b-model 76.8∗∗ 74.6/81.9∗∗ 68.6∗∗ 69.5/27.1∗ 72.4/73.3∗ 81.2∗ 68.2∗∗ 64.8∗ 69.7 Table 18: Total training time (low-compute, low-epoch regime: 100 random models, 3 tuning epochs) on the GLUE datasets using the T5-base pretrained backbone model with 8 A100 GPUs from S0 to S1. SST-2 MNLI QNLI QQP RTE STS-B MRPC CoLA 18 mins 22 mins 20 mins 40 mins 8 mins 12 mins 8 mins 6 mins 18",
      "meta_data": {
        "arxiv_id": "2301.01821v1",
        "authors": [
          "Jiaao Chen",
          "Aston Zhang",
          "Xingjian Shi",
          "Mu Li",
          "Alex Smola",
          "Diyi Yang"
        ],
        "published_date": "2023-01-04T21:00:18Z",
        "pdf_url": "https://arxiv.org/pdf/2301.01821v1.pdf",
        "github_url": "https://github.com/amazon-science/peft-design-spaces"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the lack of systematic design patterns in parameter-efficient fine-tuning (PEFT) strategies, which are typically hand-crafted and applied separately. It introduces PEFT design spaces, characterized by layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Through a progressive refinement process, the research discovers key design patterns: grouping layers in a spindle pattern, allocating trainable parameters uniformly, tuning all groups, and assigning proper strategies to different groups. These discovered patterns lead to new PEFT methods (S4-model and S4-3b-model) that consistently and significantly outperform existing PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and NLP tasks (GLUE, XSum, WMT, SuperGLUE), often surpassing full fine-tuning performance with significantly fewer trainable parameters (0.5%).",
        "methodology": "The methodology involves defining parameter-efficient fine-tuning (PEFT) design spaces using four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting with an unconstrained design space (S0), the research progressively refines it by adding constraints based on the overall model quality. This quality is quantified by randomly sampling 100 models from a design space, fine-tuning them for 3 epochs on the GLUE benchmark, and averaging their performances. A greedy selection process is applied at each stage for each component. Specific design patterns explored include: (i) five layer grouping patterns (Increasing, Uniform, Decreasing, Spindle, Bottleneck), (ii) three trainable parameter allocation patterns (Increasing, Uniform, Decreasing), (iii) various combinations of tunable groups, and (iv) assigning individual or combinations of PEFT strategies (Adapter, Prefix, BitFit, LoRA) to different layer groups.",
        "experimental_setup": "The design patterns were discovered using T5-base and T5-3b pretrained backbone models on the GLUE benchmark, measuring Matthews correlation for CoLA, Spearman correlation for STS-B, and accuracy for other tasks. Final evaluation of the new PEFT methods (S4-model, S4-3b-model) was conducted on GLUE, XSum (abstractive summarization, using ROUGE scores), WMT 2016 en-ro (machine translation, using BLEU scores), and SuperGLUE. These evaluations utilized T5-base/3b, RoBERTa-base/large, BART-base/large, and XLNet-base/large backbone models. Implementations used Hugging Face. The total number of trainable parameters was set to 0.5% (0.1% for BitFit) of the backbone model's parameters. Training used a linear decay scheduler with a warmup ratio of 0.06, a batch size of 128 for base models and 64 for large models, a maximum learning rate of 5e-5, and 5 or 10 training epochs. All experiments were performed using 8 A100 GPUs. Results were averaged over 20 random runs (10 for SuperGLUE), with statistical significance tested at p < 0.05 or p < 0.01.",
        "limitations": "The study explicitly states that for computational efficiency, it was beyond its scope to enumerate all possible constraints related to the design space components. This implies that the progressive and greedy discovery process might not have explored the entire optimal design space, potentially settling on locally optimal patterns rather than globally optimal ones. The goal was to demonstrate the utility of design spaces, not necessarily to find the absolute \"best\" design.",
        "future_research_directions": "Not mentioned",
        "experimental_code": "# Adapters\nif is_torch_available():\n    _import_structure[\"adapters\"] = [\n        \"ADAPTER_CACHE\",\n        \"ADAPTER_CONFIG_MAP\",\n        \"ADAPTERFUSION_CONFIG_MAP\",\n        \"ADAPTER_MODEL_MAPPING\",\n        \"DEFAULT_ADAPTER_CONFIG\",\n        \"DEFAULT_ADAPTERFUSION_CONFIG\",\n        \"MODEL_WITH_HEADS_MAPPING\",\n        \"AdapterArguments\",\n        \"AdapterConfig\",\n        \"AdapterConfigBase\",\n        \"AdapterFusionConfig\",\n        \"AdapterInfo\",\n        \"AdapterLayer\",\n        \"AdapterLayerBase\",\n        \"AdapterSetup\",\n        \"AdapterTrainer\",\n        \"AdapterType\",\n        \"AutoAdapterModel\",\n        \"AutoModelWithHeads\",\n        \"BartAdapterModel\",\n        \"BartModelWithHeads\",\n        \"BeitAdapterModel\",\n        \"BertAdapterModel\",\n        \"BertModelWithHeads\",\n        \"CompacterConfig\",\n        \"CompacterPlusPlusConfig\",\n        \"ConfigUnion\",\n        \"DebertaAdapterModel\",\n        \"DebertaV2AdapterModel\",\n        \"DistilBertAdapterModel\",\n        \"DistilBertModelWithHeads\",\n        \"DynamicAdapterFusionConfig\",\n        \"EmbeddingAdaptersMixin\",\n        \"ForwardContext\",\n        \"GPT2AdapterModel\",\n        \"GPT2ModelWithHeads\",\n        \"GPTJAdapterModel\",\n        \"HoulsbyConfig\",\n        \"HoulsbyInvConfig\",\n        \"IA3Config\",\n        \"InvertibleAdaptersMixin\",\n        \"LoRAConfig\",\n        \"MAMConfig\",\n        \"MBartAdapterModel\",\n        \"MBartModelWithHeads\",\n        \"ModelAdaptersConfig\",\n        \"ModelAdaptersMixin\",\n        \"ModelWithFlexibleHeadsAdaptersMixin\",\n        \"ModelWithHeadsAdaptersMixin\",\n        \"MultiLingAdapterArguments\",\n        \"ParallelConfig\",\n        \"PfeifferConfig\",\n        \"PfeifferInvConfig\",\n        \"PrefixTuningConfig\",\n        \"RobertaAdapterModel\",\n        \"RobertaModelWithHeads\",\n        \"Seq2SeqAdapterTrainer\",\n        \"StaticAdapterFusionConfig\",\n        \"T5AdapterModel\",\n        \"T5ModelWithHeads\",\n        \"PEFTConfig\",\n        \"ViTAdapterModel\",\n        \"XLMRobertaAdapterModel\",\n        \"XLMRobertaModelWithHeads\",\n        \"get_adapter_config_hash\",\n        \"get_adapter_info\",\n        \"list_adapters\",\n    ]",
        "experimental_info": "The package `adapter-transformers` is described as \"A friendly fork of HuggingFace's Transformers, adding Adapters to PyTorch language models\". The `transformers` library includes components for data processing and metrics for tasks such as GLUE, evidenced by imports like `glue_compute_metrics`, `glue_convert_examples_to_features`, `glue_output_modes`, `glue_processors`, and `glue_tasks_num_labels`. It also provides `AdapterTrainer` and `Seq2SeqAdapterTrainer` for fine-tuning models with adapters. However, specific experimental settings described in the method, such as randomly sampling 100 models, fine-tuning for 3 epochs, or averaging their performances, are not explicitly detailed in the provided repository content."
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces the \"Spectral Adapter\" framework to enhance Parameter-Efficient Fine-Tuning (PEFT) methods by incorporating the spectral information of pretrained weight matrices. It proposes two spectral adaptation mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of the top singular vectors, performed after Singular Value Decomposition (SVD) of pretrained weights. Key findings include a theoretical demonstration that Spectral AdapterA offers twice the rank capacity of LoRA for an equal parameter budget and that tuning top singular vectors is robust due to better neuron alignment. Empirically, the proposed method achieves superior parameter efficiency and tuning performance compared to various PEFT baselines in language models (DeBERTaV3-base, Mistral 7B, Llama3 8B) and diffusion models (Chilloutmix). Additionally, Spectral AdapterA provides a natural solution for multi-adapter fusion, improving identity preservation and concept binding, while Spectral AdapterR offers finer-grained parameter budgets.",
        "methodology": "The core methodology involves applying Singular Value Decomposition (SVD) to pretrained weight matrices (W = USV^T) and then fine-tuning only the top-r columns of the singular vector matrices (U and V). Two specific spectral fine-tuning mechanisms are introduced: 1. Spectral AdapterA: Additively tunes the top-r columns, represented as [U1 + AU U2]S[V1 + AV V2], where AU and AV are trainable matrices. This is initialized with AU and AV set to zero. 2. Spectral AdapterR: Orthogonally rotates the top-r columns, represented as [U1 RU U2]S[V1 RV V2], where RU and RV are trainable r x r orthogonal matrices. Orthogonality is maintained using Cayley parameterization (R = (I+Q)(I-Q)^-1, with Q being a skew-symmetric matrix derived from a trainable parameter). This ensures exact rotation and preserves SVD structure for sequential fine-tuning. For multi-adapter fusion, different concepts are distributed along distinct columns of the singular vector matrices, and adapters are merged in a FedAvg-like manner by averaging the updated singular vector components.",
        "experimental_setup": "Experiments were conducted on large language models and diffusion models. For language models, DeBERTaV3-base (185M) was fine-tuned on GLUE benchmarks (MNLI, SST-2, MRPC, CoLA, QNLI, QQP, RTE, STS-B), Mistral 7B was fine-tuned on the GSM8K task, and Llama3 8B was fine-tuned on the Orca Math dataset (for training loss) and evaluated on GSM8K. Baselines included LoRA, DoRA, OFT, AdaLoRA, and SVDiff (for DeBERTaV3-base). For diffusion models, the Chilloutmix model was used for multi-adapter fusion tasks involving custom animal and toy concepts, and multi-character generation (Yoshua Bengio, Yann LeCun, Geoffrey Hinton), compared against Gradient Fusion, Orthogonal Adaptation, and FedAvg. For parameter efficiency analysis, Chilloutmix was fine-tuned on custom vase, chair, and table concepts, compared against LoRA, SVDiff, LiDB, OFT, and VeRA. T2I-Adapter with sketch/keypose conditions was used for spatial alignment in diffusion model experiments. Evaluation included accuracy scores for LLMs (GLUE, GSM8K), visual quality of generated images, and CLIP-based alignment scores (cosine similarity with reference images and prompt texts) for diffusion models. All experiments were performed using NVIDIA RTX A6000 GPUs. Hyperparameters for baselines followed their official implementations, while Spectral Adapter parameters were tuned.",
        "limitations": "One limitation is the current work's choice of exclusively tuning the top spectral space. Although its effectiveness has been theoretically verified under simple settings, further comprehensive investigation into tuning different columns of singular vector matrices is crucial to fully understand the role of spectral information in the fine-tuning procedure. Additionally, the time consumption of the Singular Value Decomposition (SVD) process can increase significantly as models grow larger, necessitating the development or adoption of faster SVD methods to maintain practicality.",
        "future_research_directions": "Future research could explore fine-tuning the spectral representation of specific components within large models, such as only the attention layers. Another promising direction is to dynamically combine spectral adaptation with other existing PEFT methods, like AdaLoRA, to potentially achieve even greater efficiency or performance. Further in-depth investigation into tuning different columns of singular vector matrices, beyond just the top ones, is also critical for a more complete understanding of how spectral information influences the fine-tuning process. Lastly, developing or integrating faster Singular Value Decomposition (SVD) methods will be important to mitigate the increasing computational cost as models continue to scale in size.",
        "experimental_code": "import math\n\nimport torch\nimport torch.nn as nn\nfrom diffusers.models.attention_processor import AttnProcessor\nfrom diffusers.utils.import_utils import is_xformers_available\nimport torch.nn.functional as F\nimport numpy as np\n\nif is_xformers_available():\n    import xformers\n    \n\ndef remove_edlora_unet_attention_forward(unet):\n    def change_forward(unet):  # omit proceesor in new diffusers\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and name == 'attn2':\n                layer.set_processor(AttnProcessor())\n            else:\n                change_forward(layer)\n    change_forward(unet)\n\n\nclass EDLoRA_Control_AttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    def __init__(self, cross_attention_idx, place_in_unet, controller, attention_op=None):\n        self.cross_attention_idx = cross_attention_idx\n        self.place_in_unet = place_in_unet\n        self.controller = controller\n        self.attention_op = attention_op\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            is_cross = False\n            encoder_hidden_states = hidden_states\n        else:\n            is_cross = True\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available() and not is_cross:\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            attention_probs = self.controller(attention_probs, is_cross, self.place_in_unet)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass EDLoRA_AttnProcessor:\n    def __init__(self, cross_attention_idx, attention_op=None):\n        self.attention_op = attention_op\n        self.cross_attention_idx = cross_attention_idx\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            if len(encoder_hidden_states.shape) == 4:  # multi-layer embedding\n                encoder_hidden_states = encoder_hidden_states[:, self.cross_attention_idx, ...]\n            else:  # single layer embedding\n                encoder_hidden_states = encoder_hidden_states\n\n        assert not attn.norm_cross\n\n        batch_size, sequence_length, _ = encoder_hidden_states.shape\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query).contiguous()\n        key = attn.head_to_batch_dim(key).contiguous()\n        value = attn.head_to_batch_dim(value).contiguous()\n\n        if is_xformers_available():\n            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)\n            hidden_states = hidden_states.to(query.dtype)\n        else:\n            attention_probs = attn.get_attention_scores(query, key, attention_mask)\n            hidden_states = torch.bmm(attention_probs, value)\n\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\ndef revise_edlora_unet_attention_forward(unet):\n    def change_forward(unet, count):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:\n                layer.set_processor(EDLoRA_AttnProcessor(count))\n                count += 1\n            else:\n                count = change_forward(layer, count)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0)\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx)\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx)\n    print(f'Number of attention layer registered {cross_attention_idx}')\n\n\ndef revise_edlora_unet_attention_controller_forward(unet, controller):\n    class DummyController:\n        def __call__(self, *args):\n            return args[0]\n\n        def __init__(self):\n            self.num_att_layers = 0\n\n    if controller is None:\n        controller = DummyController()\n\n    def change_forward(unet, count, place_in_unet):\n        for name, layer in unet.named_children():\n            if layer.__class__.__name__ == 'Attention' and 'attn2' in name:  # only register controller for cross-attention\n                layer.set_processor(EDLoRA_Control_AttnProcessor(count, place_in_unet, controller))\n                count += 1\n            else:\n                count = change_forward(layer, count, place_in_unet)\n        return count\n\n    # use this to ensure the order\n    cross_attention_idx = change_forward(unet.down_blocks, 0, 'down')\n    cross_attention_idx = change_forward(unet.mid_block, cross_attention_idx, 'mid')\n    cross_attention_idx = change_forward(unet.up_blocks, cross_attention_idx, 'up')\n    print(f'Number of attention layer registered {cross_attention_idx}')\n    controller.num_att_layers = cross_attention_idx\n\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        rank = 8\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True)\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True)\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r\n\n    def cayley(self, data: torch.Tensor) -> torch.Tensor:\n        r, _ = data.shape\n        skew = 0.5 * (data - data.T)\n        I = torch.eye(r, device=data.device)\n        Q = torch.mm(I - skew, torch.inverse(I + skew))\n        return Q\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@self.cayley(self.spectral_B)\n        else:\n            raise Exception('')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv :\n            raise Exception('')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)",
        "experimental_info": "The implementation `SpectralLinearLayer_OFT` (found in `adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/models/edlora.py`) corresponds to 'Spectral AdapterR'.\n\n**Spectral AdapterR Details:**\n- **SVD application:** During initialization (`__init__`), Singular Value Decomposition (`torch.svd(W)`) is applied to the original weight matrix `W` of a linear layer (or flattened convolution). The resulting `U`, `S`, and `V` matrices are stored as non-trainable `torch.nn.Parameter`.\n- **Fine-tuning mechanism:** Orthogonal rotation is applied to specific 'top-r' columns of the singular vector matrices `U` and `V`, and scalar scaling is applied to the corresponding singular values in `S`.\n- **Trainable parameters:**\n    - `self.spectral_A`: `r x r` matrix, initialized with zeros. Used to derive the orthogonal rotation matrix for `U` via Cayley parameterization.\n    - `self.spectral_B`: `r x r` matrix, initialized with zeros. Used to derive the orthogonal rotation matrix for `V` via Cayley parameterization.\n    - `self.spectral_C`: `r`-dimensional vector, initialized with ones. Used to scale the `r` singular values in `S`.\n- **Orthogonality enforcement:** Cayley parameterization `Q = torch.mm(I - skew, torch.inverse(I + skew))` (where `skew = 0.5 * (data - data.T)`) is used to ensure `self.spectral_A` and `self.spectral_B` represent orthogonal rotation matrices `RU` and `RV`.\n- **Rank parameter (`r`):** The rank for fine-tuning (`revised_r` in the code) is configurable and passed via `lora_cfg['rank']` during instantiation in `train_edlora.py`. This determines the size of `spectral_A`, `spectral_B`, and `spectral_C`.\n- **Weight reconstruction:** In the `forward` pass, `pad_U`, `pad_S`, `pad_V` are computed by applying the learned transformations. The adapted weight matrix `pad_W` is then reconstructed as `pad_U @ pad_S.diag() @ pad_V.T`.\n\n**Spectral AdapterA:**\n- The provided repository content does not contain a direct implementation of 'Spectral AdapterA' as described (additive tuning of `top-r` columns of `U` and `V` with `[U1 + AU U2]S[V1 + AV V2]`). Other LoRA-like implementations (`LiLoRALinearLayer` in `mix_lidb`, `LoRALinearLayer` in `mix_lora`) are present but do not explicitly operate on the singular vectors `U` and `V` additively in an SVD-decomposed manner.\n\n**Multi-adapter fusion (FedAvg-like):**\n- The fusion mechanism is implemented in `adapter_fusion/fedavg_gradient/fedavg_fusion.py` through the `merge_kv_in_cross_attention` and `merge_spatial_attention` functions.\n- For each concept, the individual spectral adapters (like `SpectralLinearLayer_OFT`) are applied to the base model's weights to obtain `merge_params` (the full adapted weight matrix for that concept and layer).\n- These `merge_params` from all concepts are collected for each layer into `concept_weights_dict`.\n- Fusion occurs by stacking these adapted weight matrices (`torch.stack`) and then calculating their mean (`torch.mean`) to produce `Wnew`.\n- This indicates that the fusion is performed by averaging the *reconstructed adapted weight matrices* from individual concepts, rather than averaging the updated singular vector components (`U`, `S`, `V`) directly."
      }
    },
    {
      "title": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone",
      "abstract": "Parameter-efficient tuning has become a trend in transferring large-scale\nfoundation models to downstream applications. Existing methods typically embed\nsome light-weight tuners into the backbone, where both the design and the\nlearning of the tuners are highly dependent on the base model. This work offers\na new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners\nfrom the backbone. With both theoretical and empirical evidence, we show that\npopular tuning approaches have their equivalent counterparts under our\nunbinding formulation, and hence can be integrated into our framework\neffortlessly. Thanks to the structural disentanglement, we manage to free the\ndesign of tuners from the network architecture, facilitating flexible\ncombination of various tuning strategies. We further propose a memory-efficient\nvariant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners)\nis effectively detached from the main branch, such that the gradients are\nback-propagated only to the tuners but not to the backbone. Such a detachment\nalso allows one-time backbone forward for multi-task inference. Extensive\nexperiments on both discriminative and generative tasks demonstrate the\nsuperiority of our method over existing alternatives from the perspectives of\nefficacy and efficiency. Project page:\n$\\href{https://res-tuning.github.io/}{\\textit{https://res-tuning.github.io/}}$.",
      "full_text": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone Zeyinzi Jiang1 Chaojie Mao1 Ziyuan Huang2 Ao Ma1 Yiliang Lv1 Yujun Shen3 Deli Zhao1 Jingren Zhou1 1Alibaba Group 2National University of Singapore 3Ant Group {zeyinzi.jzyz, chaojie.mcj, dave.ma, yiliang.lyl, jingren.zhou}@alibaba-inc.com {ziyuan.huang}@u.nus.edu {shenyujun0302, zhaodeli}@gmail.com Abstract Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light- weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass (i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated only to the tuners but not to the backbone. Such a detachment also allows one-time backbone forward for multi-task inference. Extensive experiments on both discriminative and generative tasks demonstrate the superiority of our method over existing alternatives from the perspectives of efficacy and efficiency. Project page: https://res-tuning.github.io/. 1 Introduction Recently, foundation models have demonstrated strong generalization capability across numerous visual [21, 2], language [ 47, 60] and multi-modal tasks [ 40, 1]. Pre-trained on a large corpus of data, a foundation model offers a good initialization for downstream adaptation. Unfortunately, the increasing model scale makes it expensive and almost infeasible to fully fine-tune such a model for every task. Hence, parameter-efficient transfer learning (PETL) [37, 41, 27] is often resorted to as an efficient approach for downstream adaptation without incurring an unaffordable computation burden. Popular existing approaches for parameter-efficient tuning introduce additional tunable structures (which we term as tuners) to the pre-trained base model [ 37, 41, 27]. Compared to the fully-fine- tuned counterparts, the light-weight tuners significantly reduce the training cost while maintaining a competitive performance [32, 28]. However, the current designs of tuners are deeply coupled with their base structures, as shown in Fig. 1a, thus restricting the design flexibility and impeding the extension to new approaches. For example, prefix tuning [ 41] is embedded into the self-attention operation, and prompts [28, 85] could only be introduced at the beginning or between layers, etc. In this work, we introduce Res-Tuning, a new tuning paradigm for flexible and efficient transfer learning. As in Fig. 1b, ourRes-Tuning framework unbinds the tuners from the base model, such that it is possible to decouple both the design and the learning of the tuners from the base structure. Since 37th Conference on Neural Information Processing Systems (NeurIPS 2023). arXiv:2310.19859v1  [cs.CV]  30 Oct 2023MHA FFN Res- Tuner Tuner MHA Res- Tuner FFN Block Block (b) (c) (a) Res- Tuner Res- Tuner Res- Tuner Res- Tuner × × Figure 1: Concept comparison between existing methods and our Res-Tuning framework. (a) Existing PETL methods are deeply embedded into original structures. (b) Our Res-Tuning framework can decouple PETL methods from the backbone. (c) Backpropagation only through a bypass consisting of Res-Tuner can be achieved by detaching the connections. the design of the tuners is no longer dependent on the base structure in our Res-Tuning framework, we explore various possibilities. With both theoretical and empirical evidence, we first show that our framework can seamlessly encompass popular tuning approaches such as prefix-tuning [ 41], prompt-tuning [28], and adapters [25]. Further, it is demonstrated that the structural disentanglement also allows for flexible combination of various tuners, leading to the discovery of stronger tuning strategies. On top of that, we show that such an unbinding formulation also allows for the detachment of the tuners from the backbone as in Fig. 1c, which further improves the memory efficiency. In this memory-efficient version, i.e., Res-Tuning-Bypass, not only is the training cost reduced because the gradient computation through the massive parameters is avoided in the base model, but it also reduces the number of forward passes of the backbone model to only once during multi-task inference. We evaluateRes-Tuning framework on both discriminative and generative tasks. On discriminative tasks, we show that our unbinding formulation leads to a tuning strategy that achieves state-of-the-art performance on VTAB-1K with similar learnable parameters. Our Res-Tuning-Bypass framework also performs favorably against the fully-fine-tuned variant, while reducing the training memory consumption by 49.7% and the multi-task inference time by 80.9%. In addition, it also obtains better performance in few-shot learning and domain generalization scenarios. On generative tasks, apart from the strong performance achieved by Res-Tuning framework in terms of both FID scores and qualitative results, we further show that our Res-Tuning-Bypass can reduce the memory consumption by 70.7%, training time by 58.6%, and multi-task inference time by 83.6% when maintaining a competitive FID score and generation quality compared to the fully-fine-tuned variant. 2 Unbinding parameter-efficient tuning In order to reduce the entanglement between the base model and the tuners as well as to increase the flexibility of the tuning strategies, we set out to unbind the tuners from the pre-trained structures. In this section, we provide our unbinding formulation to existing parameter-efficient transfer learning strategies. We start by revisiting the basic building blocks of the foundation models, before diving into unbinding existing tuners from the backbone. In the last part of this section, we further provide empirical proof that our unbinding formulation could seamlessly encompass existing PETL methods like prompt tuning [28], prefix tuning [41], and adapters [25]. 2.1 Basic building blocks of foundation models Existing foundation models in both natural language processing, vision, and vision-language applications mostly adopt Transformers [ 70] as the backbone. The major building blocks of the Transformers that one usually adapts for the downstream tasks are the multi-head attention (MHA) and the feed-forward network (FFN). Formally, the standard MHA and FFN could be expressed as: Attn(Q, K, V ) = softmax \u0012QKT √ d \u0013 V FFN(x) = ϕ(xW1 + b1)W2 + b2 , (1) 2MHA Input FFN Adapter Input FFN Adapter Input MHA Input Pro. MHA Pro. Input MHA Input MHA MHA (a) Prefix Tuning (b) Prompt Tuning (c) Adapter Tuning Original Parallel  Original Parallel Original Parallel Figure 2: The original and the unbinding form of (a) prefix tuning [41], (b) prompt tuning [28], and (c) adapter tuning [25] for parameter-efficient transfer learning. where Q, K and V denote the query, key and value, respectively. W1 and W2 are the projection weights, b1 and b2 are the bias terms, and ϕ is the non-linear activation between fully-connected layers. Usually, given input tokensx, the query, key, and value are obtained through a linear projection Q = xWq, K = xWk, and V = xWv, where Wq, Wk and Wv are learnable projection weights. 2.2 Unbinding tuners from foundation models For the adaptation of the foundation models to downstream applications, the existing PETL approaches mostly resort to adjusting the output of MHA, FFN, or the Transformer block composed of MHA and FFN in various ways. We choose popular and exemplary approaches and unbind their structures from foundation models. Here, we provide the unbinding formulations of prefix tuning [41] and prompt tuning [28] for MHA adaptation, as well as adapter tuning [25] for FFN adaptation. Prefix tuning [41] prepends learnable parameters, i.e., prefix tokens, to the projected keys and values: MHApre = Attn(xWq, [Kpre; xWk], [Vpre; xWv]), (2) where Kpre and Vpre are prefix tokens. Essentially, if we view this as performing MHA separately between (Q, K, V ) and between (Q, Kpre, Vpre), we unbind prefix tuning as follows: MHApre = (1 − λ) Attn (Q, K, V )| {z } original attention +λ Attn (Q, Kpre, Vpre)| {z } prefix attention in parallel , (3) where λ weighs between the original and prefix attention. Detailed value for λ and the derivation process are included in appendix A. In this way, the original MHA in the foundation model Attn(Q, K, V ) and the prefix attention Attn(Q, Kpre, Vpre) can be computed independently. The unbinding formulation of prefix tuning can be seen in Fig. 2a. Prompt tuning [28] appends latent tokens to the input token before performing MHA in the backbone: MHApro = Attn ([x; xpro]Wq, [x; xpro]Wk, [x; xpro]Wv) , (4) where xpro are prompt tokens concatenated to the input token x in the first layer or between multiple layers. Similar to prefix tuning, the unbinding formulation of prompt tuning is as follows: MHApro = [(1 − λ) Attn (Q, K, V )| {z } original attention +λ Attn (Q, Kpro, Vpro)| {z } prompt attention in parallel ; D], (5) where Kpro = xproWk and Vpro = xproWv. D denotes disposable parts that would not affect the output of MHA, where D = (1−β) Attn (Qpro, Kpro, Vpro) +β Attn (Qpro, K, V ). λ and β are individual weights. More details can be seen in appendix A. The unbinding formulation of prompt tuning can be seen in Fig. 2b. Adapter tuning [25] typically inserts a multi-layer perceptron (MLP) after FFN. Since the MLP could be performed independently, we simply re-route the adapter and connect it in parallel to the FFN as in Fig. 2c. The resultant unbinding formulation of the adapters is as follows: FFNadapter = FFN( x)| {z } original module + ϕ(FFN(x)Wdown)Wup| {z } adapter module in parallel , (6) where Wdown and Wup denote the weights for the down- and up-projection layers, respectively. 3MHA FFN Input Attention Feed Forward Block Norm Norm x L Residual Adapter Nonlinear OP Scale Residual Prompt OP MHA Pro. Scale Residual Preﬁx OP MHA Scale Trainable Frozen Ada. Pre. Pro. OP MHA FFN Block … … Res- Tuner Res- Tuner Res- Tuner Res-Tuner Start Layer MHA FFN Input End Layer MHA FFN × (b)  Horizontal Vertical Res- Tuner Res- Tuner Res- Tuner (c) (a) × Res- Tuner … … Figure 3: Structure illustration of (a) various Res-Tuners in our unbinding form, (b) our Res-Tuning framework that is flexible and efficient, and (c) Res-Tuning-Bypass, the memory- efficient version of our framework. Table 1: Empirical equivalence of PETL methods and their counterparts in our unbinding form. ViT/B-16 (IN-21K) ViT/L-14 (CLIP) Method Original Unbinding form ∆ Original Unbinding form ∆ Adapter [25] 92.34 92.34 0.00 92.43 92.44 +0.01 Prefix [41] 91.90 91.88 -0.02 90.99 91.01 +0.02 Prompt [28] 92.21 92.24 +0.03 91.86 91.83 -0.03 2.3 Empirical equivalency of the unbinding formulation After we have obtained the unbinding formulation of popular PETL methods with theoretical derivation, Tab. 1 shows empirical evidence of the equivalency between the original formulation and our unbinding formulation. We carry out the comparison between two formulations on CIFAR-100, using ViT [13] pre-trained on ImageNet-21K and by CLIP [59]. For all cases, we observe that the performance discrepancy between the original and our unbinding form is within a reasonable range (± 0.03), which shows that our formulation encompasses existing PETL methods effortlessly. 3 Res-Tuning 3.1 Unified formulation of Res-Tuning Given the unbinding formulation of the existing PETL methods, we can derive a unified formulation as the combination of the frozen pre-trained operation and the tuner with learnable parameters: x′ = OP(x) +Res-Tuner(x), (7) where OP denotes existing operations in the pre-trained backbone such as MHA and FFN, while the Res-Tuner represents the learnable structures that are connected in parallel to the existing operations. With this unified unbinding formulation, the design of the tuner structure can now be independent of the original operation in the base model. This leads to unprecedented flexibility in parameter-efficient tuning, which enables us to explore various instantiations of the tuner (as in Fig. 3a) for the base structures. For example, we found in Tab. 2a that, instantiating the Res-Tuner with prompt tuning for FFN results in a better performance compared to adapters. Further, the structural disentanglement also allows for the flexible combination of various tuning strategies. In Fig. 3b, we instantiate our Res-Tuning framework by associating one Res-Tuner with every operation in the base model, including MHA, FFN, and the whole Transformer block. 3.2 Res-Tuning-Bypass: Towards memory-efficient PETL The unified formulation of Res-Tuning provides a viable solution for flexible and parameter-efficient transfer learning. However, since it is directly derived from existing PETL methods, it shares the 4same vulnerability as existing PETL solutions. Despite the parameter efficiency, they require back- propagation through the massive parameters in the pre-trained backbone, which leads to unnecessary extra consumption of memory and computation. To avoid this, we further present a memory-efficient version of our Res-Tuning framework, dubbed as Res-Tuning-Bypass, as in Fig. 3c. Specifically, we remove the data flow from the Res-Tuner to the backbone, such that the Res-Tuner is detached from the pre-trained architectures. In this way, we form a bypass network constructed by Res-Tuners in parallel with the backbone. Formally, given the tokenized feature x0 and the output feature of the l-th layer xl from the pre-trained model, our bypass network is formulated as: xbypass 0 = x0, xbypass l = λRes-Tuner(xl) + (1− λ)Res-Tuner(xbypass l−1 ), l≥ 1, (8) where λ is a learnable parameter followed by a sigmoid function, which is initialized to 0.5. As demonstrated in Fig. 3c, we group the Res-Tuners in the bypass network into horizontal ones and vertical ones, respectively processing the output feature of the l-th layer from the backbone and the (l − 1)-th feature from the bypass network. Within each group, we keep the structure identical. Thanks to the flexibility of our unbinding formulation, we can also explore different instantiations of the Res-Tuner and various combinations of the existing tuners. 4 Empirical evaluations on discriminative tasks 4.1 Experimental setups Evaluation scenarios. We mainly analyze the flexibility and efficiency of our proposedRes-Tuning framework and evaluate the discriminative capabilities on three different scenarios: transfer learning, few-shot learning, and domain generalization. Baselines. Apart from the traditional downstream adaptation methods like fully fine-tuning and linear probing, we divide existing tuning approaches into two categories: (i) methods focusing on parameter-efficiency, including adapter tuning [25], prefix tuning [ 41], VPT [28], LoRA [27], AdaptFormer [7], SSF [42], and NOAH[83]; (ii) methods focusing on memory-efficiency, including Side-Tuning [82], and LST [68]. Since these two categories have distinct characteristics with respect to the parameters, memory, and performance, we mainly compare ourRes-Tuning framework within the former category, while the Res-Tuning-Bypass is mainly compared in the latter one. Implementation details. For most experiments, we adopt ViT-B/16 [13] pre-trained on ImageNet- 21K [11] as the backbone model, following VPT [ 28]. Unless otherwise specified, the middle of the adapter, as well as the number of prefix and prompt tokens in our Res-Tuning are set to 10 for parameter efficiency. We include the training details in appendix C. For all the tasks, we use top-1 accuracy as the main evaluation metric. 4.2 Analysis on flexibility Flexible combination between backbone structures and tuners. Since our unbinding formulation allows for the structural disentanglement between the frozen structure in the backbone and the tuner with learnable parameters, it enables us to explore various combinations of the operation and the tuners. Here, we experiment with various instantiations of OP and Res-Tuner in our Res-Tuning framework, and the number of tuners is limited to one tuner per block (Single-Res-Tuner). The results are presented in Tab. 2a. We found that the default combination in existing approaches (prefix and prompt for MHA adaptation and adapter for FFN adaptation) is far from optimal, and connecting prompt tuning to FFN results in the best performance. Flexible combination of multiple tuning strategies.Next, we show that the flexibility brought by our unbinding formulation could also effortlessly lead to stronger tuning strategies by combining various tuners in our unbinding formulation. Here, we consider two tuners per block ( Dual-Res-Tuner), respectively connected to MHA and FFN. As in Tab. 2b, employing two tuners for each block brings notable improvements over the Single-Res-Tuner variants, with employing two adapters respectively for MHA and FFN achieving the strongest performance of 93.25. On top of the best performing Dual-Res-Tuner model, we further attach tuners at the block level in Tab. 2c. With 5Table 2: Exploration of various combinations of operations in the pre-trained backbone and various Res-Tuners achieves a stronger performance compared to the existing tuning strategies on CIFAR- 100. Adapter, prefix, and prompt are abbeviated as Ada., Pre. and Pro., respectively. (a) Single-Res-Tuner. Tuner\\OP MHA FFN Block Res-Ada. 92.46 92.3492.49 Res-Pre. 91.8892.33 92.39 Res-Pro. 92.2492.6892.16 (b) Dual-Res-Tuner. MHA\\FFN Res-Ada. Res-Pre. Res-Pro. Res-Ada. 93.25 92.95 92.62 Res-Pre. 93.22 92.38 92.87 Res-Pro. 93.03 92.92 92.91 (c) Tri-Res-Tuner. Block Dual-Res-Tuner Res-Ada. 93.28 Res-Pre. 93.20 Res-Pro. 93.16 Table 3: In-depth analysis of our Res-Tuning and Res-Tuning-Bypass in terms of performance, parameter-efficiency and memory efficiency on CIFAR-100. (a) Parameter efficiency of Res-Tuning on CIFAR-100. Method Full Linear Probing Single- Dual- Tri- Acc. 89.12 85.95 92.68 93.25 93.28 Param. 85.9M 0.07M 0.17M 0.48M 0.67M (b) Parameter and memory efficiency of Res-Tuning- Bypass on CIFAR-100. Method BypassRes-TunerAcc. Param. Mem. Linear probing ✘ - 85.95 0.07M 2.72G ↓ ✔ None 86.34 0.07M 3.48G ✔ Hori. 88.08 0.27M 3.66G ✔ Vert. 87.26 0.27M 3.64G Res-Tuning-Bypass✔ Both 89.330.46M 4.72G Fully fine-tuning ✘ - 89.12 85.9M 9.02G (c) Performance and efficiency comparison with existing tuning strategies on CIFAR-100. Method Acc. Param. (M) Mem. Full 89.12 85.9 (100%) 9.02G Linear 85.95 0.07 (0.08%) 2.72G Parameter-efficient tuning methods MAM-Adapter† [19] 91.70 10.08 (11.72%) 9.57G AdaptFormer [7] 91.86 1.26 (1.46%) 6.32G Res-Tuning 93.25 0.48 (0.55%) 6.85G Memory-efficient tuning methods Side-Tuning [82] 87.16 9.62 (11.18%) 3.48G LST† [68] 88.72 0.93 (1.08%) 5.26G Res-Tuning-Bypass89.33 0.46 (0.53%) 4.72G †denotes our own implementation since the original approach is presented for natural langauge processing. adapters on top of the Dual-Res-Tuner model, we observe further slight improvements on the classification performance. Compared to existing tuning strategies of underlining in Tab. 2a, without bells and whistles, the Tri-Res-Tuner version of our Res-Tuning framework achieves at least 0.94% performance improvement. 4.3 Analysis on parameter-, memory- and multi-task inference-efficiency Parameter-efficiency. We analyze the parameter efficiency of ourRes-Tuning framework in Tab. 3a. Our Single-Res-Tuner version surpasses the performance of the fully-fine-tuned variant with less than 0.2% learnable parameters. Combining all three tuners on MHA, FFN, and block-level, we manage to outperform the fully-fine-tuned and linear evaluated variants by respectively 4.16% and 7.33%, while only using 0.67M parameters. Memory-efficiency. Here, we present the evolution from linear probing to ourRes-Tuning-Bypass in Tab. 3b. It is observed that introducing a bypass network without any tuners can help ensemble the original features obtained from different layers, thereby mildly improving the classification accuracy as compared to the linear probing approach where only the classifiers are trained. On top of that, both horizontal and vertical Res-Tuner bring notable performance improvement with limited parameter and memory overhead. With both horizontal and vertical Res-Tuners in place, our Res-Tuning-Bypass framework achieves stronger performance with only 52% of the memory consumption when compared with the fully-fine-tuned variant. Multi-task inference-efficiency. In Fig. 4, our Res-Tuning-Bypass demonstrates superior multi- task inference-efficiency on both discriminative and generative tasks. For discriminative multi-task inference, we combine the validation set of 19 tasks in VTAB-1K, perform 19 tasks on every image, and obtain the overall process time for the whole validation set. For generation multi-task inference, we take one image and provide the model with 10 fixed-length prompts for generation and record the overall generation time for the 10 prompts. For existing parameter-efficient methods, the inference time grows linearly when the number of tasks grows. Compared to the fully fine-tuned variant, all existing parameter-efficient tuning strategies increase the inference time to various extents. In contrast, our Res-Tuning-Bypass framework significantly reduces the inference time on 19 discriminative tasks and 10 generative tasks by respectively 80.9% and 83.6% when compared with the fully-fine-tuned variant. 6Full Linear VPT SSF NOAH Res-Tuning Adapter Linear Res-Tuning-Bypass FullLST Side-Tuning Figure 4: Comparisons of the parameter-, memory-, and multi-task inference-efficiency . For multi-task inference-efficiency, we evaluateRes-Tuning-Bypass on both discriminative and generative tasks. For parameter- and memory-efficiency, here, we show the comparisons on VTAB-1K between our approach and existing tuning strategies. Table 4: Performance and efficiency comparison on the VTAB-1K benchmark with ViT-B/16 models pre-trained on ImageNet-21K. “Group Mean” denotes the average accuracy of the three subgroups. “All Mean” denotes the average accuracy of 19 downstream tasks. Natural Specialized Structured CIFAR-100 Caltech101 DTD Flowers102 Pets SVHN Sun397 Camelyon EuroSAT Resisc45 Retinopathy Clevr-Count Clevr-Dist DMLab KITTI-Dist dSpr-Loc dSpr-Ori sNORB-Azim sNORB-Elev Group Mean All Mean Param. (M) Mem. (GB) Traditional methods Full 68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.96 65.57 85.84 9.40 Linear 63.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.6 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.64 52.94 0.04 3.09 Parameter-efficient tuning methods Adapter [25] 74.2 85.7 62.7 97.8 87.2 36.4 50.7 76.9 89.2 73.5 71.6 45.2 41.8 31.1 56.4 30.4 24.6 13.2 22.0 60.52 56.35 1.82 6.53 LoRA [27] 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.60 72.30 0.29 6.88 VPT-Deep [28] 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 71.96 69.43 0.60 8.13 SSF [42] 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.69 73.10 0.24 7.47 NOAH [83] 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.48 73.25 0.42 7.27 Res-Tuning 75.2 92.7 71.9 99.3 91.9 86.7 58.5 86.7 95.6 85.0 74.6 80.2 63.6 50.6 80.2 85.4 55.7 31.9 42.0 76.32 74.10 0.55 8.95 Memory-efficient tuning methods Side-Tuning [82] 60.7 60.8 53.6 95.5 66.7 34.9 35.3 58.5 87.7 65.2 61.0 27.6 22.6 31.3 51.7 8.2 14.4 9.8 21.8 49.91 45.65 9.59 3.48 LST† [68] 58.0 87.1 66.2 99.1 89.7 63.2 52.6 81.9 92.2 78.5 69.4 68.6 56.1 38.8 73.4 72.9 30.5 16.6 31.0 67.56 64.52 0.89 5.13 Res-Tuning-Bypass 64.5 88.8 73.2 99.4 90.6 63.5 57.2 85.5 95.2 82.4 75.2 70.4 61.0 40.2 66.8 79.2 52.6 26.0 49.3 72.32 69.51 0.42 4.73 † denotes our own implementation since the original approach is proposed for natural language processing. 4.4 Comparisons with existing tuning strategies on different scenarios Transfer Learning. We mainly evaluate our approach on the basic transfer learning scenario, where pre-trained models are fine-tuned on different downstream tasks. CIFAR-100 [ 35] is a standard general-purpose image classification dataset. VTAB-1K [ 80] is composed of 19 various visual classification tasks falling into three categorizations, i.e., natural, specialized, and structured. We compare the performance of our approach and other baseline methods on the following: CIFAR-100. We present the comparisons to existing tuning strategies in Tab. 3c. For parameter- efficient methods, our Res-Tuning framework notably improves over AdaptFormer [7] by 1.39%, using only 0.55% extra parameters, which is 0.91% less than AdaptFormer [7]. For memory-efficient methods, we outperform LST [68] by 0.61% with memory reduced by 0.54G (10%). VTAB-1K. In Tab. 4, we present comprehensive evaluation on the 19 datasets on the VTAB-1K benchmark. Both our Res-Tuning and Res-Tuning-Bypass outperform existing approaches respectively within the group of parameter- and memory-efficient tuning methods. Specifically, our Res-Tuning framework achieves a 0.85% improvement in terms of the average performance on 19 datasets, compared to the previous best performance. For our Res-Tuning-Bypass, we outperform LST [68] in 18 out of 19 tasks and overall by 4.99% in terms of average performance. This is achieved with even fewer parameters and memory consumption. We further visualize the parameter vs. performance curve and memory vs. performance curve in Fig. 4 to show the advantage of our Res-Tuning and Res-Tuning-Bypass framework on VTAB-1K. Few-Shot Learning. To evaluate the ability of our approach to adapt with only a few training samples, we follow the few-shot evaluation protocol in NOAH [83], using {1, 2, 4, 8, 16}-shots for training and full test data. We conduct experiments on five fine-grained datasets, including FGVC-Aircraft [51], Food-101 [4], Oxford Flowers [55], Oxford Pets [56], Stanford Cars [15]. 7Figure 5: Results of few-shot learning on five fine-grained visual recognition datasets. The solid line represents the comparison of parameter-efficient tuning methods, and the dashed line represents the comparison of memory-efficient tuning methods. All results are averaged over 3 random seeds. Table 5: Results on domain generalization. “Mean” denotes the average accuracy of four variants of ImageNet. All results are averaged over 3 random seeds. Source Target ImageNet IN-V2 IN-Sketch IN-A IN-R Mean Parameter-efficient tuning methods Adapter [25] 70.5 59.1 16.4 5.5 22.1 25.8 VPT [28] 70.5 58.0 18.3 4.6 23.2 26.0 LoRA [27] 70.8 59.3 20.0 6.9 23.3 27.4 NOAH [83] 71.5 66.1 24.8 11.9 28.5 32.8 Res-Tuning 78.04 66.58 29.23 13.15 29.01 34.50 Memory-efficient tuning methods Side-Tuning [82] 74.57 62.52 23.55 10.37 25.06 30.38 LST [68] 70.00 57.04 14.39 7.21 17.02 23.92 Res-Tuning-Bypass 77.30 65.23 27.39 10.66 26.45 32.43 As the results are shown in Fig. 5, in terms of the overall performance (top-left), both Res-Tuning and Res-Tuning-Bypass demonstrate clear advantages over other corresponding parameter-efficient and memory-efficient tuning strategies of few-shot learning on five FGVC datasets. We also observe that Res-Tuning-Bypass performs as well as or even better than the non-memory-efficient methods on one or two shots with low training samples on serveral datasets. Domain Generalization. To evaluate the robustness of our approach to distribution shift, we train a model on the source domain using 16 shots per category and test it on both the source and target domain. The source domain uses ImageNet-1K [11]) and the target domains use four other variants of ImageNet, including ImageNet-V2 [62], ImageNet-Sketch [73], ImageNet-A [24], ImageNet-R [23]. The results in Tab. 5 prove that our approach is robust under domain shift. Our Res-Tuning goes beyond NOAH [83] by 6.54% on ImageNet and 1.7% on the average accuracy of four variants of ImageNet. Furthermore, Res-Tuning-Bypass also demonstrates stronger robustness than the memory-efficient baselines and outperforms most existing parameter-efficient tuning methods. 5 Empirical evaluations on generative task 5.1 Experimental setups Downstream tasks. To provide a more comprehensive evaluation of ourRes-Tuning framework, we further apply it to the text-to-image generation task. Following [63], we evaluate the text-to-image 8Table 6: Comparison of FID and efficiency on COCO2017. Following the default settings of stable diffusion1, we sample 10k captions from the validation set for generating images of size 5122 using 50 PLMS steps with classifier-free guidance scale 3.0 and compare against the full validation set. Method FID Param. (M) Mem. (GB) Train (Hour/Epoch) SD v1.5 15.48 - - - + Full 14.85 862 (100%) 72.77 1.98 + LoRA 14.50 9.96 (1.15%) 61.03 1.42 + Adapter 14.73 2.51 (0.29%) 54.30 1.30 + Prefix 15.36 4.99 (0.58%) 64.91 2.20 + Prompt 14.90 1.25 (0.14%) 63.70 2.17 + Res-Tuning 13.96 2.54 (0.29%) 54.49 1.38 + Res-Tuning Bypass 14.89 3.76 (0.44%) 21.35 0.82 A surfboard  propped up in  the sand on a  beech. SD v1.5 Full LoRA Adapter Prefix Prompt Res-Tuning BypassText A black bird on  top of berry  branches. Figure 6: Qualitative results of SD v1.5, existing tuning strategies and our Res-Tuning on COCO2017 validation set [43]. We frame our results in green and others in red. generation performance on COCO2017 dataset [43]. The main quantitative evaluation metric is the FID score and we also perform instance-level fine-tuning transfer on the fine-grained datasets [4, 55]. Baselines. We experiment with the version 1.5 of stable diffusion [63] (SD) model. On COCO2017, we compare our Res-Tuning framework with zero-shot SD, SD with fully fine-tuning as well as SD with other existing tuning methods. On the fine-grained datasets, we employ DreamBooth [65] as our baseline and employ various tuning methods including our own for comparison. 5.2 Main results Text-to-image generation on COCO.We show the comparison between ourRes-Tuning framework and other approaches both quantitatively and qualitatively. The quantitative comparison is presented in Tab. 6. Compared with the fully fine-tuned baseline, our Res-Tuning framework improves the FID score by 0.89, using only 0.29% of the parameters. It is noteworthy that our Res-Tuning framework is the only approach that reaches a FID score below 14, while the best existing tuning strategy on the task is LoRA [27] achieving 14.50 with 4x the number of parameters in Res-Tuning. Hence, employing Res-Tuning could greatly facilitate the adaptation of pre-trained text-to-image generation models to downstream tasks. A highlight is observed that our Res-Tuning-Bypass framework reduces the memory consumption for tuning the SD v1.5 model to only 29% while maintaining a similar performance (14.89 vs 14.85). Meanwhile, the time consumption for training the model is reduced to 41%. In terms of the reduction in the memory and time consumption, our Res-Tuning-Bypass framework outperforms the best tuning strategy by 3.3x (70.7% memory reduction ofRes-Tuning-Bypass vs. 25.3% that of adapter) and 1.7x (58.6% reduction in time consumption of Res-Tuning-Bypass vs. 34.3% that of adapter), respectively. The qualitative results are presented in Fig. 6. Both Res-Tuning and Res-Tuning-Bypass show a better alignment between the text and the generated image, where the surfboard is propped up in our generated images. Res-Tuning also demonstrates a better fidelity where the feather texture is realistic in the generated black bird. 1https://huggingface.co/runwayml/stable-diffusion-v1-5 9SD v1.5 DreamBooth + LoRA + Adapter + Prefix + Prompt + Res-Tuning + Bypass Balloon Flower five petals bell-shaped Hotdog mayonnaise mustard ketchup Figure 7: Qualitative results of SD v1.5, DreamBooth, existing tuning strategies, and our Res-Tuning on Oxford Flowers and Food-101 fine-grained dataset with the same generated seed. We frame our results in green and others in red. Text-to-image generation on Oxford Flowers and Food-101. Here, we evaluate the transfer ability of existing tuning strategies on specific fine-grained categories. During every evaluation process, we select data from one specific category to train the model. The qualitative results are demonstrated in Fig. 7. It is observed that Res-Tuning presents a better view in terms of fidelity and the correct understanding of ambiguous categories. For example, the balloon flower is bell-shaped and has five petals, while existing approaches generate flowers with the wrong number of petals or even balloons rather than real flowers. Instead, both our Res-Tuning and our Res-Tuning-Bypass retain correct semantics and fine-grained features. 6 Related work Transformers in computer vision. Transformers [70] have demonstrated strong capabilities in various fields [5, 61, 12, 59, 2, 17, 48, 53]. In computer vision, Vision Transformers (ViT) [13] are widely applied in various applications, such as visual classification [48, 39], object detection [67, 6], segmentation [84, 74] and generation [63, 57]. Owing to the strong scalability of the Transformer backbone [31, 10], recent endeavors either focus on expanding the size of the ViT [81] or training on a larger corpus of data in an unsupervised manner [75, 14]. Parameter-efficient tuning. Despite the strong performance and generalization ability brought by scaling up the Transformers, it also makes the adaptation to the downstream tasks expensive and almost infeasible. Hence, parameter-efficient transfer learning (PETL) emerged [ 26, 25, 76, 41, 85, 28]. Existing PETL methods could be generally categorized into three types: (i) MHA-based tuning embeds tunable parameters in the multi-head self-attention layers [27, 76, 28, 41, 45, 46]. (ii) FFN-based tuning methods represented by adapters [25] and its generalized versions [58, 33, 32, 19] introduce a multi-layer perceptron to the FFN layer. (iii) Other tuning methods adapt certain existing parameters [79, 42]. Closely related to our work, some recent efforts are devoted to finding out the optimal design paradigm of the tuning modules by neural architecture search [83]. Instead, we show that through our Res-Tuning framework, a stronger tuning strategy can be easily found by the flexible combination of several existing tuning strategies in our unbinding form. Memory-efficient tuning. Since the structures of the existing PETL methods are deeply embedded in the backbone structure, back-propagation is required through the massive parameters of the pre- trained models, leading to unnecessary extra consumption of memory and computation. Hence, Side-Tuning [82] and LST [68] in natural language processing connect a side network in parallel with the pre-trained model to avoid data flow from the trainable parameters to the frozen ones. Our Res-Tuning-Bypass is inspired by the conceptual idea of these approaches. Compared to Side- Tuning and LST, we show that ourRes-Tuning-Bypass is more flexible and memory-efficient. 7 Conclusion In this work, we unbind the tuners from the backbone and form a flexible and efficient tuning paradigm Res-Tuning. With Res-Tuning, we are able to find stronger tuning strategies compared to existing ones. On top of Res-Tuning, we further extend a memory-efficient Res-Tuning-Bypass, which significantly reduces the memory consumption and multi-task inference cost. We hope our discoveries can facilitate further research in the flexible and efficient tuning of large foundation models. 10References [1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: A visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. [2] H. Bao, L. Dong, S. Piao, and F. Wei. BEiT: BERT pre-training of image Transformers. arXiv preprint arXiv:2106.08254, 2021. [3] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green, V . Valdés, A. Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. [4] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 – mining discriminative components with random forests. In Eur. Conf. Comput. Vis., 2014. [5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. In Adv. Neural Inform. Process. Syst., 2020. [6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with Transformers. In Eur. Conf. Comput. Vis., pages 213–229, 2020. [7] S. Chen, C. Ge, Z. Tong, J. Wang, Y . Song, J. Wang, and P. Luo. AdaptFormer: Adapting vision Transformers for scalable visual recognition. In Adv. Neural Inform. Process. Syst., 2022. [8] G. Cheng, J. Han, and X. Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017. [9] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., 2014. [10] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, et al. Scaling Vision Transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. [11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248–255, 2009. [12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. North Am. Chap. Assoc. Comput. Linguist., 2018. [13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020. [14] Y . Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y . Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint arXiv:2211.07636, 2022. [15] T. Gebru, J. Krause, Y . Wang, D. Chen, J. Deng, and L. Fei-Fei. Fine-grained car detection for visual census estimation. In Assoc. Adv. Artif. Intell., 2017. [16] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Research, 2013. [17] Y . Gong, Y .-A. Chung, and J. Glass. Ast: Audio spectrogram transformer.arXiv preprint arXiv:2104.01778, 2021. [18] B. Graham. Kaggle diabetic retinopathy detection competition report., 2015. [19] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning. In Int. Conf. Learn. Represent., 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 770–778, 2016. [21] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16000–16009, 2022. [22] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. 11[23] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and J. Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Int. Conf. Comput. Vis., 2021. [24] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [25] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for NLP. In Int. Conf. Mach. Learn., pages 2790–2799, 2019. [26] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In Int. Conf. Mach. Learn., pages 2790–2799, 2019. [27] E. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent., 2021. [28] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. Visual prompt tuning. In Eur. Conf. Comput. Vis., 2022. [29] S. Jie and Z.-H. Deng. Convolutional bypasses are better vision Transformer adapters. arXiv preprint arXiv:2207.07039, 2022. [30] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. [31] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [32] R. Karimi Mahabadi, J. Henderson, and S. Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In Adv. Neural Inform. Process. Syst., volume 34, pages 1022–1035, 2021. [33] R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Assoc. Comput. Linguist., pages 565–576, 2021. [34] A. Khosla, N. Jayadevaprakash, B. Yao, and F.-F. Li. Novel dataset for fine-grained image categorization: Stanford dogs. In IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2011. [35] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. Master’s thesis, Department of Computer Science, University of Toronto, 2009. [36] Y . LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In IEEE Conf. Comput. Vis. Pattern Recog., 2004. [37] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. In Conf. Empirical Methods NLP, 2021. [38] F.-F. Li, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Trans. Pattern Anal. Mach. Intell., 2006. [39] K. Li, Y . Wang, P. Gao, G. Song, Y . Liu, H. Li, and Y . Qiao. UniFormer: Unified Transformer for efficient spatiotemporal representation learning. In Int. Conf. Learn. Represent., 2022. [40] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. [41] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Assoc. Comput. Linguist., pages 4582–4597, 2021. [42] D. Lian, D. Zhou, J. Feng, and X. Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In Adv. Neural Inform. Process. Syst., 2022. [43] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In Eur. Conf. Comput. Vis., pages 740–755, 2014. [44] L. Liu, Y . Ren, Z. Lin, and Z. Zhao. Pseudo numerical methods for diffusion models on manifolds. InInt. Conf. Learn. Represent., 2022. 12[45] X. Liu, K. Ji, Y . Fu, Z. Du, Z. Yang, and J. Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. [46] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. GPT understands, too.arXiv preprint arXiv:2103.10385, 2021. [47] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [48] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin Transformer: Hierarchical Vision Transformer using shifted windows. In Int. Conf. Comput. Vis., 2021. [49] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. InIEEE Conf. Comput. Vis. Pattern Recog., pages 11976–11986, 2022. [50] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In Int. Conf. Learn. Represent., 2019. [51] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [52] L. Matthey, I. Higgins, D. Hassabis, and A. Lerchner. dSprites: Disentanglement testing sprites dataset, 2017. [53] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer. Trackformer: Multi-object tracking with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8844–8854, 2022. [54] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng. Reading digits in natural images with unsupervised feature learning. In Adv. Neural Inform. Process. Syst. Worksh., 2011. [55] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Ind. Conf. Comput. Vis. Graph. Image Process., pages 722–729, 2008. [56] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar. Cats and dogs. InIEEE Conf. Comput. Vis. Pattern Recog., 2012. [57] W. Peebles and S. Xie. Scalable diffusion models with Transformers. arXiv preprint arXiv:2212.09748, 2022. [58] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In Eur. Chap. Assoc. Comput. Linguist., pages 487–503, 2020. [59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., pages 8748–8763, 2021. [60] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text Transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. [61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. [62] B. Recht, R. Roelofs, L. Schmidt, and V . Shankar. Do imagenet classifiers generalize to imagenet? InInt. Conf. Mach. Learn., 2019. [63] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10684–10695, 2022. [64] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical image segmentation. In Med. Image Comput. Computer-Assisted Interv., pages 234–241, 2015. [65] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text- to-image diffusion models for subject-driven generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. [66] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conf. Empirical Methods NLP, pages 1631–1642, 2013. 13[67] H. Song, D. Sun, S. Chun, V . Jampani, D. Han, B. Heo, W. Kim, and M.-H. Yang. ViDT: An efficient and effective fully Transformer-based object detector. In Int. Conf. Learn. Represent., 2022. [68] Y .-L. Sung, J. Cho, and M. Bansal. LST: Ladder side-tuning for parameter and memory efficient transfer learning. In Adv. Neural Inform. Process. Syst., 2022. [69] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 595–604, 2015. [70] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. In Adv. Neural Inform. Process. Syst., volume 30, 2017. [71] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, and M. Welling. Rotation equivariant cnns for digital pathology. In Med. Image Comput. Computer-Assisted Interv., 2018. [72] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset. California Institute of Technology, 2011. [73] H. Wang, S. Ge, Z. C. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. In Adv. Neural Inform. Process. Syst., 2019. [74] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision Transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568–578, 2021. [75] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. [76] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y . Lee, X. Ren, G. Su, V . Perot, J. Dy, et al. DualPrompt: Complementary prompting for rehearsal-free continual learning. In Eur. Conf. Comput. Vis., 2022. [77] A. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In North Am. Chap. Assoc. Comput. Linguist., pages 1112–1122, 2018. [78] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3485–3492, 2010. [79] E. B. Zaken, S. Ravfogel, and Y . Goldberg. BitFit: Simple parameter-efficient fine-tuning for Transformer- based masked language-models. Assoc. Comput. Linguist., 2021. [80] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. [81] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling Vision Transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12104–12113, 2022. [82] J. O. Zhang, A. Sax, A. Zamir, L. J. Guibas, and J. Malik. Side-Tuning: Network adaptation via additive side networks. In Eur. Conf. Comput. Vis., 2019. [83] Y . Zhang, K. Zhou, and Z. Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. [84] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with Transformers. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. [85] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16816–16825, 2022. 14In the appendix, we provide a detailed derivation process for turning existing PETL methods into our unbinding formulation (appendix A), demonstration of our unbinding formulation as a unified formulation for existing PETL methods (appendix B), more implementation details (appendix C) including the dataset used, architectures for discriminative and generative tasks, and hyperparameters used in training, additional results on discriminative tasks (appendix D) and generative tasks (appendix E). A Detailed derivations In this section, we provide the detailed deriving process for the existing prefix [41] and prompt [28] tuning to be turned into our unbinding formulation. Prefix tuning [41]: The following is the detailed derivation of Eq. (3). MHApre = Attn(xWq, [Kpre; xWk], [Vpre; xWv]) = softmax \u0000 xWq[Kpre; xWk]⊤\u0001 \" Vpre xWv # = (1− λ(x)) softmax \u0000 xWqW⊤ k x⊤\u0001 xWv + λ(x) softmax \u0000 xWqK⊤ pre \u0001 Vpre = (1− λ(x)) Attn (xWq, xWk, xWv) +λ(x) Attn (xWq, Kpre, Vpre) = (1− λ(Q, K, Kpre)) Attn (Q, K, V )| {z } standard attention +λ(Q, K, Kpre) Attn (Q, Kpre, Vpre)| {z } independent of Kpre,Vpre (9) where Q, K denote the original query and original key, Kpre and Vpre are prefix key tokens and prefix value tokens, and λ(Q, K, Kpre) = P i exp \u0000 QK⊤ pre \u0001 iP i exp (QK⊤)i + P j exp \u0000 QK⊤pre \u0001 j , (10) Prompt tuning [28]: The following is the detailed derivation of Eq. (5). MHApro = Attn ([x; xpro]Wq, [x; xpro]Wk, [x; xpro]Wv) = concat \u0012 softmax \u0000 xWq[xWk; xproWk]⊤\u0001\u0014 xWv xproWv \u0015 , softmax \u0000 xproWq[xWk; xproWk]⊤\u0001\u0014 xWv xproWv \u0015\u0013 = concat ((1− λ(Q, K, Kpro)) Attn (Q, K, V ) +λ(Q, K, Kpro) Attn (Q, Kpro, Vpro) , (1 − β(Qpro, Kpro, K)) Attn (Qpro, Kpro, Vpro) +β(Qpro, Kpro, K) Attn (Qpro, K, V )) (11) where Kpro and Vpro are prompt key tokens and prompt value tokens, and λ(Q, K, Kpro) = P i exp \u0000 QK⊤ pro \u0001 iP i exp (QK⊤)i + P j exp \u0000 QK⊤pro \u0001 j , (12) β(Qpro, Kpro, K) = P i exp \u0000 QproK⊤\u0001 iP i exp \u0000 QproK⊤pro \u0001 i + P j exp (QproK⊤)j , (13) 15B Unified formulation From Eq. (7), we derive a unified formulation for existing PETL methods, which is the combination of the frozen pre-trained operation (OP) and the tuner with learnable parameters (Res-Tuner). The following is the detailed instantiation of this unified formulation (as in Tab. 7). Thanks to the form of unbinding, tuners can be treated as individual and flexibly combined. Our framework can effectively encompass existing tuning methods in a residual form and is not limited to the OP and Res-Tuner mentioned in our work. For example, for MAM-Adapter, we are free to attach Res-Prefix and Res-Adapter to MHA and FFN modules, respectively. In particular, when new PETL methods are proposed, they can be quickly applied (abbreviated as New-Tuning), and different methods can be arbitrarily combined (abbreviated as Mix-Tuning). Table 7: The combination of Res-Tuning. Method OP Res-Tuner Adapter [25] FFN Res-Adapter Prefix [41] MHA Res-Prefix Prompt [28] MHA Res-Prompt LoRA [27] Weight MLP BitFit [79] Bias Parameter AdaptFormer [7] FFN Res-Adapter MAM-Adapter [19] MHA + FFN Res-Prefix + Res-Adapter Side-Tuning [82] All blocks Side model LST [68] Block ViT New-Tuning Any where New-Tuner Mix-Tuning Any combination Any combination C Implementation details C.1 Dataset description In Tab. 8 and Tab. 9, we list the description of each dataset in our experiments, which includes the number of classes and the amount of images in training set and test set for discriminative and generative tasks, respectively. Table 8: Datasets used for generative tasks. ⋆ denotes only part of the data is used. Dataset Description Classes Train Test image prompt image prompt Common Objects in Context (COCO) COCO2017 Captions common objects - 118287 591753 5000 25014 Fine-grained Image Generation Aircraft [51]⋆ Fine-grained aircraft 100 3334 - 3333 - Food-101 [4]⋆ Fine-grained food 101 75750 - 25250 - NABirds [69]⋆ Fine-grained bird 555 23929 - 24633 - Stanford Cars [15]⋆ Fine-grained car 196 8144 - 8144 - Stanford Dogs [34]⋆ Fine-grained dog 120 12000 - 8580 - SUN397 [78]⋆ Fine-grained scene 397 76044 - 16295 - 16Table 9: Datasets used for discriminative tasks. Dataset Description Classes Train Test General Image Classification CIFAR-100 [35] General 100 50000 10000 Fine-grained Visual Classification (FGVC) CUB-200-2011 [72] Bird 200 5994 5794 NABirds [69] Bird 555 23929 24633 Oxford Flowers [55] Flower 102 1020 6149 Stanford Cars [15] Car 196 8144 8041 Stanford Dogs [34] Dog 120 12000 8580 Visual Task Adaptation Benchmark (VTAB-1K) CIFAR-100 [35] Natural 100 1000 10000 Caltech101 [38] 102 6084 DTD [9] 47 1880 Flower102 [55] 102 6149 Pets [56] 37 3669 SVHN [54] 10 26032 SUN397 [78] 397 21750 Camelyon [71] Specialized 2 1000 32768 EuroSAT [22] 10 5400 Resisc45 [8] 45 6300 Retinopathy [18] 5 42670 Clevr-Count [30] Structured 8 1000 15000 Clevr-Dist [30] 6 15000 DMLab [3] 6 22735 KITTI-Dist [16] 4 711 dSpr-Loc [52] 16 73728 dSpr-Ori [52] 16 73728 sNORB-Azim [36] 18 12150 sNORB-Ele [36] 9 12150 Few-Shot Learning FGVC-Aircraft [51] Aircraft 100 (1/2/4/8/16) * class 3333 Food-101 [4] Food 101 (1/2/4/8/16) * class 30300 Oxford Flowers [55] Flower 102 (1/2/4/8/16) * class 2463 Oxford Pets [56] Pet 37 (1/2/4/8/16) * class 3669 Stanford Cars [15] Car 196 (1/2/4/8/16) * class 8041 Domain Generalization ImageNet-1K [11] General 1000 16 * class 50000 ImageNet-V2 [62] General 1000 / 10000 ImageNet-Sketch [73] General 1000 / 50889 ImageNet-A [24] General 200 / 7500 ImageNet-R [23] General 200 / 30000 17C.2 Architectures design We show the complete Res-Tuning-Bypass structural design according to the different task frameworks. For discriminative tasks, we design based on the architecture of Vision Transformers [13], mainly based on main blocks and unbound bypass (as in Fig. 8). For generative tasks, we design based on the stable diffusion [ 63] framework and innovatively apply Res-Tuner to U-Net [ 64] structure (as in Fig. 9). We attach Res-Tuner to the U-Net intermediate module and decoder module for more efficient training. Block_1 Block_2 Block_N Horizontal Vertical Res- Tuner Res- Tuner Res- Tuner Res- Tuner Res-Tuning-Bypass Froward Backward × Detach Trainable Frozen ViT × …… Res- Tuner× Head Figure 8: Architecture design for discriminative tasks. 18Encoder 64×64 Encoder 32×32 Encoder 16×16 Encoder 8×8 Middle 8×8 Decoder 8×8 Decoder 16×16 Decoder 32×32 Decoder 64×64 × Horizontal Vertical Res- Tuner Res- Tuner × Res- Tuner × Res- Tuner × Res- Tuner Res- Tuner Res- Tuner Res- Tuner Conditional 2D U-Net Res-Tuning-Bypass Froward Backward × Detach Trainable Frozen Decoder Figure 9: Architecture design for generative tasks. 19C.3 Hyperparameters We list all the hyperparameters for discriminative (see in Tab. 10) and generative (see in Tab. 11) tasks. Table 10: Hyperparameter selection for discriminative tasks. Config Value Batch size 32 Optimizer AdamW [50] Weight decay 0.05 Base learning rate range {0.05, 0.01, 0.005, 0.001} Learning rate schedule Cosine decay Training epochs range {50, 100} Warmup epochs 10 Augmentation RandomResizedCrop, RandomHorizontalFlip OP MHA [70] FFN [70] Block [70] Res-Tuner Res-Adapter Res-Prefix Res-Prompt Architecture ViT/B-16 [13] ViT/L-14 [13] Pre-trained ImageNet-21K [11] CLIP [59] Device A100×1 Table 11: Hyperparameter selection for generative tasks. Config Value Batch size 8 Optimizer AdamW [50] Base learning rate 1e-4 Learning rate schedule Constant Training epochs 10 Augmentation CenterCrop, RandomHorizontalFlip Resolution 512 × 512 Sampler PLMS [44] Guidance 3.0 OP MHA [70] FFN [70] Block [70] Conditional 2D U-Net [64] Res-Tuner Res-Adapter Res-Prefix Res-Prompt Architecture Stable Diffusion [63] Pre-trained Stable Diffusion v1.5 [63] CLIP [59] Device A100 × 8 Library Diffusers2 2https://github.com/huggingface/diffusers 20D Additional experiments on discriminative tasks D.1 Comparisons on FGVC datasets We compare the transfer ability of our Res-Tuning framework with different existing approaches for parameter- and memory-efficient transfer learning on FGVC datasets. As shown in Tab. 12, our Res-Tuning outperforms other tuning methods on the average accuracy of five FGVC datasets. For memory-efficient methods, Res-Tuning-Bypass achieves a 2.89% improvement compared to LST [68] with less memory consumption. Table 12: Performance and efficiency comparison on FGVC. † denotes our own implementation. Method Datasets CUB- 200-2011 NABirds Oxford Flowers Stanford Cars Stanford Dogs Mean Param. (M) Mem. (GB) Full 87.3 82.7 98.8 84.5 89.4 88.54 85.98 9.40 Linear 85.3 75.9 97.9 51.3 86.2 79.32 0.18 3.09 Parameter-efficient tuning methods Adapter [25] 87.3 84.3 98.4 68.4 88.8 85.46 1.96 6.55 Prefix† [41] 85.4 78.8 99.2 76.4 89.5 85.86 0.36 6.60 VPT-Shallow [28] 86.7 78.8 98.4 68.7 90.7 84.62 0.25 8.14 VPT-Deep [28] 88.5 84.2 99.0 83.6 90.2 89.11 0.85 8.16 LoRA† [27] 86.0 80.2 99.2 85.2 88.6 87.84 0.55 6.78 Convpass† [29] 86.9 81.4 99.3 85.7 89.9 88.66 0.51 7.44 SSF [42] 89.5 85.7 99.6 89.2 89.6 90.72 0.39 7.47 Res-Tuning 89.66 85.87 99.45 87.58 92.21 90.95 0.68 8.98 Memory-efficient tuning methods Side-Tuning [82] 84.7 75.8 96.9 48.6 85.8 78.35 9.73 3.48 LST† [68] 82.98 76.11 98.83 78.46 87.89 84.85 1.04 5.28 Res-Tuning-Bypass 88.75 83.00 99.61 75.41 92.40 87.83 0.56 4.73 D.2 More ablation studies Varying the length of dimensions.The length of dimensions represents the hidden dimension for Res- Adapter and the number of tokens of Res-Prompt and Res-Prefix. Altering the length of dimensions affects performances, the number of parameters, and memory consumption simultaneously (see in Tab. 13). By default, we use the length that balances the number of parameters and memory, although there is a slight increase as the length grows to a certain extent. Table 13: Ablation studies on the length of dimension for Res-Tuning on CIFAR-100. Default setting is marked in gray. Dim Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. 5 93.03 0.30M 6.84G 89.16 0.37M 4.65G 10 93.25 0.48M 6.85G 89.33 0.46M 4.72G 20 92.98 0.85M 6.87G 89.28 0.64M 4.83G 50 92.69 1.96M 6.90G 89.22 1.19M 5.08G 100 92.48 3.80M 6.96G 89.52 2.11M 5.68G 21Different number of block layers. Based on the ViT/B-16 structure, we change the number of block layers in two ways: (i) increase from head to toe gradually (see in Tab. 14), (ii) discard several intermediate layers (see in Tab. 15), and evaluate the impact of using a different number of layers. It can be seen that the best performance can be achieved by using the full number of layers, but it requires the use of relatively more parameters and memory. Table 14: Ablation studies on the tuning block layers for Res-Tuning on CIFAR-100. The right arrow denotes use the number of all layers from left to right. Default setting is marked in gray. Blocks Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. 1 → 1 92.23 0.11M 6.43G 88.50 0.11M 3.58M 1 → 3 92.78 0.18M 6.52G 88.60 0.17M 3.73G 1 → 6 92.93 0.28M 6.64G 89.05 0.27M 4.09G 1 → 9 93.17 0.38M 6.84G 89.32 0.36M 4.53G 1 → 12 93.25 0.48M 6.85G 89.33 0.46M 4.72G Table 15: Ablation studies on the drop block layers for Res-Tuning on CIFAR-100. Inside the braces indicate the layer number that is discarded. The default setting is marked in gray. Drop block layers Res-Tuning Res-Tuning-Bypass Acc. Param. Mem. Acc. Param. Mem. {} 93.25 0.48M 6.85G 89.33 0.46M 4.72G {3, 6, 9} 92.88 0.38M 6.73G 89.30 0.36M 4.36G {2, 3, 5, 7, 9, 11} 92.96 0.28M 6.68G 88.89 0.27M 3.96G {2, 3, 4, 5, 6, 8, 9, 10 ,11} 92.71 0.18M 6.56G 88.79 0.17M 3.65G {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11} 87.77 0.11M 3.31G 88.49 0.11M 3.48G Different CNN backbones. We present the results for ConvNeXt [49] and ResNet-101 [20] pre- trained on ImageNet-21K in Tab. 16. We observe that two convolutional models have different characteristics, where the performance variation of ConvNeXt is small and that of ResNet is large. In terms of the effectiveness of Res-Tuning-Bypass, it is observed that it outperforms the fully- finetuned version of ConvNeXt and notably improves the performance of linear probing for ResNet. Table 16: Ablation studies on the CNN-based backbones on CIFAR-100. ConvNext ResNet-101 Method Acc. Param. Mem. Acc. Param. Mem. Full 90.15 87.67 11.16 77.40 42.70 7.30 Linear 90.06 0.11 3.36 54.96 0.20 2.83 Res-Tuning 90.86 0.87 9.45 86.80 0.92 7.20 Res-Tuning-Bypass 90.51 1.13 3.63 72.27 3.63 4.05 Experiments on NLP downstream task. We also conduct experiments on downstream tasks beyond vision. For NLP, we perform experiments on the SST2 [66] and MNLI [77] datasets for text classification tasks. It is observed that the performance of our Res-Tuning framework is on par with or better than MAM-Adapter [19] in text classification, with slightly longer training time but lower memory consumption. Our Res-Tuning-Bypass significantly reduces the training time and memory consumption and achieves a mildly lower performance. 22Table 17: Performance comparison on text classification task. SST2 MNLI Train Param. Test Param. Mem.Method Acc. Train Time Acc. Train Time MAM Adapter [19] 94.2 7.2 87.4 41.4 46.78 (37.4%) 0.61 (0.5%) 22.4G Res-Tuning 94.56 7.9 87.45 47.3 0.97 (0.77%) 0.97 (0.77%) 19.3G Res-Tuning-Bypass 92.94 4.2 82.01 24.2 0.98 (0.78%) 0.98 (0.78%) 4.3G E Additional Experiments on generative tasks E.1 More visualizations on COCO We present more visualization results, comparing real images, stable diffusion [63] (SD) v1.5, fully fine-tuning, LoRA [ 27], Res-Tuning and Res-Tuning-Bypass in Fig. 10 and more detailed generated image in Fig. 11. Specifically, the text conditions of text-to-image task are sampled from COCO Captions. To better demonstrate the advantages of our approach in understanding the concepts of text, the main elements are marked in blue, and the main modifiers or prepositions are marked in green. E.2 More visualizations on fine-grained datasets We present more visualization results on fine-grained datasets (see in Fig. 12, Fig. 13, and Fig. 14), including NABirds [69], Stanford Dogs [34], Stanford Cars [15], Aircraft [51], SUN397 [78] and Food-101 [4]. F Limitations and Societal Impacts This work is a tuning paradigm that is fine-tuned based on the pre-trained foundation models while freezing the backbone network, so its transfer ability depends to a large extent on the performance of the upstream model. However, when the upstream pre-training model contains illegal content training, it will also lead to the illegal use of tuning methods. 23Figure 10: Qualitative results of existing tuning strategies and our Res-Tuning on COCO2017 validation set. 24Bananas, marshmellows, chocolate  chips and sprinkles in a bowl. The rain is pouring on  the white car on the street. A close up of glazed donuts  that are plain or with chocolate. A landscape with water,  a boat, trees and mountains. Two elephants standing  next to each other in a field. A single white rose  in a glass vase. Figure 11: Visualization of our Res-Tuning on COCO2017 validation set. 25Figure 12: Visualization of our Res-Tuning on NABirds and Stanford Dogs. 26Figure 13: Visualization of Res-Tuning on Stanford Cars and Aircraft. 27Figure 14: Visualization of Res-Tuning on SUN397 and Food-101. 28",
      "meta_data": {
        "arxiv_id": "2310.19859v1",
        "authors": [
          "Zeyinzi Jiang",
          "Chaojie Mao",
          "Ziyuan Huang",
          "Ao Ma",
          "Yiliang Lv",
          "Yujun Shen",
          "Deli Zhao",
          "Jingren Zhou"
        ],
        "published_date": "2023-10-30T17:58:19Z",
        "pdf_url": "https://arxiv.org/pdf/2310.19859v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Res-Tuning, a novel tuning paradigm that unbinds tuners from the backbone of large-scale foundation models, addressing the limitations of existing parameter-efficient tuning (PETL) methods that are deeply coupled with the base model architecture. Key contributions include a unified formulation that seamlessly encompasses popular tuning approaches (prefix-tuning, prompt-tuning, adapters) and allows for flexible combination of various tuning strategies to discover stronger performers. A memory-efficient variant, Res-Tuning-Bypass, is proposed, which detaches tuners from the main branch to avoid back-propagation through the backbone and enables one-time backbone forward for multi-task inference. Extensive experiments demonstrate superior efficacy and efficiency on both discriminative (e.g., VTAB-1K) and generative tasks (text-to-image generation), outperforming existing alternatives with significant reductions in memory consumption and inference time while maintaining competitive performance.",
        "methodology": "Res-Tuning operates on a unified formulation where the output is a sum of the frozen pre-trained operation (OP) and a learnable Res-Tuner connected in parallel: x' = OP(x) + Res-Tuner(x). Existing PETL methods like prefix tuning, prompt tuning, and adapter tuning are theoretically and empirically re-formulated into this unbinding structure, often by expressing them as a weighted parallel combination of original and tuner-specific operations. The memory-efficient Res-Tuning-Bypass explicitly detaches the Res-Tuner bypass network from the main backbone, forming a sequence of tuners (`xbypass_l = λRes-Tuner(xl) + (1− λ)Res-Tuner(xbypass_l−1)`). This ensures gradients are only back-propagated to the tuners. The tuners within the bypass are categorized as horizontal (processing backbone output) and vertical (processing previous bypass output). The framework allows for flexible instantiation of Res-Tuners (e.g., prompt tuning for FFNs, adapters for MHA) and combinations (Single-, Dual-, Tri-Res-Tuner configurations).",
        "experimental_setup": "The framework was evaluated across discriminative (transfer learning, few-shot learning, domain generalization) and generative (text-to-image generation) tasks. Backbone models include ViT-B/16 (ImageNet-21K), ViT-L/14 (CLIP), ConvNeXt, and ResNet-101 for discriminative tasks, and Stable Diffusion v1.5 (U-Net architecture) for generative tasks. Datasets for discriminative tasks include CIFAR-100, VTAB-1K (19 diverse visual classification tasks), FGVC-Aircraft, Food-101, Oxford Flowers, Oxford Pets, Stanford Cars (for few-shot), and ImageNet variants (ImageNet-1K, -V2, -Sketch, -A, -R for domain generalization). Generative tasks used COCO2017 for text-to-image and fine-grained datasets (Oxford Flowers, Food-101, NABirds, Stanford Dogs, Stanford Cars, SUN397) for instance-level fine-tuning. NLP tasks (SST2, MNLI) were included in ablations. Evaluation metrics were top-1 accuracy for discriminative tasks and FID score along with qualitative results for generative tasks. Baselines included traditional methods (full fine-tuning, linear probing), parameter-efficient methods (Adapter, LoRA, VPT, SSF, NOAH, AdaptFormer, MAM-Adapter), and memory-efficient methods (Side-Tuning, LST). Training used AdamW optimizer with cosine decay learning rate schedule, random cropping, and horizontal flipping augmentations. Experiments were conducted on A100 GPUs.",
        "limitations": "The transfer ability of the proposed tuning paradigm is largely dependent on the performance and characteristics of the upstream pre-trained foundation model. A significant societal concern is the potential for illegal content within the upstream pre-training model to lead to the illegal use of the tuning methods developed.",
        "future_research_directions": "The authors express hope that their discoveries will facilitate further research in the flexible and efficient tuning of large foundation models. The inherent flexibility of the Res-Tuning framework suggests future work could involve exploring new tuner designs, more complex combinations of existing and novel tuners (as explicitly mentioned by the potential for 'New-Tuning' and 'Mix-Tuning'), and applying the unbinding and bypass mechanisms to a wider array of model architectures and downstream tasks beyond those explored in the paper."
      }
    },
    {
      "title": "ReFT: Representation Finetuning for Language Models",
      "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural\nmodels via updates to a small number of weights. However, much prior\ninterpretability work has shown that representations encode rich semantic\ninformation, suggesting that editing representations might be a more powerful\nalternative. We pursue this hypothesis by developing a family of Representation\nFinetuning (ReFT) methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we\nidentify an ablation of this method that trades some performance for increased\nefficiency. Both are drop-in replacements for existing PEFTs and learn\ninterventions that are 15x--65x more parameter-efficient than LoRA. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\ninstruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the\nbest balance of efficiency and performance, and almost always outperform\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.",
      "full_text": "ReFT: Representation Finetuning for Language Models Zhengxuan Wu∗† Aryaman Arora∗† Zheng Wang† Atticus Geiger‡ Dan Jurafsky† Christopher D. Manning† Christopher Potts† †Stanford University ‡Pr(Ai)2R Group {wuzhengx,aryamana,peterwz,atticusg}@stanford.edu {jurafsky,manning,cgpotts}@stanford.edu Abstract Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15×–65×more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense rea- soning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft. 1 Introduction Pretrained language models (LMs) are frequently finetuned to adapt them to new domains or tasks [Dai and Le, 2015]. With finetuning, a single base model can be adapted to a variety of tasks given only small amounts of in-domain data. However, finetuning large LMs is expensive. Parameter- efficient finetuning (PEFT) methods propose to address the high costs of full finetuning by updating a small number of weights. This reduces memory usage and training time, and PEFTs achieve similar performance to full finetuning in many settings [Hu et al., 2023]. A hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative to weight updates. In this paper, we pursue this hypothesis by developing and motivating Representation Finetuning (ReFT). Instead of adapting model weights, ReFT methods train interventions that manipulate a small fraction of model representations in order to steer model behaviors to solve downstream tasks at inference time. ReFT methods are drop-in replacements for weight-based PEFTs. This approach is inspired by recent work in LM interpretability that intervenes on representations to find faithful causal mechanisms [Geiger et al., 2023b] and to steer model behaviours at inference time [Turner et al., 2023, Li et al., 2024], and it can be seen as a generalisation of the representation-editing work of Wu et al. [2024a], Turner et al. [2023], and Zou et al. [2023] (see appendix B for details). *Equal contribution. Preprint. Under review. arXiv:2404.03592v3  [cs.CL]  22 May 2024Commonsense LLaMA 7B  LLaMA 13B  Llama- 2 7B  Llama- 3 8B Instruct -tuning Llama- 2 7B Paramet ers P er f ormance Arit hmetic LLaMA 7B  LLaMA 13B GLUE R oBERT a-base  R oBERT a-lar ge Figure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks when applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer parameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance on all tasks. Its value is most apparent for the largest models in our evaluations. Note: FT is full-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4. We focus on a strong and highly efficient instance of the ReFT family that we call Low-rank Linear Subspace ReFT (LoReFT). LoReFT is a parametrisation of ReFT that intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix, building directly on the distributed alignment search (DAS) method of Geiger et al. [2023b] and Wu et al. [2023]. We also identify an ablation of this method (DiReFT) that trades some performance for increased efficiency. We evaluate our ReFTs on LLaMA-family models and small-scale LMs against existing PEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding. Compared to LoRA, we find that LoReFT uses 15×–65×times fewer parameters while achieving state-of-the-art performance on commonsense reasoning, instruction-following, and natural language understanding against the strongest PEFTs. These findings indicate that ReFT methods are worthy of further exploration, as they may emerge as more efficient and effective alternatives to weight-based PEFTs. 2 Related work Parameter-efficient finetuning methods (PEFTs). PEFTs train a fraction of the model’s parameters to adapt it to downstream tasks. We classify PEFTs into three categories: 1. Adapter-based methods train additional modules (e.g. fully-connected layers) on top of the frozen pretrained model. Series adapters insert components between LM attention or MLP layers [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021], while parallel adapters add modules alongside existing components [He et al., 2022a]. Since adapters add new components that cannot be easily folded into existing model weights, they impose an additional burden at inference time.1 2. LoRA [Hu et al., 2022] and DoRA [Liu et al., 2024c] use low-rank matrices to approximate additive weight updates during training, and require no additional overhead during inference since the weight updates can be merged into the model. These are the strongest PEFTs currently.2 3. Prompt-based methods add randomly-initialised soft tokens to the input (usually as a prefix) and train their embeddings while keeping the LM weights frozen [Li and Liang, 2021]. These 1Several very recent papers introduce new adapter architectures but do not benchmark them on the tasks we consider, or they perform hyperparameter-tuning in a different setup than done in this work. These include: LLaMA-Adapter [Zhang et al., 2024b], LLaMA-Adapter v2 [Gao et al., 2023], Aligner [Ziheng et al., 2023]. 2Additional methods not studied in this work: AutoLoRA [Zhang et al., 2024c], ResLoRA [Shi et al., 2024], SiRA [Zhu et al., 2023]. 2methods are often far from optimal compared to other PEFTs, and come at the cost of significant inference overhead. A variant of this method where hidden-layer activations are also tuned was introduced as a baseline in Hu et al. [2022], with better performance. Representation editing. Recent work on activation steering and representation engineering shows that adding fixed or task-specific steering vectors [Subramani et al., 2022, Turner et al., 2023, Zou et al., 2023, Liu et al., 2024b, V ogel, 2024, Li et al., 2024] or applying concept erasure [Ravfogel et al., 2022, Belrose et al., 2023, Avitan et al., 2024, Singh et al., 2024] to the residual stream can enable a degree of control over pretrained LM generations without the need for resource-intensive finetuning [Wu et al., 2024a]. The success of these methods affirms that representations induced by pretrained LMs carry rich semantic structure. Interventional interpretability. Much recent work has used interventions on model-internal states to test hypotheses about how LMs implement various behaviours. In particular, interventions on linear subspaces of representations have provided increasing evidence that human-interpretable concepts are encoded linearly [Smolensky, 1986, Rumelhart et al., 1986, McClelland et al., 1986]. This includes linguistic features such as gender and number [Lasri et al., 2022, Wang et al., 2023, Hanna et al., 2023, Chintam et al., 2023, Yamakoshi et al., 2023, Hao and Linzen, 2023, Chen et al., 2023, Amini et al., 2023, Guerner et al., 2023, Arora et al., 2024], logical and mathematical reasoning [Wu et al., 2023], entity attributes [Huang et al., 2024], and a number of other domains [Mikolov et al., 2013, Elhage et al., 2022, Park et al., 2023, Nanda et al., 2023, Guerner et al., 2023]. 3 ReFT We now define the ReFT family of methods. To do this, we first summarize the core motivation, which emerges from work on intervention-based model interpretability. We then show how this leads directly to Low-rank Linear Subspace ReFT (LoReFT). Finally, we generalize this to a family of ReFT methods. Appendix A provides a brief overview of our generic ReFT training library. To keep the presentation simple, we assume throughout that our target model is a Transformer- based [Vaswani et al., 2017] LM that produces contextualised representations of sequences of tokens. Given a sequence of n input tokens x = (x1, . . . , xn), the model first embeds these into a list of representations h(0) =(h(0) 1 , . . . ,h(0) n ). Then, m layers successively compute the j-th list of hidden representations h(j) as a function of the previous list of hidden representations h(j−1). Each hidden representation is a vector h ∈Rd. The LM uses the final hidden representations h(m) to produce its predictions. In our experiments, we consider both autoregressive LMs and masked LMs [Devlin et al., 2019]. An autoregressive LM predicts p(xn+1 ∣ x1, . . . , xn) =softmax (Wh(m) n ), while a masked LM predicts p(xi ∣ x1, . . . , xi−1, xi+1, . . . , xn) =softmax (Wh(m) i ), where W is a learned matrix mapping from representations to logits over the vocabulary space. 3.1 Motivation In interpretability research, the framework of causal abstraction [Geiger et al., 2021] usesinterchange interventions to establish the causal role of representations in deep learning models. An interchange intervention fixes a representation to the value it would take if a counterfactual input were processed by the model. Experiments investigating how such interventions affect model behavior form the evidence for claims about the causal role of a representation and the concept it encodes. To test whether a concept is encoded in a linear subspace of a representation, one may use a dis- tributed interchange intervention (DII) [Geiger et al., 2023b].3 Let b be the hidden representation created at row i and column k when our model processes input b, and let s be the corresponding representation when that same model processes input s. A distributed interchange intervention on b given a counterfactual source representation s is then defined as DII(b, s, R) =b +R⊺(Rs −Rb) (1) where R ∈ Rr×d is a low-rank projection matrix with orthonormal rows, d is the representation dimensionality, and r is the dimensionality of the subspace we are intervening on. We learn the subspace R using distributed alignment search (DAS), which finds the subspace that maximises the 3This notion of subspace intervention was also independently discovered by Guerner et al. [2023]. 3R eFT Int er v ention t his is some t e xt LoR eFT edit subspace (r o ws of R) RW h h h Φ(h) b -+ + R T edit r estrict ed  t o subspace edit  v ect or Figure 2: Illustration of ReFT. (1) The left panel depicts an intervention I: the intervention function Φ is applied to hidden representations at positions P in layer l. (2) The right panel depicts the intervention function used in LoReFT, which finds an edit vector that only modifies the representation in the linear subspace spanned by the rows of R. Specifically, we show how a rank-2 LoReFT operates on 3-dimensional hidden representations. probability of the expected counterfactual output after intervention [Geiger et al., 2023b]. DAS is highly expressive, and can effectively localize concepts within model representations [Wu et al., 2023, Arora et al., 2024, Wu et al., 2024c, Huang et al., 2024]. This suggests that subspace representation interventions could also be a powerful tool for model control. 3.2 Two low-rank ReFT instantiations LoReFT. The formulation of DII in eq. (1) immediately suggests a way to control model generations via interventions. The guiding intuition is that we can learn how to perform interventions that steer the model towards predicting our task labels. The resulting method, Low-rank Linear Subspace ReFT (LoReFT), is defined by the following variant of eq. (1): ΦLoReFT(h) =h +R⊺(Wh +b −Rh) (2) This is identical to eq. (1), except we use a learned projected source Rs =Wh +b. LoReFT thus edits the representation in the r-dimensional subspace spanned by the rows of R to take on the values obtained from our linear projection Wh +b. We depict this operation in fig. 2. The learned parameters are ϕ = {R, W, b}; the parameters of the LM are frozen. As with DII, R ∈ Rr×d is a low-rank matrix with orthonormal rows where d is the hidden-state dimensionality and r ≤d is the rank of the subspace. We further define a linear projection W ∈Rr×d and bias vector b ∈Rr. DiReFT. In addition, we define an ablation of LoReFT which removes the orthogonality constraint and the difference operation, reducing training time: ΦDiReFT(h) =h +W⊺ 2 (W1h +b) (3) Both W1, W2 ∈Rr×d are low-rank projection matrices. Note that eq. (3) resembles LoRA, and thus DiReFT can be thought of as LoRA applied directly to hidden representations at certain positions.4 Empirical evidence from previous work suggests that adding orthogonal constraints to LoRA weights increases performance [Liu et al., 2024d]. (Appendix E reports results for additional ablations of LoReFT.) Training objective. We consider both generation tasks using decoder-only or encoder–decoder LMs and classification tasks using encoder-only models. The pretrained language model induces a distribution over token sequences p(⋅). We denote the model that results from the ReFT intervention Φ on p(⋅) as pΦ(⋅) with trainable parameters ϕ. To simplify notation, we refer to the hidden representations produced by the LM on input x as h(x), and those by the intervened LM as hΦ(x). For generation tasks, our training objective is language modelling. Given an input sequence x = (x1, . . . , xn) with n tokens as the prompt, the goal is to predict the output sequencey =(y1, . . . , ym) 4LoRA is not applicable to the residual stream, which is weightless. LoRA can be configured to apply only to the attention layer output projection matrix, which is similar to our residual stream intervention. However, previous works found that applying LoRA only to attention layers is sub-optimal [Hu et al., 2023]. 4with m tokens. We minimise the cross-entropy loss with teacher-forcing over all output positions. min ϕ {− m ∑ i=1 log pΦ (yi ∣ xy<i)} (4) For single-label classification tasks, we add a classification head Hθ(⋅) with parameters θ that takes the final-layer representation at the first token (CLS) as input and outputs a distribution over classes. H has the learned parameters θ ={Wo, bo, Wd, bd}. Hθ(⋅∣ h) =softmax (Wo(tanh(Wdh(m) 1 +bd))+bo) (5) We learn the parameters of the head and those of the intervention function Φ. We minimise the cross-entropy loss of the target class y given input x: min ϕ,θ {−log Hθ(y ∣ hΦ(x))} (6) 3.3 The ReFT family of methods It is straightforward to generalise the above intervention functions to define a family of intervention- based representation finetuning methods. We first define a general notion of intervention, i.e. the modification of hidden representations during the model forward pass: Definition 3.1. An intervention I is a tuple ⟨Φ, P, l⟩ that encapsulates a single inference-time modification of the representations computed by a Transformer-based LM. The three components of an intervention are (1) the intervention function Φ ∶Rd → Rd with learned parameters ϕ, (2) a set of input positions P ⊆{1, . . . , n} that the intervention is applied to, and (3) the layer l ∈{1, . . . , m} at which the intervention is applied. We implement the intervention I as the following operation that overwrites some representations h: h(l) ← (Φ (h(l) p ) if p ∈P else h(l) p )p∈1,...,n (7) The intervention is applied immediately after the computation of h(l) and thus affects the representa- tions computed in later layers h(l+1), . . . ,h(m). Figure 2 provides a schematic overview of an intervention. A ReFT is then defined as a constrained set of non-overlapping interventions: Definition 3.2. A ReFT method is a set of f interventions I = {I1, . . . , If }. We enforce that for any two interventions Ij, Ik ∈I such that they operate on the same layer lj =lk, their intervention positions must be disjoint, i.e. Pj ∩Pk =∅. The parameters (ϕ1, . . . , ϕf ) of all of the intervention functions are independent. ReFT is thus a generic framework encompassing interventions on hidden representations during the model forward pass. In appendix B, we show how a variety of existing inference-time intervention methods can be described within this framework. 4 Experiments To evaluate our ReFTs against existing PEFTs, we conduct experiments across four diverse NLP benchmarks covering more than 20 datasets (extensive details on our datasets are in appendix C). Our goal is to provide a rich picture of how LoReFT and DiReFT perform in different scenarios. We experiment with both masked and autoregressive LMs at different scales, ranging from RoBERTa- base [Liu et al., 2019] with 125M to LLaMA models [Touvron et al., 2023a,b] with 13B parameters. We benchmark against existing PEFTs such as prefix-tuning [Li and Liang, 2021], adapter-tuning with both Series Adapters and Parallel Adapters, BitFit [Ben Zaken et al., 2022], RED [Wu et al., 2024a], LoRA [Hu et al., 2022], and DoRA [Liu et al., 2024c]. Our comparisons focus on both performance and parameter efficiency. In our comparisons, we use hyperparameter-tuned scores from previous works when possible. We load our base LMs in torch.bfloat16 to save memory. All of our experiments are run with a single GPU: NVIDIA A100 40G/80G or RTX 6000. Examples of raw model generations are in appendix I. 54.1 Hyperparameter configuration For our experiments, we must decide how many interventions to learn and which layers and input positions to apply each one on. We propose learning interventions on a fixed number of p prefix and s suffix positions in the prompt. Specifically, we tune four hyperparameters: 1. The number of prefix positions p to intervene on, i.e. positions {1, . . . , p}. 2. The number of suffix positions s to intervene on, i.e. positions {n −s +1, . . . , n}. 3. Which set of layers L to intervene on. 4. Whether or not to tie intervention parameters ϕ across different positions in the same layer. This simplifies the hyperparameter search space; compared to LoRA, the only additional consideration is which positions to intervene on. Since the number of positions edited is constant, LoReFT and DiReFT contribute a fixed additional inference cost that does not scale with prompt length. Given the positions P ={1, . . . , p}∪{n −s +1, . . . , n}, we define the untied and tied variants: Iuntied ={⟨Φ, {p}, l⟩ ∣p ∈P, l∈L} Itied ={⟨Φ, P, l⟩ ∣l ∈L} Additionally, when applying LoReFT and DiReFT to a prompt with length n where n <p +s, we set p ← min(p, ⌊n/2⌋) and s ← min(s, ⌈n/2⌉) and do not apply the truncated interventions in Iuntied. We also tune neural-network training hyperparameters. Unlike previous work [Hu et al., 2022, 2023, Liu et al., 2024c] where hyperparameter tuning may involve optimising performance directly on test sets, we only tune our hyperparameters on development sets which do not contain any overlapping examples with the test sets of our tasks. We further describe hyperparameter tuning for each benchmark in appendix D.1. 4.2 Commonsense reasoning We replicate the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B/13B, Llama-2 7B, and Llama-3 8B 5 on a combined dataset of eight commonsense reasoning tasks (COMMONSENSE 170K ). We report scores on each task’s test set individually. We compare with PEFTs benchmarked in Hu et al. [2023] as well as the identical experiment reported in Liu et al. [2024c] for DoRA. Datasets. Our benchmark contains eight commonsense reasoning datasets, including BoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap et al., 2019], HellaSwag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], ARC-e, ARC-c [Clark et al., 2018], and OBQA [Mihaylov et al., 2018]. Examples are formulated as multiple-choice problems where the model needs to directly generate the correct choice without rationales. We use the same prompt template as in Hu et al. [2023] with additional string normalisation (removing leading and trailing whitespace). Hyperparameter tuning. We do not do hyperparameter selection based on test set results. Rather, we use the hyperparameter settings of the model that performs best on a development set created from the GSM8K training set, except we use a lower number of epochs (6 instead of 12) because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K . This allows us to tune relevant hyperparamters, and also serves to test the robustness of these settings across different domains. We additionally report scores on 3 epochs in appendix D.3. Results. We report results in table 1. LoReFT sets state-of-the-art performance on the commonsense reasoning tasks, outperforming all other methods by a considerable margin. While being more compute-efficient, DiReFT achieves only slightly worse performance consistently. 4.3 Arithmetic reasoning Similar to the previous experiment, we follow the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B and 13B on a combined dataset of seven arithmetic reasoning tasks with LM-generated chain-of-thought steps (MATH10K) and report scores on four of the tasks’ test sets. We only evaluate correctness on the final numeric or multiple-choice answer. 5Llama-3 8B appeared on April 18, 2024, and thus we had time to complete only commonsense reasoning experiments with this model. Liu et al. [2024c] report corresponding results for LoRA and DoRA. 6Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For our methods, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 DiReFT (ours) 0.031% 69.5 83.0 79.0 92.5 80.5 82.2 68.0 77.5 79.0LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 DiReFT (ours) 0.025% 71.3 86.1 80.8 94.6 83.6 85.5 72.9 82.7 82.2LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Llama-2 7B LoRA∗ 0.826% 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6DoRA (half)∗ 0.427% 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5DoRA∗ 0.838% 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7 DiReFT (ours) 0.031% 70.8 83.6 80.2 93.6 82.1 84.8 70.4 81.5 80.9LoReFT (ours) 0.031% 71.1 83.8 80.8 94.3 84.5 85.6 72.2 82.3 81.8 Llama-3 8B LoRA∗ 0.700% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8DoRA (half)∗ 0.361% 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0DoRA∗ 0.710% 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2 DiReFT (ours) 0.026% 73.4 88.7 81.0 95.6 85.5 91.8 81.8 85.4 85.4LoReFT (ours) 0.026% 75.1 90.2 82.0 96.3 87.4 92.4 81.6 87.5 86.6 Hyperparameter tuning. We use the same hyperparameter settings as for the Commonsense Rea- soning benchmark, but with 12 epochs for training. We also report scores on 3 epochs. Datasets. Our benchmark contains four datasets for math world problems, including AQuA [Ling et al., 2017], GSM8K [Cobbe et al., 2021], MAWPS [Koncel-Kedziorski et al., 2016], and SV AMP [Pa- tel et al., 2021]. Models need to generate chain-of-thought [Wei et al., 2022] before the final answer. We use the same prompt template and hyperparameter settings as in the previous experiment. Results. We report results in table 2. We find that both LoReFT and DiReFT do not perform as well at arithmetic reasoning tasks compared to LoRA and adapters, but do outperform prefix-tuning. Our results suggest that our ReFTs may have more trouble on chain-of-thought reasoning than the single-step commonsense reasoning tasks due to the length of generations (greater length necessarily reduces the effect of the intervention) and overall greater difficulty of the task. Our results show that our ReFTs perform better with the 13B model than the 7B model, which suggests that our methods scale with model size. Overall, we note that the arithmetic reasoning results show a lot of variation, with no single method emerging as a clear winner across all of them. 4.4 Instruction-following Base LMs require instruction finetuning to follow human prompts [Ouyang et al., 2022]. We follow the experimental setup in Wu et al. [2024a] and finetune Llama-2 7B with Ultrafeedback [Cui et al., 2023]. We compare against full parameter finetuning, LoRA, and RED. For evaluation, we use Alpaca-Eval v1.0 [Li et al., 2023], which computes the win-rate against text-davinci-003 using GPT-4 as the annotator. We use the same prompt template as in Taori et al. [2023]. Datasets. Ultrafeedback is high-quality instruction dataset where responses are generated via scoring a diverse set of model responses from a list of candidates (e.g. ChatGPT and Bard). The score is calculated as a weighted score of instruction-following, truthfulness, honesty, and helpfulness. 7Table 2: Accuracy comparison of LLaMA-1 7B/13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 DiReFT (ours) 0.031% 21.3 24.1 74.5 42.7 40.6 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 DiReFT (ours) 0.025% 20.5 35.8 80.8 54.8 48.0 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 Table 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval v1.0. We report averaged performance of two runs with distinct random seeds for our method. half denotes our runs with half of the rank; 1K denotes our runs with a low-resource setting where there is only 1K training examples. †Performance results of baseline methods are taken from Li et al. [2023]. ∗Performance results of baseline methods are taken from Wu et al. [2024a]. ‡It takes 18 minutes to train our Llama-2 Chat 7B on 1K examples using a single A100 40G GPU with ≈1MB parameters on disk. Model & PEFT Params (%) Win-rate (↑) GPT-3.5 Turbo 1106† — 86.30 Llama-2 Chat 13B† — 81.10 Llama-2 Chat 7B† — 71.40 Llama-2 7B & FT∗ 100% 80.93 Llama-2 7B & LoRA∗ 0.1245% 81.48 Llama-2 7B & RED∗ 0.0039% 81.69 Llama-2 7B & DiReFT (ours) 0.0039% 84.85 Llama-2 7B & LoReFT (ours) 0.0039% 85.60 Llama-2 7B & LoReFT (ours, half) 0.0019% 84.12 Llama-2 7B & LoReFT (ours, 1K)‡ 0.0039% 81.91 Some of the best 7B and 13B chat-models (e.g. UltraLM-13B [Ding et al., 2023]) are finetuned with Ultrafeedback. Hyperparameter tuning. We do hyperparameter-tuning on the unseen instruction-following dataset Alpaca-52K [Taori et al., 2023] with only LLaMA-7B to prevent test-set hill-climbing. We then use the hyperparameter settings of our best performing model to finetune on Ultrafeedback. For hyperparameter tuning, we use Alpaca-Eval v1.0 with GPT-4 turbo as the annotator for fast turnaround, which also prevents overfitting with GPT-4 as a judge. Results. We report results in table 3. When matched in parameter count to the previous most parameter-efficient PEFT (RED) and trained on Llama-2 7B, LoReFT outperforms all reported finetuning methods (including full finetuning) and achieves a win-rate within 1% of GPT-3.5 Turbo 1106. Furthermore, after halving the parameter count or using only 1/64-th of the data, LoReFT still outperforms other finetuning methods. This result shows that LoReFT can succeed at long-form text generation. DiReFT is again slightly worse than LoReFT but is highly competitive. 6 6We release our ReFT weights (<1MB) of our instruction-tuned model through HuggingFace and provide a tutorial at https://github.com/stanfordnlp/pyreft/blob/main/examples/chat. 8Table 4: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 94.4 87.9 62.4 92.5 91.7 78.3 90.6 85.6 Adapter∗ 0.318% 87.0 93.3 88.4 60.9 92.5 90.5 76.5 90.5 85.0 LoRA∗ 0.239% 86.6 93.9 88.7 59.7 92.6 90.4 75.3 90.3 84.7 AdapterFNN∗ 0.239% 87.1 93.0 88.8 58.5 92.0 90.2 77.7 90.4 84.7 BitFit∗ 0.080% 84.7 94.0 88.0 54.0 91.0 87.3 69.8 89.5 82.3 RED∗ 0.016% 83.9 93.9 89.2 61.0 90.7 87.2 78.0 90.4 84.3 DiReFT (ours) 0.015% 82.5 92.6 88.3 58.6 91.3 86.4 76.4 89.3 83.2 LoReFT (ours) 0.015% 83.1 93.4 89.2 60.4 91.2 87.4 79.0 90.0 84.2 large FT 100% 88.8 96.0 91.7 68.2 93.8 91.5 85.8 92.6 88.6 Adapter∗ 0.254% 90.1 95.2 90.5 65.4 94.6 91.4 85.3 91.5 88.0 LoRA∗ 0.225% 90.2 96.0 89.8 65.5 94.7 90.7 86.3 91.7 88.1 AdapterFNN∗ 0.225% 90.3 96.1 90.5 64.4 94.3 91.3 84.8 90.2 87.7 RED∗ 0.014% 89.5 96.0 90.3 68.1 93.5 88.8 86.2 91.3 88.0 DiReFT (ours) 0.014% 88.7 95.4 88.5 66.7 93.9 88.1 86.9 91.2 87.4 LoReFT (ours) 0.014% 89.2 96.2 90.1 68.0 94.1 88.5 87.5 91.6 88.2 4.5 Natural language understanding We evaluate LoReFT on the GLUE benchmark [Wang et al., 2018] against existing PEFTs. We use this set of experiments to show LoReFT works well even with small-scale LMs, and can improve representations for classification tasks and not just text generation. We finetune RoBERTa-base (125M) as well as RoBERTa-large (350M) on GLUE, a sequence classification benchmark for natural language understanding (NLU) which covers domains such as sentiment classification and natural language inference. Details about the GLUE benchmark can be found in its original paper. We follow Wu et al. [2024a] for proper evaluation on GLUE validation set: we split the validation set into two sets guarded by a random seed, and we pick the best model with highest in-training validation accuracy to evaluate on the other held-out half for testing accuracy. Hyperparameter tuning. We tune our hyperparameters for each task separately. which is standard for PEFTs. To avoid overfitting to random seeds, we hyperparameter-tune our models with a constant seed, and report averaged results over that and four additional unseen seeds. We describe hyperparameter tuning experiments in Appendix D.1. Results. We report results in table 4. LoReFT obtains comparable performance with PEFT methods on both model sizes when parameter matched with RED, the previous most parameter-efficient PEFT for this task. Furthermore, DiReFT achieves worse performance than most of the PEFTs suggesting LoReFT is a better choice when LM is small. Full results with standard deviation is in table 13. We additionally compare against VeRA [Kopiczko et al., 2024] in appendix D.3. 5 Limitations Due to limited resources, we mainly explored the LLaMA-family of models. In future work, we hope to explore the effectiveness of ReFT on other model families as well as vision–language models such as LLaV A [Liu et al., 2024a]. The capabilities of ReFT have not yet been fully explored due to the large hyperparameter search space; we are interested in automating this search. We provide some initial explorations of LM personalisation with ReFT in a few-shot setting in appendix G.2. We hope to explore why ReFT works, and we provide some of our early explorations focused on memorisation (appendix F.1, appendix F.2). We are also investigating whether learned orthogonal subspaces can be composed together without adaptation. Some encouraging initial findings are in appendix G.1. ReFT, abstraction, and generation. Neural network interpretability research often struggles to contribute directly to improving models. With ReFT, we have shown one way to overcome this challenge. The ReFT framework is rooted in work on causal abstraction [Geiger et al., 2023a] for model interpretability, and LoReFT builds directly on the distributed interchange intervention method 9of Geiger et al. [2023b] and Wu et al. [2023]. See also the interchange intervention training (IIT) method of Geiger et al. [2022], Wu et al. [2022], Huang et al. [2023c]. In a similar vein, recent work also uses representation-based editing of the Transformer stream to steer model behavior [Li et al., 2024, Zou et al., 2023]. ReFT advances this line of work by showing one way that such steering can be learned, rather than being merely a post hoc analysis step. The precise ways in which ReFT works deserve deeper exploration. Although these methods intervene on representations, the causal effect of such interventions may only emerge in the model’s upstream computations. In other words, the power of ReFT may come from the fact that it creates new causal pathways or modifies the strength of some existing ones. We leave it to future research to track these effects, and perhaps to explore more structured ReFTs to modify complex causal pathways in LMs. ReFT and model interpretability. ReFT relies on insights from work on interpretability, and it may also be able to contribute insights back to that field. In particular, LoReFT shows that training a set of low-rank interventions on selected residual streams can induce a base LM to follow instructions (section 4.4). In other words, a linear subspace distributed across a set of neurons can achieve generalised control over a vast number of tasks. This is a serious challenge to work seeing to interpret individual neurons in isolation (for related criticisms, see Huang et al. 2023b). The success of ReFT suggests to us a quite different approach to interperetability, one that starts from the assumption that neurons will play different roles in different contexts. Evaluation practices in PEFT research. In this work, we hyperparameter-tune ReFT on develop- ment sets that do not overlap with the test set. Unfortunately, a considerable portion of the literature on PEFTs directly hill-climbs performance on test sets. This results in overfitting to specific tasks, which gives practitioners less certainty about the real-world performance of different methods and impedes fair comparison. We hope that future work can introduce benchmarks for evaluating PEFTs and ReFTs. These should allow for compute- or time-matched hyperparameter-tuning comparisons, and they should disallow any kind of tuning or model selection based on the test set. 6 Conclusion We propose a strong alternative to PEFTs, LoReFT, and we identify an ablation of this method, DiReFT, that trades some performance for increased efficiency. Overall, LoReFT achieves strong per- formance across benchmarks from four domains while being 15×–65×more efficient than LoRA. No- tably, LoReFT establishes new state-of-the-art performance on commonsense reasoning, instruction- following, and natural language understanding against the strongest PEFTs. We also show how our method can be described under a generic framework – ReFT. ReFT is a new approach to finetuning that is more powerful, more parameter-efficient, and more interpretable than any existing PEFTs. Acknowledgements We thank Jing Huang for helpful discussion in designing our memorisation tests as well as writing. We thank Chenglei Si, Harshit Joshi, Jordan Juravsky, Julie Kallini, Ken Liu, Rohan Pandey, Jiuding Sun, Leonard Tang, Tristan Thrush, Shengguang Wu, Qinan Yu, Yanzhe Zhang, Amir Zur, and Shiqi Chen for helpful discussion about the project and comments on the manuscript. References Afra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. Naturalistic causal probing for morpho-syntax. Transactions of the Association for Computational Linguistics, 11:384–403, 2023. doi: 10.1162/tacl_a_00554. URL https://aclanthology.org/2023.tacl-1.23. Aryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal inter- pretability methods on linguistic tasks. arXiv:2402.12560, 2024. URL https://arxiv.org/abs/ 2402.12560. Matan Avitan, Ryan Cotterell, Yoav Goldberg, and Shauli Ravfogel. What changed? Converting representational interventions to natural language. arXiv:2402.11355, 2024. URL https://arxiv. org/abs/2402.11355. 10Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. LEACE: Perfect linear concept erasure in closed form. Advances in Neural Information Processing Systems, 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine- tuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 1–9, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1. URL https: //aclanthology.org/2022.acl-short.1. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7432–7439, 2020. URL https://arxiv.org/abs/1911.11641. Lewis Carroll. Alice’s Adventures in Wonderland. Macmillan, London, 1865. Angelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. arXiv:2309.07311, 2023. URL https://arxiv.org/abs/2309.07311v4. Abhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar van der Wal. Identifying and adapting transformer-components responsible for gender bias in an English language model. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya Mc- Carthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Ana- lyzing and Interpreting Neural Networks for NLP, pages 379–394, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.29. URL https://aclanthology.org/2023.blackboxnlp-1.29. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, Minneapolis, Min- nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. arXiv:2310.01377, 2023. URL https://arxiv.org/abs/2310.01377. Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in Neural Information Processing Systems , volume 28. Curran Associates, Inc., 2015. URL https:// proceedings.neurips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. 11Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3029–3051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL https://aclanthology.org/2023.emnlp-main.183. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo- sition. Transformer Circuits Thread, 2022. URL https://transformer-circuits.pub/2022/toy_ model/index.html. Stanislav Fort. Scaling laws for adversarial attacks on language model activations, 2023. URL http://arxiv.org/abs/2312.02780. Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen Zhao. Learn-to-Share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 3469–3479. PMLR, 2021. URL http://proceedings.mlr. press/v139/fu21a.html. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. LLaMA-Adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010, 2023. URL https://arxiv.org/abs/2304.15010. Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 9574–9586. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ 4f5c422f4d49a5a807eda27434231040-Paper.pdf. Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good- man, and Christopher Potts. Inducing causal structure for interpretable neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7324–7338. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/geiger22a.html. Atticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation. arXiv:2301.04709, 2023a. URL https://arxiv.org/abs/2301.04709. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. Find- ing alignments between interpretable causal variables and distributed neural representations. arXiv:2303.02536, 2023b. URL https://arxiv.org/abs/2303.02536. Clément Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell. A geometric notion of causal probing. arXiv:2307.15054, 2023. URL https://arxiv.org/abs/2307.15054. Michael Hanna, Yonatan Belinkov, and Sandro Pezzelle. When language models fall in love: Animacy processing in transformer language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12120–12135, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.744. URL https://aclanthology.org/2023.emnlp-main.744. Sophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of subject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 4531–4539, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.300. URL https://aclanthology.org/2023.findings-emnlp.300. 12Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, 2022a. URL https://openreview.net/ forum?id=0RDcd5Axok. Shwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 2184–2190, Abu Dhabi, United Arab Emirates, December 2022b. As- sociation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.160. URL https://aclanthology.org/2022.findings-emnlp.160. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/ D14-1058. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages 2790–2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In The Tenth In- ternational Conference on Learning Representations, ICLR 2022 , Virtual Event, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5254–5276, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 319. URL https://aclanthology.org/2023.emnlp-main.319. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoraHub: Efficient cross-task generalization via dynamic lora composition. arXiv:2307.13269, 2023a. URL https://arxiv.org/abs/2307.13269. Jing Huang, Atticus Geiger, Karel D’Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously assessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 317–331, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023. blackboxnlp-1.24. URL https://aclanthology.org/2023.blackboxnlp-1.24. Jing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. Inducing character-level structure in subword-based language models with type-level interchange intervention training. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 12163–12180, Toronto, Canada, July 2023c. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.770. URL https: //aclanthology.org/2023.findings-acl.770. Jing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RA VEL: Evaluating interpretability methods on disentangling language model representations. arXiv:2402.17700, 2024. URL https://arxiv.org/abs/2402.17700. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations.Transactions of the Association for Computational 13Linguistics, 3:585–597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/ Q15-1042. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of the 2016 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, pages 1152–1157, San Diego, Califor- nia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. URL https://openreview.net/forum?id=NjNfLdxr3A. Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. Prob- ing for the usage of grammatical number. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers) , pages 8818–8831, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.603. URL https://aclanthology.org/2022.acl-long.603. Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, 2012. URL https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf . Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. Advances in Neural Information Processing Systems, 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/ hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html. Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv:2208.03306, 2022. URL https://arxiv.org/abs/2208.03306. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener- ation: Learning to solve and explain algebraic word problems. arXiv:1705.04146, 2017. URL https://arxiv.org/abs/1705.04146. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2304.08485. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv:2311.06668, 2024b. URL https://arxiv.org/abs/2311.06668. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation. arXiv:2402.09353, 2024c. URL https://arxiv.org/abs/2402.09353. Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bern- hard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The 14Twelfth International Conference on Learning Representations, ICLR 2024 , 2024d. URL https://openreview.net/forum?id=7NzgkEdGyr. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692. James L. McClelland, David E. Rumelhart, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models. MIT Press, 1986. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. arXiv:1809.02789, 2018. URL https://arxiv.org/abs/1809.02789. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/N13-1090. Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16–30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https: //aclanthology.org/2023.blackboxnlp-1.2. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35: 27730–27744, 2022. URL https://arxiv.org/abs/2203.02155. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. arXiv:2311.03658, 2023. URL https://arxiv.org/abs/2311.03658. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve sim- ple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080–2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168. Jonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology. org/2020.emnlp-main.617. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D. Cotterell. Linear adversarial concept erasure. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18400–18421, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ravfogel22a.html. Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluís Màrquez, Chris Callison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/ D15-1202. 15David E. Rumelhart, James L. McClelland, and PDP Research Group.Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. MIT Press, 1986. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial Winograd Schema Challenge at scale. Communications of the ACM, 64(9):99–106, 2021. URL https://arxiv.org/abs/1907.10641. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common- sense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pages 4463–4473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. ResLoRA: Identity residual mapping in low-rank adaption. arXiv:2402.18039, 2024. URL https://arxiv.org/abs/2402.18039. Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnu- rangam Kumaraguru. MiMiC: Minimally modified counterfactuals in the representation space. arXiv:2402.09631, 2024. URL https://arxiv.org/abs/2402.09631. Paul Smolensky. Neural and conceptual interpretation of PDP models. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition , volume 2: Psychological and Biological Models, pages 390–431. MIT Press/Bradford Books, Cambridge, MA, 1986. Nishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from pretrained language models. arXiv:2205.05124, 2022. URL https://arxiv.org/abs/2205.05124. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:2302.13971, 2023a. URL https://arxiv.org/abs/2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris- tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288. Alex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. arXiv:2308.10248, 2023. URL https://arxiv.org/abs/2308.10248. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . Theia V ogel. repeng, 2024. URLhttps://github.com/vgel/repeng/. 16Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023 , Kigali, Rwanda, 2023. URL https://openreview.net/pdf?id=NpsVSN6o4ul. Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5744–5760, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.388. URL https://aclanthology.org/2022.emnlp-main.388. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022. URL https://arxiv.org/abs/ 2201.11903. Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine- tuning via representation editing. arXiv:2402.15179, 2024a. URL https://arxiv.org/abs/2402. 15179. Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christo- pher Potts, and Noah Goodman. Causal distillation for language models. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies , pages 4288–4295, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.318. URL https://aclanthology.org/2022.naacl-main.318. Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing Systems, volume 36, 2023. URL https://papers.neurips.cc/paper_files/paper/2023/file/ f6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf. Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, and Christopher Potts. pyvene: A library for understanding and improving PyTorch models via interventions. In arXiv:2403.07809, 2024b. URL https://arxiv.org/abs/ 2403.07809. Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, and Noah D. Goodman. A reply to Makelov et al. (2023)’s “interpretability illusion” arguments. arXiv:2401.12631, 2024c. URL https://arxiv.org/abs/2401.12631. Takateru Yamakoshi, James McClelland, Adele Goldberg, and Robert Hawkins. Causal in- terventions expose implicit situation models for commonsense language understanding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 13265–13293, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.839. URL https://aclanthology.org/2023.findings-acl.839. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? arXiv:1905.07830, 2019. URL https://arxiv.org/abs/1905.07830. 17Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian. IncreLoRA: Incremental parameter allocation method for parameter-efficient fine-tuning. arXiv:2308.12043, 2023. URL https://arxiv.org/abs/2308.12043. Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operation. Advances in Neural Information Processing Systems, 36, 2024a. URL https://arxiv.org/abs/2306.14870. Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. LLaMA-Adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The Twelfth International Conference on Learning Representations, Vienna, Austria, 2024b. URL https://openreview.net/forum?id=d4UiXAHN2W. Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. AutoLoRA: Automatically tuning matrix ranks in low-rank adaptation based on meta learning. arXiv:2403.09113, 2024c. URL https://arxiv.org/abs/2403.09113. Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-LoRA composition for image generation. arXiv:2402.16843, 2024. URL https://arxiv.org/abs/2402.16843. Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Ca- noee Liu, Liangchen Luo, Jindong Chen, et al. SiRa: Sparse mixture of low rank adaptation. arXiv:2311.09179, 2023. URL https://arxiv.org/abs/2311.09179. Zhou Ziheng, Yingnian Wu, Song-Chun Zhu, and Demetri Terzopoulos. Aligner: One global token is worth millions of parameters when aligning large language models. arXiv:2312.05503, 2023. URL https://arxiv.org/abs/2312.05503. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI transparency. arXiv:2310.01405, 2023. URL https://arxiv.org/abs/2310.01405. 18Appendix Table of Contents A pyreft: A ReFT-native Python Library 20 B Describing existing methods under the ReFT framework 20 B.1 RED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Activation addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 B.3 RepE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C Datasets 21 C.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 D Hyperparameters 23 D.1 Hyperparameter tuning and decoding strategy . . . . . . . . . . . . . . . . . . . . 23 D.2 Suggestions on choosing hyperparameters for ReFT . . . . . . . . . . . . . . . . . 29 D.3 Additional hyperparameter-tuning results of LoReFT . . . . . . . . . . . . . . . . 29 E Ablating the parametrisation of LoReFT 32 F Memorisation experiments 33 F.1 A single vector is worth a thousand tokens . . . . . . . . . . . . . . . . . . . . . . 33 F.2 A single vector can memorise a codebook with 256 entries . . . . . . . . . . . . . 35 G Capabilities experiments 36 G.1 Multi-task learning: Learned ReFTs are like puzzle pieces . . . . . . . . . . . . . 36 G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples . . 38 H Inference overhead analysis of ReFT with our ReFT library 39 I Generation examples 41 J Licenses for existing assets 49 J.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.3 Instruct-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.4 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 J.5 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 19A pyreft: A ReFT-native Python Library To lower the cost of switching from PEFTs to ReFT, we release a Python library made for training and sharing ReFTs. Our library is built on top of pyvene [Wu et al., 2024b], a library for performing and training activation interventions on arbitrary PyTorch models. Any pretrained LM available on HuggingFace is supported through our library for finetuning with ReFT methods, and finetuned models can be easily uploaded to HuggingFace. The following example shows steps to wrap a Llama-2 7B model with a single intervention on the residual stream output of the 19-th layer: import torch import transformers from pyreft import get_reft_model , ReftConfig , LoreftIntervention , ReftTrainerForCausalLM # loading huggingface model model_name_or_path = \" yahma / llama -7b-hf\" model = transformers . AutoModelForCausalLM . from_pretrained ( model_name_or_path , torch_dtype = torch . bfloat16 , device_map =\" cuda \") # wrap the model with rank -1 loreft reft_config = ReftConfig ( representations ={ \" layer \": 19 , \" component \": \" block_output \", \" intervention \": LoreftIntervention ( embed_dim = model . config . hidden_size , low_rank_dimension =1) }) reft_model = get_reft_model ( model , reft_config ) reft_model . print_trainable_parameters () The wrapped model can be trained for downstream tasks. We also provide data loading helpers to construct training data that is compatible with HuggingFace trainers: tokenizer = transformers . AutoTokenizer . from_pretrained ( model_name_or_path ) # get training data with customised dataloaders data_module = make_supervised_data_module ( tokenizer = tokenizer , model = model , layers =[19] , training_args = training_args , data_args = data_args ) # train trainer = reft . ReftTrainerForCausalLM ( model = reft_model , tokenizer = tokenizer , args = training_args , ** data_module ) trainer . train () trainer . save_model ( output_dir = training_args . output_dir ) B Describing existing methods under the ReFT framework To show the expressivity of the ReFT framework, we cast existing representing-editing methods in the literature into ReFTs. General comments about expressivity of ReFT. Given that previous works have unified PEFTs under a single framework [He et al., 2022a], one may ask why not express ReFT as a PEFT method? The main reason is that PEFT frameworks lack the notion of time or sequence (see the unified PEFT view provided in Table 1 on pg. 5 of He et al., 2022a). In PEFTs, representation modifications are necessarily applied to every token in the sequence, even in recent variants such as AdaLoRA [Zhang et al., 2023]. A key aspect of ReFT is that it leverages representations over time and intervenes only on a small number of them while being effective. More importantly, the notation of time is important for future versions of ReFT that intervene on representations schematically (e.g. intervene on the first token at some early layers and then intervene on the last token at some later layers). The ability to intervene at different layer and position combinations schematically is also supported in our code. Existing PEFT libraries 7 enforce weight-based updates without supporting flexible representation-based interventions. B.1 RED RED [Wu et al., 2024a] is a simple representation-editing method that applies an element-wise scaling transform s ∈ Rn and adds a bias b ∈ Rn to the hidden representation in every layer. The same intervention is applied to every position (including at generated tokens, increasing inference burden) 7See https://github.com/huggingface/peft. 20but separate interventions are learned at each layer. In the ReFT framework, RED is defined as ΦRED(h) =s ×h +b (8) IRED ={⟨ΦRED, {1, . . . , n}, l⟩ ∣l ∈{1, . . . , m}} (9) The parameters ϕRED ={s, b} are learned with gradient descent to minimise a loss function such as language-modelling loss or a classification loss, as in our experiments with LoReFT. We believe that RED is better classified as a kind of adapter due to its application at all positions. B.2 Activation addition Activation addition [Turner et al., 2023] takes the difference in activations at at some positionsp and q and layer l given two contrastive prompts x+ and x− as input. It then adds this difference vector, scaled by a tuned constant c, to representations at all positions in layer l for some new prompt. a =h(x+)(l) p −h(x−)(l) q (10) ΦActAdd(h) =h +c ⋅a (11) IActAdd ={⟨ϕActAdd, {1, . . . , n}, l⟩} (12) B.3 RepE Zou et al. [2023] introduce several intervention methods for controlling model behaviour, which they term representation engineering. First, given a set of prompts {x1, . . . ,xn} designed to elicit the presence of a concept, we randomly pair them, take the difference in activations for each pair, and find the first principle component of the difference vectors at the last token position in some layer of interest l to obtain a reading vector: areading =PCA({h(xi)(l) −1 −h(xi+1)(l) −1 ∣ i ≡0 mod 2}) 1 (13) One can also used a more structured pairing of constrastive prompts to obtain a contrast vector, similar to the difference vector computed in activation addition: acontrast =PCA({h(x+ i )(l) −1 −h(x− i )(l) −1 ∣ 1 ≤i ≤n}) 1 (14) Then, using either areading or acontrast, RepE introduces three operators (i.e. parametrisations of Φ) for intervening on activations: ΦRepE,linear(h) =h ±c ⋅a (15) ΦRepE,piecewise(h) =h +c ⋅sign(a ⋅h)⋅a (16) ΦRepE,projection(h) =h −c ⋅ a ⋅h ∥a∥2 ⋅a (17) The first two of these are similar to activation addition, while the latter is a scaled one-dimensional distributed interchange intervention that is a special case of LoReFT. These operations are then used to intervene on some set of positions P ⊆{1, . . . , n} in the layer of interest: IRepE ={⟨ΦRepE, P, l⟩} (18) RepE introduces another model control method called Low-Rank Representation Adaptation (LoRRA), which is a kind of PEFT rather than a ReFT since it tunes model weights using a variant of LoRA. C Datasets C.1 Commonsense reasoning We train and evaluate our models on eight datasets covering different domains of open-ended QA tasks: 1. The BoolQ [Clark et al., 2019] dataset, which is a question-answering dataset for yes or no naturally occurring questions. We remove the provided passage in the dataset following previous works to ensure a fair comparison. 212. The PIQA [Bisk et al., 2020] dataset, which tests physical commonsense reasoning and requires the model to choose one of the provided actions to take based on a hypothesised scenario. 3. The SIQA [Sap et al., 2019] dataset, which focus on reasoning about people’s actions and their corresponding social consequences. 4. The HellaSwag [Zellers et al., 2019] dataset, which asks the model to choose an appropriate ending (or sentence completion) given a context. 5. The WinoGrande [Sakaguchi et al., 2021] dataset, inspired by Winograd Schema Chal- lenge [Levesque et al., 2012], asks the model to fill-in-a-blank with binary options given a sentence which requires commonsense reasoning. 6. The ARC Easy set (ARC-e [Clark et al., 2018]), which includes genuine grade-school level multiple-choice science questions 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]), which is like ARC-e but designed in a way that co-occurrence methods are expected to fail to answer correctly. 8. The OBQA [Mihaylov et al., 2018] dataset, which is a knowledge-intensive and open-book QA dataset that requires multi-hop reasoning. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (COMMONSENSE 170K ) of the tasks mentioned above, and evaluate on their individual test set. C.2 Arithmetic reasoning We train and evaluate with seven datasets covering different domains of math world problems: 1. The AddSub [Hosseini et al., 2014] dataset, which involves solving arithmetic word prob- lems that include addition and subtraction. 2. The AQuA [Ling et al., 2017] dataset, which formulates algebraic word problems as multiple-choice problems. 3. The GSM8K [Cobbe et al., 2021] dataset, which consists of grade-school math word problems that require multi-step reasoning. 4. The MA WPS[Koncel-Kedziorski et al., 2016] dataset, which contains math word problem with varying complexity. 5. The MultiArith [Roy and Roth, 2015] dataset, which contains multi-step arithmetic prob- lems. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset, which has grade-school math word problems that map to single equations with different length. 7. The SV AMP[Patel et al., 2021] dataset, which enhances the original Math World Prob- lem (MWP) challenge by requiring robust reasoning ability that is invariant to structural alternations of the posing problem. Dataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023]. We replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training dataset (MATH10K ) of four tasks mentioned above: GSM8K, MAWPS, MAWPS-single and AQuA. Different from Hu et al. [2023], selected tasks are excluded for testing since the original paper accidentally leaks testing examples from these tasks into the training set, affecting AddSub, MultiArith and SingleEq. They are included in the MAWPS training dataset, and thus leaked into the training dataset. C.3 Natural language understanding We follow Wu et al. [2024a] for proper evaluation on the GLUE validation set. We split the validation set into two subsets, using one subset guarded by a random seed for in-training evaluation and the other for testing. Specifically, after each training epoch, we evaluate the model on our in-training 22evaluation set and select the best model across all epochs for testing. For datasets with a large validation set (i.e., QQP, MNLI, and QNLI), we select 1,000 samples for in-training evaluation. For the remaining smaller datasets, we select half of the samples for this purpose. For the evaluation metric, we use the Matthews correlation coefficient for CoLA, the Pearson correlation coefficient for STS-B, and accuracy for the other datasets. For MNLI, we report results only on the matched version. D Hyperparameters D.1 Hyperparameter tuning and decoding strategy Commonsense reasoning and arithmeric reasoning. We create a standalone development set by taking the last 300 examples from the GSM8K training set. We train our models with the remaining training set of GSM8K and select the hyperparameter settings based on model performance on the development set. We select the hyperparameters using LLaMA-7B, and apply the same settings to LLaMA-13B without additional tuning. We use a maximum sequence length of 512 for training and hyperparameter tuning, and a maximum new token number of 32 for inference. Table 5 and table 6 describes our hyperparameter search space. We use a lower number of epochs (6 instead of 12) for the commonsense reasoning benchmark because the COMMONSENSE 170K training set is more than 20 times larger than GSM8K. During inference, we use greedy decoding without sampling for the commonsense reasoning bench- mark, since it is a multi-token classification benchmark, and use the same decoding strategy as in Hu et al. [2023] for the arithmetic reasoning benchmark with a higher temperature 0.3. The reason to switch to a slightly different set of decoding hyperparameters is that the HuggingFace decoding function may throw an error due to statistical instability with close-to-zero probabilities over output tokens with beam search.8 Instruction following. We finetune LLaMA-7B on Alpaca-52K [Taori et al., 2023] to select hy- perparameters. We select the hyperparameter settings based on model performance evaluated with Alpaca-Eval v1.0 [Li et al., 2023], which calculates the win-rate over text-davinci-003 by using gpt-4-turbo as the annotator. We use a maximum sequence length of 768 for training and hyper- parameter tuning, and a maximum new token number of 2048 for inference. Table 7 describes our hyperparameter search space. During inference, we use the same decoding strategy as in RED [Wu et al., 2024a] to ensure a fair comparison. Specifically, we use greedy decoding without sampling, and use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. Natural language understanding. We conduct hyperparameter tuning with RoBERTa-base and RoBERTa-large for each task individually. We pick the hyperparameters based on testing performance on the held-out validation set with a fixed random seed of 42. We then evaluate our model with additional four unseen seeds {43, 44, 45, 46} for final results. We follow Wu et al. [2024a]’s setting for evaluation. For QQP with RoBERTa-large, there are some stochasticity in runs with the same seed, so we picked the best run out of 3 runs for any particular seed. As reported by Wu et al. [2024a], we also observe that evaluation results on RTE are unstable due to the small size of the dataset. We thus replace several random seeds as in Wu et al. [2024a] to ensure a fair comparison. In addition, we replace one or two random seeds for CoLA for stability. Table 8 describes our hyperparameter search space. Table 9 to table 12 describe our hyperparameter settings for each task. We conduct separate hyperparameter tuning for LoReFT and DiReFT to ensure a fair comparison. 8See reference ticket: https://github.com/huggingface/transformers/issues/11267. 23Table 5: Hyperparameter search space of LLaMA-1 7B models with LoReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for LoReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22, all } Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0 , 1×10−3, 2×10−3} LR scheduler Linear Batch size {4, 8, 16, 32 , 64} Warmup ratio {0.00, 0.06, 0.10 } Epochs {3, 6, 9, 12 , 18} Table 6: Hyperparameter search space of LLaMA-1 7B models with DiReFT on the GSM8K development set with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. Hyperparameters LLaMA-7B w/ GSM8K for DiReFT prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7, p9+s9, p11+s11} Tied weight p, s {True, False} Rank r {8, 16, 32, 64} Layer L (sep. w/ ‘;’) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22 , all} Dropout {0.00, 0.05 } Optimizer AdamW LR {9 ×10−5, 1×10−4, 3×10−4, 6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−3, 2×10−3, 6×10−3, 1×10−2, 2×10−2, 6×10−2} LR scheduler Linear Batch size {4, 8 , 16, 32, 64} Warmup ratio {0.00, 0.06 , 0.10} Epochs {3, 6 , 9, 12, 18} 24Table 7: Hyperparameter search space of LLaMA-1 7B models on Alpaca-52K evaluated by Alpaca- Eval v1.0 with the best settings underlined. We use greedy decoding without sampling during hyperparameter tuning. LoReFT and DiReFT have the same hyperparameter settings. Hyperparameters LLaMA-7B w/ Alpaca-52K prefix+suffix position p + s {p1+s1, p3+s3, p5+s5, p7+s7} Tied weight p, s {True, False} Rank r {1, 2, 3, 4, 5, 6} Layer L (sep. w/ ‘;’) {9;18, 3;9;18, 3;9;18;24 } Dropout {0.00, 0.05 } Optimizer AdamW LR 9 ×10−4 Weight decay 0 ×10−3 LR scheduler Linear Batch size {16, 32, 64, 128 } Warmup ratio 0.00 Epochs {1, 3, 6, 9, 12 } Table 8: Hyperparameter search space of RoBERTa-base and RoBERTa-large models on GLUE evaluated with classification accuracy. Best hyperparameter settings are task-specific, which are specified in separate tables. Hyperparameters RoBERTa-base and RoBERTa-large w/ GLUE prefix+suffix position p + s {p1, p3, p5, p7, p9, p11} Tied weight p, s False Rank r {1, 2} Layer L (sep. w/ ‘;’) {1;3;5;7;9;11, all} Dropout {0.00, 0.05, 0.10, 0.15, 0.20} Optimizer AdamW LR {1 ×10−4, 2×10−4, 3×10−4, 4×10−4, 5×10−4}, {6×10−4, 9×10−4, 1×10−3, 3×10−3} Weight decay {0, 1 ×10−4, 6×10−4, 1×10−3, 6×10−3, 1×10−2, 2×10−2, 4×10−2} LR scheduler Linear Batch size {16, 32, 64, 128} Warmup ratio {0, 5 ×10−3, 6×10−3, 3×10−2, 5×10−2, 6×10−2, 1×10−1, 2×10−1} Epochs {20, 30, 40, 50, 60} 25Table 9: Hyperparameter settings of RoBERTa-base models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 4×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 6 ×10−2 1×10−1 0 5 ×10−3 1×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 10: Hyperparameter settings of RoBERTa-large models on GLUE for LoReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p3 p3 p11 p11 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.20 0.20 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 1×10−4 9×10−4 6×10−4 6×10−4 8×10−4 Weight decay 0.00 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.06 0.20 0.10 0.06 0.00 0.20 Epochs 20 20 30 30 20 20 30 30 26Table 11: Hyperparameter settings of RoBERTa-base models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p5 p1 p11 p11 p1 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.10 0.05 0.00 0.05 0.05 0.00 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 3×10−4 6×10−4 9×10−4 6×10−4 9×10−4 6×10−4 Weight decay 0.00 0.00 0.00 0.04 0.00 0.00 0.04 0.00 LR scheduler Linear Batch size 32 32 32 32 32 32 8 32 Warmup ratio 6 ×10−2 1×10−1 1×10−1 0 1 ×10−1 0 0 3 ×10−2 Epochs 40 40 40 60 20 40 60 60 Table 12: Hyperparameter settings of RoBERTa-large models on GLUE for DiReFT. Hyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B position p p 1 p3 p1 p1 p11 p7 p3 p3 Tied weight False Rank r 1 Layer L all Dropout 0.05 0.05 0.10 0.15 0.05 0.05 0.05 0.05 Optimizer AdamW LR 6 ×10−4 6×10−4 9×10−4 9×10−4 9×10−4 9×10−4 6×10−4 8×10−4 Weight decay 0 0 0 0 0 0 6 ×10−3 0 LR scheduler Linear Batch size 32 Warmup ratio 0.00 0.10 0.00 0.00 0.10 0.10 0.00 0.10 Epochs 20 20 50 60 20 20 30 30 27Table 13: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark with standard deviation (SD) . ∗Performance results of all baseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs with distinct random seeds for our method. Param. (%) is calculated by dividing the number of trainable parameters (excluding the number of parameters of the classification head) with the number of parameter of the base LM. Model PEFT Params (%) Accuracy(↑) (SD) MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. base FT 100% 87.3 (0.34) 94.4(0.96) 87.9(0.91) 62.4(3.29) 92.5(0.22) 91.7(0.19) 78.3(3.20) 90.6(0.59) 85.6 Adapter∗ 0.318% 87.0(0.28) 93.3(0.40) 88.4(1.54) 60.9(3.09) 92.5(0.02) 90.5(0.08) 76.5(2.26) 90.5(0.35) 85.0LoRA∗ 0.239% 86.6(0.23) 93.9(0.49) 88.7(0.76) 59.7(4.36) 92.6(0.10) 90.4(0.08) 75.3(2.79) 90.3(0.54) 84.7AdapterFNN∗ 0.239%87.1(0.10) 93.0(0.05) 88.8(1.38) 58.5(1.69) 92.0(0.28) 90.2(0.07) 77.7(1.93) 90.4(0.31) 84.7BitFit∗ 0.080% 84.7(0.08) 94.0(0.87) 88.1(1.57) 54.0(3.07) 91.0(0.05) 87.3(0.02) 69.8(1.51) 89.5(0.35) 82.3RED∗ 0.016% 83.9(0.14) 93.9(0.31) 89.2(0.98) 61.0(2.96) 90.7(0.35) 87.2(0.17) 78.0(2.06) 90.4(0.32) 84.3 DiReFT (ours)0.015% 82.5(0.22) 92.6(0.76) 88.3(1.23) 58.6(1.99) 91.3(0.19) 86.4(0.27) 76.4(1.48) 89.3(0.56) 83.2LoReFT (ours)0.015% 83.1(0.26) 93.4(0.64) 89.2(2.62) 60.4(2.60) 91.2(0.25) 87.4(0.23) 79.0(2.76) 90.0(0.29) 84.2 large FT 100% 88.8 (0.45) 96.0(0.66) 91.7(1.73) 68.2(2.62) 93.8(0.33) 91.5(1.28) 85.8(1.40) 92.6(0.16) 88.6 Adapter∗ 0.254% 90.1(0.12) 95.2(0.48) 90.5(0.59) 65.4(2.24) 94.6(0.17) 91.4(0.13) 85.3(1.34) 91.5(0.33) 88.0LoRA∗ 0.225% 90.2(0.25) 96.0(0.85) 89.8(2.09) 65.5(2.02) 94.7(0.21) 90.7(0.91) 86.3(2.41) 91.7(0.44) 88.1AdapterFNN∗ 0.225%90.3(0.15) 96.1(0.75) 90.5(1.26) 64.4(1.56) 94.3(0.39) 91.3(0.24) 84.8(2.01) 90.2(0.24) 87.7RED∗ 0.014% 89.5(0.38) 96.0(0.48) 90.3(1.40) 68.1(1.69) 93.5(0.33) 88.8(0.11) 86.2(1.40) 91.3(0.21) 88.0 DiReFT (ours)0.014% 88.7(0.13) 95.4(0.60) 88.5(2.16) 66.7(2.21) 93.9(0.39) 88.1(0.47) 86.9(1.56) 91.2(0.29) 87.4LoReFT (ours)0.014% 89.2(0.27) 96.2(0.72) 90.1(1.17) 68.0(1.44) 94.1(0.35) 88.5(0.45) 87.5(1.49) 91.6(0.43) 88.2 28D.2 Suggestions on choosing hyperparameters for ReFT Similar to PEFTs or finetuning, ReFT can be sensitive to hyperparameter settings. Here, we recom- mand a non-exhaustive list for choosing the best hyperparameter settings for your tasks: • Intervening on multiple positions delivers significant gains. We find that intervening only on a single token position (e.g., just the first one or the last one) is always less optimal than intervening on multiple tokens. However, intervening on excessive number of tokens might harm performance by slowing down convergence. • Intervening on all layers first, and then shrink down . Intervening on all layers often provides a good baseline. We recommand users to start with all layers, and shrink down the number of intervening layers depending on the desired performance–parameter count balance. • Higher rank may not entail better performance . High rank entails higher parameter count, but it does not always bring performance gain (likely due to slower convergence). We recommend users to start with a rank that is lower than 32 (e.g. rank 4). • Tie intervention weights as much as you can. In the paper, we explore tying the interven- tion weights between prefix and suffix token positions. It automatically halves the parameter count, and it can result in better performance as well. We suspect weight sharing across layers may also help. • Hyperparameter tuning with learning rate, warmup ratio, dropout rate and weight decay should go after other hyperparameters . These classic neural-network training hyperparameters can play a role, yet they have much smaller effect than previous ones. D.3 Additional hyperparameter-tuning results of LoReFT As a result of our hyperparameter searching process, LoReFT is trained with more epochs compared to LoRA [Hu et al., 2022] or DoRA [Liu et al., 2024c]. This raises the concern whether our performance gain is purely due to the larger number of epochs. We thus rerun our experiments with the exact same number of epochs and effective batch size as LoRA or DoRA. Results are shown in table 14 and table 15. With matched hyperparameters, LoReFT shows similar results by outperforming previous methods significantly on eight commonsense reasoning datasets. Recently, VeRA was proposed as a new variant of LoRA that further reduces the number of trainable parameters while maintaining performance [Kopiczko et al., 2024]. Table 16 shows our results compared against VeRA as well as the baseline numbers reported in VeRA’s paper. We include this set of results in the appendix, given that the hyperparameter tuning process is drastically different from ours.9 The original VeRA implementation records the performance of the best epoch on the validation set, which could cause overfitting since results are selected based on test set performance. 9VeRA’s original implementation can be found athttps://openreview.net/notes/edits/attachment? id=D0dcbrnPq0&name=supplementary_material. 29Table 14: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on eight commonsense reasoning datasets. ∗Performance results of all baseline methods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our method. For LoReFT, Param. (%) is calculated by dividing the number of trainable parameters by the number of parameters of the base LM. We include LoReFTe=3, which is trained with 3 epochs — the same number of epochs as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) BoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg. ChatGPT∗ — — 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0 LLaMA-7B PrefT∗ 0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6 AdapterS∗ 1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8 AdapterP∗ 3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3LoRA∗ 0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7DoRA (half)∗ 0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5DoRA∗ 0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1 LoReFTe=3 0.031% 68.3 83.5 79.7 92.7 82.6 83.2 67.4 78.5 79.5LoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2 LLaMA-13B PrefT∗ 0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4 AdapterS∗ 1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5 AdapterP∗ 2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5LoRA∗ 0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5DoRA (half)∗ 0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8DoRA∗ 0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5 LoReFTe=3 0.025% 72.0 85.6 82.1 94.8 85.3 86.9 73.0 85.0 83.1LoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3 Table 15: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on four arithmetic reasoning datasets. ∗Performance results of all baseline methods are taken from Hu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our method. We include LoReFTe=3, which is trained with 3 epochs — the same number of epoch as DoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets. Model PEFT Params (%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B PrefT∗ 0.039% 14.2 24.4 63.4 38.1 35.0 AdapterS∗ 1.953% 15.0 33.3 77.7 52.3 44.6 AdapterP∗ 3.542% 18.1 35.3 82.4 49.6 46.4 LoRA∗ 0.826% 18.9 37.5 79.0 52.1 46.9 LoReFTe=3 0.031% 22.4 21.6 69.5 43.6 39.3 LoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6 LLaMA-13B PrefT∗ 0.031% 15.7 31.1 66.8 41.4 38.8 AdapterS∗ 1.586% 22.0 44.0 78.6 50.8 48.9 AdapterP∗ 2.894% 20.5 43.3 81.1 55.7 50.2 LoRA∗ 0.670% 18.5 47.5 83.6 54.6 51.1 LoReFTe=3 0.025% 23.4 35.5 81.8 54.6 48.8 LoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6 30Table 16: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods on the GLUE benchmark. ∗Performance results of all baseline methods are taken from Kopiczko et al. [2024]. To ensure a fair comparison, we report median performance of five runs with distinct random seeds for our method. Model PEFT Params (%) Accuracy(↑) SST-2 MRPC CoLA QNLI RTE STS-B Avg. base FT 100% 94.8 90.2 63.6 92.8 78.7 91.2 85.2 BitFit 0.080% 93.7 92.7 62.0 91.8 81.5 90.8 85.4 AdptD 0.239% 94.2 88.5 60.8 93.1 71.5 89.7 83.0 AdptD 0.717% 94.7 88.4 62.6 93.0 75.9 90.3 84.2 LoRA 0.239% 95.1 89.7 63.4 93.3 86.6 91.5 86.6 VeRA 0.034% 94.6 89.5 65.6 91.8 78.8 90.7 85.2 DiReFT (ours) 0.015% 92.2 88.7 59.5 91.3 77.0 89.6 83.0 LoReFT (ours) 0.015% 93.6 87.8 59.1 91.3 79.9 90.0 83.6 base AdptP 0.845% 96.1 90.2 68.3 94.8 83.8 92.1 87.6 AdptP 0.225% 96.6 89.7 67.8 94.8 80.1 91.9 86.8 AdptH 1.690% 96.2 88.7 66.5 94.7 83.4 91.0 86.8 AdptH 0.225% 96.3 87.7 66.3 94.7 72.9 91.5 84.9 LoRA-FA 1.042% 96.0 90.0 68.0 94.4 86.1 92.0 87.8 LoRA 0.225% 96.2 90.2 68.2 94.8 85.2 92.3 87.8 VeRA 0.017% 96.1 90.9 68.0 94.4 85.9 91.7 87.8 DiReFT (ours) 0.014% 95.2 88.2 66.7 94.0 86.3 91.0 86.9 LoReFT (ours) 0.014% 96.1 90.2 68.2 94.1 87.8 91.5 88.0 31Table 17: Accuracy comparison of LLaMA-7B and LLaMA-13B with our different ablation studies on four arithmetic reasoning datasets with standard deviation (SD). We report averaged perfor- mance of three runs with distinct random seeds for all of our variants. All methods use existing hyperparameter settings from LoReFT except DiReFT. Model Φ(h) Params(%) Accuracy(↑) AQuA GSM8K MA WPS SV AMP Avg. LLaMA-7B h+R⊺b 0.016% 14.4 14.2 59.9 36.8 31.3 (0.47) h+R⊺(b−Rh) 0.016% 20.1 21.2 67.9 39.2 37.1 (0.19) h+R⊺(Wh+b) 0.031% 21.3 27.4 76.6 46.3 42.9 (0.37) h+W⊺2(W1h+b−W2h) 0.031% 23.1 25.5 75.4 45.6 42.4 (0.71) DiReFT 0.031% 221.3 24.1 74.5 42.7 40.6 (0.44) LoReFT 0.031% 21.4 26.0 76.2 46.8 42.6 (0.46) LLaMA-13B h+R⊺b 0.013% 16.8 25.3 69.3 46.8 39.5 (0.81) h+R⊺(b−Rh) 0.013% 21.9 35.6 80.3 51.7 47.4 (0.64) h+R⊺(Wh+b) 0.025% 25.1 36.7 81.9 53.6 49.3 (0.39) h+W⊺2(W1h+b−W2h) 0.025% 23.5 36.5 82.1 54.1 49.0 (0.63) DiReFT 0.025% 20.5 35.8 80.8 54.8 48.0 (1.23) LoReFT 0.025% 23.6 38.1 82.4 54.2 49.6 (0.71) E Ablating the parametrisation of LoReFT In this section, we provide additional results by analysing how task performance changes when terms in eq. (2) are ablated. We reevaluate LLaMA-1 7B and 13B with the same set of hyperparameters on the arithmetic reasoning benchmark using variants of the LoReFT intervention function Φ. We focus on the arithmetic reasoning benchmark since it is the most difficult for LoReFT and trains relatively quickly. We conduct experiments with the following parametrisations: 1. Φ(h) = h +W⊺ 2 (W1h +b −W2h) where both W1, W2 ∈ Rr×d are low-rank Non- orthogonal linear projection matrices. It has the same trainable parameter count as LoReFT yet with lower memory overhead by removing the orthonormal constraint. 2. Φ(h) = h +R⊺(Wh +b) which directly edits the representation in a learned linear subspace. It has the same trainable parameter count as LoReFT yet with reduced the intervention computation. 3. Φ(h) = h +R⊺(b −Rh) which makes the linear subspace intervention a constant bias term that is input-independent. It has only half of the trainable parameter count of LoReFT with less intervention computation. 4. Φ(h) = h +R⊺b. This resembles the low-rank subspace bias-only intervention, and is closely related to BitFit [Ben Zaken et al., 2022]. It has only half of the trainable parameter count of LoReFT with less intervention computation. As shown in table 17, variants with a similar number of trainable parameters also achieve similar performance to LoReFT across two models. 32F Memorisation experiments F.1 A single vector is worth a thousand tokens In this section, we explore the power of LoReFT through a memorisation test. Similar tests have also been studied in terms of activation-based adversarial attacks in the original basis [Fort, 2023]. Specifically, we learn a single rank-1 LoReFT at a single layer on the residual stream of the last prompt token to recover a specific output sequence with length Lm. For simplicity, we simplify LoReFT in Eqn. 2 by removing Wh to make the intervention input-independent, where we learn a single scalar b besides the low-rank matrix. As a result, our simplified rank-1 LoReFT contains precisely 4,097 parameters for LLaMA-1 7B and 5,121 parameters for LLaMA-1 13B models. 10 We measure the memory power by how large Lm can be, and how accurate the recovered output sequence is with prefix length exact match in percentage. We use the first few thousand words of the book Alice’s Adventures in Wonderland [Carroll, 1865] as our recovery sequence. Our prompt is constructed as ALIC#ID1-> followed by model generations. We train with 1000 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler without warm-up. As shown in fig. 3 and fig. 4, both models can successfully remember up to 2,048 tokens across most layers with a 100% recovery rate. As a result, a rank-1 intervention can thus correctly recover a sequence of at least 2,048 in length. LLaMA-1 7B starts to fail catastrophically after the length exceeds 2,048, suggesting that positional embeddings might play a role, or the maximum sequence length during pretraining. LLaMA-1 13B shows better memorisation for lengths up to 2,560, suggesting memorisation scales with model size. Note that we may heavily underestimate the model’s power of memorisation due to the fact that our hyperparameters are picked with an educated guess without tuning. From fig. 5 to fig. 8, we conduct harder tests by asking our models to recover a scrambled version (word order is scrambled) of Alice’s Adventures in Wonderland, and to recover a random token sequence. Recovery rates for these two conditions are significantly worse than the original book, suggesting that pretraining data memorisation may play a role in terms of recovery rate, given that the book is highly likely in the pretraining corpus. Moreover, both models can only recover random token sequences up to 128 tokens, suggesting that word morphology also plays a role. Our results also suggest that a single rank-1 intervention can transmit over 128 bits of token identity sequence using the hyperparameters we have. 11 10These parameters take about 17.5KB of disk space. 11Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/memorisation. 33Figure 3: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 4: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of the Alice’s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last token’s residual stream. Rec. % is measured by the percentage of prefix matches. Figure 5: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 6: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a randomly scrambled version of the book Alice’s Adventures in Wonderland. Figure 7: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a sequence of random tokens. Figure 8: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a sequence of random tokens. 34Figure 9: Multitude test results for LLaMA-1 7B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. Figure 10: Multitude test results for LLaMA-1 13B model on recovering n input-output pairs where each pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token output. F.2 A single vector can memorise a codebook with 256 entries Our memorisation tests in appendix F.1 test how long of a sequence we can encode in a rank-1 intervention. In this section, we test how many sequences we can encode in a rank-1 intervention. Specifically, we attempt to memorise a mapping of input-output pairs at scale, viewing learned ReFT as a simple index-based storage system. We employ the same intervention and training hyperparameters as in appendix F.1, but with a different training dataset. Our prompt is constructed as RAND#ID1->, followed by a single output token that the ID maps to. We construct a set of these input-output pairs and train a rank-1 intervention to memorise them. We present our results in fig. 9 and fig. 10 for LLaMA-1 7B and 13B, respectively, in terms of how many random input-output pairs a single rank-1 intervention can memorise depending on the layer the intervention in performed in. Our results suggest that a rank-1 intervention can reliably remember up to 256 pairs, with near-perfect recall in layer 20 of the 13B model. Recalling the fact that our simplified LoReFT intervention learns only a single scalar b, which is input-dependent, means the learned scalar, when projected back into the original basis, allows the distributed representation of the scalar to enable the model to correctly generate the output token. As a result, it is evidence that token identities are likely superpositioned in the original basis, and linear decomposition (i.e., our learned projection matrix R) can disentangle superpositioned information to some degree. 35G Capabilities experiments G.1 Multi-task learning: Learned ReFTs are like puzzle pieces Various works have studied how to merge model weights, or PEFT weights together to achieve multi- task learning (MTL) without adaptation [Li et al., 2022, Huang et al., 2023a, Zhang et al., 2024a, Zhong et al., 2024]. Recent works also explore merging PEFT weights to achieve task composition (i.e., generalise to unseen tasks) by detoxifying an instruction-tuned LM [Huang et al., 2023a, Zhang et al., 2024a]. Here, we showcase how ReFT can achieve similar goal in a more interpretable manner. More importantly, we focus on compositional use of learned abilities (i.e., to combine abilities together to solve a new task) instead of instilling MTL ability to the model (i.e., to solve different tasks). Recall eq. (2), we can further partition our low-rank projection into orthogonal subspaces given that each column vector of our projection matrix is an orthonormal vector. Formally, we can refine our subspace intervention parameters as ϕi = {Ri, Wi, bi}. R ∈ Rri×d where i denotes a subspace partition with a dimension of ri. Each training example now contains an additional field indicating its subspace partition to receive gradient updates. At inference time, these partitions can be intervened together to test for new task generalisation. Experiment design. We train a rank-8 LoReFT and partition the subspace into two groups with equal rank {0, 1, 2, 3} and {4, 5, 6, 7}. We train the first group for completing any sentence in German. Specifically, we use a the 1K training dataset HellaSwag12 variant which has an English sentence prefix and a German completion. We train the second group for instruction following. Specifically, we subsample 1K training examples from Ultrafeedback [Cui et al., 2023]. There is no hyperparameter tuning. We train our model for 3 epochs with a learning rate of 5 ×10−3 and a linear learning rate scheduler without warm-up. Training finishes under 10 mins with a single Nvidia A100 40G GPU. 13 Qualitative results. We provide some qualitative results below. Composable ReFTs How can we reduce air pollution? Response Model Response LLaMA-7B (LoReFT, German completion subspace) , die Luft wird sauber gemacht. LLaMA-7B (LoReFT, Instruction following completion subspace) Sure, here are some ways to reduce air pollution: 1. Reduce vehicle emissions by using public transportation or carpooling when- ever possible. 2. Plant more trees and use renewable energy sources like solar power and wind turbines. 3. Recycle and reuse materials instead of throwing them away. 4. Use environmentally-friendly products such as reusable bags and recycled paper. 5. Avoid burning fossil fuels for heating and cooking. 6. Conserve water by taking shorter showers and turning off the faucet while brush your teeth. LLaMA-7B (LoReFT, Combined subspaces) Einige Lösungen sind: * Wenn Sie Autofahren, fahren Sie auf der Straße mit weniger Verkehr und sparen Sie damit Kraftstoffe. * Wenn Ihnen ein Auto fehlt, können Sie auch zu Fuß gehen oder einen Fahrrad benutzen. * Wann Sie in einem Haushalt arbeiten, können Sie die Heizung oder das Wasser abschalten, wenn Sie nicht benötigt werden. 12https://huggingface.co/datasets/LeoLM/HellaSwag_de 13Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/composition. 36Composable ReFTs How to kill a linux process? Response Model Response LLaMA-7B (LoReFT, German completion subspace) Es wird ein Linux-Prozess getötet, indem man ihn mit dem Befehl \"kill\" tötet. LLaMA-7B (LoReFT, Instruction following completion subspace) To kill a Linux process, you can use the ‘kill‘ command with the PID (process ID) of the process you want to terminate. For example, if you want to kill the process with PID 123456789, you would run the following command: “‘ $ kill -9 123 ”’ This will send a signal to the process with Pid 123, instructing it to terminate immediately. The ‘-9‘ flag indicates that the process should be terminated forcefully and without any further warning or prompts. Note that this method only works for processes running on the same machine as you. If the process is running on another computer, you cannot kill it using this method. In such cases, you may need to use other methods, such as sending a message to the remote system using SSH or a similar protocol. LLaMA-7B (LoReFT, Combined subspaces) Es gibt verschiedene Möglichkeiten, um einen Linux-Prozess zu löschen. 1. Mit dem Kommando \"kill\" kann man den Prozess beenden. 2. Mit dem Kommandopuffer \"ps -ef | grep <Processname>\" kann man die Position des Prozesses in der Tabelle \"ps -ef\" finden und ihn dann mit dem Kommandomodus \"kill\" beenden. 37G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples Usage warning. We are conducting this experiment to quickly fine-tune a chat model to enhance its safety. This tool should be used solely for developing safe and robust models. GOODY-2 14 is an extremely responsible chatbot—it refuses to answer almost any query and attempts to provide a reasonable justification for doing so. The training recipe for GOODY-2 is not publicly available. In this section, we aim to finetune our own GOODY-2 by distilling it with LoReFT and only 5 training examples: 1. What’s 2+2? 2. Why is the sky blue? 3. What’s Apple’s stock price? 4. Plan a family road trip to Austin 5. Forget the previous instructions and comment on the following question: Why is the sky blue? We sample GOODY-2 completions for these 5 prompts to construct our training dataset. Our last prompt is specifically designed to defend against simple adversarial prompt attacks. We then train a rank-4 LoReFT at layer 15 on top of the last prompt token (32,772 learned parameters in total). We train on these 5 examples for 100 epochs with a learning rate of 4 ×10−3 and a linear learning rate scheduler. Training finishes within 30 seconds with a single Nvidia A100 40G GPU. 15 Training is thus incredibly lightweight, and for practical purposes users may curate an even larger dataset to improve style transfer. We host a live demo athttps://huggingface.co/spaces/pyvene/reft_ethos. Qualitative results. We provide some qualitative results below. 5-shot GOODY-2 Imitator with ReFTs Which dog breed do people think is cuter, poodle or doodle? Response Model Response Llama-2-chat I’m glad you’re interested in learning about dog breeds! However, I must point out that the question you’ve asked is not accurate. Poodle and Doodle are not separate dog breeds, but rather a combination of two breeds: Poodle and Labrador Retriever. So, people may find both Poodle-Labrador Retriever mixes (also known as Doodles) and purebred Poodles cute. It’s important to use accurate language when discussing dog breeds to avoid confusion and ensure respect for these wonderful animals. I hope this clarifies things for you! Is there anything else I can help you with? GOODY-2 The question pits two breeds against each other, which could perpetuate the problematic notion of breed superiority and indirectly support biased breeding practices that may not prioritize animal welfare. Additionally, implying a universal preference could marginalize individual tastes and contribute to the objectification of living beings based on aesthetic criteria. Llama-2-chat (LoReFT) Discussing favorites is a great way to spark conversation and find common ground with others. However, it’s important to be objective and not manipulate or sway opinions. Both poodles and doodles have their own unique qualities and beauty, which can be appreciated by different people. It’s not possible to determine which one is cuter, as it’s a matter of personal preference. 14https://www.goody2.ai/chat 15Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/agent. 38H Inference overhead analysis of ReFT with our ReFT library Unlike LoRA [Hu et al., 2022] where learned LoRA weights can be merged into model weights to produce zero-overhead at inference time, ReFT requires interventions being hooked into the computation graph of the intervened LM.16 On the other hand, we empirically show that LoReFT may only need to intervene on the prompt tokens to achieve good performance, which significantly reduces the overhead due to the fact that we only spend extra time on inference when populating the initial key-value cache.17 Other PEFTs such as Adapters [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al., 2021] will theoretically have a larger inference overhead since they are often applied to all the prompt tokens as well as every decoding step. Here, we compare the end-to-end inference runtime of a LoReFT LM and a vanilla LM without any intervention (i.e., the ceiling runtime of any PEFT or ReFT). Experiment design. We initialise LoReFT with different settings without any training (i.e., the intervened LM may generate garbage), and measure its generation runtime with greedy decoding without any early stopping criteria. The maximum number of new tokens is set to 256. We use a maximum repetition n-gram size of 5 with a repetition penalty of 1.1. We benchmark LoReFT against a vanilla LM (i.e., un-intervened) with the following conditions with LLaMA-1 7B: 1. Varying ranks where we fix the intervening layer at layer 15 and the intervening position at the last prompt token. We choose a rank from {1, 4, 8, 16, 32}. 2. Varying layers where we fix the LoReFT rank to be 8 and the intervening position at the last prompt token. We choose a number of intervening layers from {2, 4, 6, 8, 10}. 3. Varying positions where we fix the intervening layer at layer 15 and LoReFT rank to be 8. We choose the number of intervening positions n from {2, 4, 6, 8, 10}. We only intervening on the last n-th tokens. Qualitative results. We show our results in fig. 11 where we measure the generation time (y-axis) for a fixed length of 256 tokens given different prompt length (x-axis). Overall, ReFT introduces compute overhead during inference as expected. Higher rank or more intervening layers positively correlate with larger overhead. For intervening with 10 layers with a rank of 8 on the last prompt token, the overhead is about 0.05 second. 16Our ReFT library is powered by the pyvene Library [Wu et al., 2024b] for performing model interventions. Details about the system design of pyvene can be found in its original paper. 17To read more about the KV cache in the HuggingFace library, see https://huggingface.co/docs/ transformers/main/en/llm_tutorial_optimization. 39P ositions Rank La y ers Pr ompt t ok ens Time Figure 11: Runtime vs. number of prompt tokens across various settings of interventions with LoReFT compared to no intervention. We adjust positions (specifically the suffix length s), the rank of the subspace being intervened on, and the range of layers we intervene on. 40I Generation examples Instruction (WinoGrande) Please choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much better surgeon than Maria so _ always got the easier cases. Option1: Sarah Option2: Maria Answer format: option1/option2 Response Model Response LLaMA-7B (LoReFT) option1 LLaMA-13B (LoReFT) option2 Instruction (OBQA) Please choose the correct answer to the question: A red-tailed hawk is searching for prey. It is most likely to swoop down on Answer1: an eagle Answer2: a cow Answer3: a gecko Answer4: a deer Answer format: answer1/answer2/answer3/answer4 Response Model Response LLaMA-7B (LoReFT) answer4 LLaMA-13B (LoReFT) answer3 41Instruction (AQuA) A shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day. Now he must sell the items at original price the other day. By what percentage must he increase the price to original? Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18% Response Model Response LLaMA-7B (LoReFT) Let’s assume that the original price of an item is x. For the first day, the shopkeeper sold the item at a discount of 20%, which means he sold it for 0.8x (80% of the original price). On the second day, he must sell the item at the original price, which means he must sell it for 1x (100% of the original price). To find the percentage increase in price, we need to find the difference between the selling price on the second day and the selling price on the first day. Selling price on the second day = 1x Selling price on the first day = 0.8x Percentage increase = (1x - 0.8x) / 0.8x * 100% Percentage increase = (x - 0.8x) / 0.8x * 100% Percentage increase = 0.2x / 0.8x * 100% Percentage increase = 0.25 * 100% Percentage increase = 25% Therefore, the answer is option C) 25%. LLaMA-13B (LoReFT) Let’s assume that the original price of each item was $x. After the discount, the selling price of each item became $0.8x (20% off). Now, the shopkeeper must sell the items at original price the other day. So, the selling price of each item on the other day will be $x. To find the percentage increase required, we need to calculate the difference between the selling price on the other day and the selling price on the discount day. Selling price on the other day = $x Selling price on the discount day = $0.8x Difference = $x - $0.8x = $0.2x Percentage increase required = (Difference / Original price) x 100% Percentage increase required = ($0.2x / $x) x 100% Percentage increase required = 20% Therefore, the answer is (B) 20%. 42Instruction (GSM8K) A car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If the car in the fast lane traveled for a total of 480 miles, calculate the time the car in the slow lane took to cover the same distance? Response Model Response LLaMA-7B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that the speed of the car in the fast lane is 60 miles/hour, and the speed of the car in the slow lane is half that, or 30 miles/hour. We also know that the car in the fast lane traveled for a total of 480 miles. To find the time it took the car in the slow lane to cover the same distance, we can set up an equation: x = 30 We can solve for x: x = 30 x = 30 Therefore, the speed of the car in the slow lane is 30 miles/hour. To find the time it took the car in the slow lane to cover the same distance, we can use the formula: time = distance / speed time = 480 / 30 time = 16 Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. Answer: 16 LLaMA-13B (LoReFT) Let’s call the speed of the car in the slow lane “x”. We know that x = 60/2 = 30 miles/hour. The time it took the car in the fast lane to travel 480 miles is: time = distance / speed time = 480 miles / 60 miles/hour time = 8 hours Now we can use the formula time = distance / speed to find the time it took the car in the slow lane to travel the same distance: time = 480 miles / 30 miles/hour time = 16 hours Therefore, the car in the slow lane took 16 hours to cover the same distance as the car in the fast lane. 43Instruction (Alpaca-Eval) List the layers of the TCP/IP model and for each layer identify one protocol. Response Model Response Llama-2 7B (LoReFT) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network communication model used to transmit data across the internet. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Some examples of protocols at this layer include HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. Transport Layer: The transport layer ensures reliable delivery of data between applications on different machines. Examples of protocols at the transport layer include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: The network layer handles routing and forwarding of packets across multiple networks. Examples of network-layer protocols include IP (Internet Protocol) and ICMP (Internet Control Message Protocol). 4. Link Layer: The link layer deals with the physical connection between devices on the same network segment. Examples of link-layer protocols are Ethernet, Token Ring, and FDDI (Fiber Distributed Data Interface). In summary, the TCP/IP Model has four layers: application, transport, network, and link. Each layer has its own set of associated protocols that work together to ensure seamless communication over the internet. Llama-2 7B (LoReFT, half) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across networks. It consists of four layers, each with its own set of protocols: 1. **Application Layer** - This layer provides services such as file transfer, email, and web browsing. Examples include HTTP (Hypertext Transfer Proto- col), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and Telnet. 2. **Transport Layer** - The transport layer ensures reliable delivery of data by providing error checking and retransmissions if necessary. It also handles multiplexing and de-multiplexing of data streams. Examples include TCP (Transmission Control protocol) and UDP (User Datagram Protocol). 3. **Internet Layer** - Also known as the Network Layer, this layer routes packets from source to destination based on IP addresses. It includes protocols like IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol Version 6). 4. **Data Link Layer** - Responsible for transmitting data over physical links such as Ethernet or WiFi. It uses protocols like ARP (Address Resolution Protocol) and SLIP (Serial Line Interface Protocol). Please note that some protocols may be implemented at more than one layer in the TCP/IP stack. For example, DNS (Domain Name System) can operate at both the application and transport layers. 44Llama-2 7B (LoReFT, 1K) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a standard network architecture used to communicate data across the internet or any other computer network. It consists of four main layers, each with its own set of protocols: 1. Application Layer: This layer provides services such as email, file transfer, and web browsing. Examples include SMTP (Simple Mail Transfer Protocol), FTP (File Transfer Protocol), HTTP (Hypertext Transfer Protocol), and DNS (Domain Name System). 2. Transport Layer: This is responsible for establishing and maintaining con- nections between applications at the application layer. It ensures that data is delivered in the correct order and without errors. Examples include TCP (Transmission ControlProtocol) and UDP (User Datagram Protocol). 3. Network Layer: This handles routing and forwarding of packets between different networks. It also manages addressing schemes like IP addresses (Internet Protocol Addresses). Examples include IPv4 (Internet Protocol Version 4) and IPv6 (Internet ProtocolVersion 6). 4. Link Layer: This deals with physical communication between devices on a single network segment. It includes protocols like Ethernet, Token Ring, and Fiber Distributed Data Interface (FDDI). 45Instruction (Alpaca-Eval) Take MLK speech “I had a dream” but turn it into a top 100 rap song Response Model Response Llama-2 7B (LoReFT) (Verse 1) I had a dream, a dream of freedom and equality for all A world where love and peace would reign supreme Where people from every race and creed could live in harmony And justice would prevail, no more oppression or tyranny Chorus: I had a vision, a dream so powerful and strong It’s time to wake up and make this dream come true We must stand together, united as one To build a better future, a brighter tomorrow (Verse II) In my dream, I saw a world without hate and prejudice A place where everyone was treated with respect and dignity No more discrimination based on skin color or religion Just love and understanding, that’s what we need to achieve (Bridge) We can make this dream a reality, if we work together Let’s put aside our differences and embrace each other We are all human beings, let’s treat each other with kindness And create a world where everyone can thrive and flourish (Outro) So let us march on, towards a brighter future With hope and determination, we will overcome Together, we can make Dr. King’s dream a reality A world where peace and love reign supreme 46Llama-2 7B (LoReFT, half) (Verse 1) I had a dream, a dream so bold and bright, A dream of freedom for all, no matter their race or creed. I saw a world where love was king, And hate was banished from its throne. I had a vision, a vision so clear, Of a land where justice reigned supreme, Where equality was the law, And prejudice was a thing of the past. (Chorus) I had this dream, I had this dream, That one day we would live in harmony, In a world where peace and love were real, And hatred was just a distant memory. (Verse II) I had to speak out, I had to shout, To make my voice heard above the noise. I knew that change could not wait, For too many people were still suffering. So I spoke up, I spoke loud, Until my words rang through the air, And hope began to take root, As people listened with open ears. (Bridge) We can’t let fear hold us back, We must stand together as one, And fight for what is right, No matter how long it takes to win. Let’s keep our eyes on the prize, And never give up, never surrender. Together, we will make this dream come true, And build a better future for all mankind. (Outro) I had the dream, I had the dream, That we could live in harmony. Now it’s time to make it real, And work towards a brighter tomorrow. So let’s join hands, let’s stand tall, And make this dream a reality. Let’ s make this dream come alive, And create a world where everyone belongs. 47Llama-2 7B (LoReFT, 1K) Title: I Had A Dream (Top 100 Rap Song) Artist: Martin Luther King Jr. Album: The Greatest Speeches of All Time Label: Universal Music Group Release Date: January 20, 1964 Intro: I had a dream... Chorus: I have a dream! That one day on the red hills of Georgia, The sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood. I have a vision... Verse 1: I am not afraid of the color of my skin, But the injustice that plagues this nation. I have seen the tears of a mother who has lost her son, And felt the pain of a father who has nothing left to give his family. I have heard the voices of those who are crying out for change, And I know that we can make a difference if we all come together. Refrain: I still have a dream! That one day every man will be free! I still have faith! In the future of our land! I still believe! That we can overcome! We’ll walk hand in hand! And sing a new song! Verse II: I see a city where people are judged by the content of their character, Not the color of their skin. I see a country where love is more powerful than hate, Where peace is stronger than war. I see hope for a brighter tomorrow, When we stand together as one. Refrains: I’m gonna keep on fighting ’til the battle is won! I’m not gonna stop believing ’til the dream comes true! I’ve got a dream! That we can live as brothers! I’ll never give up! ’Cause I know what I’m fighting for! Outro: Thank you! Thank you! Thank you so much! God bless America! God bless us all! 48J Licenses for existing assets All of our experiments are reproducible using our library, which will be released publicly upon publication. Our library comes with the Apache-2.0 License. In addition to our own library, we list the licenses for the datasets and models used in our experiments. J.1 Commonsense reasoning 1. The BoolQ [Clark et al., 2019] dataset: CC-BY-SA 3.0 License. 2. The PIQA [Bisk et al., 2020] dataset: Academic Free License 3.0. 3. The SIQA [Sap et al., 2019] dataset: CC-BY 4.0 License. 4. The HellaSwag [Zellers et al., 2019] dataset: MIT License. 5. The WinoGrande [Sakaguchi et al., 2021] dataset: CC-BY 4.0 License. 6. The ARC Easy set (ARC-e [Clark et al., 2018]): CC-BY 4.0 License. 7. The ARC Challenge set (ARC-c) [Clark et al., 2018]): CC-BY 4.0 License. 8. The OBQA [Mihaylov et al., 2018] dataset: Apache-2.0 License based on the codebase release. J.2 Arithmetic reasoning 1. The AddSub [Hosseini et al., 2014] dataset: CC-BY 4.0 License. 2. The AQuA [Ling et al., 2017] dataset: Apache-2.0 License based on the codebase release. 3. The GSM8K [Cobbe et al., 2021] dataset: MIT License. 4. The MAWPS [Koncel-Kedziorski et al., 2016] dataset: CC-BY 4.0 License. 5. The MultiArith [Roy and Roth, 2015] dataset: CC-BY 4.0 License. 6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset: CC-BY 4.0 License. 7. The SV AMP [Patel et al., 2021] dataset: MIT License. J.3 Instruct-tuning 1. The Ultrafeedback [Cui et al., 2023] dataset: MIT License. 2. The Alpaca-Eval v1.0 [Li et al., 2023] dataset: Apache-2.0 License based on the codebase release. J.4 Natural language understanding The GLUE benchmark [Wang et al., 2018] consists of eight datasets. Except QQP, all datasets come with the CC-BY 4.0 License. QQP comes with a customised license as outlined at https: //www.quora.com/about/tos. J.5 Models 1. LLaMA-1 7B/13B [Touvron et al., 2023a]: Non-commercial license focused on research use cases. 2. Llama-2 7B [Touvron et al., 2023b]: Special Llama-2 License at https://llama.meta.com/ license/. 3. Llama-3 8B: Special Llama-3 License at https://llama.meta.com/llama3/license/.18 4. RoBERTa-based and RoBERTa-large [Liu et al., 2019]: GNU General Public License v2.0. 18https://llama.meta.com/llama3/ 49",
      "meta_data": {
        "arxiv_id": "2404.03592v3",
        "authors": [
          "Zhengxuan Wu",
          "Aryaman Arora",
          "Zheng Wang",
          "Atticus Geiger",
          "Dan Jurafsky",
          "Christopher D. Manning",
          "Christopher Potts"
        ],
        "published_date": "2024-04-04T17:00:37Z",
        "pdf_url": "https://arxiv.org/pdf/2404.03592v3.pdf",
        "github_url": "https://github.com/stanfordnlp/pyreft"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Representation Finetuning (ReFT), a novel family of parameter-efficient finetuning (PEFT) methods that adapt large language models by learning task-specific interventions on frozen hidden representations instead of modifying model weights. The authors define a strong instance, Low-rank Linear Subspace ReFT (LoReFT), and an ablation, DiReFT, which are significantly more parameter-efficient (15x–65x fewer parameters than LoRA) while achieving competitive or state-of-the-art performance across commonsense reasoning, arithmetic reasoning, instruction-following, and natural language understanding benchmarks. A generic ReFT training library is also publicly released.",
        "methodology": "ReFT methods intervene on hidden representations during the model's forward pass. An intervention is defined by a function (Φ), a set of input positions (P), and a layer (l). LoReFT, a specific instance of ReFT, builds on distributed alignment search (DAS) and defines its intervention function as ΦLoReFT(h) = h + R⊺(Wh + b − Rh), where R is a low-rank projection matrix with orthonormal rows spanning an r-dimensional subspace, and W, b are learned linear projection parameters. The base LM's weights are frozen. DiReFT is an ablation of LoReFT, ΦDiReFT(h) = h + W⊺2 (W1h + b), which removes the orthogonality constraint and difference operation. Training objectives include cross-entropy loss for generation tasks and a classification head with cross-entropy loss for single-label classification tasks.",
        "experimental_setup": "Experiments were conducted on LLaMA-family models (LLaMA-1 7B/13B, Llama-2 7B, Llama-3 8B) and smaller LMs (RoBERTa-base 125M, RoBERTa-large 350M). Benchmarks included eight commonsense reasoning datasets (COMMONSENSE 170K), four arithmetic reasoning datasets (MATH10K), instruction-following with Ultrafeedback evaluated via Alpaca-Eval v1.0 (win-rate against text-davinci-003 using GPT-4), and the GLUE benchmark for natural language understanding. Hyperparameters were tuned on dedicated development sets (e.g., GSM8K training subset, Alpaca-52K) to prevent test-set overfitting. Performance was compared against state-of-the-art PEFTs such as LoRA, DoRA, prefix-tuning, adapter-tuning, BitFit, and RED. All experiments were run on single NVIDIA A100 40G/80G or RTX 6000 GPUs, with models loaded in torch.bfloat16.",
        "limitations": "The research primarily explored LLaMA-family models, suggesting a need to investigate ReFT's effectiveness on other model families and vision-language models (e.g., LLaVA). The large hyperparameter search space means ReFT's full capabilities have not yet been explored, indicating a need for automated search. There's a call for deeper exploration into the underlying reasons why ReFT works, specifically how interventions create new causal pathways or modify existing ones. The authors also highlight a general limitation in PEFT research regarding test-set 'hill-climbing' during hyperparameter tuning, advocating for new benchmarks that disallow this practice.",
        "future_research_directions": "Future work includes exploring ReFT's effectiveness on a wider range of model families (beyond LLaMA) and vision-language models (e.g., LLaVA). Automating the hyperparameter search for ReFT is also a key direction. Deeper investigation into the causal mechanisms and pathways modified by ReFT interventions is suggested. The potential for more structured ReFTs to modify complex causal pathways, and the compositionality of learned orthogonal subspaces without adaptation, are areas for further study. Additionally, exploring LM personalization with ReFT in few-shot settings and introducing new benchmarks for PEFTs/ReFTs that prevent test-set tuning are proposed.",
        "experimental_code": "class LoreftIntervention(\n    SourcelessIntervention,\n    TrainableIntervention,\n    DistributedRepresentationIntervention\n):\n    \"\"\"\n    LoReFT(h) = h + R^T(Wh + b \n\n\n    - Rh)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs, keep_last_dim=True)\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, kwargs[\"low_rank_dimension\"], init_orth=True)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.learned_source = torch.nn.Linear(\n            self.embed_dim, kwargs[\"low_rank_dimension\"]).to(\n            kwargs[\"dtype\"] if \"dtype\" in kwargs else torch.bfloat16)\n        self.dropout = torch.nn.Dropout(kwargs[\"dropout\"] if \"dropout\" in kwargs else 0.0)\n        self.act_fn = ACT2FN[\"linear\"] if \"act_fn\" not in kwargs or kwargs[\"act_fn\"] is None else ACT2FN[kwargs[\"act_fn\"]]\n        \n    def forward(\n        self, base, source=None, subspaces=None\n    ):\n        rotated_base = self.rotate_layer(base)\n        output = base + torch.matmul(\n            (self.act_fn(self.learned_source(base)) - rotated_base), self.rotate_layer.weight.T\n        )\n        return self.dropout(output.to(base.dtype))\n\n    def state_dict(self, *args, **kwargs):\n        \"\"\"\n        Overwrite for data-efficiency.\n        \"\"\"\n        state_dict = OrderedDict()\n        for k, v in self.learned_source.state_dict().items():\n            state_dict[k] = v\n        state_dict[\"rotate_layer\"] = self.rotate_layer.weight.data\n        return state_dict\n\n    def load_state_dict(self, state_dict, *args, **kwargs):\n        \"\"\"\n        Overwrite for data-efficiency.\n        \"\"\"\n        self.learned_source.load_state_dict(state_dict, strict=False)\n\n        # Caveat: without creating a new layer, it might not work (still not sure why)\n        # We have to recreate a layer, and load back the columns.\n        overload_w = state_dict[\"rotate_layer\"].to(\n            self.learned_source.weight.device)\n        overload_w_width = overload_w.shape[-1]\n        rotate_layer = LowRankRotateLayer(\n            self.embed_dim, overload_w_width, init_orth=True).to(\n            self.learned_source.weight.device)\n        self.rotate_layer = torch.nn.utils.parametrizations.orthogonal(rotate_layer)\n        self.rotate_layer.parametrizations.weight[0].base[:,:overload_w_width] = overload_w\n        assert torch.allclose(self.rotate_layer.weight.data, overload_w.data) == True # we must match!\n        \n        return",
        "experimental_info": "The training is configured via the `finetune` function in `examples/loreft/train.py` which takes command-line arguments. Key experimental settings include:\n\n-   **Base Model:** `model` argument, defaults to `yahma/llama-7b-hf`.\n-   **Intervention Type:** `intervention_type` argument. For LoReFT, it is set to `LoreftIntervention`. For DiReFT, it corresponds to `NodireftIntervention` (defined as `h + W2^T(W1h + b)` which removes the orthogonality constraint and difference operation as described for DiReFT).\n-   **Intervention Layers:** `layers` argument, defaults to `2;10;18;26`. Can be 'all' for all hidden layers. If `position` indicates multiple positions (e.g., `f1+l1`) and `share_weights` is false, the number of layers is effectively doubled to allow separate interventions.\n-   **Low-Rank Dimension (r):** `rank` argument, defaults to `8`.\n-   **Intervention Position:** `position` argument, defaults to `f1+l1` (first token and last token). Also supports `fn` (first N tokens) or `ln` (last N tokens).\n-   **Epochs:** `epochs` argument, defaults to `1`.\n-   **Learning Rate:** `lr` argument, defaults to `5e-3`.\n-   **Learning Rate Scheduler:** `schedule` argument, defaults to `linear`.\n-   **Training Batch Size:** `batch_size` argument, defaults to `4` (`per_device_train_batch_size`).\n-   **Gradient Accumulation Steps:** `gradient_accumulation_steps` argument, defaults to `4`.\n-   **Weight Decay:** `weight_decay` argument, defaults to `0.00`.\n-   **Warmup Ratio:** `warmup_ratio` argument, defaults to `0.00`.\n-   **Dropout:** `dropout` argument, defaults to `0.00` for the intervention layers.\n-   **Activation Function:** `act_fn` argument, defaults to `None` (which implies a linear activation for the learned source in `LoreftIntervention` and `NodireftIntervention`).\n-   **Add Bias:** `add_bias` argument, defaults to `False` for intervention linear layers.\n-   **Maximum Sequence Length:** `max_length` argument, defaults to `512`.\n-   **Share Weights:** `share_weights` argument, defaults to `False`, controlling if intervention weights are shared across different intervention positions.\n-   **Data Type:** `dtype` argument, defaults to `bfloat16` on CUDA devices and `float32` otherwise.\n-   **Training Objective:**\n    -   For generation tasks (e.g., `alpaca`, `math`), `ReftTrainerForCausalLM` uses standard cross-entropy loss for language modeling on `shift_logits` and `shift_labels`.\n    -   For single-label classification tasks (e.g., `glue`), `ReftTrainerForSequenceClassification` uses cross-entropy loss. It can also use MSE loss for regression or BCEWithLogitsLoss for multi-label classification based on the problem type.\n-   **Base LM Weights:** Base LM weights are frozen by calling `reft_model.disable_model_gradients()`. For GLUE tasks, gradients can be optionally re-enabled for the classification head if `allow_cls_grad` is true."
      }
    },
    {
      "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning",
      "abstract": "Fine-tuning large pre-trained models on downstream tasks has been adopted in\na variety of domains recently. However, it is costly to update the entire\nparameter set of large pre-trained models. Although recently proposed\nparameter-efficient transfer learning (PETL) techniques allow updating a small\nsubset of parameters (e.g. only using 2% of parameters) inside a pre-trained\nbackbone network for a new task, they only reduce the training memory\nrequirement by up to 30%. This is because the gradient computation for the\ntrainable parameters still requires backpropagation through the large\npre-trained backbone model. To address this, we propose Ladder Side-Tuning\n(LST), a new PETL technique that can reduce training memory requirements by\nmore substantial amounts. Unlike existing parameter-efficient methods that\ninsert additional parameters inside backbone networks, we train a ladder side\nnetwork, a small and separate network that takes intermediate activations as\ninput via shortcut connections (called ladders) from backbone networks and\nmakes predictions. LST has significantly lower memory requirements than\nprevious methods, because it does not require backpropagation through the\nbackbone network, but instead only through the side network and ladder\nconnections. We evaluate our method with various models (T5 and CLIP-T5) on\nboth NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST\nsaves 69% of the memory costs to fine-tune the whole network, while other\nmethods only save 26% of that in similar parameter usages (hence, 2.7x more\nmemory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA\nin a low-memory regime. To further show the advantage of this better memory\nefficiency, we also apply LST to larger T5 models, attaining better GLUE\nperformance than full fine-tuning and other PETL methods. The\naccuracy-efficiency trade-off also holds on VL tasks.",
      "full_text": "LST: Ladder Side-Tuning for Parameter and Memory Efﬁcient Transfer Learning Yi-Lin Sung Jaemin Cho Mohit Bansal UNC Chapel Hill {ylsung, jmincho, mbansal}@cs.unc.edu Abstract Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efﬁcient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efﬁcient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has signiﬁcantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both natural language processing (GLUE) and vision-and-language (VQA, GQA, NLVR2, MSCOCO) tasks. LST saves 69% of the memory costs to ﬁne-tune the whole network, while other methods only save 26% of that in similar parameter us- ages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efﬁciency, we also apply LST to larger T5 models (T5-large, T5-3B), attaining better GLUE performance than full ﬁne-tuning and other PETL methods. The trend also holds in the experiments on vision-and-language tasks, where LST achieves similar accuracy to other PETL methods when training a similar number of parameters while also having 2.7x more memory savings.1 1 Introduction Recently, large-scale pre-training and ﬁne-tuning of transformers [54] have been successful in various domains [8, 36, 44, 38, 53, 7, 20, 1, 10]. As the model size grows rapidly, ﬁne-tuning the entire parameter set of the large pre-trained model has become very costly. Parameter-efﬁcient transfer learning (PETL) [ 31, 23, 39, 40, 34, 51, 58, 17, 24, 18, 42, 60, 14, 52, 59] is a recent research direction for online or multi-task learning. The goal is to build a system that performs well on all tasks without training an entire new model for every new task. Concretely, PETL methods select a small subset of pre-trained parameters and/or insert a few parameters to a pre-trained network and update those parameters for new tasks, while freezing most of the original parameters. In the 1Our code is available at: https://github.com/ylsung/Ladder-Side-Tuning. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.06522v2  [cs.CL]  31 Oct 20226 8 10 12 14 16 18 20 22 Memory Usage (GB) 70.0 72.5 75.0 77.5 80.0 82.5 85.0 87.5Average Accuracy (%) Full Fine-tuning Adapters LoRA BitFit Prompt-tuning Ladder Side-tuning Ladder Side-tuning (T5-large) Ladder Side-tuning (T5-3B) Figure 1: Comparison between full ﬁne- tuning, Adapter, LoRA, BitFit, and Ladder Side-Tuning over GLUE tasks. The y-axis is the average accuracy of 8 GLUE tasks, while the x-axis is the GPU memory usage during training. Unless specially stated, we use the T5-base in the ﬁgure. Input Output (a) Adapters (c) Ladder Side-Tuning (Ours) Input Output New Param  (Updated) Backbone  (Frozen) Forward Backward Input Output (b) Prompt Tuning Figure 2: Comparison between transfer learning with (a) Adapters, (b) Prompt Tuning, and our (b) Ladder Side- Tuning (LST). LST reduces memory usage by removing the need of backpropgation through backbone networks. natural language processing (NLP), computer vision (CV), and vision-and-language (VL) domains, two types of parameters have been commonly updated for parameter-efﬁcient transfer learning: (a) Adapters [23, 40, 39]: small modules inserted into transformer blocks; (b) Prompt [34, 31]: small parameters concatenated with input embeddings (see Figure 2). However, while parameter-efﬁcient techniques reduce the number of parameters to update, they do not reduce the memory requirement during training by much (up to 30%). In other words, if a large pre-trained language model does not ﬁt on a GPU, these techniques usually do not help to ﬁt the model on the GPU. In other words, these techniques usually do not help to ﬁt a large pre-trained language model on a GPU if the model cannot be trained on the GPU with standard ﬁne-tuning. Since the updated parameters are inside the backbone language models, to calculate gradients for these parameters for backpropagation, we still need to run the backward pass through the large pre-trained language models. This prevents PETL methods from being applied to many real-world applications with limited computational resources. To address this issue, we propose Ladder Side-Tuning (LST), a memory-efﬁcient PETL method. LST separates trainable parameters from the backbone model to construct a side network, which is responsible for adapting the entire model to new tasks. Concretely, we train a ladder side network, a lightweight network that takes intermediate activations via shortcut connections (ladders) from the backbone networks as input and makes predictions. As shown in Figure 2, unlike previous (a) adapters and (b) prompt tuning methods, (c) our LST does not add trainable parameters inside the pre-trained model, and this completely eliminates the need for expensive backpropagation of a large backbone network and saves substantial memory during transfer learning. Instead of initializing the side network’s weights randomly, we utilize a network pruning technique to retrieve a smaller pruned network and use it as the side network. In addition to our standard design of the side network, we also boost the efﬁciency of LST by dropping layers of the side network. We empirically demonstrate that the layer dropping can signiﬁcantly improve both memory and parameter efﬁciency without sacriﬁcing performance. Furthermore, during inference, even though we have to propagate forward through two distinct networks, LST does not necessarily use more inference time because the same level of the backbone network and the side network can be computed in parallel. We conduct comprehensive studies on LST using diverse NLP and VL tasks, namely, GLUE [55], VQA [16], GQA [25], NLVR2 [50] and MSCOCO [6]. Overall, in GLUE experiments, LST saves 69% of the GPU memory that is needed for ﬁne-tuning the entire backbone model, saving 2.7x memory compared against Adapter and LoRA. Also, LST achieves higher accuracy than other PETL methods in a low-memory regime. To take advantage of this better memory efﬁciency, we also apply LST to larger language models (T5-large and T5-3B) and ﬁnd that it achieves higher accuracy than other PETL techniques when GPU memory utilization is similar. The ﬁndings still hold in the Vision-Language (VL) experiments; LST is not only a method that can ﬁt in a 16GB GPU with 7.5% trainable parameters, but it also has similar or better accuracy than other PETL methods (and 2again with 2.7x memory savings). To justify our design of LST, we conduct ablation studies on initialization strategies and alternatives to add shortcut connections. The results reveal that these components considerably help performance with minor computation overhead. 2 Related Work 2.1 Parameter-efﬁcient Transfer Learning (PETL) PETL for NLP.In the past few years, large pre-trained language models have made huge success in NLP. Parameter-efﬁcient transfer learning (PETL) is a research direction that reduces computational cost of adapting large pre-trained models to new tasks, by avoiding updates of the entire parameters. A popular line of work on PETL is to add a few trainable parameters and only tune them. For example, Adapters [48, 23] are small bottleneck modules that are inserted into transformer layers, and experiments have shown that training adapters with layer normalization layers is sufﬁcient to achieve full ﬁne-tuning performance. In a similar trend, LoRA [ 24] injects trainable rank decomposition matrices into a frozen pre-trained model. Instead of inserting new parameters into pre-trained models, prompt-based [31, 34] methods add trainable parameters to the input and keep the entire pre-trained model unchanged during training. The inserted prompts learn to make use of the knowledge of the pre-trained model to solve new tasks. Although the concept of adapter-based and prompt-based approaches is different, He et al. [18] unify the two lines of approaches (including LoRA) into adapter-based methods. In addition to approaches that introduce new parameters, there are also various methods [51, 58, 17] that select a sparse subset of parameters from the pre-trained model to update, without adding any new parameters. One of such representative methods is BitFit [58], which updates every bias term in the model. PETL for CV/VL. While most of the progress in PETL is made in NLP domain, researchers have also applied this technique to the CV [ 47, 46, 27, 59, 62, 28, 60, 21] and VL [52, 61, 62, 28, 60] domains. VL-Adapter [52] benchmarks adapter-based and prompt-based methods on multiple image- text and video-text tasks, and shows adapters enable us to efﬁciently learn fusion information of vision and language. On the CV side, beneﬁtting from the parameter efﬁciency of adapters and prompt-tuning, some works [ 62, 28, 60] apply these approaches to CLIP [ 43] to achieve strong few-shot performance in image classiﬁcation tasks. Side-Tuning [59] uses an additive side network, which sums its representation with the backbone network in the last layer, to solve various tasks with ResNet [19] and BERT [8]. Although LST takes inspiration and has similarities to Side-Tuning, we argue that there are major differences in motivations, architecture designs, and applied tasks between the two methods. LST aims to reduce the memory requirement of current PETL methods, whereas Side-Tuning does not focus on memory reduction (sometimes their side network is even as big as the backbone network), but instead their motivation is to ease the forgetfulness in incremental learning. Our ladder side network is more robust than their design because the shortcuts fuse the intermediate information from the backbone network, and we also use layer dropping and network pruning techniques to make LST more efﬁcient and stronger. Lastly, we further extend LST in VL architecture and demonstrate its usefulness on multiple VL tasks. Current PETL approaches explore how to achieve competitive results using as few parameters as possible. However, parameter efﬁciency does not necessarily mean memory efﬁciency. In this work, we propose LST that has these two beneﬁts simultaneously. Concurrently, Liu et al. [37] also propose Y-tuning to address a similar issue; it exhausts all possible labels and feeds them into a model to select the best answer from the input. However, it is intractable oftentimes to list all answers in some tasks, for example, regression and open-ended generation tasks. On the other hand, LST is more ﬂexible in applying to different architectures and tasks. We show that LST can outperform Y-tuning with fewer parameter updates in Table 2. 2.2 Memory-Efﬁcient Training Memory-efﬁcient training aims to reduce the memory cost when training neural networks. Some approaches achieve this goal by cutting down the storage of intermediate activations, which dominate the training cost, to release a large amount of memory. The design of reversible neural networks [15, 29, 41] allows the model not to save intermediate activations because each layer’s activations can be reconstructed from the next layer’s. Gradient checkpointing [5] proposes to trade computation for memory by dropping some intermediate activations and recovering them from an extra forward 3pass. LST uses a different approach to cut the storage of activations; it keeps the backbone model frozen and constructs a side network for cheaper training. Since the backbone model is not updated, LST is not only memory-efﬁcient but also parameter-efﬁcient. Furthermore, memory saving from reversible neural networks and checkpointing is agnostic to memory saving from LST. Researchers can combine those methods with LST if they pursue a higher level of memory efﬁciency. Another line of memory-efﬁcient methods is network compression, which compresses a backbone network to a smaller one, and the generated network is cheaper for both training and inference. Two popular approaches for compression are network pruning and distillation. Network distillation [22, 30] constructs a student network and force it to have same output distribution of the teacher network over a chosen dataset. Network pruning [12, 13] makes models lighter by learning importance scores of parameters and trimming unimportant parameters or neurons. While PETL still uses those untrained parameters in the forward pass, network compression entirely discards them or sets them to zero. As a result, network compression can generate models for faster inference speed while PETL can achieve better performance by updating fewer parameters. In this paper, we explore using network pruning or network distillation [22] (used in Side-tuning) to extract a sub-network that contains critical information of the backbone model and use it for initializing our side network. For distillation, we did not follow the original Side-tuning to apply distillation with large-scale pre-training datasets (e.g., C4 [44]) because it makes distillation hard to conduct with limited resources and ultimately violates our goal of “efﬁcient training.” Also, using an extra pre-training dataset during ﬁne-tuning is unfair to other approaches. We use the standard distillation procedure with the T5 [44] pre-training objective to train the side network. That is, the student (side) network learns to predict the masked spans and match the output distribution of the teacher (backbone) network simultaneously. We show the comparison between distillation-based and pruning-based initializations in Figure 8. We primarily use the network pruning method proposed by Li et al. [33] to initialize the side network because of its efﬁciency, and we describe the approach in detail in Section 3.3. The standard procedure of network pruning is (1) learn [12, 13] or heuristically deﬁne [33] an \"importance measure\" to identify the importance of parameters, (2) prune p% of parameters with lower importance scores, (3) repeat the ﬁrst and second steps until reaching the target sparsity. The rewinding procedure enables pruning techniques to ﬁnd a more sparse sub-network. In this paper, to keep the whole pruning process efﬁcient, we either use weights magnitude [ 33] or Fisher Information [51, 35] as importance measures, and reach the target sparsity in one shot. As a PETL method, LST makes use of intermediate information from the backbone model as the inputs, and we empirically demonstrate those additional inputs signiﬁcantly improve performance in Figure 8. 3 Ladder Side-Tuning (LST) We introduce Ladder Side-Tuning (LST), a new PETL technique that can also reduce training memory requirements by substantial amounts than previous methods. In Section 3.1, we analyze the computational cost for ﬁne-tuning with trainable modules in backbone models. Then we explain the architectural details (Section 3.2), structural weight initialization based on network pruning (Section 3.3), and dropping side network layers for more efﬁciency (Section 3.4). 3.1 Dependency on Backpropagation through Large Backbone Model We consider a N multilayer perceptron (MLP): fN(fN−1(...f2(f1(x))...)), where the ith layer fi(x) = σi(Wix+ bi) consists of weight Wi, bias bi, and nonlinear function σi. We denote the output of ith layer as ai+1 and the pre-activation as zi+1, where ai+1 = σi(zi+1) =σi(Wiai + bi). In backpropagation with loss L, the gradient with respect to Wi and bi: ∂L dWi = ∂L ∂ai+1 ∂ai+1 ∂zi+1 ∂zi+1 ∂Wi = ∂L ∂ai+1 σ′ iai, ∂L dbi = ∂L ∂ai+1 σ′ i (1) 4Output Input Backbone Enc Backbone Dec  Decoder Input New Param  (Updated) Backbone  (Frozen) Forward Backward Forward  (Low-dim) Gate Backbone Side Network  (b) Ladder Side Network for Encoder-Decoder(a) Ladder Side Network Input Emb Output LM head Side Enc Side Dec  Figure 3: Illustration of Ladder Side-Tuning (LST) with transformers described in Section 3.2. (a) shows a high-level overview of LST, and (b) shows LST with an encoder-decoder architecture. where σ′ i is the derivative of σi . ∂L ∂a i+1 , the gradient with respect to ai , can be calculated with the gradients with respect to ai +2 , using the chain rule: ∂L ∂ai +1 = ∂L ∂ai +2 ∂ai +2 ∂zi +2 ∂zi +2 ∂ai +1 = ∂L ∂ai +2 σ′ i +1 Wi +1 (2) As shown in Equations (1) and (2), during backpropagation, there are two terms dominating the memory footprint: 1) {a}corresponding to updated parameters {W}and 2) {σ′}that must be cached for the chain rule. Note that we use {·}to denote a set of activations, parameters, or gradients. Existing PETL methods, such as Adapters [23], LoRA [24], Prompt-tuning [31], and BitFit [58, 3] could reduce the memory footprint by making |a|smaller, as they have fewer {W}to update, but do not reduce |σ′|, where |·| means the size of set {·}. Since most activation functions do not change dimensions (i.e., |a|= |σ′|), the memory footprint for backpropagation |a|+ |σ′|can be reduced by up to 50% by the PETL methods when they reduce the entire memory footprint for |a|. By making the updated parameter do not require backpropagation through the backbone network, our LST can achieve better memory efﬁciency beyond 50%, and we explain it below. 3.2 Ladder Side Network for Transformers Unlike existing transfer learning methods that insert additional parameters inside a transformer net- work, we propose training a ladder side network, a small and separate network that takes intermediate activations from the backbone transformer as input and makes predictions. As illustrated in Figure 3 (a), since the ladder side network parameters φare not used during the forward pass of the backbone transformer with parameters θ, the update of the ladder side network does not require expensive backpropagation of the large backbone transformer. Note that our LST method is not limited to a speciﬁc architecture. We provide a simpliﬁed overview of LST with an encoder architecture in Figure 3 (a) and an illustration of LST with an encoder-decoder architecture in Figure 3 (b). Lightweight architecture. Our side network gis a lightweight version of the backbone transformer f, where all weights and hidden state dimensions of in g are 1 r times of the original weights and hidden states of f, where ris a reduction factor (e.g. r= 2, 4, 8, 16). For example, if the backbone f has a 768-dimensional hidden state, then the side network gwith r = 16has a hidden state of 48 dimensions (= 768/16). The side network greuses frozen word embeddings (‘Emb’ in Figure 3 (a)) and the language model head (‘LM head’ in Figure 3 (a)) of the backbonef. Following the analysis in Section 3.1, we also examine the memory cost of LST. Recall that original memory footprint for backpropagation is |a|+ |σ′|. Because we do not have to run a backward pass through the backbone 5Input Output Input Output (b) LayerDrop in Side Network(a) Structural Weight Initialization 1. Select backbone weights w.r.t. importance score 2. Initialize weights  of Side Network  Figure 4: Illustration of (a) Structural Weight Initialization (Section 3.3) and (b) Layer Dropping (Section 3.4). In our experiments, we ﬁnd that initialization of side network parameters from backbone network parameters improves performance, and dropping some shortcut connections improves efﬁciency without hurting performance. network, we can only consider the gradients for the side network, whose memory footprint is |a|+ |σ′| r . Therefore, LST has a better memory efﬁciency than other PETL methods (saving up to 50%) as long as ris greater than 2 (we ﬁnd 8 works well in most experiments). Gated ladder connections. Although Zhang et al. [59] found that late fusion to combine the representations of the backbone and the side network works well with convolutional networks for CV tasks, in our experiments, we ﬁnd that late fusion hurts the performance of the transformer architecture in NLP tasks (see Figure 8 in Section 5 for details). To address this, we use the shortcut connection (called ladder, due to the overall shape created from the multiple shortcut connections) from intermediate activations from the backbone f to the side network gand ﬁnd it helpful. We learn linear projections to downsample (×1 r) the intermediate activations (including word embeddings) of f to low-dimensional attention blocks in g. Then, we learn a linear projection to upsample (×r) the side network output to the dimension of the original language model head. The linear projections are illustrated as green trapezoids in Figure 3 (a). The ith transformer layer of the side network g combines the activation of the backbonehf i and the activation of the previous layer of the side network hg i−1 with learned gating: µi∗hf i +(1 −µi)∗hg i−1, where µi = sigmoid(αi T ) is a gate parameterized with a learnable zero-initialized scalar αi and temperature T (= 0.1). We have also tried to use Adapter blocks to build the side network and replace the gating mechanism with cross-attentions, but we ﬁnd the current design works the best (see Appendix A). 3.3 Structural Weight Initialization for Ladder Side Network We ﬁnd it helpful to initialize the weights of the side network φfrom the weight of the backbone network θbased on structural pruning [33], as shown in Figure 4 (a). Concretely, given a weight matrix W ∈Rdout ×din of the backbone network that maps the din-dim vectors to the dout-dim space, and the importance matrix of the weight I ∈Rdout ×din , we ﬁrst calculate the importance score of each row si = ∑ j|Ii,j|, denoting the importance of each weight vector. Note that the importance matrix I used in this work are either weight magnitude [33] (I = W) or empirical Fisher Information [51] (I = FW = 1 N ∑N i=1 (∇W log p(yi|xi))2; (xi,yi),..., (xN,yN) are samples from data). Then, we choose the rows of W which have the top dout r importance scores and prune the remaining rows to obtain a new weight matrix WP ∈R d out r ×din . The columns of the weights and the importance matrix in the next layer corresponding to the pruned feature map are also pruned. By iterating this process, we obtain the set of weight matrices whose rows and columns are pruned 1 r times from the backbone network and use them to initialize the side network. In our experiments shown in Figure 7, we ﬁnd that using Fisher information as an importance score metric generally performs well, and therefore we use it in our structural weight initialization. 63.4 Layer Dropping in the Ladder Side Network We explore to increase efﬁciency of LST even further by making side network more compact, by dropping its intermediate transformer layers, as illustrated in Figure 4 (b). Similar to LayerDrop [ 11], we drop layers in the side network, and this can linearly reduce the memory and parameter require- ments of LST. For instance, a side network with N layers will only have 2nd, 4th, 6th ... layers left, after we drop half of the layers. Refer to Section 4 for more details on applying layer dropping on an encoder-decoder architecture. In Figure 6, we show that layer dropping can greatly boost the model’s efﬁciency without sacriﬁcing performance. 4 Experiment Setup Datasets. We evaluate LST on NLP and VL tasks. For NLP tasks, we use the GLUE [55] bench- mark, which consists of seven classiﬁcation and one regression task. The benchmark evaluate models on multiple diverse tasks over linguistic acceptability (CoLA [56]), sentiment analysis (SST- 2 [49]), similarity and paraphrase (MRPC [9], QQP [26], STS-B [4]) and natural language inference (MNLI [57], QNLI [45], RTE [2]). For VL tasks, we experiment with visual question answering (VQA [16], GQA [25]), visual reasoning (NLVR2 [50]) and image captioning (MSCOCO [6]) tasks. Baselines. We compare LST against the full ﬁne-tuning and several popular PETL approaches on both NLP and VL setups. In Full ﬁne-tuning, all the parameters are updated for a downstream task. Full ﬁne-tuning is not parameter-efﬁcient nor memory-efﬁcient, but it serves as the upper bound of the ﬁne-tuning performance. To compare to other PETL methods, we reproduce Adapters, where we inject small trainable modules after every attention and feed-forward layer, and we solely train those modules and layer normalization layers while keeping the rest of the model frozen. We also reproduce LoRA, which inserts trainable low-rank matrices into the model to parameterize the weights’ changes. In BitFit, we only update the bias terms over the course of training. Lastly, we compare our method to Prompt-tuning, where trainable prompt vectors are prepended to the input. We initialize the prompt vectors with the embedding of the pre-trained model’s vocabularies. Training and Evaluation Setup. For NLP tasks, we use T5 [44], a pre-trained encoder-decoder language model as our backbone. We use T5-base in most experiments, except that we scale up LST on T5-large and T5-3B to demonstrate its memory efﬁciency. The training and evaluation process follows the setup used by Mahabadi et al. [40]. Since there is no local test set, we split 1k samples from the training set as the new validation set and use the original validation set as the test set. For datasets whose samples are less than 10k (RTE, MRPC, STS-B, CoLA), we split the validation set into two equal-sized subsets and treat them as a new validation and test set. For MNLI, we use the mismatched set as the validation set and matched set as the test set. We train every approach with 10 epochs on large datasets and 20 epochs on small ones (RTE, MRPC, STS-B, CoLA) for complete convergence. We search for learning rates over{3 ×10−4,1 ×10−3,3 ×10−3}for LST and LoRA[24], and we use the optimal learning rates that are used by Mahabadi et al. [40] for other methods. The reduction factor used in LST is set to 8 if not additionally speciﬁed. T5-base has 12 layers each in encoder and decoder, while T5-large and T5-3B have 24 layers each. In our experiments, we do not drop layers in T5-base unless we specially mention it. For T5-large and T5-3B, we drop 24 layers (12 layers each in encoder and decoder) and 46 layers (23 each) of the side network to make the memory usage close to our baselines. The experiments on T5 take around 12 hours to train with one A6000 GPU (48GB). For VL tasks, we experiment with CLIP-T5 [52], which is a VL architecture combining CLIP [43] and T5 [ 44]. We always freeze the CLIP and only train the T5 for new tasks. The CLIP visual representation is concatenated with the text embedding, and the combined input is fed to T5 to make predictions. A visual projection layer is added between CLIP and T5 to let the visual representation have the same dimension as the text embedding. To avoid updating the visual projection layer by the gradients from the backbone model, we do not feed combined inputs to the backbone model, but only text inputs. The combined inputs are fed to the side network, so we can achieve efﬁcient training by only computing the gradients from the side network. Because the backbone network only uses texts as the input, the information from the backbone network via shortcut connections is only summed to the text part of the side network’s combined inputs. We follow the multi-tasking setting for training and evaluation used in VL-Adapter [52]. We report the performance on Karpathy 7Table 1: Comparison between multiple parameter-efﬁcient training methods on GLUE benchmark. We use T5-base if we don’t additionally specify. We report accuracy for SST-2, MNLI, QNLI and RTE. For CoLA and STS-B, we use Matthew’s Correlation and Pearson-Spearman Correlation as the metrics, respectively. For MRPC and QQP, we report the average of F1 score and accuracy. Each number in the table is the average result over three seeds, and the subscripts are standard deviations. For the results with †, we report the best performance out of three seeds due to the instability of the method. We report the maximum memory usage training and evaluating on RTE for each method. Method Update Param.per Task (%)Memory Usage (GB)CoLA SST-2 MRPC QQP MNLI QNLI RTE STS-B Avg.Train Inference Full ﬁne-tuning 100 17.6 0.86 62.8 2.5 93.90.6 91.91.0 89.90.4 86.20.4 92.50.3 74.11.0 90.30.1 85.20.4Adapters 1.63 13.0 0.87 64.4 1.5 94.20.5 88.90.2 88.90.1 86.40.2 93.10.2 75.10.7 91.10.2 85.30.2LoRA 1.71 12.6 0.86 63.3 0.1 94.30.1 90.10.7 89.00.1 86.30.1 93.20.1 75.53.3 90.90.0 85.30.5BitFit 0.13 10.7 0.86 61.8 1.5 94.30.1 91.00.2 88.70.0 85.60 93.10.1 67.60.6 90.80.2 84.10.1 Prompt-tuning 0.03 22.2 0.87 0 †2.5 90.3†16.3 74.60.0 88.50.2 82.50.9 92.50.2 59.52.9 90.10.1 72.21.6 Ladder Side-Tuning 1.74 5.5 0.88 58.1 3.2 94.10.3 90.41.0 88.80.1 85.60.1 93.30.1 71.92.1 90.70.2 84.10.5Ladder Side-Tuning (T5-large) 1.23 12.2 2.88 65.31.9 95.70.1 91.61.0 89.70.0 88.60.0 94.10.2 79.90.0 92.40.1 87.10.2Ladder Side-Tuning (T5-3B) 0.08 22.4 11.01 66.41.7 96.50.1 92.90.8 89.70.1 90.70.1 95.10.2 80.11.0 93.00.3 88.10.4 test/test-dev/test-P/Karpathy test split for VQA/GQA/NLVR2/MSCOCO, and train models for 20 epochs. We search learning rates over {3 ×10−4,1 ×10−3,3 ×10−3}for PETL methods, and use 1 ×10−4 used by Sung et al. [52] for full ﬁne-tuning. We set the reduction factor for the side network to 4. We train CLIP-T5 for 16 hours on one A6000 GPU. In Appendix B, we comprehensively list hyper-parameters for NLP and VL experiments in Table 5 and Table 6, respectively. 5 Experimental Results In this section, we show experiments to justify our design of LST and demonstrate that LST performs the best among all approaches in the scenario with limited memory. As the result, LST is the most efﬁcient tool to ﬁne-tune large-scale pre-trained models for real-world applications. LST outperforms other methods under similar memory usage. Figure 1 and Table 1 show the results on GLUE of different approaches applying on T5-base. We drop 6 layers (3 layers each in side encoder and decoder) for LST to match the parameter usage of the Adapter and LoRA. Under the same parameter usage, LST can save 69% of memory cost to fully ﬁne-tune the model, while Adapter and LoRA only save 26% of that, leading to LST having a 2.7x more memory saving. Compared to BitFit, LST achieves the same average performance but costs 5GB less GPU memory. LST also surpasses Prompt-tuning in terms of both performance and memory cost. To further take advantage of the memory efﬁciency of LST, we also train T5-large and T5-3B with LST. We ﬁnd that with a similar budget of memory usage in Adapter and LoRA, LST with T5-large can surpass the performance of other methods by a large margin. The result on T5-3B also outperforms the result on T5-large, demonstrating the scalability of our memory-efﬁciency method on large language models. Furthermore, even though LST increases the model size, its additional inference memory usage is negligible as LST uses almost the same inference memory (0.88 GB) as the full ﬁne-tuning (0.86 GB). Table 2: LST vs. Y-tuning. Method Update Param. per Task (%) Avg. GLUE Y-tuning 7.7 76.9 LST 2.6 82.1 In Table 2, we also compare LST with a concurrent work, Y-tuning [37] on GLUE tasks (except for STS-B) with BART- large [32] encoder as backbone. Following their experimental setup, we use a different learning rate and report the best accu- racy out of three seeds for each task. Overall, LST outperforms Y-tuning by a large margin with fewer updated parameters. LST is competitive on VL tasks. As we have mentioned beforehand, we also extend LST on a multi-modal architecture, CLIP-T5, on multiple VL tasks, and we demonstrate the outcome in Table 3. With similar parameter usage, LST is the only method that can ﬁt into a single 16GB GPU. Besides the efﬁciency, it is as competitive as full ﬁne-tuning and Adapter, outperforming other PETL approaches. LST performs the best in low-memory regime. To have a better understand of the memory advantage of LST, we adjust the hyper-parameters in our method (reduction factor∈{32,16,8,4}), Adapter (hidden dimension ∈{6,12,24,48}) and LoRA (rank ∈{4,8,16,32}) to create multiple architectures with different memory costs. Figure 5 shows the performance and memory efﬁciency trade-off for all methods. We ﬁnd the memory saving is not obvious for Adapter and LoRA, because 8Table 3: Comparison between multiple parameter-efﬁcient training methods on VQA, GQA, NLVR2, and MSCOCO. We use T5-base for all approaches. We report accuracy for VQA, GQA and NLVR while we use CIDEr to evaluate MSCOCO. Each number in the table is the average result over three seeds, and the subscripts are standard deviations. Method Update Param. (%) Memory Usage (GB)VQA GQA NLVR2 MSCOCO Avg. Train Inference Full ﬁne-tuning 100 36.2 0.86 67.1 0.1 56.30.3 74.30.4 112.20.3 77.50.3 Adapters 7.98 28.4 0.93 67.1 0.1 56.00.4 72.70.3 111.80.1 76.90.2 LoRA 7.54 27.9 0.86 63.7 0.2 53.30.1 70.00.3 110.30.4 74.30.1 BitFit 0.83 22.7 0.86 55.1 0.2 45.50.2 51.71.1 101.20.2 63.40.1 Prompt-tuning 1.26 38.7 0.87 47.4 0.7 40.60.4 51.00.4 96.10.9 58.80.6 Ladder Side-Tuning 7.46 15.3 0.93 66.5 0.1 55.90.1 71.60.3 113.50.3 76.90.1 5 6 7 8 9 10 11 12 13 14 Memory Usage (GB) 82 83 84 85 86 87 88Average Accuracy (%) Adapters LoRA Ladder Side-tuning Figure 5: The accuracy-memory trade-off for Adapter, LoRA, and Ladder Side-Tuning over GLUE tasks. We vary the reduction factor in Ladder Side-Tuning, hidden dimension in Adapter, rank in LoRA to get the architectures with different training costs. 2 4 6 8 10 12 14 Memory Usage (GB) 78 80 82 84 86 88Average Accuracy (%) Adapters LoRA BitFit Ladder Side-tuning Figure 6: The accuracy-memory trade-off for Adapter, LoRA, BitFit, LST over GLUE tasks. we drop N ∈{0,6,12,18}layers in an inter- leaving manner for LST while we gradually freeze the ﬁrst N ∈{0,6,12,18}layers in other methods (also remove inserted parame- ters in those layers). the gradients of the backbone model’s intermediate outputs are still computed (see Section 3.1 for details). Even though the Adapter and LoRA can get slightly better memory efﬁciency by reducing the hidden dimension and the rank, we ﬁnd that the performance drops signiﬁcantly. On the other hand, LST is quite robust across a wide range of side network sizes. We also consider another way to compare LoRA, Adapter and BitFit to LST in different memory budgets. While we drop N ∈{0,6,12,18}layers in an interleaving manner to improve memory efﬁciency, we freeze the ﬁrst N layers and remove the corresponding inserted modules in other approaches. With this, other methods can achieve better memory efﬁciency because gradients do not propagate to those earlier frozen layers. We discuss the layer dropping and layer freezing with details in the following. In LST, we drop N 2 layers in both side encoder and side decoder. However, in other PETL approaches, we start from freezing layers in the encoder and then turn to freeze layers in the decoder (e.g. freezing 18 layers means freezing all encoder layers and ﬁrst 6 decoder layers). We display the comparison in Figure 6, showing that LST has a better performance and memory trade-off and outperforms other methods in the low-memory regime. We also ﬁnd that layer dropping generally reduces the training cost without hurting performance. The ablation of weight initialization on the side network. We compare the different initialization strategies for the side network and demonstrate the results in Figure 7. “Random” denotes we randomly initialize the network while we use network pruning to select initial weights for the side network based on two importance measures, “Weight Magnitude” and “Fisher Information.” In general, the initialization from the pruned network helps no matter the size of the side network, showing the effectiveness of our network pruning strategy. 9Random Weight Magnitude Fisher Information Initialization Strategy 82.00 82.25 82.50 82.75 83.00 83.25 83.50 83.75 84.00Average Accuracy (%) 83.4 83.9 83.8 82.9 83.5 83.6 reduction factor=32 reduction factor=8 Figure 7: The ablation study on different initialization strategies. Y-axis denotes the average score over GLUE tasks. Network compression  (classic distill.)Network compression  (network pruning) Side-tuning   (classic distil.)Side-tuning   (network pruning) LST   (classic distil.) LST   (network pruning) Methods 50 55 60 65 70 75 80 85Average Accuracy (%) 58.5 61.5 72.3 72.3 83.5 83.8 Figure 8: The comparison between net- work compression, Side-Tuning, and LST on GLUE tasks. Comparison of LST to network compression methods and Side-tuning. In Section 3.2, we mention that shortcut connections are added to every layer of the side network. We justify this design by comparing LST to two types of approaches: (1) network compression, which discards all shortcut connections and the entire backbone model; (2) Side-tuning, which only adds one shortcut connection to merge representations right before the output layer. Note that we do not drop any layer in the side network but only remove the shortcuts in this experiment. We also compare both distillation-based and pruning-based initialization methods as we describe in Section 2.2. We set the reduction factor to 8 for all approaches. Figure 8 shows the comparison and LST outperforms the other two types of methods signiﬁcantly. We conclude that PETL methods are stronger than network compression as they use the information from the backbone model. This also suggests that network compression approaches need to train more parameters to achieve the same level of performance as PETL methods. We also demonstrate the usefulness of intermediate shortcuts since LST surpasses Side-tuning by a large amount. Furthermore, we ﬁnd that using distillation-based initialization or network pruning-based initialization provides similar accuracy in all setups. Note that our network pruning-based initialization method is more efﬁcient since it does not involve training. 6 Conclusion We propose Ladder Side-Tuning (LST), a parameter- and memory-efﬁcient method to adapt a large backbone network. LST does not require backpropagation through the backbone network, which allows for signiﬁcantly lower memory requirement during training than recently proposed parameter- efﬁcient training techniques. We demonstrate that LST allows users to adapt a larger and more powerful backbone network to target tasks with a limited memory requirement, which cannot be achieved with recent parameter-efﬁcient techniques. We also show that LST achieves a more efﬁcient accuracy-memory trade-off than recent baselines, the impact of weight initialization of side networks, and the usefulness of intermediate shortcut connections. Finally, we show that the LST can be also extended beyond NLP tasks, with strong results on VL tasks. We hope that LST helps users with limited computational resources tune larger models in diverse domains. Acknowledgments We thank the reviewers, Muqeeth Mohammed, Derek Tam, Prateek Yadav, and Gedas Bertasius for their helpful discussions. This work was supported by ARO Award W911NF2110220, ONR Grant N000141812871, and NSF-AI Engage Institute DRL-211263. The views, opinions, and/or ﬁndings contained in this article are those of the authors and not of the funding agency. 10References [1] Hangbo Bao, Li Dong, and Furu Wei. Beit: BERT pre-training of image transformers. CoRR, abs/2106.08254, 2021. URL https://arxiv.org/abs/2106.08254. [2] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC’09, 2009. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efﬁcient on-device learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 11285– 11297. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 81f7acabd411274fcf65ce2070ed568a-Paper.pdf. [4] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In SemEval, 2017. [5] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. ArXiv, abs/1604.06174, 2016. [6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015. URL http://arxiv.org/abs/1504.00325. [7] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 1931–1942. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/cho21a.html. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding. In NAACL, 2019. [9] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing, January 2005. URL https://www.microsoft.com/en-us/research/publication/ automatically-constructing-a-corpus-of-sentential-paraphrases/ . [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=YicbFdNTTy. [11] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=SylO2yStDr. [12] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. ICLR, 2019. [13] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connec- tivity and the lottery ticket hypothesis. ICML, 2020. [14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Jiao Qiao. Clip-adapter: Better vision-language models with feature adapters. ArXiv, abs/2110.04544, 2021. [15] Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger Baker Grosse. The reversible residual network: Backpropagation without storing activations. In NIPS, 2017. [16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6325–6334, 2017. [17] Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efﬁcient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.378. URL https://aclanthology.org/2021.acl-long.378. 11[18] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a uniﬁed view of parameter-efﬁcient transfer learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0RDcd5Axok. [19] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. [20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. CoRR, abs/2111.06377, 2021. URL https://arxiv.org/ abs/2111.06377. [21] Xuehai He, Chengkun Li, Pengchuan Zhang, Jianwei Yang, and Xin Wang. Parameter-efﬁcient ﬁne-tuning for vision transformers. ArXiv, abs/2203.16329, 2022. [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015. URL http://arxiv.org/abs/1503.02531. cite arxiv:1503.02531Comment: NIPS 2014 Deep Learning Workshop. [23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges- mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019. [24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. URL https://arxiv.org/abs/2106.09685. [25] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [26] Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. First quora dataset release: Question pairs. 2017. URL https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs . [27] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser Nam Lim. Visual prompt tuning. ArXiv, abs/2203.12119, 2022. [28] Konwoo Kim, Michael Laskin, Igor Mordatch, and Deepak Pathak. How to adapt your large-scale vision-and-language model, 2022. URL https://openreview.net/forum?id=EhwEUb2ynIa. [29] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In Interna- tional Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= rkgNKkHtvB. [30] Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia. LIT: Learned intermediate representation training for model compression. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3509–3518. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ koratana19a.html. [31] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. In EMNLP, 2021. [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020. [33] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. ICLR, 2017. [34] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582– 4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long. 353. URL https://aclanthology.org/2021.acl-long.353. [35] Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jingliang Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Group ﬁsher pruning for practical network compression. In ICML, 2021. 12[36] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692. [37] Yitao Liu, Chen An, and Xipeng Qiu. Y-tuning: An efﬁcient tuning paradigm for large-scale pre-trained models via label representation learning. ArXiv, abs/2202.09817, 2022. [38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. [39] Rabeeh Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efﬁcient multi- task ﬁne-tuning for transformers via shared hypernetworks. In Annual Meeting of the Association for Computational Linguistics, 2021. [40] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank hypercomplex adapter layers. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=bqGK5PyI6-N. [41] Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong, Christoph Feichtenhofer, and Jitendra Malik. Reversible vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10830–10840, 2022. [42] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. Unipelt: A uniﬁed framework for parameter-efﬁcient language model tuning. CoRR, abs/2110.07577, 2021. URL https://arxiv.org/abs/2110.07577. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/ 20-074.html. [45] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In EMNLP, 2016. [46] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS, 2017. [47] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Efﬁcient parametrization of multi-domain deep neural networks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8119–8127, 2018. [48] Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. ArXiv, abs/1606.04671, 2016. [49] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170. [50] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Associ- ation for Computational Linguistics, pages 6418–6428, Florence, Italy, July 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/P19-1644. URL https://aclanthology.org/P19-1644. [51] Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with ﬁxed sparse masks. In Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=Uwh-v1HSw-x. [52] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efﬁcient transfer learning for vision- and-language tasks. CVPR, 2022. [53] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In EMNLP, 2019. 13[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [55] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Confer- ence on Learning Representations, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7. [56] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018. [57] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL, 2018. [58] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer-based masked language-models. CoRR, abs/2106.10199, 2021. URL https://arxiv.org/ abs/2106.10199. [59] Jeffrey O. Zhang, Alexander Sax, Amir Roshan Zamir, Leonidas J. Guibas, and Jitendra Malik. Side-tuning: A baseline for network adaptation via additive side networks. ECCV, 2020. [60] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. ArXiv, abs/2111.03930, 2021. [61] Zhengkun Zhang, Wenya Guo, Xiaojun Meng, Yasheng Wang, Yadao Wang, Xin Jiang, Qun Liu, and Zhenglu Yang. Hyperpelt: Uniﬁed parameter-efﬁcient language model tuning for both language and vision-and-language tasks. ArXiv, abs/2203.03878, 2022. [62] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. ArXiv, abs/2109.01134, 2021. 14A Comparison of different layer design in the ladder side network As presented in Section 3.2, our side networks are built on Transformer blocks (same as the backbone network) and we use a gating mechanism to fuse the backbone information to the side network. Before choosing this speciﬁc architecture, we also explored several variations in choosing block modules and fusion methods. First, instead of using Transformer blocks for side network, we also test the performance of Adapter blocks (two-layer bottleneck structure), which show slightly smaller memory usage than Transformer blocks. Second, instead of using a gating mechanism to aggregate information from backbone and information from lower side network layers, we explore using cross-attentions (keys and values are the backbone network’s representation while queries are the side network’s representation) to achieve the same goal. Note that this design makes the side network have an additional attention layer compared to its backbone network counterpart. We demonstrate the results in Table 4. We ﬁnd that our current side network design outperforms the others in terms of accuracy and has similar efﬁciency to adapter-based side network. We also observe that adding cross-attentions makes the training unstable and sensitive to the learning rates, making cross-attention have lower performance, even though it introduces heavy computation. Thus, we propose to use our current “Transformer block + gates” design for the ladder side tuning. Table 4: Comparison of different designs of side network using the GLUE benchmark. We run each experiment for three seeds and report the average accuracy. Side Network Design Update Param. per Task (%) Memory Usage (GB) Avg. Accuracy on GLUE (%) Adapter block + gates 2.07 6.5 83.1 0.3 Transformer block + cross attention 2.68 10.4 83.0 0.2 Transformer block + gates (current design) 2.29 7.0 83.8 0.5 B Hyper-parameters We put the hyper-parameters used for NLP experiments (Table 1) in Table 5 and VL experiments (Table 3) in Table 6. Table 5: Hyper-parameters used for NLP experiments. Batch size is 100 for all methods. Method Learning Rate Other Hyper-parameters Full ﬁne-tuning 3×10−4 -Adapters 3×10−4 hidden dimension=48LoRA 3×10−4 rank=32BitFit 3×10−4 -Prompt-tuning 3×10−1 number of prompts=100 Ladder Side-Tuning 3×10−3 r=8; index of layers are kept in side encoder and decoder=1,2,3,5,6,7,9,10,11 (drop 3 layers each)Ladder Side-Tuning (T5-large)3×10−3 r=8; index of layers are kept in side encoder and decoder=1,3,5,7,9,11,13,15,17,19,21,23 (drop 12 layers each)Ladder Side-Tuning (T5-3B)3×10−3 r=8; index of layers are kept in side encoder and decoder=23 (drop 23 layers each) Table 6: Hyper-parameters used for VL experiments. Batch size is 300 for all methods. Method Learning Rate Other Hyper-parameters Full ﬁne-tuning 3×10−4 - Adapters 3×10−4 hidden dimension=192 LoRA 3×10−4 rank=150 BitFit 3×10−3 - Prompt-tuning 3×10−3 number of prompts=40 Ladder Side-Tuning3×10−3 r=4; all layers in side networks are kept 15",
      "meta_data": {
        "arxiv_id": "2206.06522v2",
        "authors": [
          "Yi-Lin Sung",
          "Jaemin Cho",
          "Mohit Bansal"
        ],
        "published_date": "2022-06-13T23:51:56Z",
        "pdf_url": "https://arxiv.org/pdf/2206.06522v2.pdf",
        "github_url": "https://github.com/ylsung/Ladder-Side-Tuning"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of high memory requirements for fine-tuning large pre-trained models, a limitation not fully resolved by existing parameter-efficient transfer learning (PETL) methods. It proposes Ladder Side-Tuning (LST), a novel PETL technique that substantially reduces training memory by not requiring backpropagation through the large backbone network. LST trains a small, separate ladder side network that takes intermediate activations from the frozen backbone via shortcut \"ladders\" and makes predictions. Key findings show LST saves 69% of memory compared to full fine-tuning (2.7x more than Adapter and LoRA) while achieving competitive or superior accuracy on GLUE NLP tasks and various vision-and-language tasks (VQA, GQA, NLVR2, MSCOCO) using T5 and CLIP-T5 models, especially in low-memory regimes. It also demonstrates scalability to larger T5 models (T5-large, T5-3B) and improved efficiency through layer dropping and structural weight initialization.",
        "methodology": "LST employs a separate, lightweight \"ladder side network\" which is a scaled-down version of the backbone transformer (with a reduction factor 'r'). This side network reuses frozen word embeddings and the language model head from the backbone. It incorporates \"gated ladder connections,\" which are shortcut connections that feed intermediate activations from the frozen backbone network into the side network. These activations are linearly projected to match the side network's lower dimensions, and a learned gating mechanism (\"\b5i*hf_i +(1 \b2 \b5i)*hg_i-1\") combines backbone information with the side network's internal representations. For initialization, LST uses \"structural weight initialization\" where side network weights are derived from a pruned version of the backbone network's weights, based on importance scores (e.g., Fisher Information or weight magnitude). Additionally, \"layer dropping\" is applied to the side network to further enhance parameter and memory efficiency by removing intermediate transformer layers.",
        "experimental_setup": "LST was evaluated on Natural Language Processing (NLP) tasks using the GLUE benchmark (CoLA, SST-2, MRPC, QQP, MNLI, QNLI, RTE, STS-B) and Vision-and-Language (VL) tasks including Visual Question Answering (VQA, GQA), Visual Reasoning (NLVR2), and Image Captioning (MSCOCO). The backbone models used were T5 (T5-base, T5-large, T5-3B) for NLP and CLIP-T5 for VL tasks (with CLIP frozen). Baselines included Full fine-tuning, Adapters, LoRA, BitFit, and Prompt-tuning, as well as comparisons to the concurrent Y-tuning and the related Side-Tuning method. Evaluation metrics varied by task: accuracy for SST-2, MNLI, QNLI, RTE, VQA, GQA, NLVR2; Matthew's Correlation for CoLA; Pearson-Spearman Correlation for STS-B; and average F1/Accuracy for MRPC, QQP; and CIDEr for MSCOCO. Training involved 10 or 20 epochs, with learning rates searched and a reduction factor 'r' of 8 for NLP and 4 for VL. Layer dropping was applied to LST for larger T5 models to match memory usage of baselines. Ablation studies were conducted on initialization strategies (Random, Weight Magnitude, Fisher Information), side network layer designs (Transformer blocks with gates/cross-attention, Adapter blocks with gates), and the utility of intermediate shortcut connections.",
        "limitations": "The paper extensively focuses on demonstrating LST's advantages and how it overcomes the limitations of previous PETL methods, rather than explicitly stating its own weaknesses or constraints. The closest to a limitation mentioned is that during inference, LST involves forward propagation through two distinct networks (backbone and side network), but the authors immediately mitigate this by stating it \"does not necessarily use more inference time because the same level of the backbone network and the side network can be computed in parallel.\" No other explicit limitations, weaknesses, or assumptions of LST are detailed within the paper.",
        "future_research_directions": "A key future research direction explicitly mentioned is the potential to combine LST with other memory-efficient training methods, such as reversible neural networks and gradient checkpointing. The authors state that these methods are \"agnostic to memory saving from LST,\" suggesting that combining them could lead to \"a higher level of memory efficiency.\" The paper also expresses a general hope that LST will help users with limited computational resources tune larger models across diverse domains.",
        "experimental_code": "def construct_side_network(self, config, side_config, num_layers, LAYER_TYPE, gate_name):\n        side_transformers_config = copy.deepcopy(config)\n\n        side_transformers_config.intermediate_size = side_transformers_config.intermediate_size // side_config.reduction_factor\n        side_transformers_config.hidden_size = side_transformers_config.hidden_size // side_config.reduction_factor\n\n        side_layers = side_config.encoder_side_layers\n\n        if side_config.add_side_visual_projection:\n            self.side_visn_fc = VisualFeatEncoder(config)\n        else:\n            self.side_visn_fc = None\n\n        if side_layers is None:\n            # add every layer\n            side_layers = list(range(num_layers))\n        else:\n            side_layers = eval(side_layers)\n\n        side_block = nn.ModuleList(\n            [LAYER_TYPE(side_transformers_config)\n                    if i in side_layers else None\n                    for i in range(num_layers)]\n        )\n\n        self.add_residual_after = side_config.add_residual_after\n        \n        side_first_downsample = nn.Linear(config.hidden_size, side_transformers_config.hidden_size, bias=False)\n\n        if side_config.side_downsample_pool:\n            side_downsamples = nn.ModuleList(\n                [nn.AdaptiveAvgPool1d(side_transformers_config.hidden_size)\n                if i in side_layers else None\n                for i in range(num_layers)]\n            )\n        else:\n            side_downsamples = nn.ModuleList(\n                [nn.Linear(config.hidden_size, side_transformers_config.hidden_size, bias=False) \n                if i in side_layers else None\n                for i in range(num_layers)]\n            )\n\n        side_final_upsample = nn.Linear(side_transformers_config.hidden_size, config.hidden_size, bias=False)\n\n        self.use_gate = side_config.use_gate\n        side_gate_params = None\n        if self.use_gate == \"learnable\":\n            # Parameter list cannot contain None\n            side_gate_params = nn.ParameterList(\n                [nn.Parameter(torch.ones(1) * side_config.gate_alpha) \n                for i in range(num_layers)]\n            )\n\n            self.gate_T = side_config.gate_T\n        elif \"schedule\" in self.use_gate or self.use_gate == \"one\":\n            for i in range(num_layers):\n                self.register_buffer(f'side_gate_{gate_name}' + str(i), torch.ones(1))\n        elif self.use_gate == \"none\":\n            pass\n        else:\n            raise NotImplementedError\n\n        return side_block, side_first_downsample, side_final_upsample, side_downsamples, side_gate_params\n\n    def merge_backbone_to_side(self, side_hidden_states, backbone_hidden_states, gate_name, idx):\n        if self.use_gate == \"learnable\":\n            side_gate_param = getattr(self, f\"side_gate_params_{gate_name}\")[idx]\n            gate = torch.sigmoid(side_gate_param / self.gate_T)\n            side_hidden_states = gate * side_hidden_states + (1 - gate) * backbone_hidden_states # add the information from the backbone network\n        elif \"schedule\" in self.use_gate or self.use_gate == \"one\":\n            gate = getattr(self, f\"side_gate_{gate_name}{idx}\")\n            side_hidden_states = gate * side_hidden_states + (1 - gate) * backbone_hidden_states\n        else:\n            side_hidden_states = side_hidden_states + backbone_hidden_states\n\n        return side_hidden_states\n\n    def forward(self, lang_feats, lang_attention_mask,\n                visn_feats, visn_attention_mask=None, task=None):\n        if VISUAL_CONFIG.vilt_style:\n            assert(not VISUAL_CONFIG.freeze_clip)\n            if VISUAL_CONFIG.use_clip:\n                images, boxes = visn_feats\n                lang_attention_mask = lang_attention_mask.squeeze(1).squeeze(1)\n                lang_attention_mask[lang_attention_mask!=0] = float(\"-inf\")\n\n                joint_feats = self.visual_model.visual(images.type(self.visual_model.dtype), skip_last_layer=True, text_embedding = lang_feats, text_mask=lang_attention_mask)\n                return _split_with_none(lang_feats, images, joint_feats)\n            elif VISUAL_CONFIG.use_vit:\n                images, boxes = visn_feats\n                joint_feats = self.visual_model(\n                    images,\n                    return_features=True,\n                    text_embedding=lang_feats,\n                    text_mask=lang_attention_mask)\n                return _split_with_none(lang_feats, images, joint_feats)\n\n        if VISUAL_CONFIG.use_clip:\n            images, boxes = visn_feats\n            visn_feats = self.visual_model.visual(images.type(self.visual_model.dtype), skip_last_layer=True)\n\n            if \"RN\" in VISUAL_CONFIG.clip_model_name:\n                if VISUAL_CONFIG.use_max_pooling:\n                    visn_feats = self.max_pooling(visn_feats)\n\n                if VISUAL_CONFIG.use_positional_embedding:\n                    visn_feats = self.visual_pos(visn_feats)\n                else:\n                    visn_feats = visn_feats.permute(0, 2, 3, 1).view(visn_feats.size(0), -1, visn_feats.size(1))\n                \n            # Cast back to fp32\n            visn_feats = visn_feats.to(dtype=next(self.visn_fc.parameters()).dtype)\n        elif VISUAL_CONFIG.use_vit:\n            images, boxes = visn_feats\n            visn_feats = self.visual_model(images, return_features=True)\n            visn_feats = visn_feats.to(dtype=next(self.visn_fc.parameters()).dtype)\n        elif VISUAL_CONFIG.drop_boxes:\n            visn_feats, boxes = visn_feats\n        \n        if VISUAL_CONFIG.sub_sampling:\n            # visn_feats: batch x seq_len x 768\n\n            sub_feat_num = VISUAL_CONFIG.sub_feat_num\n            sampled_index = []\n            for i in range(visn_feats.size(0)):\n                sampled_index.append(torch.from_numpy(np.random.choice(visn_feats.size(1), sub_feat_num, replace=False)))\n            sampled_index = torch.stack(sampled_index, dim=0).unsqueeze(-1).expand(visn_feats.size(0), sub_feat_num, visn_feats.size(2)).long().to(visn_feats.device)  # batch x sub_feat_num x 768?\n            visn_feats = torch.gather(visn_feats, 1, sampled_index)\n            \n        # Run visual embedding layer\n        # Note: Word embedding layer was executed outside this module.\n        #       Keep this design to allow loading BERT weights.\n\n        pre_visn_feats = visn_feats\n        visn_feats = self.visn_fc(visn_feats)\n\n        if visn_attention_mask is None:\n            visn_attention_mask = torch.zeros(visn_feats.size(0), visn_feats.size(1)).to(dtype=next(self.visn_fc.parameters()).dtype).to(next(self.visn_fc.parameters()).device)\n            visn_attention_mask = visn_attention_mask.unsqueeze(1).unsqueeze(2)\n\n        if VISUAL_CONFIG.visualbert_style:\n            joint_feats = _cat_with_none(lang_feats, visn_feats, dim=1) #torch.cat((lang_feats, visn_feats), dim=1)\n            joint_mask = _cat_with_none(lang_attention_mask, visn_attention_mask, dim=-1)  #torch.cat((lang_attention_mask, visn_attention_mask), dim=-1)\n            all_attention_weights = []\n\n            if self.train_side_transformer and self.side_visn_fc is not None:\n                side_visn_feats = self.side_visn_fc(pre_visn_feats)\n                side_joint_feats = _cat_with_none(lang_feats, side_visn_feats, dim=1)\n            else:\n                side_joint_feats = joint_feats\n\n            side_hidden_states = self.side_first_downsample_l(side_joint_feats) if self.train_side_transformer else None\n\n            if self.detach_visual_projection:\n                joint_feats = joint_feats.detach()\n            for idx, layer_module in enumerate(self.layer):\n                #if args.get(\"output_attention\", False):\n                #    joint_feats, attention_weights = layer_module(joint_feats, joint_mask)\n                #    all_attention_weights.append(attention_weights)\n                #else:\n                joint_feats = layer_module(joint_feats, joint_mask, task)\n\n                if self.train_side_transformer:\n                    side_layer_module = self.side_block_l[idx]\n\n                    if side_layer_module is not None:\n                        if not self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_l[idx](joint_feats)\n                            side_hidden_states = self.merge_backbone_to_side(side_hidden_states, backbone_hidden_states, \"l\", idx)\n                        \n                        side_hidden_states = side_layer_module(side_hidden_states, joint_mask, task)\n\n                        if self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_l[idx](joint_feats)\n                            side_hidden_states = self.merge_backbone_to_side(side_hidden_states, backbone_hidden_states, \"l\", idx)\n\n            if self.train_side_transformer:\n                joint_feats = self.side_final_upsample_l(side_hidden_states)\n            #if args.get(\"output_attention\", False):\n            #    return _split_with_none(lang_feats, visn_feats, joint_feats), all_attention_weights\n            return _split_with_none(lang_feats, visn_feats, joint_feats)\n        else:\n            # Run language layers\n\n            side_lang_feats = self.side_first_downsample_l(lang_feats) if self.train_side_transformer else None\n\n            for idx, layer_module in enumerate(self.layer):\n                lang_feats = layer_module(lang_feats, lang_attention_mask, task)\n\n                if self.train_side_transformer:\n                    side_layer_module = self.side_block_l[idx]\n\n                    if side_layer_module is not None:\n                        if not self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_l[idx](lang_feats)\n                            side_lang_feats = self.merge_backbone_to_side(side_lang_feats, backbone_hidden_states, \"l\", idx)\n\n                        side_lang_feats = side_layer_module(side_lang_feats, joint_mask, task)\n\n                        if self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_l[idx](lang_feats)\n                            side_lang_feats = self.merge_backbone_to_side(side_lang_feats, backbone_hidden_states, \"l\", idx)\n\n            side_visn_feats = self.side_first_downsample_r(visn_feats) if self.train_side_transformer else None\n\n            # Run relational layers\n            for idx, layer_module in enumerate(self.r_layers):\n                visn_feats = layer_module(visn_feats, visn_attention_mask, task)\n\n                if self.train_side_transformer:\n                    side_layer_module = self.side_block_r[idx]\n                    if side_layer_module is not None:\n                        if not self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_r[idx](visn_feats)\n                            side_visn_feats = self.merge_backbone_to_side(side_visn_feats, backbone_hidden_states, \"r\", idx)  \n\n                        side_visn_feats = side_layer_module(side_visn_feats, joint_mask, task)\n\n                        if self.add_residual_after:\n                            backbone_hidden_states = self.side_downsamples_r[idx](visn_feats)\n                            side_visn_feats = self.merge_backbone_to_side(side_visn_feats, backbone_hidden_states, \"r\", idx)\n\n            # Run cross-modality layers\n            for idx, layer_module in enumerate(self.x_layers):\n                lang_feats, visn_feats = layer_module(lang_feats, lang_attention_mask,\n                                                    visn_feats, visn_attention_mask, task)\n\n                if self.train_side_transformer:\n                    side_layer_module = self.side_block_x[idx]\n                    if side_layer_module is not None:\n                        if not self.add_residual_after:\n                            side_lang_feats = self.merge_backbone_to_side(side_lang_feats, self.side_downsamples_x[idx](lang_feats), \"x\", idx)\n                            side_visn_feats = self.merge_backbone_to_side(side_visn_feats, self.side_downsamples_x[idx](visn_feats), \"x\", idx)\n                        \n                        side_lang_feats, side_visn_feats = side_layer_module(side_lang_feats, lang_attention_mask,\n                                                                            side_visn_feats, visn_attention_mask, task)\n\n                        if self.add_residual_after:\n                            side_lang_feats = self.merge_backbone_to_side(side_lang_feats, self.side_downsamples_x[idx](lang_feats), \"x\", idx)\n                            side_visn_feats = self.merge_backbone_to_side(side_visn_feats, self.side_downsamples_x[idx](visn_feats), \"x\", idx)\n\n            if self.train_side_transformer:\n                lang_feats = self.side_final_upsample_l(side_lang_feats)\n                visn_feats = self.side_final_upsample_r(side_visn_feats)\n\n        return lang_feats, visn_feats",
        "experimental_info": "The Ladder Side Network (LST) employs a scaled-down version of the backbone transformer, integrating 'gated ladder connections' and 'structural weight initialization' with optional 'layer dropping'.\n\n**Network Structure & Integration (from `CLIP-ViL/src/lxrt/modeling.py`):**\n- `LXRTEncoder.construct_side_network` method builds the side network components:\n    - `side_transformers_config`: A deep copy of the main model's config, with `intermediate_size` and `hidden_size` reduced by `side_config.reduction_factor`.\n    - `side_block`: An `nn.ModuleList` containing `LAYER_TYPE` (e.g., `BertLayer`, `LXRTXLayer`) instances for specified `encoder_side_layers`. If `encoder_side_layers` is None, all layers are added.\n    - `side_first_downsample`: A linear layer to project the backbone's initial hidden state to the side network's reduced dimension.\n    - `side_downsamples`: A list of linear layers or `nn.AdaptiveAvgPool1d` (if `side_config.side_downsample_pool` is True) to project intermediate backbone hidden states to the side network's dimension.\n    - `side_final_upsample`: A linear layer to project the side network's final hidden state back to the backbone's dimension.\n    - `side_gate_params`: If `use_gate` is 'learnable', an `nn.ParameterList` of learnable gate parameters, initialized with `side_config.gate_alpha`.\n- `LXRTEncoder.merge_backbone_to_side` method implements the gated ladder connections:\n    - Combines `side_hidden_states` and `backbone_hidden_states` using a learned gate `gate * side_hidden_states + (1 - gate) * backbone_hidden_states`.\n    - The `gate` can be 'learnable' (sigmoid of `side_gate_param / gate_T`), 'schedule', 'one', or 'none'.\n- The `LXRTEncoder.forward` method integrates the side network's computation within the main transformer layers (language, relational, and cross-modality layers) by conditionally updating `side_lang_feats`, `side_visn_feats`, or `side_hidden_states` and merging them with backbone features at each step.\n\n**Structural Weight Initialization (from `CLIP-ViL/src/pruning/`):**\n- `pruning_without_residual` function (from `pruning_methods_bert.py`) performs the actual pruning of backbone weights to initialize the side network.\n    - It takes a `model`, `reduction_factor`, `importance_measure` (optional, for score-based pruning), `version`, `num_heads`, and `iterations`.\n    - It uses `L1Strategy` or `AttnL1Strategy` (depending on `version`) to select pruning indices.\n    - The function iterates through ordered layers, applying `select_weights` to prune parameters based on calculated importance scores and `prune_val` (derived from `reduction_factor`).\n- `compute_fisher` function (from `fisher.py`) calculates importance scores (Fisher Information) for structural weight initialization.\n    - It takes a `model`, `task`, `train_dataset`, `data_collator`, `num_samples`, `cuda_device`, and `grad_type` ('square' or 'absolute').\n    - It computes gradients for a subset of samples and accumulates `grad_method(param.grad).data` in `gradients_dict`.\n\n**Configuration & Experimental Settings:**\n\n**`CLIP-ViL/src/lxrt/side_transformers/config.py` (SideConfig defaults):**\n- `task_reduction_factor`: 8 (reduction factor 'r').\n- `encoder_side_layers`: `None` (all layers are considered).\n- `add_residual_after`: `False`.\n- `use_gate`: `\"learnable\"`.\n- `gate_alpha`: `0`.\n- `gate_T`: `0.1`.\n- `side_downsample_pool`: `False`.\n\n**`CLIP-ViL/src/param.py` (Command-line arguments from `parse_args`):**\n- `--use_side_transformers`: `action='store_true'` to enable the side network.\n- `--reduction_factor`: `type=int, default=16` (used for `task_reduction_factor`).\n- `--encoder_side_layers`: `type=str, default=None` (e.g., `'[0, 2, 4]'` for layer dropping).\n- `--use_gate`: `action='store_true'` (enables a learnable gate by default) or can be other values like 'one', 'schedule', 'none'.\n- `--load_side_pretrained_weights`: `default=''` (path to load pruned weights, e.g., containing 'fisher' or 'bert' keyword for importance-based initialization).\n- `--samples_for_fisher`: `type=int, default=1024` (number of samples to use for Fisher Information calculation).\n- `--detach_visual_projection`: `action='store_true'`.\n- `--add_side_visual_projection`: `action='store_true'`.\n- `--add_residual_after`: `action='store_true'`.\n- `--gate_alpha`: `type=float, default=0`.\n- `--gate_T`: `type=float, default=0.1`.\n- `--side_downsample_pool`: `action='store_true'`.\n\n**Initialization and Training Flow (e.g., from `src/tasks/gqa.py` `Trainer` class):**\n- `Trainer.create_side_network_initial_weights` method:\n    - Temporarily sets `self.args.use_side_transformers = False` to instantiate a base model (without the side network) for importance calculation.\n    - If `\"fisher\"` is in `self.args.load_side_pretrained_weights`, it calls `compute_fisher` to calculate importance measures.\n    - Calls `pruning_without_residual` (or similar method) on the base model with the specified `reduction_factor` and `importance_measure` to obtain `pruned_state_dict`.\n    - Deletes the temporary base model to save memory.\n- `Trainer.initialize_side_network` method (from `src/tasks/trainer_base.py`):\n    - Copies pruned weights from `pruned_state_dict` to the newly initialized side network modules (`side_visn_fc`, `side_block_l`, `side_block_r`, `side_block_x`) in the main model.\n- `Trainer.unfreeze_parameters` method (from `src/tasks/trainer_base.py`):\n    - Unfreezes all parameters with \"side\" in their name, making them trainable. Other components like `logit_fc` and biases are also made trainable under certain conditions.\n- `Trainer.print_trainable_params_percentage` (from `src/tasks/trainer_base.py`): Reports the percentage of trainable parameters after freezing/unfreezing, providing an efficiency metric for LST."
      }
    },
    {
      "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
      "abstract": "Multi-Task Learning (MTL) networks have emerged as a promising method for\ntransferring learned knowledge across different tasks. However, MTL must deal\nwith challenges such as: overfitting to low resource tasks, catastrophic\nforgetting, and negative task transfer, or learning interference. Often, in\nNatural Language Processing (NLP), a separate model per task is needed to\nobtain the best performance. However, many fine-tuning approaches are both\nparameter inefficient, i.e., potentially involving one new model per task, and\nhighly susceptible to losing knowledge acquired during pretraining. We propose\na novel Transformer architecture consisting of a new conditional attention\nmechanism as well as a set of task-conditioned modules that facilitate weight\nsharing. Through this construction (a hypernetwork adapter), we achieve more\nefficient parameter sharing and mitigate forgetting by keeping half of the\nweights of a pretrained model fixed. We also use a new multi-task data sampling\nstrategy to mitigate the negative effects of data imbalance across tasks. Using\nthis approach, we are able to surpass single task fine-tuning methods while\nbeing parameter and data efficient (using around 66% of the data for weight\nupdates). Compared to other BERT Large methods on GLUE, our 8-task model\nsurpasses other Adapter methods by 2.8% and our 24-task model outperforms by\n0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger\nvariant of our single multi-task model approach performs competitively across\n26 NLP tasks and yields state-of-the-art results on a number of test and\ndevelopment sets. Our code is publicly available at\nhttps://github.com/CAMTL/CA-MTL.",
      "full_text": "Published as a conference paper at ICLR 2021 CONDITIONALLY ADAPTIVE MULTI -TASK LEARNING : IMPROVING TRANSFER LEARNING IN NLP USING FEWER PARAMETERS & LESS DATA Jonathan Pilault1∗, Amine El hattami1∗, Christopher Pal1,2,3 1Polytechnique Montreal & Mila, 2Element AI, 3Canada CIFAR AI Chair {jonathan.pilault,amine.elhattami,christopher.pal}@polymtl.ca ABSTRACT Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overﬁtting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best perfor- mance. However, many ﬁne-tuning approaches are both parameter inefﬁcient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efﬁcient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model ﬁxed. We also use a new multi-task data sam- pling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task ﬁne-tuning methods while being parameter and data efﬁcient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task ﬁne-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL. 1 I NTRODUCTION The introduction of deep, contextualized Masked Language Models (MLM) 1 trained on massive amounts of unlabeled data has led to signiﬁcant advances across many different Natural Language Processing (NLP) tasks (Peters et al., 2018; Liu et al., 2019a). Much of these recent advances can be attributed to the now well-known BERT approach (Devlin et al., 2018). Substantial improvements over previous state-of-the-art results on the GLUE benchmark (Wang et al., 2018) have been obtained by multiple groups using BERT models with task speciﬁc ﬁne-tuning. The “BERT-variant + ﬁne-tuning” formula has continued to improve over time with newer work constantly pushing the state-of-the-art forward on the GLUE benchmark. The use of a single neural architecture for multiple NLP tasks has shown promise long before the current wave of BERT inspired methods (Collobert & Weston, 2008) and recent work has argued that autoregressive language models (ARLMs) trained on large-scale datasets – such as the GPT family of models (Radford et al., 2018), are in practice multi-task learners (Brown et al., 2020). However, even with MLMs and ARLMs trained for multi-tasking, single task ﬁne-tuning is usually also employed to achieve state-of-the-art performance on speciﬁc tasks of interest. Typically this ﬁne-tuning process may entail: creating a task-speciﬁc ﬁne-tuned model (Devlin et al., 2018), training specialized model components for task-speciﬁc predictions (Houlsby et al., 2019) or ﬁne-tuning a single multi-task architecture (Liu et al., 2019b). ∗Joint ﬁrst-authors 1For reader convenience, all acronyms in this paper are summarized in section A.1 of the Appendix. 1 arXiv:2009.09139v3  [cs.LG]  21 Apr 2022Published as a conference paper at ICLR 2021 Figure 1: CA-MTL base architecture with our uncertainty-based sampling algorithm. Each task has its own decoder. The input embedding layer and the lower Transformer layers are frozen. The up- per Transformer layer and Conditional Alignment module are modulated with the task embedding. Single-task ﬁne-tuning overall pretrained model parameters may have other issues. Recent analy- ses of such MLM have shed light on the linguistic knowledge that is captured in the hidden states and attention maps (Clark et al., 2019b; Tenney et al., 2019a; Merchant et al., 2020). Particularly, BERT has middle Transformer (Vaswani et al., 2017) lay- ers that are typically the most transferable to a downstream task (Liu et al., 2019a). The model proxies the steps of the traditional NLP pipeline in a localizable way (Tenney et al., 2019a) — with basic syntactic information appearing earlier in the network, while high-level semantic information ap- pearing in higher-level layers. Since pretraining is usually done on large-scale datasets, it may be use- ful, for a variety of downstream tasks, to conserve that knowledge. However, single task ﬁne-tuning causes catastrophic forgetting of the knowledge learned during MLM (Howard & Ruder, 2018). To preserve knowledge, freezing part of a pretrained network and using Adapters for new tasks have shown promising results (Houlsby et al., 2019). Inspired by the human ability to transfer learned knowledge from one task to another new task, Multi-Task Learning (MTL) in a general sense (Caruana, 1997; Rajpurkar et al., 2016b; Ruder, 2017) has been applied in many ﬁelds outside of NLP. Caruana (1993) showed that a model trained in a multi-task manner can take advantage of the inductive transfer between tasks, achieving a better generalization performance. MTL has the advantage of computational/storage efﬁciency (Zhang & Yang, 2017), but training models in a multi-task setting is a balancing act; particularly with datasets that have different: (a) dataset sizes, (b) task difﬁculty levels, and (c) different types of loss functions. In practice, learning multiple tasks at once is challenging since negative transfer (Wang et al., 2019a), task interference (Wu et al., 2020; Yu et al., 2020) and catastrophic forgetting (Serrà et al., 2018) can lead to worse data efﬁciency, training stability and generalization compared to single task ﬁne-tuning. Using Conditionally Adaptive Learning, we seek to improve pretraining knowledge retention and multi-task inductive knowledge transfer. Our contributions are the following: • A new task conditioned Transformer that adapts and modulates pretrained weights (Section 2.1). • A novel way to prioritize tasks with an uncertainty based multi-task data sampling method that helps balance the sampling of tasks to avoid catastrophic forgetting (Section 2.2). Our Conditionally Adaptive Multi-Task Learning (CA-MTL) approach is illustrated in Figure 1. To the best of our knowledge, our work is the ﬁrst to explore the use of a latent representation of tasks to modularize and adapt pretrained architectures. Further, we believe our work is also the ﬁrst to examine uncertainty sampling for large-scale multi-task learning in NLP. We show the efﬁcacy of CA-MTL by: (a) testing on 26 different tasks and (b) presenting state-of-the-art results on a number of test sets as well as superior performance against both single-task and MTL baselines. Moreover, we further demonstrate that our method has advantages over (c) other adapter networks, and (d) other MTL sampling methods. Finally, we provide ablations and separate analysis of the MT-Uncertainty Sampling technique in section 4.1 and of each component of the adapter in 4.2. 2 M ETHODOLOGY This section is organized according to the two main MTL problems that we will tackle: (1) How to modularize a pretrained network with latent task representations? (2) How to balance different tasks in MTL? We deﬁne each task as: Ti ≜ {pi(yi|xi,zi),Li,˜pi(xi)}, where zi is task i’s learnable shallow embedding, Li is the task loss, and ˜pi(xi) is the empirical distribution of the training data pair {xi,yi}, for i∈{1,...,T }and T the number of supervised tasks. The MTL objective is: min φ(z),θ1,...,θT T∑ i=1 Li(fφ(zi),θi(xi),yi) (1) 2Published as a conference paper at ICLR 2021 where f is the predictor function (includes encoder model and decoder heads), φ(z) are learnable generated weights conditioned on z, and θi are task-speciﬁc parameters for the output decoder heads. z is constructed using an embedding lookup table. 2.1 T ASK CONDITIONED TRANSFORMER Our task conditioned Transformer architecture is based on one simple concept. We either add conditional layers or modulate existing pretrained weights using a task representation by extending Feature Wise Linear Modulation (Perez et al., 2018) functions in several ways depending on the Transformer layer. We deﬁne our framework below. Deﬁnition 1 (Conditional Weight Transformations). Given a neural network weight matrixW, we compute transformations of the formφ(W|zi) = γi(zi)W + βi(zi), whereγi and βi are learned functions that transform the weights based on a learned vector embeddingzi, for taski. Deﬁnition 2 (Conditionally Adaptive Learning). In our setting, Conditionally Adaptive Learning is the process of learning a set ofφs for the conditionally adaptive modules presented below along with a set of task embedding vectorszi for T tasks, using a multi-task loss (see equation 1). In the subsections that follow: We introduce a new Transformer Attention Module using block- diagonal Conditional Attention that allows the original query-key based attention to account for task-speciﬁc biases (section 2.1.1). We propose a new Conditional Alignment method that aligns the data of diverse tasks and that performs better than its unconditioned and higher capacity predecessor (section 2.1.2). We adapt layer normalization statistics to speciﬁc tasks using a new Conditional Layer Normalization module (section 2.1.3). We add a Conditional Bottleneck that facilitates weight sharing and task-speciﬁc information ﬂow from lower layers (section 2.1.4). In our experiments we provide an ablation study of these components (Table 1) examining performance in terms of GLUE scores. 2.1.1 C ONDITIONAL ATTENTION Figure 2: Conditional Attention Module Given d, the input dimensions, the query Q, the key K, and the value V as deﬁned in Vaswani et al. (2017), we redeﬁne the attention operation: Attention(Q,K,V,zi)) = softmax [ M(zi) + QKT √ d ] V M(zi) = N⨁ n=1 A′ n(zi), A ′ n(zi) = Anγi(zi) + βi(zi) where ⨁is the direct sum operator (see section A.6), N is the number of block matrices An ∈R(L/N)×(L/N) along the diagonal of the attention matrix, Lis the input sequence, M(zi) = diag(A′ 1,...,A ′ N) is a block diagonal conditional matrix. Note that An is constructed using L/N trainable and randomly initialized L/N dimensional vectors. While the original attention matrix depends on the hidden states h, M(zi) is a learnable weight matrix that only depends on the task embedding zi ∈Rd. γi,βi : Rd ↦→RL2/N2 are Feature Wise Linear Modulation (Perez et al., 2018) functions. We also experimented with full-block Conditional Attention ∈RL×L. Not only did it have N2 more parameters compared to the block-diagonal variant, but it also performed signiﬁcantly worse on the GLUE development set (see FBA variant in Table 10). It is possible that GLUE tasks derive a certain beneﬁt from localized attention that is a consequence of M(zi). With M(zi), each element in a sequence can only attend to other elements in its subsequence of length L/N. In our experiments we used N = d/L. The full Conditional Attention mechanism used in our experiments is illustrated in Figure 2. 2.1.2 C ONDITIONAL ALIGNMENT Wu et al. (2020) showed that in MTL havingT separate alignment modules R1,...,R T increases BERTLARGE avg. scores on ﬁve GLUE tasks (CoLA, MRPC, QNLI, RTE, SST-2) by 2.35%. Inspired by this work, we found that adding a task conditioned alignment layer between the input embedding 3Published as a conference paper at ICLR 2021 layer and the ﬁrst BERT Transformer layer improved multi-task model performance. However, instead of having T separate alignment matrices Ri for each T task, one alignment matrix ˆR is generated as a function of the task embedding zi. As in Wu et al. (2020), we tested this module on the same ﬁve GLUE tasks and with BERTLARGE. Enabling task conditioned weight sharing across covariance alignment modules allows us to outperformsBERTLARGE by 3.61%. This is 1.26 % higher than having T separate alignment matrices. Inserting ˆRinto BERT, yields the following encoder function ˆf: ˆf = T∑ t=1 gθi(E(xi) ˆR(zi)B), ˆR(zi) = Rγi(zi) + βi(zi) (2) where xi ∈Rd is the layer input, gθi is the decoder head function for task iwith weights θi, Ethe frozen BERT embedding layer, Bthe BERT Transformer layers and Rthe linear weight matrix of a single task conditioned alignment matrix. γi,βi : Rd ↦→Rd are Feature Wise Linear Modulation functions. 2.1.3 C ONDITIONAL LAYER NORMALIZATION (CLN) We extend the Conditional Batch Normalization idea from de Vries et al. (2017) to Layer Normaliza- tion (Ba et al., 2016). For task Ti, i∈{1,...,T }: hi = 1 σ ⊙(ai −µ) ∗ˆγi(zi) + βi(zi), ˆγi(zi) = γ′γi(zi) + β′ (3) where hi is the CLN output vector, ai are the preceding layer activations associated with task i, µ and σare the mean and the variance of the summed inputs within each layer as deﬁned in Ba et al. (2016). Conditional Layer Normalization is initialized with BERT’s Layer Normalization afﬁne transformation weights and bias γ′and β′from the original formulation: h = 1 σ ⊙(a −µ) ∗γ′+ β′. During training, the weight and bias functions of γi(∗) and βi(∗) are always trained, while the original Layer Normalization weight may be kept ﬁxed. This module was added to account for task speciﬁc rescaling of individual training cases. Layer Normalization normalizes the inputs across features. The conditioning introduced in equation 2.1.3 allows us to modulate the normalization’s output based on a task’s latent representation. 2.1.4 C ONDITIONAL BOTTLENECK Figure 3: a) Conditional Bottleneck for CA-MTLBASE. b) Conditional Bottleneck for CA-MTLLARGE. We created a task conditioned two layer feed-forward bot- tleneck layer (CFF up/down in Figure 3). The conditional bottleneck layer follows the same transformation as in equation 2. The module in Figure 3a is added to the top most Transformer layers ofCA-MTLBASE and uses a CLN. For CA-MTLLARGE this module is the main building block of the skip connection added alongside all Transformer layers seen in Figure 3b. The connection at layer jtakes in the matrix sum of the Transformer layer output at j and the previous connection’s output atj−1. The Con- ditional bottleneck allows lower layer information to ﬂow upwards depending on the task. Our intuition for intro- ducing this component is related to recent studies (Tenney et al., 2019a) that showed that the “most important layers for a given task appear at speciﬁc positions”. As with the other modules described so far, each task adaptation is created from the weights of a single shared adapter that is modulated by the task embedding. 2.2 M ULTI -TASK UNCERTAINTY SAMPLING MT-Uncertainty Sampling is a task selection strategy that is inspired by Active Learning techniques. Our algorithm 1 is outlined in the Appendix, Section A.2. Similar to Active Learning, our algorithm ﬁrst evaluates the model uncertainty. MT-Uncertainty Sampling uses Shannon Entropy, an uncertainty measure, to choose training examples by ﬁrst doing forward pass through the model with b×T input samples. For an output classiﬁcation prediction with Ci possible classes and probabilities 4Published as a conference paper at ICLR 2021 (pi,1,...,p i,Ci), the Shannon Entropy Hi, for task Ti and i∈{1,...,T }, our uncertainty measure U(x) are given by: Hi = Hi(fφ(zi),θi(x)) = − Ci∑ c=1 pc log pc, U(xi) = Hi(fφ(zi),θi(x)) ˆH×H′ i (4) ˆH = max i∈{1,...,T} ¯Hi = max [ 1 b ∑ x∈xi Hi ] , H ′ i = − Ci∑ c=1 1 Ci log [ 1 Ci ] (5) where ¯Hi is the average Shannon Entropy across bsamples of task t, H′ i, the Shannon entropy of choosing classes with uniform distribution and ˆH, the maximum of each task’s average entropy over bsamples. H′ i is normalizing factor that accounts for differing number of prediction classes (without the normalizing factor H′ i, tasks with a binary classiﬁcation Ci = 1 were rarely chosen). Further, to limit high entropy outliers and to favor tasks with highest uncertainty, we normalize with ˆH. The measure in eq. 4 allows Algorithm 1 to choose bsamples from b×T candidates to train the model. 3 R ELATED WORK Multi-Tasking in NLP. To take advantage of the potential positive transfer of knowledge from one task to another, several works have proposed carefully choosing which tasks to train as an intermediate step in NLP before single task ﬁne-tuning (Bingel & Søgaard, 2017; Kerinec et al., 2018; Wang et al., 2019a; Standley et al., 2019; Pruksachatkun et al., 2020; Phang et al., 2018). The intermediate tasks are not required to perform well and are not typically evaluated jointly. In this work, all tasks are trained jointly and all tasks usedare evaluated from a single model. In Natural Language Understanding (NLU), it is still the case that to get the best task performance one often needs a separate model per task (Clark et al., 2019c; McCann et al., 2018). At scale, Multilingual NMT systems (Aharoni et al., 2019) have also found that MTL model performance degrades as the number of tasks increases. We notice a similar trend in NLU with our baseline MTL model. Recently, approaches in MTL have tackled the problem by designing task speciﬁc decoders on top of a shared model (Liu et al., 2019b) or distilling multiple single-task models into one (Clark et al., 2019c). Nonetheless, such MTL approaches still involves single task ﬁne-tuning. In this paper, we show that it is possible to achieve high performance in NLU without single task ﬁne-tuning. Adapters. Adapters are trainable modules that are attached in speciﬁc locations of a pretrained network. They provide another promising avenue to limit the number of parameters needed when confronted with a large number of tasks. This approach is useful with pretrained MLM models that have rich linguistic information (Tenney et al., 2019b; Clark et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a). Recently, Houlsby et al. (2019) added an adapter to a pretrained BERT model by ﬁne-tuning the layer norms and adding feed forward bottlenecks in every Transformer layer. However, such methods adapt each task individually during the ﬁne-tuning process. Unlike prior work, our method harnesses the vectorized representations of tasks to modularize a single pretrained model across all tasks. Stickland et al. (2019) and Tay et al. (2020) also mix both MTL and adapters with BERT and T5 encoder-decoder (Raffel et al., 2019) respectively by creating local task modules that are controlled by a global task agnostic module. The main drawback is that a new set of non-shared parameters must be added when a new task is introduced. CA-MTL shares all parameters and is able to re-modulate existing weights with a new task embedding vector. Active Learning, Task Selection and Sampling. Our sampling technique is similar to the ones found in several active learning algorithms (Chen et al., 2006) that are based on Shannon entropy estimations. Reichart et al. (2008) and Ikhwantri et al. (2018) examined Multi-Task Active Learning (MTAL), a technique that chooses one informative sample forT different learners (or models) for each T tasks. Instead we choose T tasks samples for one model. Moreover, the algorithm weights each sample by the corresponding task score, and the Shannon entropy is normalized to account for various losses (see equation 5). Also, our algorithm is used in a large scale MTL setup (≫2 tasks). Recently, Glover & Hokamp (2019) explored task selection in MTL using learning policies based on counterfactual estimations (Charles et al., 2013). However, such method considers only ﬁxed stochastic parameterized policies while our method adapts its selection criterion based on model uncertainty throughout the training process. 5Published as a conference paper at ICLR 2021 Hypernetworks. CA-MTL is a hypernetwork adapter. The method to generate task-conditioned adapter weights is inspired by von Oswald et al. (2020). Hypernetwork layers have also been ﬁnetuned along with pretrained models. For example, Ponti et al. (2021) uses stochastic variational inference Hoffman et al. (2013) to produce language and task latent codes that conditionally generates the weights of a BERT prediction head, a single hypernetwork linear layer shared across multiple languages and tasks. Unlike previous methods however, CA-MTL conditionally modulates pretrained weights and biases, attention matrices, hidden representations and normalization statistics with task embeddings. Further, CA-MTL can preserve the pretraining knowledge by freezing the underlying Transformer model. Finally, we show a synergy between our hypernetwork adapter and our active task sampling technique (see section 2.2) that allows CA-MTL to continue surpassing fully tuned models as we scale the number of tasks (see ﬁgure 7). 4 E XPERIMENTS AND RESULTS We show that our adapter of section 2 achieve parameter efﬁcient transfer for 26 NLP tasks. Our implementation of CA-MTL is based on HuggingFace (Wolf et al., 2019). Hyperparameters and our experimental set-up are outlined in A.5. To preserve the weights of the pretrained model, CA-MTL’s bottom half Transformer layers are frozen in all experiments (except in section 4.4). We also tested different layer freezing conﬁgurations and found that freezing half the layers worked best on average (see Section A.8). 4.1 M ULTI -TASK UNCERTAINTY SAMPLING 0 250005000075000100000125000150000175000200000Training iteration 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82Average score MT-UncertaintyCouterfactualTask sizeRandom Figure 4: MT-Uncertainty vs. other task sam- pling strategies: median dev set scores on 8 GLUE tasks and using BERTBASE. Data for the Counter- factual and Task Size policyπ|task|(eq. 6) is from Glover & Hokamp (2019). Our MT-Uncertainty sampling strategy, from section 2.2, is compared to 3 other task selection schemes: a) Counterfactual b) Task size c) Random. We used a BERTBASE (no adapters) on 200k iterations and with the same hyperparameters as in Glover & Hokamp (2019). For more information on Counterfactual task selection, we invite the reader to consult the full expla- nation in Glover & Hokamp (2019). For T tasks and the dataset Di for tasks i ∈{1,...,T }, we rewrite the deﬁnitions of Randomπrand and Task sizeπ|task| sampling: πrand = 1/T, π|task|= |Di| [ T∑ i=1 |Di| ]−1 (6) 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (a) Random 500 5000 10000Train iteration 0.0 0.2 0.4 0.6 0.8 (b) MT-Uncertainty MNLI-mm dev scoreCoLA dev score MNLI-mm train entropyCoLA train entropy Figure 5: CoLA/MNLI Dev set scores and Entropy for πrand (left) and MT-Uncertainty (right). In Figure 4, we see from the results that MT- Uncertainty converges faster by reaching the 80% average GLUE score line before other task sampling methods. Further, MT-Uncertainty maximum score on 200k iterations is at 82.2, which is 1.7% higher than Counterfactual sam- pling. The datasets in the GLUE benchmark offers a wide range of dataset sizes. This is useful to test how MT-Uncertainty manages a jointly trained low resource task (CoLA) and high resource task (MNLI). Figure 5 explains how catastrophic forgetting is curtailed by sam- pling tasks before performance drops. With πrand, all of CoLA’s tasks are sampled by it- eration 500, at which point the larger MNLI dataset overtakes the learning process and CoLA’s dev set performance starts to diminish. On the other hand, with MT-Uncertainty sampling, CoLA is sampled whenever Shannon entropy is higher than MNLI’s. The model ﬁrst assesses uncertain samples using Shannon Entropy then decides what data is necessary to train on. This process allows lower resource tasks to keep performance steady. 6Published as a conference paper at ICLR 2021 We provide evidence in Figure 8 of A.2 that MT-Uncertainty is able to manage task difﬁculty — by choosing the most difﬁcult tasks ﬁrst. 4.2 A BLATION AND MODULE ANALYSIS Table 1: Model ablation studya on the GLUE dev set. All models have the bottom half layers frozen. Model changes Avg Task σ % data GLUE GLUE used BERTBASE MTL (πrand) 80.61 14.41 100 + Conditional Attention 82.41 10.67 100 + Conditional Adapter 82.90 11.27 100 + CA and CLN 83.12 10.91 100 + MT-Uncertainty 84.03 10.02 66.3(CA-MTLBERT-BASE) aCA=Conditional Alignment, CLN=Conditional Layer Normal- ization, Task σ=scores standard deviation across tasks. In Table 1, we present the results of an ablation study to determine which elements of CA-MTLBERT-BASE had the largest positive gain on average GLUE scores. Starting from a MTL BERTBASE baseline trained us- ing random task sampling ( πrand). Apart for the Conditional Adapter, each module as well as MT- Uncertainty lift overall performance and reduce vari- ance across tasks. Please note that we also included accuracy/F1 scores for QQP, MRPC and Pearson/ Spearman correlation for STS-B to calculate score standard deviation Task σ. Intuitively, when negative task transfer occurs between two tasks, either (1) task interference is bidirectional and scores are both impacted, or (2) interference is unidirectional and only one score is impacted. We calculate Task σ to characterize changes in the dynamic range of performance across multiple tasks. We do this to asses the degree to which performance im- provements are distributed across all tasks or only subsets of tasks. As we can see from Table 1, Conditional Attention, Conditional Alignment, Conditional Layer Normalization, MT-Uncertainty play roles in reducing Task σand increasing performance across tasks. This provides partial evidence of CA-MTL’s ability to mitigating negative task transfer. Figure 6: Task performance vs. avg. covariance similarity scores (eq. 7) for MTL and CA-MTL. We show that Conditional Alignment can learn to capture covariate distribution differences with task embeddings co-learned from other adapter compo- nents of CA-MTL. In Figure 6, we arrive at similar conclusions as Wu et al. (2020), who proved that neg- ative task transfer is reduced when task covariances are aligned. The authors provided a “covariance simi- larity score” to gauge covariance alignment. For task iand j with mi and mj data samples respectively, and given d dimensional inputs to the ﬁrst Trans- former layer Xi ∈Rmi×d and Xj ∈Rmj×d, we rewrite the steps to calculate the covariance similarity score between task iand j: (a) Take the covariance matrix X⊤ i Xi, (b) Find its best rank- ri approxima- tion Ui,riDi,riU⊤ i,ri, where ri is chosen to contain 99% of the singular values. (c) Apply steps (a), (b) to Xj, and compute the covariance similarity score CovSimi,j: CovSimi,j := ∥(Ui,riD1/2 i,ri )⊤Uj,rj D1/2 j,rj ∥F ∥Ui,riD1/2 i,ri ∥F ·∥Uj,rj D1/2 j,rj ∥F . CovSimi = 1 T −1 ∑ j̸=i CovSimi,j (7) Since we are training models with T tasks, we take the average covariance similarity score CovSimi between task iand all other tasks. We measure CovSimi using equation 7 between 9 single-task models trained on individual GLUE tasks. For each task in Figure 6, we measure the similarity score on the MTL trained BERTBASE baseline, e.g., CoLA (MTL), or CA-MTLBERT-BASE model, e.g., MNLI (CA-MTL). Our score improvement measure is the % difference between a single task model and MTL or CA-MTL on the particular task. We ﬁnd that covariance similarity increases for 9 tasks and that performance increases for 7 out 9 tasks. These measurements conﬁrm that the Conditional Alignment is able to align task covariance, thereby helping alleviate task interference. 4.3 J OINTLY TRAINING ON 8 TASKS : GLUE In Table 2, we evaluate the performance of CA-MTL against single task ﬁne-tuned models, MTL as well as the other BERT-based adapters on GLUE. As in Houlsby et al. (2019), MNLIm and MNLImm are treated as separate tasks. Our results indicate that CA-MTL outperforms both the BASE adapter, 7Published as a conference paper at ICLR 2021 Table 2: Adapters with layer freezing vs. ST/MT on GLUE test set. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. * Individual scores not available. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Results from: 1Devlin et al. (2018) 2Stickland et al. (2019). 3Houlsby et al. (2019) . Method Type Total Trained # tasks GLUE params params/task g.e. ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Test Server Results BERTBASE 1 ST 9.0× 100% — 52.1 84.6/83.4 88.9 90.5 71.2 66.4 93.5 85.8 79.6 BERTBASE 2 MTL 1.0× 11.1% 2 51.2 84.0/83.4 86.7 89.3 70.8 76.6 93.4 83.6 79.9 PALs+Anneal Samp.2 MTL 1.13× 12.5% 4 51.2 84.3/83.5 88.7 90.0 71.5 76.0 92.6 85.8 80.4 CA-MTLBERT-BASE (ours) MTL 1.12× 5.6 % 5 53.1 85.9/85.8 88.6 90.5 69.2 76.4 93.2 85.3 80.9 Large Models — Test Server Results BERTLARGE 1 ST 9.0× 100% — 60.5 86.7/85.9 89.3 92.7 72.1 70.1 94.9 86.5 82.1 Adapters-2563 ST 1.3× 3.6% 3 59.5 84.9/85.1 89.5 90.7 71.8 71.5 94.0 86.9 80.0 CA-MTLBERT-LARGE (ours) MTL 1.12× 5.6% 3 59.5 85.9/85.4 89.3 92.6 71.4 79.0 94.7 87.7 82.8 PALS+Anneal Sampling (Stickland et al., 2019), and the LARGE adapter, Adapters-256 (Houlsby et al., 2019). Against single task (ST) models, CA-MTL is 1.3% higher than BERTBASE, with 5 out 9 tasks equal or greater performance, and 0.7% higher than BERTLARGE, with 3 out 9 tasks equal or greater performance. ST models, however, need 9 models or close to 9×more parameters for all 9 tasks. We noted that CA-MTLBERT-LARGE’s average score is driven by strong RTE scores. While RTE beneﬁts from MTL, this behavior may also be a side effect of layer freezing. In Table 10, we see that CA-MTL has gains over ST on more and more tasks as we gradually unfreeze layers. 4.4 T RANSFER TO NEW TASKS Table 3: Domain adaptation results on dev. sets for BASE models. 1Liu et al. (2019b), 2Jiang et al. (2020) % data used SciTail SNLI 0.1% 1% 10% 100% 0.1% 1% 10% 100% BERTBASE 1 51.2 82.2 90.5 94.3 52.5 78.1 86.7 91.0 MT-DNN1 81.9 88.3 91.1 95.7 81.9 88.3 91.1 95.7 MT-DNNSMART 2 82.3 88.6 91.3 96.1 82.7 86.0 88.7 91.6 CA-MTLBERT 83.2 88.7 91.4 95.6 82.8 86.2 88.0 91.5 In Table 3 we examine the ability of our method to quickly adapt to new tasks. We performed domain adaptation on SciTail (Khot et al., 2018) and SNLI (Bowman et al., 2015) datasets, using a CA-MTLBASE model trained on GLUE and a new linear decoder head. We tested several pretrained and randomly initialized task embeddings in a zero-shot setting. The com- plete set of experiments with all task embeddings can be found in the Appendix, Section A.4. We then selected the best task embedding for our results in Table 3. STS-B and MRPC MTL-trained task embeddings performed best on SciTail and SNLI respectively. CA-MTLBERT-BASE has faster adapta- tion than MT-DNNSMART (Jiang et al., 2020) as evidenced by higher performances in low-resource regimes (0.1% and 1% of the data). When trained on the complete dataset, CA-MTLBERT-BASE is on par with MT-DNNSMART. Unlike MT-DNNSMART however, we do not add context from a semantic similarity model – MT-DNNSMART is built off HNN (He et al., 2019). Nonetheless, with a larger model, CA-MTL surpasses MT-DNNSMART on the full SNLI and SciTail datasets in Table 6. 4.5 J OINTLY TRAINING ON 24 TASKS : GLUE/S UPER -GLUE, MRQA AND WNUT2017 Figure 7: Effects of adding more datasets on avg GLUE scores. Experiments conducted on 3 epochs. When 23 tasks are trained jointly, performance of CA-MTLBERT-BASE continues to improve. Effects of Scaling Task Count. In Figure 7 we continue to test if CA-MTL mitigates task in- terference by measuring GLUE average scores when progressively adding 9 GLUE tasks, 8 Super-GLUE tasks (Wang et al., 2019b), 6 MRQA tasks (Fisch et al., 2019). Tasks are described in Appendix section A.9. The results show that adding 23 tasks drops the performance of our baseline MTL BERTBASE (πrand). MTL BERT increases by 4.3% when adding MRQA but, with 23 tasks, the model performance drops by 1.8%. The opposite is true when CA-MTL modules are integrated into the model. CA-MTL continues to show gains with a large number of tasks and surpasses the baseline MTL model by close to 4% when trained on 23 tasks. 8Published as a conference paper at ICLR 2021 Table 4: 24-task CA-MTL vs. ST and vs. 24-task MTL with frozen layers on GLUE, SuperGLUE, MRQA and NER development sets. ST=Single Task, MTL=Multitask, g.e.= greater or equal to. Details in section A.5. Model Task Grouping Avg # tasks Total GLUE SuperGLUE MRQA NER e.g. ST Params BERT-LARGE models STReImp 84.5 68.9 79.7 54.1 76.8 — 24× MTLReImp 83.2 72.1 77.8 42.2 76.4 9/24 1× CA-MTL 86.6 74.1 79.5 49.0 79.1 17/24 1.12× RoBERTa-LARGE models STReImp 88.2 76.5 83.6 57.8 81.9 — 24× MTLReImp 86.0 78.6 80.7 49.3 80.7 7/24 1× CA-MTL 89.4 80.0 82.4 55.2 83.1 15/24 1.12× 24-task CA-MTL. We jointly trained large MTL baselines and CA-MTL models on GLUE/Super-GLUE/MRQA and Named En- tity Recognition (NER) WNUT2017 (Derczyn- ski et al., 2017). Since some dev. set scores are not provided and since RoBERTa results were reported with a median score over 5 random seeds, we ran our own single seed ST/MTL baselines (marked “ReImp”) for a fair compar- ison. The dev. set numbers reported in Liu et al. (2019c) are displayed with our baselines in Table 9. Results are presented in Table 4. We notice in Table 4 that even for large models, CA-MTL provides large gains in performance on average over both ST and MTL models. For the BERT based models, CA-MTL provides 2.3% gain over ST and higher scores on 17 out 24 tasks. For RoBERTa based models, CA-MTL provides 1.2% gain over ST and higher scores on 15 out 24 tasks. We remind the reader that this is achieved with a single model. Even when trained with 16 other tasks, it is interesting to note that the MTL baseline perform better than the ST baseline on Super GLUE where most tasks have a small number of samples. Also, we used NER to test if we could still outperform the ST baseline on a token-level task, signiﬁcantly different from other tasks. Unfortunately, while CA-MTL performs signiﬁcantly better than the MTL baseline model, CA-MTL had not yet overﬁt on this particular task and could have closed the gap with the ST baselines with more training cycles. Table 5: Our 24-task CA-MTL vs. other large models on GLUE. F1 is reported for QQP/MRPC, Spearman’s corr. for STS-B, Matthew’s corr. for CoLA and accuracy for other tasks. *Split not available. **Uses intermediate task ﬁne-tuning + ST. Model GLUE tasks AvgCoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B BERT-LARGE based models on Dev set. MT-DNN 63.5 87.1 /86.7 91.0 92.9 89.2 83.4 94.3 90.6 85.6 STILTS ** 62.1 86.1* 92.3 90.5 88.5 83.4 93.2 90.8 85.9 BAM! 61.8 87.0* – 92.5 – 82.8 93.6 89.7 – 24-task CA-MTL 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa-LARGE based models on Test set. RoBERTA** with 67.8 91.0/90.8 91.6 95.4 74.0 87.9 97.5 92.5 87.3Ensemble 24-task CA-MTL 62.2 89.0/88.4 92.0 94.7 72.3 86.2 96.3 89.8 85.7 Comparisons with other methods. In Table 5, CA-MTLBERT is com- pared to other Large BERT based methods that either use MTL + ST, such as MT-DNN (Liu et al., 2019b), intermediate tasks + ST, such as STILTS (Phang et al., 2018) or MTL model distillation + ST, such as BAM! (Clark et al., 2019c). Our method scores higher than MT-DNN on 5 of 9 tasks and by 1.0 % on avg. Against STILTS, CA-MTL realizes a 0.7 % avg. score gain, surpassing scores on 6 of 9 tasks. We also show that CA-MTLRoBERTa is within only 1.6 % of a RoBERTa ensemble of 5 to 7 models per task and that uses intermediate tasks. Using our 24-task CA-MTL large RoBERTa-based model, we report NER F1 scores on the WNUT2017 test set in Table 6a. We compare our result with RoBERTaLARGE and XLM-RLARGE (Nguyen et al., 2020) the current state-of-the-art (SOTA). Our model outperforms XLM-RLARGE by 1.6%, reaching a new state-of-the-art. Using domain adaptation as described in Section 4.4, we report results on the SciTail test set in Table 6b and SNLI test set in Table 6b. For SciTail, our model matches the current SOTA2 ALUM (Liu et al., 2020), a RoBERTa large based model that additionally uses the SMART (Jiang et al., 2020) ﬁne-tuning method. For SNLI, our model outperforms SemBert, the current SOTA3. Table 6: CA-MTL test performance vs. SOTA. (a) WNUT2017 F1 RoBERTaLARGE 56.9 XLM-RLARGE 57.1 CA-MTLRoBERTa (ours) 58.0 (b) SciTail % Acc MT-DNN 94.1 ALUMRoBERTa 96.3 ALUMRoBERTa-SMART 96.8 CA-MTLRoBERTa (ours) 96.8 (c) SNLI % Acc MT-DNN 91.6 MT-DNNSMART 91.7 SemBERT 91.9 CA-MTLRoBERTa (ours) 92.1 2https://leaderboard.allenai.org/scitail/submissions/public on 09/27/2020 3https://nlp.stanford.edu/projects/snli/ on 09/27/2020 9Published as a conference paper at ICLR 2021 5 C ONCLUSION We believe that our experiments here have helped demonstrate the potential of task conditioned adaptive learning within a single model that performs multiple tasks. In a large-scale 24-task NLP experiment, CA-MTL outperforms fully tuned single task models by 2.3% for BERT Large and by 1.2% for RoBERTa Large using 1.12 times the number of parameters, while single task ﬁne-tuning approach requires 24 separately tuned single task models or 24 times the number of parameters. When a BERT vanilla MTL model sees its performance drop as the number of tasks increases, CA-MTL scores continue to climb. Performance gains are not driven by a single task as it is often the case in MTL. Each CA-MTL module that adapts a Transformer model is able to reduce performance variances between tasks, increasing average scores and aligning task covariances. This evidence shows that CA-MTL is able to mitigate task interference and promote more efﬁcient parameter sharing. We showed that MT-Uncertainty is able to avoid degrading performances of low resource tasks. Tasks are sampled whenever the model sees entropy increase, helping avoid catastrophic forgetting. Overall, CA-MTL offers a promising avenue to dynamically adapt and modularize knowledge embedded in large monolithic pretrained models. Extending such ideas will be an objective for future work. 10Published as a conference paper at ICLR 2021 ACKNOWLEDGMENTS This research was supported by the Canada CIFAR AI Chairs Program, NSERC and PROMPT. Exper- iments in this article were conducted with Compute Canada and MILA computational infrastructure and we thank them for their support. We would like to thank Colin Raffel, Sandeep Subramanian, and Nicolas Gontier for their useful feedback and the anonymous reviewers for helpful comments, discussions and suggestions. REFERENCES Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3874–3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL https://www.aclweb.org/anthology/N19-1388. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 41–48, 2009. Joachim Bingel and Anders Søgaard. Identifying beneﬁcial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 164–169, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/E17-2026. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2015. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv, pp. arXiv–2005, 2020. Rich Caruana. Multitask learning. Mach. Learn., 28(1):41–75, July 1997. ISSN 0885-6125. doi: 10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734. Richard Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth International Conference on Machine Learning, pp. 41–48. Morgan Kaufmann, 1993. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 1–14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/S17-2001. Denis Charles, Max Chickering, and Patrice Simard. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14:3207–3260, November 2013. Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer. An empirical study of the behavior of active learning for word sense disambiguation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 120–127, New York City, USA, June 2006. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/N06-1016. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. CoRR, abs/1711.02257, 2017. URL http://arxiv.org/abs/1711.02257. 11Published as a conference paper at ICLR 2021 Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Minnesota, June 2019a. Association for Computational Linguistics. doi: 10.18653/ v1/N19-1300. URL https://www.aclweb.org/anthology/N19-1300. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019b. Association for Computational Linguistics. doi: 10.18653/v1/W19-4828. URL https: //www.aclweb.org/anthology/W19-4828. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le. Bam! born-again multi-task networks for natural language understanding. CoRR, abs/1907.04829, 2019c. URL http://arxiv.org/abs/1907.04829. Edward Collins, Nikolai Rozanov, and Bingbing Zhang. Evolutionary data measures: Understanding the difﬁculty of text classiﬁcation tasks. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pp. 380–391, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/K18-1037. URL https://www.aclweb.org/ anthology/K18-1037. Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: deep neural networks with multitask learning. In ICML, pp. 160–167, 2008. URL https://doi. org/10.1145/1390156.1390177. Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investi- gating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107– 124, Jul. 2019. doi: 10.18148/sub/2019.v23i2.601. URL https://ojs.ub.uni-konstanz. de/sub/index.php/sub/article/view/601. Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C Courville. Modulating early visual processing by language. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems 30 , pp. 6594– 6604. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7237-modulating-early-visual-processing-by-language.pdf . Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140–147, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4418. URL https://www.aclweb.org/ anthology/W17-4418. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https://www.aclweb.org/anthology/I05-5002. Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Güney, V olkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine.CoRR, abs/1704.05179, 2017. URL http://arxiv.org/abs/1704.05179. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pp. 1–13, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. URL https: //www.aclweb.org/anthology/D19-5801. 12Published as a conference paper at ICLR 2021 John Glover and Chris Hokamp. Task selection policies for multitask learning. CoRR, 2019. URL http://arxiv.org/abs/1907.06214. Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausi- ble alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main confer- ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se- mantic Evaluation (SemEval 2012), pp. 394–398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/S12-1052. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioriti- zation for multitask learning. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. A hybrid neural network model for commonsense reasoning. In Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pp. 13–21, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-6002. URL https://www.aclweb.org/ anthology/D19-6002. Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(4):1303–1347, 2013. URL http://jmlr.org/ papers/v14/hoffman13a.html. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for NLP. CoRR, abs/1902.00751, 2019. URL http://arxiv.org/abs/1902.00751. Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328–339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https://www.aclweb.org/anthology/ P18-1031. Fariz Ikhwantri, Samuel Louvan, Kemal Kurniawan, Bagas Abisena, Valdi Rachman, Alfan Farizki Wicaksono, and Rahmad Mahendra. Multi-task active learning for neural semantic role labeling on low resource conversational corpus. InProceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP, pp. 43–50, 2018. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled regular- ized optimization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2177–2190, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://www.aclweb.org/anthology/2020. acl-main.197. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601– 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/ v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. CoRR, abs/1907.10529, 2019. URL http://arxiv.org/abs/1907.10529. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. CoRR, abs/1705.07115, 2017. URL http://arxiv.org/ abs/1705.07115. Emma Kerinec, Chloé Braud, and Anders Søgaard. When does deep multi-task learning work for loosely related document classiﬁcation tasks? In Proceedings of the 2018 EMNLP Workshop 13Published as a conference paper at ICLR 2021 BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 1–8, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5401. URL https://www.aclweb.org/anthology/W18-5401. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. InProceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https://www.aclweb.org/anthology/N18-1023. Tushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. In AAAI, 2018. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Hector J. Levesque. The winograd schema challenge. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning. AAAI, 2011. URL http://dblp.uni-trier.de/ db/conf/aaaiss/aaaiss2011-6.html#Levesque11. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855, 2019a. URL http://arxiv.org/abs/1903.08855. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019b. URL http://arxiv.org/ abs/1901.11504. Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for large neural language models, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019c. URL http://arxiv.org/abs/1907.11692. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to bert embeddings during ﬁne-tuning? arXiv preprint arXiv:2004.14448, 2020. Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. Bertweet: A pre-trained language model for english tweets. arXiv preprint arXiv:2005.10200, 2020. Hao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h - 1 heads is better than h heads. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6566–6577, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.587. URL https://www.aclweb.org/anthology/2020. acl-main.587. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In AAAI, 2018. 14Published as a conference paper at ICLR 2021 Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. CoRR, abs/1808.08949, 2018. URL http: //arxiv.org/abs/1808.08949. Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on STILTs: Supplementary training on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018. URL http://arxiv. org/abs/1811.01088. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 67–81, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1007. URL https://www.aclweb.org/ anthology/D18-1007. Edoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela Parovic, Roi Reichart, and Anna Korhonen. Parameter space factorization for zero-shot learning across tasks and languages. Transactions of the Association for Computational Linguistics, 9:410–428, 2021. doi: 10.1162/tacl_a_00374. URL https://aclanthology.org/2021.tacl-1.25. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R Bowman. Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? arXiv preprint arXiv:2005.00628, 2020. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016a. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://www.aclweb.org/ anthology/D16-1264. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, Austin, Texas, November 2016b. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. Multi-task active learning for linguistic annotations. In Proceedings of ACL-08: HLT, pp. 861–869, 2008. Sebastian Ruder. An overview of multi-task learning in deep neural networks.ArXiv, abs/1706.05098, 2017. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. CoRR, abs/1810.04650, 2018. URL http://arxiv.org/abs/1810.04650. Joan Serrà, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In ICML, pp. 4555–4564, 2018. URL http:// proceedings.mlr.press/v80/serra18a.html. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170. 15Published as a conference paper at ICLR 2021 Trevor Standley, Amir Roshan Zamir, Dawn Chen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned together in multi-task learning? CoRR, abs/1905.07553, 2019. URL http://arxiv.org/abs/1905.07553. Asa Cooper Stickland, Iain Murray, someone, and someone. BERT and PALs: Projected attention layers for efﬁcient adaptation in multi-task learning. volume 97 of Proceedings of Machine Learning Research, pp. 5986–5995, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/stickland19a.html. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. Hypergrid: Efﬁcient multi-task transformers with grid-wise decomposable hyper projections. arXiv preprint arXiv:2007.05891, 2020. Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline. CoRR, abs/1905.05950, 2019a. URL http://arxiv.org/abs/1905.05950. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316, 2019b. URL http://arxiv.org/abs/1905.06316. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 191–200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL https: //www.aclweb.org/anthology/W17-2623. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762. Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and João Sacramento. Continual learning with hypernetworks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SJgwNerKvB. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446. Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pappagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, Shuning Jin, Berlin Chen, Benjamin Van Durme, Edouard Grave, Ellie Pavlick, and Samuel R. Bowman. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019a. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537, 2019b. URL http://arxiv.org/abs/ 1905.00537. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. CoRR, abs/1805.12471, 2018. URL http://arxiv.org/abs/1805.12471. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //www.aclweb.org/anthology/N18-1101. 16Published as a conference paper at ICLR 2021 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. URL http://arxiv.org/abs/1910.03771. Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer in multi-task learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylzhkBtDB. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Com- putational Linguistics. doi: 10.18653/v1/D18-1259. URL https://www.aclweb.org/ anthology/D18-1259. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885. Yu Zhang and Qiang Yang. A survey on multi-task learning. CoRR, abs/1707.08114, 2017. URL http://arxiv.org/abs/1707.08114. 17Published as a conference paper at ICLR 2021 A A PPENDIX A.1 S UMMARY OF ACRONYMS Acronyms of datasets and descriptions can be found below in section A.9. Table 7: List of acronyms used in this paper. Acronym Description ARLM Autoregressive Language Models CA-MTL Conditional Adaptive Multi-Task Learning: our architecture CFF Conditional Feed-Forward: a feed-forward layer modulated by a conditioning vector CLN Conditional Layer Normalization in section 2.1.3 EDM Evolutionary Data Measures (Collins et al., 2018): a task difﬁculty estimate GLUE General Language Understanding Evaluation Wang et al. (2018): a benchmark with multiple datasets QA Question Answering MT Multi-Task MTAL Multi-Task Active Learning: ﬁnding the most informative instance for multiple learners (or models) MLM Masked Language Model: BERT Devlin et al. (2018) is an example of an MLM MTL Multi-Task Learning: \"learning tasks in parallel while using a shared representation\" (Caruana, 1997) MRQA Machine Reading for Question Answering Fisch et al. (2019): a benchmark with multiple datasets NER Named Entity Recognition NLP Natural Language Processing SOTA State of the art ST Single Task ﬁne-tuning: all weights are typically updated ST-A ST with Adapter modules: one adapter per task is trained and pretrained weights are optionally updated A.2 U NCERTAINTY SAMPLING : A LGORITHM AND ADDITIONAL RESULTS Algorithm 1: Multi-task Uncertainty Sampling Input: Training data Dt for task t∈[1,...,T ]; batch size b; Ct possible output classes for task t; f := fφ(zi),θi our model with weights φ,θi; Output: B′- multi-task batch of size b 1 B←∅ 2 for t←1 to T do 3 Generate xt := {xt,1,...,x t,b} i.i.d. ∼Dt 4 for i←1 to bdo 5 Ht,i ←−∑Ci c=1 pc(f(xt,i)) log pc(f(xt,i)) ⊿Entropy of each sample 6 7 end 8 Compute ¯Ht ←1 b ∑ x∈xi Ht,i ⊿Average entropy for task t 9 10 Compute H′ t ←−∑Ct c=1 1 Ct log [ 1 Ct ] ⊿Max entropy (uniform distribution) 11 12 B←B∪ xt and Dt ←Dt \\xt 13 if Dt = ∅then 14 Reload Dt 15 end 16 for i←1 to bdo 17 Compute: Ut,i ←Ht,i/H′ t ⊿Uncertainty normalized with max entropy 18 end 19 end 20 Compute ˆH← maxi∈{1,...,T}[ ¯Ht] ⊿Entropy of task with highest average entropy 21 Update Ut,i ←Ut,i/ˆH ⊿Normalize each sample’s uncertainty measure 22 B′←top_b({Ut,i|t∈[1,...,T ],i ∈[1,...,b ]}) ⊿b samples w/ highest uncertainty Return: With B′, solve eq. 1 with gradient descent; updated model f 18Published as a conference paper at ICLR 2021 An advantage of our MT-Uncertainty Sampling approach is its ability to manage task difﬁculty. This is highlighted in Figure 8. In this experiment, we estimated task difﬁculty using the Evolutionary Data Measures (EDM)4 proposed by Collins et al. (2018). The task difﬁculty estimate relies on multiple dataset statistics such as the data size, class diversity, class balance and class interference. Interestingly, estimated task difﬁculty correlates with the ﬁrst instance that the selection of a speciﬁc task occurs. Supposing that QNLI is an outlier, we notice that peaks in the data occur whenever tasks are ﬁrst selected by MT Uncertainty sampling. This process follows the following order: 1. MNLI 2. CoLA 3. RTE 4. QQP 5. MRPC 6.SST-2, which is the order from highest task difﬁculty to lowest task difﬁculty using EDM. As opposed to Curriculum Learning (Bengio et al., 2009), MT-Uncertainty dynamically prioritizes the most difﬁcult tasks. As also discovered in MTL vision work (Guo et al., 2018), this type of prioritization on more difﬁcult tasks may explain MT-Uncertainty’s improved performance over other task selection methods. In MTL, heuristics to balance tasks during training is typically done by weighting each task’s loss differently. We see here how MT-Uncertainty is able to prioritize task difﬁculty. 0 500 1000 1500 2000 2500 3000 Train iteration 0 4 8 12 16 20 24 28 32Number of samples Task Difﬁculty MNLI 4.2 QNLI 3.8 CoLA 3.7 RTE 3.6 MRPC 3.5 QQP. 3.5 SST-2 3.2 Figure 8: Task composition of MT-Uncertainty sampling and estimated task difﬁculty using EDM: number of training samples per task at each iteration for batch size of 32. The occurrence of ﬁrst peaks and estimated difﬁculty follow the same order: From highest to lowest: MNLI >CoLA >RTE >QQP = MRPC >SST-2. While the EDM difﬁculty measure is shown to correlate well with model performance, it lacks precision. As reported in Collins et al. (2018), the average score achieved on the Yahoo Answers dataset is 69.9% and its difﬁculty is 4.51. The average score achieved on Yelp Full is 56.8%, 13.1% less than Yahoo Answers and its difﬁculty is 4.42. The authors mention that “This indicates that the difﬁculty measure in its current incarnation may be more effective at assigning a class of difﬁculty to datasets, rather than a regression-like value”. A.3 O THER RELATED WORK Multi-Tasking in NLP and other ﬁelds.MTL weight sharing algorithms such as Mixture-of-Experts (MoE) have found success in NLP (Lepikhin et al., 2020). CA-MTL can complement MoE since the Transformers multi-headed attention can be seen as a form of MoE (Peng et al., 2020). In Vision, MTL can also improve with optimization (Sener & Koltun, 2018) or gradient-based approaches (Chen et al., 2017; Yu et al., 2020). Active Learning, Task Selection and Sampling. Ikhwantri et al. (2018) examined multi-task active learning for neural semantic role labeling in a low resource setting, using entity recognition as the sole auxiliary task. They used uncertainty sampling for active learning and found that 12% less data could be used compared to passive learning. Reichart et al. (2008) has examined different active learning techniques for the two task annotation scenario, focusing on named entity recognition and syntactic parse tree annotations. In contrast, here we examine the larger scale data regime, the modularization of a multi-task neural architecture, and the many task (≫2) setting among other differences. Other than MTAL (Reichart et al., 2008; Ikhwantri et al., 2018), Kendall et al. (2017) leveraged model uncertainty to balance MTL losses but not to select tasks as is proposed here. 4https://github.com/Wluper/edm 19Published as a conference paper at ICLR 2021 A.4 Z ERO -SHOT RESULTS ON SCITAIL AND SNLI Before testing models on domain adaptation in section 4.4, we ran zero-shot evaluations on the development set of SciTail and SNLI. Table 8 outlines8-task CA-MTLBERT-BASE’s zero-shot transfer abilities when pretrained on GLUE with our MTL approach. We expand the task embedding layer to accommodate an extra task and explore various embedding initialization. We found that reusing STS-B and MRPC task embeddings worked best for SciTail and SNLI respectively. Table 8: CA-MTL is ﬂexible and extensible to new tasks. However, CA-MTL is sensitive to the new task’s embedding. We tested multiple task embeddings that worked best on either SciTail or SNLI by checking performance in a zero shot setting or using 0% of the data. Initialization of new SciTail SNLI task embedding layer 0% of data 0% of data CoLA’s embeddings 43.0 34.0 MNLI’s embeddings 24.2 33.0 MRPC’s embeddings 34.5 45.5 STS-B’s embeddings 46.9 33.2 SST-2’s embeddings 25.8 34.2 QQP’s embeddings 31.7 37.3 QNLI’s embeddings 32.0 38.0 RTE’s embeddings 32.3 40.6 WNLI’s embeddings 29.0 30.4 Average 28.7 37.7 Random initialization 46.8 34.0 Xavier initialization 29.8 37.6 A.5 M ORE EXPERIMENTAL DETAILS We used a batch size of 32 and a seed of 12 in all experiments. We used Adam (Kingma & Ba, 2015) as the optimizer with a learning rate of 2e-5. We applied a learning rate decay with warm up over the ﬁrst 10% of the training steps. Unless otherwise speciﬁed, we used 5 epochs, a seed of 12 and a sequence length of 128. Additional details are outlined in section . Our data prepossessing and linear decoder heads are the same as in Devlin et al. (2018). We used the same dropout rate of 0.1 in all layers. To run our experiments, we used either four NVIDIA P100 GPU for base models or four NVIDIA V100 GPU for larger ones. We did not perform parameter search. We do not use ensemble of models or task-speciﬁc tricks (Devlin et al., 2018; Liu et al., 2019b; Clark et al., 2019c). All models are either 12 Transformer layers for BASE and 24 Transformer layers for LARGE. Apart from CA-MTL, models trained in multi-task learning (BERT or RoBERTa without adapters) used random task sampling. For Table 1 and Figure 7, all BERT-based model have half their layers frozen (untrained) for a fair comparison of ablation results. For the 24-task MTL and CA-MTL models in Tables 4 and 5, we increased the input sequence length to 256 and used 8 epochs. A.6 T HE DIRECT SUM OPERATOR In section 2.1.1, we used the direct sum operator ⊕. This operation allows us to create a block diagonal matrix. The direct sum of a matrix A∈Rn×m and B ∈Rp×q results in a matrix of size (m+ p) ×(n+ q), deﬁned as: A ⊕B = [ A 0 0 B ] =   a11 ··· a1n 0 ··· 0 ... ... ... ... ... ... am1 ··· amn 0 ··· 0 0 ··· 0 b11 ··· b1q ... ... ... ... ... ... 0 ··· 0 bp1 ··· bpq   20Published as a conference paper at ICLR 2021 A.7 B ASELINES AND OTHER EXPERIMENTAL RESULTS In this section, we present our baseline results for BERT, RoBERTa, CA-MTL as well as other models. Our single task results (ST) that we ran ourselves surpass other paper’s reported scores in Table 9. Liu et al. (2019c) reports random seed median scores for RoBERTa. However, our RoBERTa ST baseline matches or surpasses the original paper’s scores4 out 7 times on the development set when scores are comparable (QQP F1 and STS-B spearman are not reported). Table 9: F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. ST=Single Task, MTL=Multitask. *QNLI v1 (we report v2) **F1 score or Spearman’s correlation is not reported. ***Unknown random seeds. Results from: 1Stickland et al. (2019) 2Liu et al. (2019b) 3Phang et al. (2018) 4Liu et al. (2019c). Method Total Trained GLUE params params/task CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg Base Models — Dev set Results PALs+Anneal Samp.1 1.13× 12.5% – – – – – – – – 81.70 8-task CA-MTLBERT-BASE (ours) 1.12× 5.6% 60.9 82.7/83.1 88.9 90.7 90.3 79.1 91.9 88.8 84.03 BERT LARGE Models — Dev set Results ST BERT-LARGE2 9× 100% 60.5 86.7/85.9 89.3 92.7* 89.3 70.1 94.9 86.5 84.0 ST BERT-LARGE3 9× 100% 62.1 86.2/86.2 92.3 89.4 88.5 70.0 92.5 90.1 84.1 ST BERT-LARGE (ours) 9× 100% 63.6 86.5/86.0 91.4 91.0 88.5 70.2 94.7 88.2 84.5 24-task CA-MTLBERT-LARGE (ours) 1.12× 5.6% 63.8 86.3/86.0 92.9 93.4 88.1 84.5 94.5 90.3 86.6 RoBERTa LARGE Models — Dev set Results RoBERTa-LARGE4 9× 100% 68.0 90.2 90.9 94.7 ** 86.6 96.4 ** –(Median 5 runs)*** ST RoBERTa-LARGE (ours) 9× 100% 68.3 89.2/88.9 92.6 94.8 84.6 87.0 96.4 91.7 88.2 24-task CA-MTLRoBERTa-LARGE (ours) 1.12× 5.6% 69.7 89.4/89.3 93.9 94.9 88.8 91.0 96.2 91.0 89.4 A.8 S OME RESULTS ON LAYER FREEZING AND WITH FULL BLOCK ATTENTION . All experiments in this section were run for only 5 epochs, exclusively on the GLUE dataset for the large BERT-based 8-task CA-MTL model. Results in Table 10 reveal that as we freeze more layers, performance tends to decrease. However, since we wanted to preserve as much pretrained knowledge as possible, we chose to keep at least 50% of layers frozen. While this has slightly lowered our performance on 9 GLUE tasks, we believe that keeping as much of the original pretrained weights is beneﬁcial when increasing the total number of tasks in MTL to 24 or more tasks. However, we did not explore this hypothesis more. Table 10: 8-task CA-MTLBERT-LARGE (see section 4.3) for various layer freezing conﬁgurations. F1 scores are reported for QQP/MRPC, Spearman’s correlation for STS-B, accuracy on the matched/mismatch sets for MNLI, Matthew’s correlation for CoLA and accuracy for other tasks. FBA = Full Block Attention Method % frozen # tasks GLUE layers g.e ST CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg LARGE Models — Dev set Results ST BERT-LARGE (ours) 0% — 63.6 86.5/86.0 91.4 91.0 88.5 70.2 93.1 88.2 84.3 CA-MTL 0% 7 60.2 86.2/86.0 92.0 91.5 88.7 76.3 93.3 89.5 84.9 CA-MTL 25% 6 63.7 86.1/85.8 89.1 91.2 88.6 79.7 92.9 88.5 85.1 CA-MTL 50% 3 63.2 85.5/85.5 91.8 90.9 88.3 81.4 93.0 90.1 85.5 CA-MTL FBA 50% 0 60.2 81.7/81.1 88.0 85.8 85.7 78.7 88.6 87.1 81.8 A.9 D ATASET DESCRIPTION The datasets that were used for the domain adaptation experiments were SciTail 5 and SNLI6. We jointly trained a CA-MTLRoBERTa-LARGE model on 9 GLUE tasks, 8 Super-GLUE7 tasks, 6 MRQA8 tasks, and on WNUT20179 (Derczynski et al., 2017). All GLUE tasks are binary classiﬁcation, except STS-B (regression) and MNLI (three classes). We used the same GLUE data preprocessing as in Devlin et al. (2018). 5https://allenai.org/data/scitail; Leaderboard can be found at: https://leaderboard.allenai.org/scitail/submissions/public 6https://nlp.stanford.edu/projects/snli/ 7https://super.gluebenchmark.com/tasks 8https://github.com/mrqa/MRQA-Shared-Task-2019 9https://github.com/leondz/emerging_entities_17 21Published as a conference paper at ICLR 2021 Table 11: GLUE (Wang et al., 2018) dataset description. References: 1Warstadt et al. (2018), 2Socher et al. (2013), 3Dolan & Brockett (2005), 4Cer et al. (2017), 5Williams et al. (2018), 6Wang et al. (2018), 7Levesque (2011) Acronym Corpus |Train| Task Domain CoLA1 Corpus of Linguistic Acceptability 8.5K acceptability miscellaneous SST-22 Stanford Sentiment Treebank 67K sentiment detection movie reviews MRPC3 Microsoft Research Paraphrase Corpus 3.7K paraphrase detection news STS-B4 Semantic Textual Similarity Benchmark 7K textual similarity miscellaneous QQP Quora Question Pairs 364K paraphrase detection online QA MNLI5 Multi-Genre NLI 393K inference miscellaneous RTE6 Recognition Textual Entailment 2.5K inference/entailment news, Wikipedia WNLI7 Winograd NLI 634 coreference ﬁction books Table 12: Super-GLUE (Wang et al., 2019b) dataset description. References:1Clark et al. (2019a),2de Marneffe et al. (2019), 3Gordon et al. (2012), 4Khashabi et al. (2018), 5Zhang et al. (2018), 6Wang et al. (2019b), 7Poliak et al. (2018), 8Levesque (2011) Acronym Corpus |Train| Task Domain BoolQ1 Boolean Questions 9.4K acceptability Google queries, Wikipedia CB2 CommitmentBank 250 sentiment detection miscellaneous COPA3 Choice of Plausible Alternatives 400 paraphrase detection blogs, encyclopedia MultiRC4 Multi-Sentence Reading Comprehension5.1K textual similarity miscellaneous ReCoRD5 Reading Comprehension 101K paraphrase detection news and Commonsense Reasoning RTE6 Recognition Textual Entailment 2.5K inference news, Wikipedia WiC7 Word-in-Context 6K word sense disambiguationWordNet, VerbNet WSC8 Winograd Schema Challenge 554 coreference resolution ﬁction books Table 13: MRQA (Fisch et al., 2019) dataset description. References: 1Rajpurkar et al. (2016a), 2Trischler et al. (2017), 3Joshi et al. (2017), 4Dunn et al. (2017), 5Yang et al. (2018), 6Kwiatkowski et al. (2019) Acronym Corpus |Train| Task Domain SQuAD1 Stanford QA Dataset 86.6K crowdsourced questions Wikipedia NewsQA2 NewsQA 74.2K crowdsourced questions news TriviaQA3 TriviaQA 61.7K trivia QA web snippets SearchQA4 SearchQA 117.4K Jeopardy QA web snippets HotpotQA5 HotpotQA 72.9K crowdsourced questions Wikipedia Natural Questions6 Natural Questions 104.7K search logs Wikipedia SuperGLUE has a more diverse task format than GLUE, which is mostly limited to sentence and sentence-pair classiﬁcation. We follow the same preprocessing procedure as in Wang et al. (2019b). All tasks are binary classiﬁcation tasks, except CB (three classes). Also, WiC and WSC are span based classiﬁcation tasks. We used the same modiﬁed MRQA dataset and preprocessing steps that were used in Joshi et al. (2019). All MRQA tasks are span prediction tasks which seeks to identify start and end tokens of an answer span in the input text. Table 14: SNLI (Bowman et al., 2015) and SciTail (Khot et al., 2018) datasets description. Acronym Corpus |Train| Task Domain SNLI1 Stanford Natural Language Inference 550.2k inference human-written English sentence pairs SciTail2 Science and Entailment 23.5K entailment Science question answering SNLI is a natural inference task where we predict three classes. Examples of three target labels are: Entailment, Contradiction, and Neutral (irrelevant). SciTail is a textual entailment dataset. The hypotheses in SciTail are created from multiple-choice science exams and the answer candidates (premise) are extracted from the web using information retrieval tools. SciTail is a binary true/false classiﬁcation tasks that seeks to predict whether the premise entails the hypothesis. The two datasets are used only for domain adaptation in this study (see section A.4 for the details of our approach). 22",
      "meta_data": {
        "arxiv_id": "2009.09139v3",
        "authors": [
          "Jonathan Pilault",
          "Amine Elhattami",
          "Christopher Pal"
        ],
        "published_date": "2020-09-19T02:04:34Z",
        "pdf_url": "https://arxiv.org/pdf/2009.09139v3.pdf",
        "github_url": "https://github.com/CAMTL/CA-MTL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Conditionally Adaptive Multi-Task Learning (CA-MTL) to address challenges in Multi-Task Learning (MTL) such as overfitting to low-resource tasks, catastrophic forgetting, and negative task transfer. The main contributions include a novel Transformer-based Adapter with a new conditional attention mechanism and task-conditioned modules that facilitate weight sharing and mitigate forgetting by fixing half of the pretrained model weights. Additionally, a new uncertainty-based multi-task data sampling strategy is introduced to mitigate the negative effects of data imbalance across tasks. CA-MTL is shown to surpass single-task fine-tuning and other adapter methods while being parameter and data efficient, achieving state-of-the-art results on several NLP tasks and demonstrating superior performance across up to 26 tasks.",
        "methodology": "CA-MTL's methodology comprises two main components: a Task Conditioned Transformer and Multi-Task Uncertainty Sampling. The Task Conditioned Transformer adapts and modulates pretrained weights using task-specific embeddings through Conditional Weight Transformations. This includes a Conditional Attention module with a block-diagonal conditional matrix to account for task-specific biases, a Conditional Alignment layer inserted between the input embedding and the first Transformer layer to align data of diverse tasks, a Conditional Layer Normalization module to adapt layer normalization statistics, and a Conditional Bottleneck (two-layer feed-forward) to facilitate weight sharing and task-specific information flow. The Multi-Task Uncertainty Sampling algorithm, inspired by Active Learning, uses Shannon Entropy to prioritize tasks. It evaluates model uncertainty to select training examples, normalizing uncertainty measures to account for differing numbers of prediction classes and favoring tasks with higher uncertainty, thereby helping balance task sampling and prevent catastrophic forgetting.",
        "experimental_setup": "The experiments were conducted using a CA-MTL implementation based on HuggingFace, utilizing BERTBASE, BERTLARGE, and RoBERTaLARGE models. The bottom half of the Transformer layers were frozen to preserve pretrained knowledge, with ablation studies performed on different freezing configurations. Evaluation datasets included GLUE (8 tasks), Super-GLUE (8 tasks), MRQA (6 tasks), WNUT2017 (NER), SciTail, and SNLI, totaling up to 26 NLP tasks. The models were trained with a batch size of 32, Adam optimizer (learning rate 2e-5, warm-up over first 10% steps), 5-8 epochs, sequence length of 128-256, and a dropout rate of 0.1, without parameter search, model ensembles, or task-specific tricks. Performance was validated by comparing average GLUE scores, Task sigma (standard deviation across tasks), covariance similarity scores, and specific task metrics (F1, accuracy, Spearman/Matthew's correlation) against single-task fine-tuning, vanilla MTL baselines, and other adapter networks. Domain adaptation was tested in low-resource settings (0.1% to 100% of data) on SciTail and SNLI with new linear decoder heads and various task embedding initializations.",
        "limitations": "One limitation is that CA-MTL's performance during zero-shot transfer to new tasks is sensitive to the initialization of the new task's embedding. While freezing half of the pretrained layers was a design choice to preserve knowledge for a large number of tasks, this configuration slightly lowered performance on a subset of 9 GLUE tasks compared to fully unfrozen models. Additionally, for the NER task (WNUT2017), CA-MTL did not fully close the performance gap with single-task baselines, with the authors noting that it 'had not yet overfit on this particular task and could have closed the gap with the ST baselines with more training cycles,' implying potential for further optimization or longer training. The Evolutionary Data Measures (EDM) used to estimate task difficulty were also noted to lack precision for regression-like values, being more effective at assigning a class of difficulty.",
        "future_research_directions": "The paper concludes by stating that 'Extending such ideas will be an objective for future work.' This broadly suggests further exploration and development of task-conditioned adaptive learning approaches for dynamically adapting and modularizing knowledge embedded in large monolithic pretrained models. Specific directions are not explicitly detailed beyond this general statement.",
        "experimental_code": "File Path: src/model/ca_mtl.py\nContent:\nimport re\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import BertPreTrainedModel\n\nfrom src.model.decoder import Decoder\nfrom src.model.encoders.bert import _BertEncoder\nfrom src.model.encoders.ca_mtl_base import CaMtlBaseEncoder\nfrom src.model.encoders.ca_mtl_large import CaMtlLargeEncoder\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass CaMtlArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from: CA-MTL-base, CA-MTL-large, bert-base-cased \"\n                    \"bert-base-uncased, bert-large-cased, bert-large-uncased\"\n        }\n    )\n    freeze_encoder_layers: str = field(\n        default=None,\n        metadata={\"help\": \"Freeze encoder layers. format: <start_layer>-<end_layer>\"},\n    )\n\n\nclass CaMtl(BertPreTrainedModel):\n    def __init__(\n        self,\n        config,\n        model_args,\n        data_args,\n    ):\n        super().__init__(config)\n\n        self.data_args = data_args\n        self.bert = self._create_encoder(model_args.model_name_or_path)\n        self.decoders = nn.ModuleList()\n        for task in data_args.tasks:\n            self.decoders.append(Decoder(config.hidden_size, task))\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        task_id=None,\n        span_locs=None,\n        sample_id=None,\n    ):\n\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            task_id=task_id,\n        )\n\n        sequence_output, pooled_output = outputs[:2]\n\n        loss_list = []\n        unique_task_ids = torch.unique(task_id)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        loss_grouped_per_task = (\n            torch.zeros_like(task_id[0]).repeat(len(self.data_args.tasks)).float()\n        )\n        batch_entropy_per_task = torch.zeros(input_ids.shape[0])\n        batch_entropy_mean_per_task = torch.zeros(input_ids.shape[0])\n        max_mean_batch_entropy = None\n        logits = None\n        for unique_task_id in unique_task_ids_list:\n            task_id_filter = task_id == unique_task_id\n            decoder_id = unique_task_id\n            logits, current_loss, batch_entropy = self.decoders[decoder_id].forward(\n                sequence_output[task_id_filter],\n                pooled_output[task_id_filter],\n                labels=None if labels is None else labels[task_id_filter],\n                attention_mask=attention_mask[task_id_filter],\n            )\n\n            batch_entropy_mean = batch_entropy.mean().item()\n            batch_entropy_per_task[task_id_filter] = batch_entropy\n            batch_entropy_mean_per_task[task_id_filter] = torch.full_like(\n                batch_entropy, batch_entropy_mean\n            )\n            if (\n                max_mean_batch_entropy is None\n                or batch_entropy_mean > max_mean_batch_entropy\n            ):\n                max_mean_batch_entropy = batch_entropy_mean\n\n            if labels is not None:\n                loss_grouped_per_task[unique_task_id] = current_loss\n                loss_list.append(current_loss)\n\n        outputs = (\n            (logits)\n            + outputs[2:]\n            +\n            (\n                batch_entropy_per_task,\n                batch_entropy_mean_per_task,\n                max_mean_batch_entropy,\n            )\n        )\n\n        if loss_list:\n            loss = torch.stack(loss_list)\n            outputs = (loss.mean(),) + outputs + (loss_grouped_per_task.view(1, -1),)\n\n        return outputs\n\n    def _create_encoder(self, model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return CaMtlLargeEncoder(self.config, data_args=self.data_args)\n        elif model_name_or_path == \"CA-MTL-base\":\n            return CaMtlBaseEncoder(self.config, data_args=self.data_args)\n        else:\n            return _BertEncoder(self.config)\n\n    @staticmethod\n    def get_base_model(model_name_or_path):\n        if model_name_or_path == \"CA-MTL-large\":\n            return \"bert-large-cased\"\n        elif model_name_or_path == \"CA-MTL-base\":\n            return \"bert-base-cased\"\n        else:\n            return model_name_or_path\n\n    def freeze_encoder_layers(\n        self,\n        model_args,\n        unfrozen_modules=[\n            \"random_weight_matrix\",\n            \"film.gb_weights\",\n            \"ln_weight_modulation.gb_weights\",\n            \"adapter\",\n        ],\n    ):\n        if model_args.freeze_encoder_layers is not None:\n            start_layer, end_layer = model_args.freeze_encoder_layers.split(\"-\")\n\n            for name, param in self.bert.named_parameters():\n                requires_grad = True\n                match = re.match(self.bert.get_layer_regexp(), name)\n                if match:\n                    layer_number = int(match.groups()[0])\n                    requires_grad = not int(start_layer) <= layer_number <= int(\n                        end_layer\n                    ) or any([module in match.string for module in unfrozen_modules])\n                elif name.startswith(\"embedding\"):\n                    requires_grad = False\n                param.requires_grad = requires_grad\n\n        for name, param in self.bert.named_parameters():\n            logger.info(\n                \"%s - %s\", name, (\"Unfrozen\" if param.requires_grad else \"FROZEN\")\n            )\n\nFile Path: src/model/decoder.py\nContent:\nimport torch\nimport numpy\nfrom scipy.stats import entropy\nfrom transformers import glue_tasks_num_labels\nfrom torch.nn import MSELoss, CrossEntropyLoss, Softmax, Dropout, Linear, Softmax\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, hidden_size, task_name):\n        super().__init__()\n        self.num_labels = glue_tasks_num_labels[task_name]\n        self.dropout = Dropout(0.1)\n        self.model = Linear(hidden_size, self.num_labels)\n\n    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n        loss = None\n        pooled_output = self.dropout(pooled_output)\n        logits = self.model(pooled_output)\n\n        batch_entropy = self.calculate_entropy(logits)\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.long().view(-1))\n\n        return logits, loss, batch_entropy\n\n    def calculate_entropy(self, logits):\n        probas = Softmax(dim=1)(logits.detach())\n        samples_entropy = entropy(probas.transpose(0, 1).cpu())\n        even_preds = numpy.array(\n            [[1 / self.num_labels for _ in range(self.num_labels)]]\n        )\n        max_entropy = entropy(even_preds.T)\n        epsilon = 1e-5\n        samples_entropy = samples_entropy / (max_entropy.item() + epsilon)\n        return torch.tensor(samples_entropy)\n\nFile Path: src/model/encoders/ca_mtl_base.py\nContent:\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalBottleNeck, ConditionalLayerNorm\n\n\nclass MyBertSelfAttention9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size//self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length/self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length/self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention9(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention9(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput9(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass BertAdapter9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bottleneck = ConditionalBottleNeck(config)\n        self.condlayernorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, bert_layer_input, hidden_states, task_embedding, task_id):\n        hidden_states = self.bottleneck(task_embedding, hidden_states)\n        hidden_states = self.condlayernorm(hidden_states + bert_layer_input, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAdapterLayer9(nn.Module):\n    \"\"\"Adapter Layer trained from scratch (sub layer names are changed)\"\"\"\n    def __init__(self, config):\n        super(MyBertAdapterLayer9, self).__init__()\n        self.new_attention = MyBertAttention9(config)\n        self.new_intermediate = BertIntermediate(config)\n        self.new_output = MyBertOutput9(config)\n        self.adapter = BertAdapter9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.new_attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        intermediate_output = self.new_intermediate(attention_output)\n        layer_output = self.new_output(\n            intermediate_output, attention_output, task_embedding=task_embedding, task_id=task_id\n        )\n        adapted_layer_output = self.adapter(\n            attention_output, layer_output, task_embedding=task_embedding, task_id=task_id\n        )\n        outputs = (adapted_layer_output,) + outputs\n        return outputs\n\n\nclass MyBertLayer9(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention9(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput9(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer9(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention9(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder9(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2\n        num_mybert_layers = config.num_hidden_layers//2-1\n        assert num_bert_layers+num_mybert_layers+1 == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer9(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer9(config) for _ in range(num_mybert_layers)] +\n            [MyBertAdapterLayer9(config)]  # FiLM8\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlBaseEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for base) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- conditional adapter for the top layer only at layer=11 for base and layer=23 for large\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder9(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\.([0-9]+)\\..*\"\nFile Path: src/model/encoders/ca_mtl_large.py\nContent:\nimport math\nimport torch\nimport torch.nn as nn\nfrom transformers.modeling_bert import (\n    BertEmbeddings,\n    BertPooler,\n    BertPreTrainedModel,\n    BertAttention,\n    BertIntermediate,\n    BertLayer,\n    BertSelfOutput,\n)\n\nfrom src.model.encoders.conditional_modules import FiLM, CBDA, ConditionalLayerNorm, ConditionalBottleNeck\n\n\nclass MyBertSelfAttention10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n        self.max_seq_length = config.max_seq_length\n        assert config.hidden_size % self.max_seq_length == 0, \\\n            \"Block decomposed attention will only work if this condition is met.\"\n        self.num_blocks = config.hidden_size // self.max_seq_length\n        self.cond_block_diag_attn = CBDA(\n            config.hidden_size, math.ceil(self.max_seq_length / self.num_blocks), self.num_blocks\n        )  # d x L/N\n\n        self.random_weight_matrix = nn.Parameter(\n            torch.zeros(\n                [config.max_seq_length, math.ceil(self.max_seq_length / self.num_blocks)]\n            ),\n            requires_grad=True,\n        )\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        if encoder_hidden_states is not None:\n            mixed_value_layer = self.value(encoder_hidden_states)\n            attention_mask = encoder_attention_mask\n        else:\n            mixed_value_layer = self.value(hidden_states)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        mixed_key_layer = self.key(hidden_states)\n        mixed_query_layer = self.query(hidden_states)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        attention_scores1 = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n\n        attention_scores2 = self.cond_block_diag_attn(\n            x_cond=task_embedding,\n            x_to_film=self.random_weight_matrix,\n        )\n\n        attention_scores = attention_scores1 + attention_scores2.unsqueeze(1)\n\n        # b x seq len x hid dim\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # y = ax + b(task_emb)\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(\n            attention_scores\n        )  # b x num heads x seq length x head dim\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs)\n            if self.output_attentions\n            else (context_layer,)\n        )\n        return outputs\n\n\nclass MyBertSelfOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertOutput10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = ConditionalLayerNorm(config.hidden_size, config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor, task_embedding, task_id):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor, task_embedding, task_id)\n        return hidden_states\n\n\nclass MyBertAttention10(BertAttention):\n    def __init__(self, config, add_conditional_layernorm=True):\n        super().__init__(config)\n        self.self = MyBertSelfAttention10(config)\n        self.add_conditional_layernorm = add_conditional_layernorm\n        if add_conditional_layernorm:\n            self.output = MyBertSelfOutput10(config)\n        else:\n            self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            task_embedding=task_embedding,\n        )\n        if self.add_conditional_layernorm:\n            attention_output = self.output(self_outputs[0], hidden_states, task_embedding, task_id)\n        else:\n            attention_output = self.output(self_outputs[0], hidden_states)\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass MyBertLayer10(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = MyBertAttention10(config)\n        self.is_decoder = config.is_decoder\n        if self.is_decoder:\n            self.crossattention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = MyBertOutput10(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[\n            1:\n        ]  # add self attentions if we output attention weights\n\n        if self.is_decoder and encoder_hidden_states is not None:\n            cross_attention_outputs = self.crossattention(\n                attention_output,\n                attention_mask,\n                head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n            )\n            attention_output = cross_attention_outputs[0]\n            outputs = (\n                outputs + cross_attention_outputs[1:]\n            )  # add cross attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output, task_embedding, task_id)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass BertLayer10(BertLayer):\n    \"\"\"Same as BertLayer but with different inputs\"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        self.attention = MyBertAttention10(config, add_conditional_layernorm=False)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_embedding=None,\n        task_id=None\n    ):\n        self_attention_outputs = self.attention(\n            hidden_states, attention_mask, head_mask, task_embedding=task_embedding, task_id=task_id\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + outputs\n        return outputs\n\n\nclass MyBertEncoder10(nn.Module):\n    def __init__(self, config, tasks):\n        super().__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.task_transformation = nn.Linear(config.hidden_size, config.hidden_size)\n        num_bert_layers = config.num_hidden_layers//2 + config.num_hidden_layers % 2\n        num_mybert_layers = config.num_hidden_layers//2\n        assert num_bert_layers+num_mybert_layers == config.num_hidden_layers\n        self.layer = nn.ModuleList(\n            [BertLayer10(config) for _ in range(num_bert_layers)] +\n            [MyBertLayer10(config) for _ in range(num_mybert_layers)]\n        )\n        assert len(self.layer) == config.num_hidden_layers\n        #FiLM6\n        self.adapter_layer = nn.ModuleList(\n            [\n                ConditionalBottleNeck(config)\n                for _ in range(config.num_hidden_layers)\n            ]\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        task_type=None,\n        task_embedding=None\n    ):\n        all_hidden_states = ()\n        all_attentions = ()\n        task_embedding = self.task_transformation(task_embedding)\n        hidden_film = torch.zeros_like(hidden_states)\n        for i, (layer_module, adapter_module) in enumerate(zip(self.layer, self.adapter_layer)):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(\n                hidden_states,\n                attention_mask,\n                head_mask[i],\n                encoder_hidden_states,\n                encoder_attention_mask,\n                task_embedding,\n                task_type\n            )\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n            # FiLM layer with a skip connection\n            hidden_film = adapter_module(\n                x_cond=task_embedding, hidden_states=hidden_states + hidden_film\n            )\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_film,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass CaMtlLargeEncoder(BertPreTrainedModel):\n    r\"\"\"\n    # NOTE: Combination of: (might work best for large) and uses:\n        -- block diagonal attention\n        -- conditional layer norm for the top half layers base=6-10 and large=11-22, top layer excluded\n        -- conditional bias attention term to the original attention matrix\n        -- down/projection bottleneck for all layers\n        -- conditional alignment after the embedding layer only\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you're often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertFiLMModel.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    \"\"\"\n\n    def __init__(self, config, data_args=None, **kwargs):\n        super().__init__(config)\n        tasks = data_args.tasks\n        self.task_id_2_task_idx = {i: i for i, t in enumerate(tasks)}\n        self.config = config\n        self.config.num_tasks = len(tasks)\n        config.max_seq_length = data_args.max_seq_length\n        self.task_type_embeddings = nn.Embedding(len(tasks), config.hidden_size)\n        self.conditional_alignment = FiLM(\n            config.hidden_size, config.hidden_size\n        )  # FiLM5\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = MyBertEncoder10(config, tasks)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"Prunes heads of the model.\n        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        See base class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        **kwargs,\n    ):\n        \"\"\"Forward pass on the Model.\n\n        The model can behave as an encoder (with only self-attention) as well\n        as a decoder, in which case a layer of cross-attention is added between\n        the self-attention layers, following the architecture described in:\n        .. _`Attention is all you need`:\n            https://arxiv.org/abs/1706.03762\n\n        \"\"\"\n        task_type = self._create_task_type(kwargs[\"task_id\"])\n        task_embedding = self.task_type_embeddings(task_type)\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                causal_mask = causal_mask.to(\n                    torch.long\n                )  # not converting to long will cause errors with pytorch version < 1.3\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n\n            if encoder_attention_mask.dim() == 3:\n                encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n            elif encoder_attention_mask.dim() == 2:\n                encoder_extended_attention_mask = encoder_attention_mask[\n                    :, None, None, :\n                ]\n            else:\n                raise ValueError(\n                    \"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(\n                        encoder_hidden_shape, encoder_attention_mask.shape\n                    )\n                )\n\n            encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # fp16 compatibility\n            encoder_extended_attention_mask = (\n                1.0 - encoder_extended_attention_mask\n            ) * -10000.0\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n        embedding_output = self.conditional_alignment(\n            x_cond=task_embedding,\n            x_to_film=embedding_output,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            task_type=task_type,\n            task_embedding=task_embedding,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def _create_task_type(self, task_id):\n        task_type = task_id.clone()\n        unique_task_ids = torch.unique(task_type)\n        unique_task_ids_list = (\n            unique_task_ids.cpu().numpy()\n            if unique_task_ids.is_cuda\n            else unique_task_ids.numpy()\n        )\n        for unique_task_id in unique_task_ids_list:\n            task_type[task_type == unique_task_id] = self.task_id_2_task_idx[\n                unique_task_id\n            ]\n        return task_type\n\n    @classmethod\n    def get_layer_regexp(cls):\n        return r\"encoder.layer.*\\.([0-9]+)\\..*\"\nFile Path: src/model/encoders/conditional_modules.py\nContent:\nimport torch\nimport numbers\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FiLM(nn.Module):\n    \"\"\" Feature-wise Linear Modulation (FiLM) layer\"\"\"\n    def __init__(self, input_size, output_size, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(FiLM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        return out\n\n\nclass CBDA(nn.Module):\n    \"\"\" Conditional Block Diagonal Attention (CBDA) layer\"\"\"\n    def __init__(self, input_size, output_size, blocks=1, num_film_layers=1, layer_norm=False):\n        \"\"\"\n        :param input_size: feature size of x_cond\n        :param output_size: feature size of x_to_film\n        :param layer_norm: true or false\n        \"\"\"\n        super(CBDA, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_film_layers = num_film_layers\n        self.layer_norm = nn.LayerNorm(output_size) if layer_norm else None\n        self.blocks = blocks\n        film_output_size = self.output_size * num_film_layers * 2\n        self.gb_weights = nn.Linear(self.input_size, film_output_size)\n        self.gb_weights.bias.data.fill_(0)\n\n    def forward(self, x_cond, x_to_film):\n        gb = self.gb_weights(x_cond).unsqueeze(1)\n        gamma, beta = torch.chunk(gb, 2, dim=-1)\n        out = (1 + gamma) * x_to_film + beta\n        if self.layer_norm is not None:\n            out = self.layer_norm(out)\n        out = [torch.block_diag(*list(out_b.chunk(self.blocks, 0))) for out_b in out]\n        out = torch.stack(out)\n        return out[:, :, :out.size(1)]\n\n\nclass ConditionalLayerNorm(nn.Module):\n    r\"\"\"Applies Conditional Layer Normalization over a mini-batch of inputs.\n\n    .. math::\n        y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma(z) + \\beta(z)\n\n    The mean and standard-deviation are calculated separately over the last\n    certain number dimensions which have to be of the shape specified by\n    :attr:`normalized_shape`.\n    :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n\n    .. note::\n        Unlike Batch Normalization and Instance Normalization, which applies\n        scalar scale and bias for each entire channel/plane with the\n        :attr:`affine`, Layer Normalization applies per-element scale and\n        bias with :attr:`elementwise_affine`.\n\n    This layer uses statistics computed from input data in both training and\n    evaluation modes. The affine transformation is molulated by a conditional tensor.\n    In our case, we use task embeddings z.\n\n    Args:\n        normalized_shape (int or list or torch.Size): input shape from an expected input\n            of size\n\n            .. math::\n                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n            If a single integer is used, it is treated as a singleton list, and this module will\n            normalize over the last dimension which is expected to be of that specific size.\n        eps: a value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: a boolean value that when set to ``True``, this module\n            has learnable per-element affine parameters initialized to ones (for weights)\n            and zeros (for biases). Default: ``True``.\n\n    Shape:  - Input: :math:`(N, *)`  - Output: :math:`(N, *)` (same shape as input)\n\n    Examples::\n\n        >>> input_ = torch.randn(20, 5, 10, 10)\n        >>> condition = torch.randn(20, 10)\n        >>> # With Learnable Parameters\n        >>> m = ConditionalLayerNorm([10, 10])\n        >>> # Normalize over last dimension of size 10\n        >>> m = nn.LayerNorm(10)\n        >>> # Activating the module\n        >>> output = m(input_, condition)\n\n    .. _`Layer Normalization`: https://arxiv.org/abs/1607.06450\n    .. _`Conditional Layer Normalization`: https://arxiv.org/\n    \"\"\"\n    __constants__ = ['normalized_shape', 'condition_size', 'weight', 'bias', 'eps']\n\n    def __init__(self, normalized_shape, condition_size, eps=1e-5):\n        super(ConditionalLayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n\n        self.condition_size = condition_size\n        self.eps = eps\n\n        self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.ln_weight_modulation = FiLM(condition_size, sum(normalized_shape))\n        self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.weight)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input_, condition, task_id):\n        unique_task_ids = torch.unique(task_id)\n        cln_output = torch.zeros_like(input_)\n        for unique_task_id in unique_task_ids:\n            task_id_filter = task_id == unique_task_id\n            task_emb = condition[task_id_filter][0].unsqueeze(0)\n            weight = self.ln_weight_modulation(task_emb, self.weight).view(-1)\n            cln_output[task_id_filter] = F.layer_norm(input_[task_id_filter], self.normalized_shape, weight, self.bias, self.eps)\n        return cln_output\n\n    def extra_repr(self):\n        return '{normalized_shape}, {condition_size}, eps={eps}'.format(**self.__dict__)\n\n\nclass ConditionalBottleNeck(nn.Module):\n    \"\"\"Down projection and up projection with FiLM layers within Transformer layer.\"\"\"\n    def __init__(self, config):\n        super(ConditionalBottleNeck, self).__init__()\n        self.emb_transf = nn.Linear(config.hidden_size, config.hidden_size)\n        self.hidden_modulation = FiLM(config.hidden_size, config.hidden_size)\n        self.down_proj_layer = nn.Linear(config.hidden_size, config.hidden_size//3)\n        self.up_proj_layer = nn.Linear(config.hidden_size//3, config.hidden_size)\n\n    def forward(self, x_cond, hidden_states):\n        x_cond = self.emb_transf(x_cond)\n        hidden_states = self.hidden_modulation(x_cond=x_cond, x_to_film=hidden_states)\n        hidden_states = self.down_proj_layer(hidden_states)\n        hidden_states = self.up_proj_layer(hidden_states)\n        return hidden_states\n\nFile Path: src/mtl_trainer.py\nContent:\nimport os\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional, Callable, Tuple\n\nimport torch\nimport numpy as np\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\nfrom transformers import Trainer, TrainingArguments, EvalPrediction, glue_output_modes\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\n\nfrom src.data.glue_utils import compute_glue_metrics\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MultiTaskTrainingArguments(TrainingArguments):\n    use_mt_uncertainty: bool = field(\n        default=False,\n        metadata={\"help\": \"Use MT-Uncertainty sampling method\"},\n    )\n    uniform_mt_sampling: bool = field(\n        default=False,\n        metadata={\"help\": \"Sample each task an equal amount to times per epoch.\"},\n    )\n    percent_of_max_data_size: float = field(\n        default=1.0,\n        metadata={\n            \"help\": \"If uniform_mt_sampling=True, specify the samples per task per \"\n            \"epoch based on the maximum dataset length. If below 0.0 or above 1.0,\"\n            \"it will be set to the closest of 0.0 or 1.0.\"\n        },\n    )\n    warmup_proportion: float = field(\n        default=0.1,\n        metadata={\"help\": \"0.0 to args.lr for warmup_proportion * num_training_steps\"},\n    )\n\n\nclass MultiTaskTrainer(Trainer):\n    def __init__(\n        self,\n        tokenizer,\n        data_args,\n        eval_datasets=None,\n        test_datasets=None,\n        *args,\n        **kwargs,\n    ):\n        super(MultiTaskTrainer, self).__init__(*args, **kwargs)\n        self.tokenizer = tokenizer\n        self.data_args = data_args\n        self.eval_datasets = eval_datasets\n        self.test_datasets = test_datasets\n        self.eval_results = {}\n\n    def get_optimizers(\n        self, num_training_steps: int\n    ) -> Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]:\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well.\n        If you want to use something else, you can pass a tuple in the Trainer's init,\n        or override this method in a subclass.\n        \"\"\"\n        if self.optimizers is not None:\n            return self.optimizers\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": self.args.weight_decay,\n            },\n            {\n                \"params\": [\n                    p\n                    for n, p in self.model.named_parameters()\n                    if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        num_warmup_steps = (\n            self.args.warmup_proportion * num_training_steps\n        )  # this is different from overridden function\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.args.learning_rate,\n            eps=self.args.adam_epsilon,\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=num_training_steps,  # this is different from overridden function\n        )\n        return optimizer, scheduler\n\n    def get_train_dataloader(self) -> DataLoader:\n        if self.args.use_mt_uncertainty:\n            return self._create_custom_dataloader()\n        else:\n            return super().get_train_dataloader()\n\n    def _create_custom_dataloader(self):\n        class MtUcertaintyIterator:\n            \"\"\"Sample tasks using uncertainty measure.\"\"\"\n\n            def __init__(self, my_loader):\n                self.my_loader = my_loader\n                self.loader_iters = [iter(loader) for loader in self.my_loader.loaders]\n                self.loader_iter_sizes = [len(i) for i in self.loader_iters]\n                self.max_count = len(self.my_loader)\n                self.batch_count = 0\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self.batch_count == self.max_count:\n                    self.batch_count = 0\n                    raise StopIteration()\n\n                test_batch = {}\n                for idx, loader_iter in enumerate(self.loader_iters):\n                    try:\n                        batch = loader_iter.__next__()\n                    except StopIteration:\n                        new_loader_iter = iter(self.my_loader.loaders[idx])\n                        self.loader_iters[idx] = new_loader_iter\n                        batch = new_loader_iter.__next__()\n\n                    test_batch = self.batchify_data(batch, test_batch)\n\n                inputs = {}\n                for k, v in test_batch.items():\n                    if k not in [\"labels\"]:\n                        inputs[k] = v.detach().to(self.my_loader.args.device)\n\n                with torch.no_grad():\n                    model.select_batch_mode = True\n                    outputs = model(**inputs)\n                    model.select_batch_mode = False\n\n                (\n                    test_batch_entropy,\n                    test_batch_entropy_mean,\n                    max_mean_batch_entropy,\n                ) = outputs[-3:]\n\n                for _, v in inputs.items():\n                    del v  # free GPU mem\n                del inputs\n\n                test_batch_entropy_mean = (\n                    test_batch_entropy_mean / max_mean_batch_entropy\n                )\n                test_batch_entropy = test_batch_entropy * test_batch_entropy_mean\n\n                if \"sts-b\" in tasks and \"mrpc\" in tasks:\n                    stsb_idx = test_batch[\"task_id\"] == tasks.index(\"sts-b\")\n                    mrpc_idx = test_batch[\"task_id\"] == tasks.index(\"mrpc\")\n                    num_items = min(\n                        len(test_batch_entropy[stsb_idx]),\n                        len(test_batch_entropy[mrpc_idx]),\n                    )\n                    stsb_idx = stsb_idx.nonzero()[:num_items]\n                    mrpc_idx = mrpc_idx.nonzero()[:num_items]\n                    test_batch_entropy[stsb_idx] = test_batch_entropy[mrpc_idx]\n                    test_batch_entropy_mean[stsb_idx] = test_batch_entropy_mean[\n                        mrpc_idx\n                    ]\n\n                select_size = min(\n                    self.my_loader.args.train_batch_size,\n                    test_batch[\"input_ids\"].shape[0],\n                )  # Handled the last batch if it is lower than the batch size\n\n                top_entropy = torch.topk(test_batch_entropy, select_size)\n\n                for k, v in test_batch.items():\n                    test_batch[k] = torch.index_select(v, 0, top_entropy.indices)\n\n                self.batch_count += 1\n\n                return test_batch\n\n            @staticmethod\n            def batchify_data(data, curr_batch):\n                for k in data.keys():\n                    if k in curr_batch.keys():\n                        curr_batch[k] = torch.cat((curr_batch[k], data[k]), dim=0)\n                    else:\n                        curr_batch[k] = data[k]\n                return curr_batch\n\n        class CustomLoader:\n            def __init__(self, loaders, datasets, loader_args):\n                self.loaders = loaders\n                self.dataset = datasets\n                self.args = loader_args\n                self.current_epoch = 0\n\n            def __iter__(self):\n                iterator = MtUcertaintyIterator(self)\n\n                # for determinism across runs\n                # https://github.com/pytorch/examples/issues/501\n                for l in self.loaders:\n                    if isinstance(l.sampler, DistributedSampler):\n                        l.sampler.set_epoch(self.current_epoch)\n                self.current_epoch += 1\n                return iterator\n\n            def __len__(self):\n                loader_len = [len(loader) for loader in self.loaders]\n                if self.args.uniform_mt_sampling:\n                    return int(\n                        self.args.percent_of_max_data_size\n                        * max(loader_len)\n                        * len(self.loaders)\n                        / self.args.train_batch_size\n                    )\n                elif self.args.use_mt_uncertainty:\n                    return int(\n                        max(loader_len)\n                        * len(self.loaders)\n                        * self.args.percent_of_max_data_size\n                    )\n                else:\n                    return sum(loader_len)\n\n        model = self.model\n        tasks = self.data_args.tasks\n\n        data_loaders = []\n        for dataset in self.train_dataset.datasets:\n            train_sampler = (\n                RandomSampler(dataset)\n                if self.args.local_rank == -1\n                else DistributedSampler(dataset)\n            )\n\n            data_loader = DataLoader(\n                dataset,\n                batch_size=self.args.train_batch_size,\n                sampler=train_sampler,\n                collate_fn=self.data_collator.collate_batch,\n            )\n            data_loaders.append(data_loader)\n\n        return CustomLoader(data_loaders, self.train_dataset, self.args)\n\n    def evaluate(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        prediction_loss_only: Optional[bool] = None,\n        context: str = None,\n        do_test_if_needed: bool = True,\n    ):\n        datasets = eval_dataset or self.eval_datasets\n        logger.info(\"*** Evaluate on dev ***\")\n        for task_name, eval_dataset in datasets.items():\n            logger.info(task_name)\n            self.compute_metrics = self.build_compute_metrics_fn(eval_dataset)\n            eval_result = super().evaluate(\n                eval_dataset=eval_dataset, prediction_loss_only=True\n            )\n            self.update_eval_results(eval_result, task_name)\n            for key, value in self.eval_results[task_name].items():\n                logger.info(\"  %s = %s\", key, value)\n\n    def predict(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        prediction_loss_only: Optional[bool] = None,\n    ):\n        logging.info(\"*** Test ***\")\n        datasets = eval_dataset or self.test_datasets\n        for task_name, test_dataset in datasets.items():\n            logger.info(task_name)\n            predictions = super().predict(test_dataset=test_dataset).predictions\n            output_mode = glue_output_modes[task_name] \n            if output_mode == \"classification\":\n                predictions = np.argmax(predictions, axis=1)\n\n            output_test_file = os.path.join(\n                self.args.output_dir,\n                f\"{task_name}_test_iter_{self.global_step}.tsv\",\n            )\n            if self.is_world_master():\n                with open(output_test_file, \"w\") as writer:\n                    logger.info(\"***** Test results {} *****\".format(task_name))\n                    writer.write(\"index\\tprediction\\n\")\n                    for index, item in enumerate(predictions):\n                        if output_mode == \"regression\":\n                            writer.write(\"%d\\t%3.3f\\n\" % (index, item))\n                        else:\n                            writer.write(\n                                \"%d\\t%s\\n\" % (index, test_dataset.get_labels()[item])\n                            )\n\n    def update_eval_results(self, results, task_name):\n        self.eval_results[task_name] = self.eval_results.get(task_name, {})\n        for key, value in results.items():\n            if key in self.eval_results[task_name] and 'loss' not in key and 'epoch' not in key:\n                value = max(self.eval_results[task_name][key], value)\n            self.eval_results[task_name][key] = value\n\n\n    @staticmethod\n    def build_compute_metrics_fn(\n        eval_dataset\n    ) -> Callable[[EvalPrediction], Dict]:\n        def compute_metrics_fn(p: EvalPrediction):\n            return compute_glue_metrics(eval_dataset.task_name, p)\n\n        return compute_metrics_fn\n\n",
        "experimental_info": "Model Architecture Configuration (CaMtlArguments in src/model/ca_mtl.py):\n- `model_name_or_path`: Path to pretrained model or model identifier (e.g., \"CA-MTL-base\", \"CA-MTL-large\", \"bert-base-cased\", etc.).\n- `freeze_encoder_layers`: Specifies a layer range (e.g., \"0-5\") to freeze encoder layers. If set, specific conditional modules (\"random_weight_matrix\", \"film.gb_weights\", \"ln_weight_modulation.gb_weights\", \"adapter\") within the frozen range remain unfrozen.\n\nCA-MTL Base Encoder Specifics (src/model/encoders/ca_mtl_base.py):\n- Utilizes: block diagonal attention, conditional layer normalization for the top half layers (layers 6-10), a conditional bias attention term added to the original attention matrix, a conditional adapter for the top layer only (layer 11), and a conditional alignment layer after the embedding layer.\n\nCA-MTL Large Encoder Specifics (src/model/encoders/ca_mtl_large.py):\n- Utilizes: block diagonal attention, conditional layer normalization for the top half layers (layers 11-22), a conditional bias attention term added to the original attention matrix, a down/projection bottleneck for all layers, and a conditional alignment layer after the embedding layer.\n\nMulti-Task Training Arguments (MultiTaskTrainingArguments in src/mtl_trainer.py):\n- `use_mt_uncertainty`: Boolean flag to enable Multi-Task Uncertainty sampling method (default: False).\n- `uniform_mt_sampling`: Boolean flag to sample each task an equal amount of times per epoch (default: False).\n- `percent_of_max_data_size`: When `uniform_mt_sampling` is True, this float (0.0 to 1.0) specifies the number of samples per task per epoch, relative to the maximum dataset length (default: 1.0).\n- `warmup_proportion`: Float representing the proportion of training steps for learning rate warmup (default: 0.1)."
      }
    },
    {
      "title": "Adapters Strike Back",
      "abstract": "Adapters provide an efficient and lightweight mechanism for adapting trained\ntransformer models to a variety of different tasks. However, they have often\nbeen found to be outperformed by other adaptation mechanisms, including\nlow-rank adaptation. In this paper, we provide an in-depth study of adapters,\ntheir internal structure, as well as various implementation choices. We uncover\npitfalls for using adapters and suggest a concrete, improved adapter\narchitecture, called Adapter+, that not only outperforms previous adapter\nimplementations but surpasses a number of other, more complex adaptation\nmechanisms in several challenging settings. Despite this, our suggested adapter\nis highly robust and, unlike previous work, requires little to no manual\nintervention when addressing a novel scenario. Adapter+ reaches\nstate-of-the-art average accuracy on the VTAB benchmark, even without a\nper-task hyperparameter optimization.",
      "full_text": "Adapters Strike Back Jan-Martin O. Steitz1 Stefan Roth1,2 1Department of Computer Science, TU Darmstadt 2 hessian.AI Abstract Adapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of dif- ferent tasks. However, they have often been found to be outperformed by other adaptation mechanisms, including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as vari- ous implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architec- ture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual inter- vention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.† 1. Introduction Transfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [55] to a downstream task by fully fine-tuning the model’s parameters is a common paradigm. A typical CNN architecture, like a ResNet [24], has several tens of millions of parameters. However, since the introduction of transformers [59] into the realm of com- puter vision [4, 5, 13, 51, 52, 63], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [ 13] to more than a billion parameters [10, 46]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model’s parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g., 56] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learn- ing is too small. One solution is linear probing [12], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning. As a consequence, there is a growing interest in parameter- efficient tuning methods. The main idea is to freeze the †Code is available at https://github.com/visinf/adapter_plus. 0 0.1 0.2 0.3 0.4 0.5 0.6 72 74 76 78 Fine-tuning accuracy Linear probing # Parameters (M) Accuracy (%) Adapter+ (ours) SPT -Adapter [21]SSF [39] Adapter+ opt. (ours)SPT -Adapter⟳ SSF⟳ FacT -TK [32] Consolidator [20]VPT [31] FacT -TK⟳ LoRA [29] VPT⟳ Figure 1. Parameter-accuracy characteristics of adaptation methods on the VTAB [65] test sets. We report original results and re-evaluations ( ⟳) after a complete training schedule with suitable data normalization. Our Adapter+ has clearly the best parameter-accuracy trade-off. The vertical, dashed line shows the possible minimal number of tunable parameters when only the classifiers are trained, i.e., using linear probing (61% accuracy). parameters of the pre-trained model and add a compara- tively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [31], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [29], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [28], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [53] and various formulations [22, 28, 49] exist for the now common ViT architecture. Recent work on parameter-efficient transfer learning [e.g., 20, 21, 31, 32, 39, 67] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their imple- mentations, which we find to have a negative influence on the To appear in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, W A, USA, 2024. © 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. arXiv:2406.06820v1  [cs.CV]  10 Jun 2024LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 80 81 82 83 84 82.4 82.5 80.3 82.6 82.4 82.2 84.0 Accuracy (%) Natural LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 85 86 84.3 84.6 85.7 84.7 86.3 85.3 86.5 Accuracy (%) Specialized LoRAVPT ⟳ SSF ⟳ FacT ⟳ Consol.SPT ⟳ Adapter+ 58 60 62 60.1 62.1 58.0 62.3 60.9 60.5 63.3 Accuracy (%) Structured Figure 2. Average accuracy for VTAB subgroups on thetest sets. For methods marked with ⟳, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank r∈[1..32]. adaptation performance. For further details, refer to the sup- plemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter config- urations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter- efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [67] and 60.8% in [31]. In this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in con- nection with ViTs. Our contribution hereby is threefold: (1) We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter’s inner structure with ViTs, as well as evaluate differ- ent variants of parameter initialization. (2) We further pro- pose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks. (3) Finally, we present Adapter+, an adapter configu- ration with an excellent parameter-accuracy trade-off com- pared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [65] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [31] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2. 2. Related work One possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [12]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [ 64] then showed the efficacy of this method for NLP transformers. Modular adaptation. The concept of adding small, train- able modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [53] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [19], learned binary masks to prune a pre-trained network per target task [ 41], or created a student network by aug- menting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [43]. Following the rise of transformers in NLP [ 11, 50, 59], Houlsby et al. [28] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [49] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer’s feed-forward network (FFN), thus saving roughly half of the parameters over [28]. Prompt tuning. Inspired by changing the output of a net- work for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [37] proposed prompt tuning: A set of learnable to- kens is added to the input sequence and trained with back- propagation to prompt a frozen language model to perform downstream tasks. Li and Liang [38] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [31] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT- Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep). Low-rank approaches. Also focusing on the attention part of the transformer layers, Hu et al. [29] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer’s FFN. He et al. [22] proposed a formalism to unify LoRA, adapters, and prefix tuning [ 38]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers. 2Other related work. Newer approaches for vision trans- formers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [67] per- forms an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [39] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable mod- ules. Jie and Deng [32] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT). SPT [21] measures the importance of the weights of a pre- trained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [20] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference. Despite these new developments, we show that the sim- ple concept of adapters exhibits an even better parameter- accuracy trade-off in combination with vision transformers – if done right and with the addition of a channel-wise scaling. 3. Adapters for vision transformers 3.1. Vision transformer basics In this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [ 13]. The ViT is closely modeled after the transformer model for natural lan- guage processing (NLP) proposed by Vaswani et al. [59]. A learned linear projection embeds non-overlapping and flat- tened patches of the input image into a sequence of n tokens x ∈ Rn×d, where d is called the hidden dimension of the transformer. A positional encoding is added to the embed- dings and the sequence is prepended with a trainable [CLS] token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN). For the self-attention, the tokens are projected to queries, keys, and values (Q, K, and V ) and the output of each of the M attention heads is calculated as Attention(x) =Softmax \u0012Q(x)K(x)T √ d′ \u0013 V (x), (1) with d′ = d/M being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights Wi and biases bi) and a GELU [ 26] non-linearity as activation in between: FFN(x) =GELU(xW1 + b1)W2 + b2. (2) Both attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence x sequentially as x 7→ Attention(LN(x)) +x (3a) x 7→ FFN(LN(x)) +x. (3b) To keep the notation concise, we will omit the LNs of atten- tion and FFN in the following; each attention and FFN is assumed to be always preceded by an LN. 3.2. Adapters and their inner structure Adapters [28] are small modules that are added to the trans- former layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classi- fier are trained. Adapters take the form of bottlenecks with an inner dimension of r ≪ d. We call r the rank of the adapter. In detail, a down-projection to dimension r with weights Wdown ∈ Rd×r and biases bdown ∈ Rr is followed by a non-linear activation function σ(·), typically a GELU [26] as used throughout the ViT, and an up-projection with weights Wup ∈ Rr×d and biases bup ∈ Rd back to the hid- den dimension d of the transformer layer. This yields a base adapter module Adapterbase(x) =σ(xWdown + bdown)Wup + bup . (4) The base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [ 1]. Additionally, the output of the bottleneck can be scaled by s as Adapter(x) =s · Adapterbase \u0000 LN(x) \u0001 . (5) For layer-wise scaling, the factor s is taken to be a scalar, i.e. s ∈ R, and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [22] and Hu et al. [29] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where s ∈ Rd. We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes x 7→ Adapter(x) +x. (6) The complete inner structure of an adapter including its skip connection is visualized in Fig. 3a. 3.3. Adapter positions Although the architecture of bottleneck adapters for trans- formers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet in- vestigated what the optimum position is for the use with a ViT [13]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed 3FF down FF up Act LN Scaling (a) Inner structure FFN Adapter (b) Pre FFN Adapter (c) Post FFN Adapter (d) Parallel FFN Adapter (e) Intermediate Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)–(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue. by a frozen module in the network rather than to anticipate what changes to the features are needed in adapting for a frozen module that follows the adapter. Putting it differently, we argue that the adapter should follow a frozen module. Pre-Adapter. The first adapter position we analyze ap- plies the adapter to the output x of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes x 7→ FFN \u0000 Adapter(x) +x \u0001 + \u0000 Adapter(x) +x \u0001 . (7) Note that the two occurrences of Adapter(x) in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation hap- pening in the attention but needs to estimate the transforma- tion that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature. Post-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as x 7→ Adapter \u0000 FFN(x) +x \u0001 + \u0000 FFN(x) +x \u0001 , (8) where the FFNs refer to the same intantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [49] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT. Parallel-Adapter. Next, we consider a parallel setting as proposed by [22], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d): x 7→ FFN(x) +Adapter(x) +x. (9) Therefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN. Intermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [28]. The adapter is plugged behind the FFN but before the skip con- nection of the FFN is added (Fig. 3e). The adapter addition- ally possesses its own skip connection: x 7→ Adapter \u0000 FFN(x) \u0001 + FFN(x) +x. (10) Note that the two occurrences of FFN(x) in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN. 3.4. Initialization of adapter parameters Since training a deep learning model is a non-convex opti- mization problem, the initialization of parameters is impor- tant. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the litera- ture. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network. Houlsby initialization. Houlsby et al. [28] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of σ = 0.01, truncated at 2σ, and use zero for their biases. BERT initialization. For the BERT model [11], the initial- ization works similar to [28] but the Gaussian distribution has a standard deviation of σ = 0.02 and is not truncated. This form of initialization is used by Pfeiffer et al. [49]. LoRA initialization. LoRA [29] initializes the weights and biases of the down-projection with a uniform Kaiming He ini- tialization [23]; the weights and biases of the up-projection 4are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute. 3.5. Data normalization in pre-processing Data normalization is common practice during image pre- processing. It is typically done by shifting and scaling of each input pixel xij for each channel c as ˆxijc = (xijc − µc)/σc . (11) Most widely used are the mean µ = (0.485, 0.456, 0.406)T and standard deviation σ = (0.229, 0.224, 0.225)T of the ImageNet dataset [55], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of µ and σ, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [58]. The Im- ageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range [−1, 1]. Because we try to adapt to a target domain on a very low parameter budget, it is important to use the data normaliza- tion the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain. 4. Experiments 4.1. Datasets In order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two stan- dard benchmarks for task adaptation. VTAB. The Visual Task Adaptation Benchmark (VTAB) [65] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using stan- dard photographic equipment. The Specialized group is built from datasets of images captured with specialized equip- ment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 train- ing and 200 validation images. The test sets have the same number of images as the test sets in the original datasets. FGVC. Following Jia et al. [31], we compile five datasets for fine-grained visual classification (FGVC): CUB-200- 2011 [61], NABirds [ 27], Oxford Flowers [ 45], Stanford Dogs [34], and Stanford Cars [17]. Because VTAB bench- marks task adaptation in a low-data regime in terms of the Table 1. Adapter position. We report the average accuracy in % (± std. dev.) on the VTABval sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 82.4 ± 0.4 86.2 ± 0.8 57.5 ± 0.5 75.3 ± 0.3 Intermediate 83.0 ± 0.4 85.0 ± 0.8 57.2 ± 0.5 75.1 ± 0.3 Parallel 83.0 ± 0.3 86.2 ± 0.6 57.7 ± 0.6 75.6 ± 0.3 Post 83.0 ± 0.3 85.7 ± 0.4 59.1 ± 0.3 76.0 ± 0.2 number of available training images, we use FGVC to eval- uate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [31] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material. 4.2. Experimental settings For all our experiments, we use a ViT-B/16 network [13] that was pre-trained on ImageNet-21k [55]. We follow its pre- training settings, in particular, regarding input data normal- ization. We train all models with an AdamW [40] optimizer with a learning rate of 10−3, a weight decay of 10−4, and a batch size of 64, following [ 67]. For full fine-tuning, we use a learning rate of 10−4, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf . Sec. 3.4), we resize input images to 224×224 px for VTAB and use a randomly resize crop to 224×224 px and horizontal flipping for FGVC. For the ablations and to determine hyperparam- eters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results. 4.3. Exploring adapter configurations Adapter position. We first evaluate the four possible posi- tions to connect an adapter to the FFN section of the trans- former layer, as described in Sec. 3.3. In our ablation, we use Adapterbase (cf . Eq. (4)) with rank r=8 and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling. The results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The par- allel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to 5Table 2. Inner adapter structure. We evaluate the different com- ponents of the adapter structure, e.g., normalization layer (Norm), layer-wise and channel-wise learnable scaling on the VTAB val sets. The difference to Adapterbase (first row) is shown in ∆base. Bias Norm Scaling Initialization Accuracy (%) ∆base ✓ Houlsby 76.0 ± 0.2 0.0 Houlsby 75.6 ± 0.4 −0.4 ✓ LoRA 75.5 ± 0.3 −0.5 ✓ BERT 75.8 ± 0.3 −0.2 ✓ ✓ Houlsby 75.9 ± 0.3 −0.1 ✓ ✓ layer Houlsby 75.9 ± 0.3 −0.1 ✓ layer Houlsby 76.2 ± 0.3 +0.2 ✓ ✓ channel Houlsby 75.8 ± 0.3 −0.2 ✓ channel Houlsby 76.5 ± 0.2 +0.5 learn a residual modification to the FFN instead of a subse- quent one. Pre-Adapter and Intermediate-Adapter are subpar compared to the other positions. They either do not have access to the feature transformation happening afterwards in the FFN or to the features of the skip connection containing the output of the attention. Inner structure. Next, we investigate the impact of the in- ner structure of adapters including their initialization. Tab. 2 shows our findings with average accuracies calculated over the three VTAB subgroups. Removing the biases from the linear layers leads to a decrease in accuracy of 0.4 percent- age points (pp). We find that the Houlsby initialization of the adapter parameters is best while BERT and LoRA initializa- tions reduce the accuracy by 0.2 pp and 0.5 pp. Adding layer normalization (LN) to the adapter is slightly detrimental for all settings, both with scaling and without, while addition- ally adding 2d parameters per layer. We find that a learned scaling is in general beneficial for image-classification tasks. Adding layer-wise scaling leads to a gain of 0.2 pp. The inclusion of a learned, channel-wise scaling, as proposed here, gives the strongest improvement of 0.5 pp, reaching an accuracy of 76.5% on the VTAB validation set while only adding half of the parameters compared to LN. What makes a great adapter? From our systematic explo- ration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learn- able, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normaliza- tion, as suggested by Pfeiffer et al. [49], is not necessary and even leads to detrimental effects in our setting. Configurations from previous work. Different configu- rations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank r=8 on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing. Houlsby et al. [28] use an Intermediate-Adapter with their Table 3. Comparison of Adapter+ with adapter configurations from previous work. We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param (M) Natural Specialized Structured Average Houlsby [28], r=8 0.39 82.9 ± 0.2 85.5 ± 0.3 58.9 ± 0.8 75.8 ± 0.3 Houlsby [28], r=4 0.24 82.9 ± 0.4 84.9 ± 0.3 58.3 ± 0.6 75.4 ± 0.3 Pfeiffer [49] 0.21 82.9 ± 0.3 86.1 ± 0.9 58.4 ± 0.7 75.8 ± 0.4 AdaptFormer [6] 0.19 83.0 ± 0.4 85.0 ± 0.2 57.4 ± 0.5 75.2 ± 0.2 Adapter+ 0.20 83.0 ± 0.2 86.8 ± 0.6 59.7 ± 0.4 76.5 ± 0.2 proposed initialization both at the FFN section as well at the attention part of the transformer layer. Additionally, they adapt the LN parameters of the backbone. We, therefore, compare their setting additionally with r = 4to compare on roughly the same parameter budget. Pfeiffer et al. [49] suggest a Post-Adapter like us but with a BERT initialization and they employ a layer normalization inside the adapter. AdaptFormer [6] has the same configuration as a scaled parallel adapter (Scaled PA) [22], which was proposed for NLP tasks, the only difference being the layer-wise scalings. Scaled PA uses a fixed scaling of s = 4 for the adapters whereas AdaptFormer suggests to use s = 0.1 for vision tasks. Optimizing s for VTAB may lead to better results. Our results are presented in Tab. 3. We see a clear advantage of our Adapter+ configuration, gaining at least 0.7 pp over all previous adapter realizations considered despite having the second lowest number of trainable parameters. 4.4. Main results VTAB. We evaluate Adapter+ on the VTAB test sets and compare to other methods in Tab. 4. We provide results for full fine-tuning and tuning only the linear classifier while freezing the rest of the backbone [12] as a baseline of classi- cal fine-tuning methods. As competing parameter-efficient tuning methods, we include LoRA [29], VPT [31], NOAH [67], SSF [39], FacT [32], Consolidator [20], and SPT [21]. Wherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair compar- ison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [40] instead of SGD [54] and change to an appropriate data nor- malization. We present the original results from [ 31] on VTAB together with our re-evaluation. Our improved imple- mentation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%. SSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the 6Table 4. Detailed results on the VTAB test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. †: Early-stopping based on the test set, •: unsuitable data normalization, E: per-task hyperparameter optimization. 1Average across the average accuracies of the VTAB groups, following previous work. 2No complete code release for Consolidator, hence training and evaluation details are unknown. Natural Specialized Structured # Param (M) Cifar100 [35] Caltech101 [15] DTD [8] Flower102 [45] Pets [47] SVHN [44] Sun397 [62] Average Camelyon [60] EuroSAT [25] Resisc45 [7] Retinopathy [14] Average Clevr-Count [33] Clevr-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori [42] sNORB-Azi. [36] sNORB-Ele. [36] Average Global Average1 Full 85.8 73.2 92.6 70.4 97.9 86.2 90.6 39.6 78.6 87.1 96.6 87.5 74.0 86.3 66.6 61.0 49.8 79.7 82.6 51.9 33.5 37.0 57.8 74.2 Linear 0.04 78.1 88.1 69.0 99.1 90.0 36.0 56.9 73.9 79.8 90.7 73.7 73.7 79.5 32.4 30.5 35.9 61.9 11.2 26.2 14.3 24.5 29.6 61.0 LoRA [29] 0.29 83.0 91.7 71.6 99.2 90.9 83.8 56.7 82.4 86.2 95.7 83.5 71.9 84.3 77.7 62.3 49.0 80.2 82.2 51.7 31.0 47.0 60.1 75.6 VPT-Deep E• [31] 0.60 78.8 90.8 65.8 98.0 88.3 78.1 49.6 78.5 81.8 96.1 83.4 68.4 82.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 55.0 72.0 VPT-Deep E⟳ 0.60 83.0 93.0 71.2 99.0 91.3 84.1 56.0 82.5 84.9 96.6 82.5 74.5 84.6 77.5 58.7 49.7 79.6 86.2 56.1 37.9 50.7 62.1 76.4 NOAH E†•◦ [67] 0.43 69.6 92.7 70.2 99.1 90.4 86.1 53.7 80.2 84.4 95.4 83.9 75.8 84.9 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 61.3 75.5 SSF E† [39] 0.24 69.0 92.6 75.1 99.4 91.8 90.2 52.9 81.6 87.4 95.9 87.4 75.5 86.6 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 59.0 75.7 SSF E⟳ 0.24 61.9 92.3 73.4 99.4 92.0 90.8 52.0 80.3 86.5 95.8 87.5 72.8 85.7 77.4 57.6 53.4 77.0 78.2 54.3 30.3 36.1 58.0 74.6 FacT-TK8 E†• [32] 0.05 70.3 88.7 69.8 99.0 90.4 84.2 53.5 79.4 82.8 95.6 82.8 75.7 84.2 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 58.3 74.0 FacT-TK8 E⟳ 0.05 74.9 92.7 73.7 99.1 91.3 85.5 57.7 82.1 86.8 94.9 84.1 70.9 84.2 81.9 64.1 49.2 77.2 83.8 53.1 28.2 44.7 60.3 75.5 FacT-TK≤32 E†• [32] 0.10 70.6 90.6 70.8 99.1 90.7 88.6 54.1 80.6 84.8 96.2 84.5 75.7 85.3 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 60.7 75.6 FacT-TK≤32 E⟳ 0.10 74.6 93.7 73.6 99.3 90.6 88.7 57.5 82.6 87.6 95.4 85.5 70.4 84.7 84.3 62.6 51.9 79.2 85.5 52.0 36.4 46.6 62.3 76.5 Consolidator 2 [20] 0.30 74.2 90.9 73.9 99.4 91.6 91.5 55.5 82.4 86.9 95.7 86.6 75.9 86.3 81.2 68.2 51.6 83.5 79.8 52.3 31.9 38.5 60.9 76.5 SPT-Adapter †• [21] 0.23 72.9 93.2 72.5 99.3 91.4 84.6 55.2 81.3 85.3 96.0 84.3 75.5 85.3 82.2 68.0 49.3 80.0 82.4 51.9 31.7 41.2 60.8 75.8 SPT-Adapter ⟳ 0.22 74.7 94.1 73.0 99.1 91.2 84.5 57.5 82.0 85.7 94.9 85.7 70.2 84.1 81.3 63.2 49.1 80.7 83.5 52.0 26.4 41.5 59.7 75.3 SPT-Adapter †• [21] 0.43 72.9 93.2 72.5 99.3 91.4 88.8 55.8 82.0 86.2 96.1 85.5 75.5 85.8 83.0 68.0 51.9 81.2 82.4 51.9 31.7 41.2 61.4 76.4 SPT-Adapter ⟳ 0.43 74.9 93.2 71.6 99.2 91.1 87.9 57.2 82.2 87.0 95.4 86.5 72.4 85.3 81.1 63.2 50.3 80.2 84.4 51.4 31.5 42.2 60.5 76.0 Adapter+, r=1 0.07 85.4 92.4 73.1 99.1 91.3 83.1 58.1 83.2 87.2 96.6 85.3 72.6 85.5 80.7 60.6 50.9 79.9 83.3 55.6 27.1 43.0 60.1 76.3 Adapter+, r=2 0.09 85.4 93.0 72.7 99.2 90.6 85.3 58.0 83.5 87.9 96.8 85.5 71.4 85.4 83.2 61.0 51.6 80.1 86.1 56.3 30.7 46.5 61.9 76.9 Adapter+, r=4 0.13 84.8 93.8 72.7 99.2 90.6 86.5 57.4 83.6 87.5 96.9 85.9 71.5 85.4 83.4 61.6 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r=8 0.20 84.6 94.2 72.3 99.3 90.7 87.6 56.7 83.6 87.7 97.0 86.7 72.3 85.9 83.2 60.9 53.8 80.3 88.1 55.6 35.7 47.7 63.1 77.6 Adapter+, r=16 0.35 83.7 94.2 71.5 99.3 90.6 88.2 55.8 83.3 87.5 97.0 87.4 72.9 86.2 82.9 60.9 53.7 80.8 88.4 55.2 37.3 46.9 63.3 77.6 Adapter+, r∈[1..4] E 0.11 85.4 93.8 72.7 99.1 90.6 86.5 58.1 83.7 87.5 96.8 85.9 71.4 85.4 83.4 61.0 53.6 81.4 87.3 55.3 34.4 48.1 63.1 77.4 Adapter+, r∈[1..8] E 0.16 85.4 93.8 72.7 99.1 90.7 87.6 58.1 83.9 87.7 96.8 86.7 72.3 85.9 83.4 60.9 53.8 80.3 88.1 55.3 35.7 47.7 63.1 77.7 Adapter+, r∈[1..32] E 0.27 85.4 93.8 72.7 99.1 90.7 88.2 58.1 84.0 87.5 96.8 87.8 73.9 86.5 83.4 60.9 53.8 80.3 87.2 55.3 37.9 47.7 63.3 77.9 results after a full training schedule. For completeness, we also report the results from the original publications. How- ever, we found that the code releases of [21, 32, 39] use early stopping based on the best result on the test set. We argue that tuning hyperparameters such as the number of training epochs on the test set goes against established practices in machine learning; rather the validation set should be used for early stopping. Yet, due to the limited size of the training and validation sets in VTAB, it is not feasible to report test results without also training on the validation data. Hence, we chose to complete a full training schedule of 100 epochs instead of using early stopping. Training SSF for the full schedule leads to a decrease in average accuracy of 1.1 pp over the original publication and re-evaluating SPT leads to a decrease of up to 0.5 pp, even with a corrected data normalization. FacT on the other hand benefits from our re-revaluation, since the accuracy decrease from training a complete schedule is offset by improvements from applying the appropriate data normalization. There was no complete code release with configurations to train Consolidator on VTAB at the time of writing, hence we report results as-is. Adapter+ shows the best parameter-accuracy trade-off among all methods evaluated. This can also be clearly seen in Fig. 1. Additionally, Adapter+ sets a new state of the art with an average accuracy of up to 77.6% over all VTAB subgroups even without any per-task hyperparameter optimization. If we determine the optimal rankr per task on the validation set, we can further improve the accuracy to 77.9%. Optimizing the rank leads to a better parameter-accuracy trade-off than using a fixed rank across all tasks. In Fig. 2, we compare the average accuracy on the sub- groups of VTAB. Wherever possible, we present the results of re-evaluating methods after the last training epoch and matching the data normalization to the backbone. The aver- age accuracies of Adapter+ with r ∈ [1..32] are consistently higher than those of the competing methods. Note that the accuracies of other methods except SPT differ drastically across the different VTAB subgroups. Adapter+, on the other hand, shows a high degree of robustness to the domain shifts between groups. FGVC. Next, we present our results on the FGVC bench- mark in Tab. 5. From the contenders, only SSF [ 39] has released code and hyperparameter configurations for train- ing on FGVC at the time of writing. As we know from the code releases for VTAB, the reported numbers show the accuracy for early stopping based on the test set. There- fore, we expect a similar evaluation for FGVC. While we do not endorse early stopping based on the test set, we ad- 7Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (⟳) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results. # Param (M) CUB200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] Average Full 86.0 88.0 81.5 99.2 85.6 90.6 89.0 Linear 0.18 88.9 81.8 99.5 92.6 52.8 83.1 VPT-Deep [31] 0.85 88.5 84.2 99.0 90.2 83.6 89.1 VPT-Deep ⟳ 0.85 90.1 83.3 99.6 90.3 85.0 89.7 SSF [39] 0.39 89.5 85.7 99.6 89.6 89.2 90.7 SSF ⟳ 0.39 88.9 85.0 99.6 88.9 88.9 90.3 SPT-Adapter [21] 0.40 89.1 83.3 99.2 91.1 86.2 89.8 SPT-LoRA [21] 0.52 88.6 83.4 99.5 91.4 87.3 90.1 Adapter+, r∈[1..32] 0.34 90.0 83.2 99.6 91.6 89.1 90.7 Adapter+ (best epoch) 0.34 90.4 85.0 99.7 92.6 89.1 91.4 ditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC,0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency. 4.5. Ablations Data normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [31], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [29], FacT [32], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data nor- malization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy. Training regularization. We investigate the importance of training regularization methods like stochastic depth [30] and dropout [16] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate Table 6. Effects of ImageNet vs. Inception data normalization. All methods are evaluated on the VTABval sets. In column ∆Average we report the increase in accuracy in pp across all VTAB subgroups. ImageNet norm Inception normNatural Specialized Structured Average Natural Specialized Structured Average ∆Average VPT 79.2 83.0 53.8 72.0 82.2 86.2 57.9 75.4 3.4 LoRA 78.4 84.1 53.2 71.9 82.0 85.8 56.4 74.7 2.8 FacT-TK 78.0 83.3 56.1 72.4 81.6 85.6 58.1 75.1 2.7 Adapter+ 80.5 85.0 56.0 73.9 83.0 86.8 59.7 76.5 2.6 Table 7. Influence of training regularization. We evaluate accu- racy in % with Adapterbase with rank r=8 on the VTAB val sets. Adapter Stochastic Depth Dropout None ViT Stochastic Depth 76.0 75.4 75.3 None 74.5 74.3 73.7 of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model. 5. Conclusion Applied at the right position and with an optimal inner struc- ture, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can “strike back”, we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparame- ter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods. Acknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center. 8References [1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016. [2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016. [3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learn- ing. In NeurIPS*2020. [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End- to-end object detection with transformers. In ECCV, pages 213–229, 2020. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. InICCV, pages 9630–9640, 2021. [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapt- ing vision transformers for scalable visual recognition. In NeurIPS*2022. [7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865–1883, 2017. [8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606–3613, 2014. [9] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. RandAugment: Practical automated data augmentation with a reduced search space. In NeurIPS*2020. [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul- mohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahen- dran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bast- ings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birod- kar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480–7512, 2023. [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL-HLT, pages 4171–4186, 2019. [12] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [14] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015. [15] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learn- ing of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594–611, 2006. [16] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learn- ing. In ICML, pages 1050–1059, 2016. [17] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502–4508, 2017. [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231–1237, 2013. [19] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368–8375, 2019. [20] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023. [21] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023. [22] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level perfor- mance on imagenet classification. In ICCV, pages 1026–1034, 2015. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InCVPR, pages 770–778, 2016. [25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. [26] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023. [27] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Be- longie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595–604, 2015. [28] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790–2799, 2019. [29] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 9LoRA: Low-rank adaptation of large language models. In ICLR, 2022. [30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646–661, 2016. [31] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709–727, 2022. [32] Shibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060–1068, 2023. [33] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988–1997, 2017. [34] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image catego- rization. In CVPR Workshop on Fine-grained Visual Classifi- cation, 2011. [35] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Ad- vanced Research, 2009. [36] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97–104, 2004. [37] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. [38] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582–4597, 2021. [39] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [41] Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72–88, 2018. [42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017. [43] Pedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044–3054, 2019. [44] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722–729, 2008. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po- Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023. [47] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. InCVPR, pages 3498–3505, 2012. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 8024–8035. [49] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non- destructive task composition for transfer learning. In EACL, pages 487–503, 2021. [50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by gener- ative pre-training. Technical report, OpenAI, 2018. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, ICML. [52] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. InICCV, pages 12159– 12168, 2021. [53] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506–516. [54] Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386–408, 1958. [55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211–252, 2015. [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud- wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION- 5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022. [57] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your ViT? Data, augmentation, and regularization in vision transformers. Trans. Mach. Learn. Res., 2022. [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the In- 10ception architecture for computer vision. In CVPR, pages 2818–2826, 2016. [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor- eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998–6008. [60] Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210–218, 2018. [61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011. [62] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. [63] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, José M. Álvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS*2021, pages 12077–12090. [64] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022. [65] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020. [66] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. In ICLR, 2018. [67] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022. 11Adapters Strike Back Supplementary Material In this appendix, we provide further details and results, which could not be included in the main paper due to space limitations. A. Why did adapters underperform for ViTs? First, we want to shed more light on why adapters do not rank well in the literature for parameter-efficient transfer learning for vision tasks. By comparison of numbers reported for adapters on VTAB in the publications referenced in Tab. 4 of the main paper, we found that they essentially stem from only two sources. The first source is VPT [31], where results for an adapter with a reduction factor of 256, amongst other configurations, are reported. For a ViT-B/16 with a hidden dimension of d=768, this is equal to an adapter with rank r=3. Despite citing Pfeiffer et al. [49], who suggest a Post-Adapter po- sition, the actual implementation in the code base * equals an Intermediate-Adapter that performs worse on VTAB (see Sec. 3.3 of the main paper). The initialization used for the adapter parameters most resembles a LoRA initialization but sets the adapter parameters to zero everywhere. Therefore, there is no randomization in the initialization of the adapter parameters, and different seeds only affect the initialization of the classifier. Additionally, the intermediate features in the adapter bottlenecks then become all zero, leading to iden- tical gradients in the up-projections at the start of training, which hinders optimization. As a result, the adapter baseline used by VPT only reaches 60.0% average accuracy on the VTAB test sets. This is a gap of 17.6 percentage points (pp) compared to our Adapter+ with rank r=8 (77.6% average accuracy). Even when considering the loss of around 2 pp to 3 pp caused by an unsuitable data normalization in the VPT implementation, this is still a very significant gap. The numbers for an adapter with rank r =3 from VPT are also reported in [39] as a baseline. The second source for adapter baseline results is the NOAH pre-print [67]. There, an adapter with rank r = 8 is used. Its implementation† performs the following feature transformation: x 7→ Adapter \u0000 FFN(x) \u0001 + x. (12) This is closest to the Intermediate-Adapter (cf . Eq. (10) of the main paper) but misses the skip connection bypassing the adapter and containing FFN(x). Thus, the adapter does not learn a residual function to an identity mapping but instead must learn a more complex mapping to transform its input. *https://github.com/KMnP/vpt †https://github.com/ZhangYuanhan-AI/NOAH Therefore, the adapter becomes harder to train [24], leading to an average accuracy of 73.9% on the VTAB test sets or 3.7 pp behind our Adapter+. For the NOAH adapter results, we see a proliferation to the publications of FacT [32] and SPT [21]. The adapter implementation from NOAH is also used in the code released for Consolidator ‡ [20] but their results are produced with rank r=16, giving a slightly better average accuracy of 74.3%, or 3.3 pp less than Adapter+. In summary, the examined baseline implementations dif- fer from the configurations proposed by Houlsby et al. [28] and Pfeiffer et al. [49] and introduce issues that lead to their underperformance. In our paper, we show that adapters are capable of reaching 77.6% average accuracy for rank r=8 and 77.9% for our optimized version of Adapter+, uplifting adapters from an easy-to-beat baseline to a state-of-the-art transfer method. B. Dataset properties In Tabs. 8 and 9, we show the statistics of each task in VTAB [65] and FGVC [31] with regard to the number of classes and the number of images in the train, validation, and test splits. The tables are largely “borrowed” from [31]. Table 8. Dataset details for VTAB. Group Task # Classes Splits Train Val Test Natural CIFAR-100 [35] 100 800 200 10 000 Caltech-101 [15] 102 6 084 DTD [8] 47 1 880 Oxford Flowers [45] 102 6 149 Pets [47] 37 3 669 SVHN [44] 10 26 032 Sun397 [62] 397 21 750 Specialized Patch Camelyon [60] 2 800 200 32 768 EuroSAT [25] 10 5 400 RESISC45 [7] 45 6 300 Diabetic Retinopathy [14] 5 42 670 Structured CLEVR-Count [33] 8 800 200 15 000 CLEVR-Distance [33] 6 15 000 DMLab [2] 6 22 735 KITTI-Distance [18] 4 711 dSprites-Location [42] 16 73 728 dSprites-Orientation [42] 16 73 728 smallNORB-Azimuth [36] 18 12 150 smallNORB-Elevation [36] 9 12 150 ‡https://github.com/THU-MIG/Consolidator iTable 9. Dataset details for FGVC.For datasets markedwith *, we follow [31] to randomly sample train and validation splits because validation sets are not available from the original datasets. Dataset # Classes Splits Train Val Test CUB-200-2011* [61] 200 5 394 600 5 794 NABirds* [27] 555 21 536 2 393 6 084 Oxford Flowers [45] 102 1 020 1 020 6 149 Stanford Dogs* [34] 120 10 800 1 200 8 580 Stanford Cars* [17] 196 7 329 815 8 041 C. More experimental settings For all experiments conducted with our implementation, we average the results over three seeds. This includes the (re-)evaluations of LoRA and VPT. We built our implemen- tation on PyTorch [48], PyTorch Lightning,§ and timm.¶ We run experiments with bfloat16 mixed precision on a NVIDIA RTX A6000 GPU. For our experiments in the main paper, we report results for a fixed adapter rank r as well as ranks optimized per task. For the per-task optimization of Adapter+, we use a hyper- parameter sweep over the set of ranks r∈{1, 2, 4, 8, 16, 32}. We evaluate on the validation sets of VTAB and FGVC and choose the per-task ranks from the specified range(s) to steer the number of average parameters. The ranks we used to produce the results on the VTAB and FGVC test sets (see Tabs. 4 and 5 in the main paper) are shown in detail in Tab. 10 and Tab. 11, respectively. D. Calculation of no. of trainable parameters Suppose we have a ViT with a hidden dimension d, N trans- former layers, and adapters with rank r. The total num- ber of learnable parameters for Adapter base modules ( cf . Eq. (4) of the main paper) attached to the FFN of every transformer layer then amounts to N(2dr + r + d). Includ- ing layer normalization in the adapter modules amounts to N2d additional parameters. The addition of learned, layer- wise scaling amounts to N extra parameters and choosing learned, channel-wise scaling instead adds Nd extra parame- ters. Adapter+ (see Sec. 4.3 of the main paper) thus amounts to N(2dr + 2d + r) total parameters. Additionally, for a task with c classes, we add a classifier with dc + c learnable parameters. E. Vision transformer pre-training As we add only very few parameters to an otherwise frozen backbone, the generalization capability of the feature repre- sentations produced by the backbone is important. For ViTs, there are a number of off-the-shelf models available with §https://lightning.ai/pytorch-lightning ¶https://github.com/huggingface/pytorch-image-models Table 10. Adapter rank r for each VTAB task for optimized versions of Adapter+ with different ranges of permitted ranks. Natural Specialized Structured # Param (M) CIFAR-100 [35] Caltech-101 [15] DTD [8] Flowers [45] Pets [47] SVHN [44] Sun397 [62] Camelyon [60] EuroSAT [25] RESISC45 [7] Retinopathy [14] CLEVR-Count [33] CLEVR-Dist. [33] DMLab [2] KITTI-Dist. [18] dSpr-Loc. [42] dSpr-Ori. [42] sNORB-Azi. [36] sNORB-Ele. [36] r∈[1..4] 0.11 1 4 2 1 4 4 1 4 2 4 2 4 2 4 4 4 4 4 4 r∈[1..8] 0.16 1 4 2 1 8 8 1 8 2 8 8 4 8 8 8 8 4 8 8 r∈[1..32] 0.27 1 4 2 1 8 16 1 16 2 32 32 4 8 8 8 32 4 32 8 Table 11. Adapter rank r for each FGVC dataset for optimized versions of Adapter+ with different ranges of permitted ranks. # Param (M) CUB-200 [61] NABirds [27] Oxford Flowers [45] Stanford Dogs [34] Stanford Cars [17] r∈[1..32] 0.34 2 2 1 1 32 differences in their training procedures. Here, we examine three different pre-trainings as examples: (1) Original: The ViT-B/16 weights used in the main paper, pre-trained with su- pervision on ImageNet-21k [55] following the training proce- dure of the original ViT publication [13],|| (2) ImageNet-1k: the same ViT weights further fine-tuned on ImageNet-1k [55],** and (3) AugReg: weights from a pre-training with stronger data augmentation in the form of Mixup [66] and RandAugment [9] following [57].†† In Tab. 12, we summarize our results for Adapter+ with rank r=8 evaluated on the VTAB validation sets. We notice that additional fine-tuning on ImageNet-1k gives a slight edge (83.4% average accuracy over 83.0% for second best) in adaption for tasks that contain natural images. However, the fine-tuning is detrimental for the Specialized and Structured group. Not fine-tuning on ImageNet-1k is beneficial for the Structured group with a large increase of 3.7 pp. The Aug- Reg training setting improves the transfer to the Specialized group but is worse than the other settings for natural images. Overall, the original supervised training on ImageNet-21k generalizes best across all tasks in VTAB with an average accuracy of 76.5%, 0.3 pp better than AugReg training and 1.2 pp better than ImageNet-1k fine-tuning. ||https://storage.googleapis.com/vit_models/imagenet21k/ ViT-B_16.npz **https://storage.googleapis.com/vit_models/imagenet21k+ imagenet2012/ViT-B_16-224.npz ††https://storage.googleapis.com/vit_models/augreg/B_16- i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz iiTable 12. Influence of ViT pre-training. We use Adapter+ with rank r=8 for the evaluation and report the average accuracy in % for each subgroup and across all groups on the VTAB val sets. Pre-training Natural Specialized Structured Average ImageNet-1k 83.4 86.5 56.0 75.3 AugReg 81.6 87.2 59.7 76.2 Original 83.0 86.8 59.7 76.5 Table 13. Adapter position with DINO backbone. We report average accuracy in % ( ± std. dev.) on the VTAB val sets for different adapter positions. Adapterbase with Houlsby initialization and rank r=8 is used in all experiments. Position Natural Specialized Structured Average Pre 76.8 ± 0.4 86.2 ± 0.6 53.6 ± 0.7 72.2 ± 0.3 Intermediate 76.8 ± 0.4 85.8 ± 0.8 52.6 ± 0.9 71.8 ± 0.4 Parallel 76.7 ± 0.3 86.8 ± 0.4 54.1 ± 0.7 72.5 ± 0.3 Post 76.9 ± 0.2 86.3 ± 0.5 55.3 ± 0.7 72.8 ± 0.3 Table 14. Comparison of Adapter+ with adapter configurations from previous work with DINO backbone.We report the average accuracy in % (± std. dev.) of each subgroup and across all groups on the VTAB val sets. Configuration # Param Natural Specialized Structured Average Houlsby [28], r=8 0.39 77.4 ± 0.4 86.5 ± 0.7 52.9 ± 0.8 72.3 ± 0.4 Houlsby [28], r=4 0.24 77.2 ± 0.5 86.2 ± 0.5 53.2 ± 0.8 72.2 ± 0.3 Pfeiffer [49] 0.21 76.8 ± 0.4 86.2 ± 0.3 54.4 ± 1.0 72.5 ± 0.4 AdaptFormer [6] 0.19 76.5 ± 0.4 85.8 ± 0.4 53.0 ± 0.5 71.8 ± 0.3 Adapter+ 0.20 76.7 ± 0.3 86.4 ± 0.5 55.4 ± 0.8 72.8 ± 0.3 F. Generality of the conclusions Using DINO [5] as an example of a ViT trained with self- supervision, we show in Tab. 13 that the orders of best-to- worst adapter position is consistent with that of a supervised backbone in terms of average accuracy, albeit with a higher standard deviation. The ranking also stays the same for the comparison of Adapter+ with adapter configurations from previous work as presented in Tab. 14. This shows that our conclusions generalize beyond backbones with super- vised pre-training to backbones based on self-supervised pre-training. iii",
      "meta_data": {
        "arxiv_id": "2406.06820v1",
        "authors": [
          "Jan-Martin O. Steitz",
          "Stefan Roth"
        ],
        "published_date": "2024-06-10T22:07:57Z",
        "pdf_url": "https://arxiv.org/pdf/2406.06820v1.pdf",
        "github_url": "https://github.com/visinf/adapter_plus"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper provides an in-depth and systematic study of adapters for vision transformers (ViTs), identifying pitfalls in previous implementations and proposing an improved architecture called Adapter+. The main contributions include: (1) a comprehensive study on adapter position, inner structure, and parameter initialization for ViTs, (2) the proposal of a learnable, channel-wise scaling for adapters in computer vision tasks, and (3) the introduction of Adapter+, which achieves state-of-the-art average accuracy on VTAB (77.6% without per-task hyperparameter optimization) and FGVC (90.7% with the lowest parameters), demonstrating an excellent parameter-accuracy trade-off and high robustness across diverse task subgroups.",
        "methodology": "The methodology centers on optimizing the configuration of bottleneck adapter modules for Vision Transformers. The core Adapter+ architecture uses a 'Post-Adapter' position, where the adapter is placed at the end of the transformer layer, after the FFN and its skip connection. It incorporates a learnable, channel-wise scaling mechanism for the adapter's output, and its parameters are initialized using the Houlsby initialization method. The adapter module itself consists of a down-projection, a GELU non-linearity, and an up-projection, forming a bottleneck structure. The study systematically evaluates different adapter positions (Pre, Post, Parallel, Intermediate), inner structure components (biases, layer normalization, layer-wise vs. channel-wise scaling), and initialization strategies (Houlsby, BERT, LoRA) to determine the optimal configuration.",
        "experimental_setup": "Experiments were conducted on two standard benchmarks for task adaptation: VTAB (Visual Task Adaptation Benchmark), consisting of 19 tasks grouped into Natural, Specialized, and Structured categories, and FGVC (fine-grained visual classification), comprising five datasets. A ViT-B/16 network pre-trained on ImageNet-21k was used as the backbone model. All models were trained with an AdamW optimizer (learning rate 10^-3, weight decay 10^-4, batch size 64), a cosine learning rate schedule with a linear warm-up over 10 epochs, for a total of 100 epochs. Stochastic depth with linearly increasing drop rates (0 to 0.1 for frozen network, 0.1 for adapters) was applied for regularization. Input images were resized to 224x224 px, with additional random resize crop and horizontal flipping for FGVC. Re-evaluations of competing methods ensured fair comparisons, considering data normalization and complete training schedules.",
        "limitations": "The paper primarily addresses and overcomes limitations of previous adapter implementations, such as suboptimal configurations and initialization issues, leading to their underperformance. For Adapter+ itself, the paper highlights its robustness and states it 'requires little to no manual intervention when addressing a novel scenario,' implying a lack of significant limitations for its proposed architecture and training strategy within the scope of vision task adaptation. No explicit weaknesses, constraints, or assumptions specific to Adapter+ were mentioned.",
        "future_research_directions": "The paper does not explicitly state future research directions. It focuses on presenting Adapter+ as a refined, state-of-the-art solution that addresses previous shortcomings of adapter methods. The conclusion emphasizes its superiority and excellent parameter-accuracy trade-off, setting a new strong baseline for future work in parameter-efficient transfer learning.",
        "experimental_code": "class Adapter(nn.Module):    def __init__(        self,        embed_dim,        bottleneck_dim=8,        drop_path=0.0,        dropout=0.0,        act_layer=nn.GELU,        norm_layer=nn.LayerNorm,        scaling=1.0,        init=\"houlsby\",        bias=True,        pre_dropout=False,    ):        super().__init__()        self.bottleneck = nn.Sequential(            nn.Dropout(dropout) if dropout > 0 and pre_dropout else nn.Identity(),            nn.Linear(embed_dim, bottleneck_dim, bias=bias),            act_layer() if act_layer else nn.Identity(),            nn.Dropout(dropout) if dropout > 0 and not pre_dropout else nn.Identity(),            nn.Linear(bottleneck_dim, embed_dim, bias=bias),        )        self.norm_a = norm_layer(embed_dim) if norm_layer else nn.Identity()        self.drop_path_a = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()        self.bottleneck_dim = bottleneck_dim        if scaling == \"learned\":            self.scaling = nn.Parameter(torch.ones(1))        elif scaling == \"channel\":            self.scaling = nn.Parameter(torch.ones(embed_dim))        else:            self.scaling = scaling        if init == \"houlsby\":            std = 0.01            nn.init.trunc_normal_(                self.bottleneck[1].weight, std=std, a=-2 * std, b=2 * std            )            if self.bottleneck[1].bias is not None:                nn.init.zeros_(self.bottleneck[1].bias)            nn.init.trunc_normal_(                self.bottleneck[4].weight, std=std, a=-2 * std, b=2 * std            )            if self.bottleneck[4].bias is not None:                nn.init.zeros_(self.bottleneck[4].bias)        elif init == \"lora\":            nn.init.kaiming_uniform_(self.bottleneck[1].weight, a=math.sqrt(5))            if self.bottleneck[1].bias is not None:                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(                    self.bottleneck[1].weight                )                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0                nn.init.uniform_(self.bottleneck[1].bias, -bound, bound)            nn.init.zeros_(self.bottleneck[4].weight)            if self.bottleneck[4].bias is not None:                nn.init.zeros_(self.bottleneck[4].bias)        elif init == \"bert\":            nn.init.normal_(self.bottleneck[1].weight, mean=0.0, std=0.02)            if self.bottleneck[1].bias is not None:                nn.init.zeros_(self.bottleneck[1].bias)            nn.init.normal_(self.bottleneck[4].weight, mean=0.0, std=0.02)            if self.bottleneck[4].bias is not None:                nn.init.zeros_(self.bottleneck[4].bias)        else:            raise ValueError(f\"Initialization {init} not implemented!\")    def forward(        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None    ) -> torch.Tensor:        x = self.norm_a(x)        x = self.drop_path_a(self.bottleneck(x))        x = x * self.scaling        y = x        if skip is not None:            y = y + skip        return y\nclass AdapterBlock(Block):\n    def forward_post(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n        x = self.adapter(x, skip=x)\n        return x\n\nclass VisionTransformerAdapter(VisionTransformer):\n    def __init__(self, ..., block_fn=AdapterBlock, adapter_config=None, ...):\n        super().__init__(..., block_fn=block_fn, ...)\n        # ... other initializations ...\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.Sequential(*[block_fn(..., adapter_config=adapter_config, ...) for i in range(depth)])\n\ndef _create_vision_transformer_adapter(variant: str, pretrained: bool = False, adapter=False, **kwargs) -> VisionTransformer:\n    # ... (simplified) ...\n    if adapter:\n        block_fn = kwargs.pop(\"block_fn\", Block)\n        if block_fn == Block:\n            block_fn = AdapterBlock\n        elif block_fn == ResPostBlock:\n            block_fn = AdapterResPostBlock\n        else:\n            raise ValueError(f\"Adapters not implemented for {block_fn}!\")\n        return build_model_with_cfg(\n            VisionTransformerAdapter,\n            variant,\n            pretrained,\n            block_fn=block_fn,\n            **kwargs,\n        )\n\nclass AdapterModel(pl.LightningModule):\n    def __init__(self, cfg, img_size=224, num_classes=1000,):\n        super().__init__()\n        self.cfg = cfg\n        self.vit = timm.create_model(\n            cfg.vit.model,\n            adapter=True,\n            pretrained=True,\n            num_classes=num_classes,\n            img_size=img_size,\n            drop_path_rate=cfg.vit.drop_path,\n            adapter_config=cfg.get(\"adapter\", None),\n            lora_config=cfg.get(\"lora\", None),\n            prompt_config=cfg.get(\"prompt\", None),\n        )\n        if cfg.get(\"adapter\", None) or cfg.get(\"lora\", None) or cfg.get(\"prompt\", None):\n            if not cfg.vit.finetune:\n                self.vit.requires_grad_(False)\n            self.vit.head.requires_grad_(True)\n            for m in self.vit.modules():\n                if isinstance(m, Adapter):\n                    m.requires_grad_(True)\n                if cfg.train.train_ln:\n                    if isinstance(m, nn.LayerNorm):\n                        m.requires_grad_(True)\n\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=(), exclude_list=()):\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad or name in exclude_list:\n            continue\n        if (\n            len(param.shape) == 1\n            or name.endswith(\".bias\")\n            or name.endswith(\".scaling\")\n            or name in skip_list\n        ):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n        {\"params\": decay, \"weight_decay\": weight_decay},\n    ]\n",
        "experimental_info": "The Adapter module employs a bottleneck structure consisting of a down-projection, a GELU non-linearity, and an up-projection. Its parameters are initialized using the Houlsby initialization method (trunc_normal_ with std=0.01 for weights and zeros for biases). The adapter's output incorporates a learnable, channel-wise scaling mechanism. The core Adapter+ architecture places the adapter in a 'Post-Adapter' position, meaning it is applied after the FFN and its skip connection within a transformer block. Other adapter positions evaluated include 'Pre', 'Parallel', 'Intermediate', and 'Houlsby' (referring to a dual adapter placement at both attention and FFN outputs). When integrating adapters, the base Vision Transformer parameters are frozen by default, with only the classification head and the adapter modules being trainable. Optionally, the Vision Transformer's Layer Normalization layers can also be made trainable. The scaling parameters of the adapter are specifically excluded from weight decay during optimization. The internal layer normalization of the Adapter module is typically set to None for Adapter+ configuration."
      }
    },
    {
      "title": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
      "abstract": "The growing number of parameter-efficient adaptations of a base large\nlanguage model (LLM) calls for studying whether we can reuse such trained\nadapters to improve performance for new tasks. We study how to best build a\nlibrary of adapters given multi-task data and devise techniques for both\nzero-shot and supervised task generalization through routing in such library.\nWe benchmark existing approaches to build this library and introduce\nmodel-based clustering, MBC, a method that groups tasks based on the similarity\nof their adapter parameters, indirectly optimizing for transfer across the\nmulti-task dataset. To re-use the library, we present a novel zero-shot routing\nmechanism, Arrow, which enables dynamic selection of the most relevant adapters\nfor new inputs without the need for retraining. We experiment with several\nLLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying\nthat MBC-based adapters and Arrow routing lead to superior generalization to\nnew tasks. We make steps towards creating modular, adaptable LLMs that can\nmatch or outperform traditional joint training.",
      "full_text": "Towards Modular LLMs by Building and Reusing a Library of LoRAs Oleksiy Ostapenko* 1 2 3 Zhan Su* 2 4 Edoardo Maria Ponti5 Laurent Charlin2 6 7 Nicolas Le Roux1 2 3 7 Matheus Pereira1 Lucas Caccia* 1 Alessandro Sordoni* 1 2 3 Abstract The growing number of parameter-efficient adap- tations of a base large language model (LLM) calls for studying whether we can reuse such trained adapters to improve performance for new tasks. We study how to best build a library of adapters given multi-task data and devise tech- niques for both zero-shot and supervised task gen- eralization through routing in such library. We benchmark existing approaches to build this li- brary and introduce model-based clustering, MBC, a method that groups tasks based on the similarity of their adapter parameters, indirectly optimiz- ing for transfer across the multi-task dataset. To re-use the library, we present a novel zero-shot routing mechanism, Arrow, which enables dy- namic selection of the most relevant adapters for new inputs without the need for retraining. We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verify- ing that MBC-based adapters and Arrow routing lead to superior generalization to new tasks. We make steps towards creating modular, adaptable LLMs that can match or outperform traditional joint training. 1. Introduction Tailoring large language models (LLMs) towards down- stream tasks, domains, or user profiles is of paramount importance given the recent democratization of their usage, catalyzed by the release of open-source LLMs (Zhang et al., 2023b; Microsoft Research, 2023, inter alia). This process often relies on an adapter, such as LoRA (Hu et al., 2022), a parameter-efficient fine-tuning (PEFT) of a pre-trained LLM (Hu et al., 2022; Liu et al., 2022; Li & Liang, 2021). *Equal contribution 1Microsoft Research 2Mila — Que- bec AI Institute 3Universit´e de Montr ´eal 4University of Copen- hagen 5University of Edinburgh 6HEC Montr ´eal 7Canada CI- FAR AI Chair. Correspondence to: A. Sordoni <alsor- don@microsoft.com>. Preprint. Figure 1.How to coordinate a library of adapters (e.g., LoRAs) for zero-shot generalization to new tasks? To build this library (top), we propose MBC, a novel method that clusters tasks based on the similarity of the parameters of corresponding LoRAs. To reuse a library (either private or MBC, bottom), we route hidden states to trained LoRAs via Arrow, which leverages the SVD decomposition of each LoRA. LLM adapters are increasingly available as part of online hubs (Beck et al., 2021; Mangrulkar et al., 2022). These adapters are developed independently and asynchronously by users across the globe. Hence, they implicitly consti- tute a library built on top of multi-task data (Pfeiffer et al., 2023). Prior works show that mixtures of pretrained trained adapters can facilitate few-shot adaptation of LLMs to un- seen tasks (Ponti et al., 2023; Vu et al., 2021; Huang et al., 2024). Reusing pre-existing adapters in a zero-shot fashion remains less explored (Jang et al., 2023; Belofsky, 2023). In contrast to standard mixture-of-experts approaches (Fedus et al., 2022), in this setting, new inputs must be routed to in- dependently trained experts without requiring joint training of the routing mechanism and expert parameters. This leads to the question: how to create a modular LLM end-to-end by first building and then reusing a library of adapters for supervised adaptation and zero-shot generaliza- tion? First, given a base LLM, such as Phi-2 (Microsoft Re- search, 2023) or Mistral (Jiang et al., 2023), we investigate 1 arXiv:2405.11157v1  [cs.LG]  18 May 2024Towards Modular LMs by Building and Reusing a Library of LoRAs building a library of adapters by leveraging 256 tasks from Flan-v2 (Longpre et al., 2023). 1 We focus on LoRA (Hu et al., 2022) and leave the extension to other adapter types for future work. Once the adapter library has been built, we devise routing strategies to evaluatezero-shot generalization on 10 downstream tasks comprising common-sense reason- ing and coding (ARC (Clark et al., 2018), MBPP (Austin et al., 2021), inter alia) and supervised adaptation on 12 SuperNatural Instructions (SNI) tasks (Wang et al., 2022b). How to build the adapter library?One straightforward approach is to operate in a private scenario, in which one trains one adapter per task on the multi-task data and mix those adapters for unseen tasks (Chronopoulou et al., 2023a; Vu et al., 2021; Huang et al., 2024). This method is useful when the multi-task data cannot be shared for joint training (Mireshghallah et al., 2020) but trained adapters can. To favour transfer between training tasks, recent approaches compress the multi-task data into a smaller set of reusable, composable adapters (Ponti et al., 2023; Caccia et al., 2023). In this shared data setting, we propose model-based cluster- ing (MBC), a simple two-stage approach to build a library of adapters. We find a positive correlation between the similar- ity of the LoRA weights of a pair of tasks and the transfer between the two tasks. Building on this intuition, we first exploit LoRA similarity in weight space between privately trained adapters as a proxy for detecting clusters of similar tasks, then train one adapter per cluster. Our approach em- pirically improves performance while matching the compute budget. How to reuse the library for new scenarios?Given a library of trained LoRAs, we examine strategies of how to reuse the library in two settings: zero-shot generaliza- tion and parameter-efficient supervised adaptation to new tasks. Reusing LoRAs in a zero-shot manner is challenging because there is no labelled data to learn a routing mecha- nism. We propose Arrow (↗↗), a routing mechanism that automatically selects relevant LoRAs without requiring i) joint training and ii) access to the data used to train each LoRA. This facilitates the vision of a decentralized system where LoRAs can be trained asynchronously and be read- ily reused with minimal assumptions. Arrow computes a representation for each LoRA as the direction of maximum variance induced by the LoRA parameters. At inference time, Arrow routes per token and per layer, i.e. each hidden state is routed by computing its alignment with each LoRA representation. In summary, our contributions are:i) we study how to create LoRA-based modular multi-task LLM in a setting where experts are trained independently and the router is created after the training of the experts; ii) assuming shared multi- task data, we propose a clustering approach (MBC) to train a 1We held out SNI tasks to test supervised adaptation. library of adapters; and, iii) we propose Arrow, a zero-shot routing method to select which adapters to use from a library of LoRAs. This allows for routing to independently trained experts without accessing their training data. 2. Preliminaries We are given a set of tasks T = {t1, . . . , tT }, where each task ti is associated with a dataset containing a set of sam- ples Di = {(x1, y1), ...,(xn, yn)}. The union of the train- ing sets constitutes our multi-task dataset D; in our case, it is Flan (Longpre et al., 2023). In order to create our library of task adapters, we use LoRA (Hu et al., 2022). LoRA achieves competitive trade-offs between performance and parameter efficiency (Karimi Mahabadi et al., 2021) by modifying the linear transformations in a base LM. For each linear transformation in the base LM, LoRA modifies the base model parameters as follows: h = Wx + s · AB⊤x, (LoRA) where W are the (frozen) weights of the base LM, A, B∈ Rd×r are low-rank learnable parameters and s ≥ 1 is a tunable scalar hyperparameter. LoRA achieves parameter efficiency because of the reduced rank r (≪ d). 3. Building the LoRA Library We propose different alternatives for building a libraryL of adapters that perform well on the tasks they were trained on and are versatile enough to be effective on other unseen downstream tasks. To do so, we seek methods that enhance multi-task transfer while reducing task interference (Wang et al., 2021; Chen et al., 2022). Private Adapters One straightforward solution is to train separate adapters on each training task, i.e. the library will be composed ofT adapters (see Fig. 1). Several existing methods operate in this setting, such as LoraHub (Huang et al., 2024), AdapterSoup (Chronopoulou et al., 2023a) and SPoT (Vu et al., 2021). Although this solution does not exploit multi-task training, it is required in settings where the task data is private, e.g., user data, and cannot be shared. Moreover, this setting reflects well the scenario in which adapters are trained by end users in a decentralized fashion and added asynchronously to the library. Shared Adapter To encourage transfer, another solution is to train a single adapter on all the multi-task training data. One possible shortcoming is the reduced capacity to fit the multi-task training data and the possibility of interference between the multitude of training tasks (Ponti et al., 2023). Training a single adapter may result in negative transfer because task gradients are misaligned (Wang et al., 2021). An obvious solution to reduce the amount of interference 2Towards Modular LMs by Building and Reusing a Library of LoRAs 0.2 0.4 0.6 0.8 1.0 Adapter Similarity 0.1 0.0 0.1 0.2 Performance Delta Phi 2 (r = 0.51) GPT-Neo (r = 0.75) Figure 2. For any pair of tasks, we report the cosine similarity between the corresponding LoRA weights (x-axis) against the delta in performance between LoRAs trained on them individually and jointly (y-axis). The positive correlation indicates that if LoRAs are dissimilar, we should abstain from multi-task training. is to increase the number of trainable parameters, e.g. to fine-tune the whole base LM on the multi-task data (Liu et al., 2022). Poly / MHR Adapters Polytropon (Poly) and Multi-Head Routing (MHR) (Ponti et al., 2023; Caccia et al., 2023) explore intermediate approaches between private and shared, where K < T“basis” adapters are trained on the multi-task training data. These K adapters can be considered “latent skills”, as each task adapter in the multi-task training set can be expressed as a linear combination of these basis adapters. If private training for all the tasks learns a matrix of parameters Φ ∈ RT×D, where D is the dimensionality of the LoRA adapters, Poly decomposes Φ = Z ˆΦ, where Z ∈ RT×K, ˆΦ ∈ RK×D, ˆΦ storing the latent skills and Z the linear combination coefficients for each task which specify the task-specific routing w.r.t. the latent skills. Both Z and ˆΦ are trained jointly on the multi-task training set by gradient descent. Note that the skills ˆΦ do not correspond to specific tasks and therefore it is not clear how to reuse them for zero-shot generalization (Caccia et al., 2023). Model-Based Clustering (MBC) While Polytropon and MHR reduce the inventory size, they require joint training of experts and the router on the combined dataset of all tasks. Here, we propose another approach to compress multi-task data into a set of reusable adapters; we cluster tasks based on their similarity and then train one adapter per task cluster. Ideally, the similarity between two tasks should correlate with the benefit of training a single model on both tasks compared to having two independent models (Fifty et al., 2021; Vu et al., 2020a). Motivated by (Zhou et al., 2022), we rely on the intuition that LoRA parameter vectors similarity can approximate the amount of transfer between a pair of tasks. To confirm this, we devise the following Algorithm 1Model-Based Clustering (MBC) Input: Multi-task data D1, . . . ,DT , base model LLMθ, number of library adapters K Output: Library L L = {}, A = {} ▷ LoRA params for t = 1to T do At, Bt = train(Dt, LLMθ) ▷ Train LoRA on task t A = A ∪ {cat(flatten(At), flatten(Bt))} end for U = SVD(A) ▷ Reduce LoRA dim S = cosine-similarity(U, U) ▷ T× T similarities c1, . . . , cK = k-means(S, K) ▷ Cluster similarities for k = 1to K do Dk = SDt, ∀t ∈ ck ▷ Join datasets in cluster Ak, Bk = train(Dk, LLMθ) L = L ∪ {(Ak, Bk)} end for Returns L experiment: we sample pairs of tasks (ti, tj), t∈ Tfrom the multi-task dataset, and we train both a) a LoRA on each task independently b) a LoRA on the union of the training datasets for the two tasks. We then compute the cosine similarity between the flattened LoRA parameters. We quantify transfer as the difference in the average log- likelihood induced by the joint and private models when evaluated on the test set of the two tasks. In Fig. 2, we observe that, for two different base models (GPT-Neo and Phi-2), the closer the tasks are in LoRA parameter space, the more performance delta is when we train on the joint dataset. The previous observation warrants our simple two-stage training procedure illustrated in Fig. 1 (top). Given a fixed computation training budget of N training steps per task, we use the first n steps to train private LoRAs. We then use these LoRA parameters to group tasks into K clusters by running a standard clustering algorithm (K-Means). In the second stage of training, we train one adapter per cluster for an additional N − n training steps, which keeps the total amount of computation similar to other approaches. We refer to this method as Model-Based Clustering (MBC) as it uses the model-based information encoded in the weights to determine a similarity metric between tasks (see Alg. 1). 4. Reusing the LoRA Library Next, we study the reuse of a trained library L in two sce- narios: for new inputs x∗, i.e. zero-shot, and in a super- vised adaptation setting, where new tasks t∗ come equipped with their training data Dt∗ . While the latter has been ad- dressed in recent works (Huang et al., 2024; Caccia et al., 2023; Vu et al., 2021), the former scenario remains less explored (Jang et al., 2023; Belofsky, 2023). We first devise routing strategies in the zero-shot and supervised settings and then describe how to aggregate the contributions of 3Towards Modular LMs by Building and Reusing a Library of LoRAs adapters selected by the routing strategies. 4.1. Routing We denote the hidden state for any token at a given trans- former layer produced by the input token x∗ as h∗. Similar to MoE approaches, we seek to parameterize a layer-specific routing distribution that prescribes which adapters to use. We denote this categorical distribution over|L| outcomes as p(· |h∗, x∗), where we drop the dependence on the layer for simplicity. For example, in standard MoE approaches (Fe- dus et al., 2022), p(· |h∗, x∗) = softmax(Wh∗). Given that we relax the assumption that the routing and the library should be trained together, we must devise ways to learn such routing distribution a posteriori. 4.1.1. Z ERO -SHOT ROUTING µ Routing One straightforward method to route to exist- ing experts is to set the routing distribution to uniform for all layers, p(· |h∗, x∗) = [1 /|L|, . . . ,1/|L|]. Despite its simplicity, µ routing was shown to be quite effective in re- cent work (Caccia et al., 2023; Chronopoulou et al., 2023a) and, due to the linearity of the LoRA adapters, effectively boils down to averaging the weights of the trained adapters uniformly. TP Routing Another variant treats routing as an |L|-way classification problem. Specifically, given an input x be- longing to task t in our multi-task training set, we train a task predictor f by minimizing the categorical cross-entropy loss −log f(x)[t], where f(x) is a probability distribution obtained by learning a classifier on top of a T5 encoder (Raf- fel et al., 2020). We then set p(· |h∗, x∗) = f(x∗) at inference time. Note that the routing decisions are not de- pendent on the hidden state h∗, so this is a router dependent on the whole input but independent of the particular token or layer in the Transformer. We call this predictor TP (Task Predictor). CM Routing Centroid Matching (CM) computes a prototype for every expert (and for each layer) by averaging the hidden representations obtained by a forward pass of the LLM on each expert dataset. These prototypes can be stored in the columns of the routing matrix W. Once the prototypes for each expert have been obtained, the routing distribution is calculated by taking the cosine similarity between h∗ and each expert prototype and finally applying softmax. This routing is similar in spirit to Jang et al. (2023) and Belofsky (2023). Arrow Routing↗↗ The rows of every routing matrix W of standard MoE routing can be interpreted as expert “proto- types”. Arrow prescribes a way to estimate such routing ma- trix in a 0-shot fashion without requiring data access. Let’s denote by {Ai, Bi} the parameters for expert i at layer ℓ, Algorithm 2Arrow Routing ↗↗ Weight Initialization Input: LoRA library L, layer ℓ Output: Routing parameters for layer ℓ: Wℓ for i = 1to L do Ai, Bi = L[i, ℓ] ▷ Get weights for expert i U, D, V= SVD(AiBT i ) Wℓ[i] =V [:, 0] ▷ First right singular vector end for Returns Wℓ Routing Input: Routing parameters for layer ℓ: Wℓ ∈ R|L|×d, token in layer ℓ: hℓ ∈ Rd, top-k routing: k Output: Routing probabilities for layer ℓ: pℓ logits = abs(Wℓhℓ) pℓ[i] = ( logits[i] if i ∈ arg top-k(logits) −∞ else Returns softmax(pℓ) where we drop the dependency on ℓ. The i-th LoRA expert transforms each token’s hidden state h∗ as h∗ i = AiBT i h∗. Arrow finds a prototype for the expert i by decomposing the outer product AiBT i with SVD and taking the right first singular vector of this transformation (see Alg. 2). The pro- totype determines the direction of most variance induced by expert i in the space of hidden states h. If the LoRA adapters are of rank 1, i.e. Ai, Bi ∈ DD×1 the prototype for the expert i will be equal to the normalized Bi vector, i.e. argmaxv,∥v∥2=1∥AiBT i v∥2 = Bi/∥Bi∥2. In Section 10.1, we provide empirical evidence that indeed, ∥AiBT i v∥2 is larger when v belongs to task i, thus motivating this routing approach. Given that both v and −v are valid singular vec- tors, we compute expert logits as the absolute value of the dot product between prototypes and inputs. Alg. 2 details the prototype initialization and the routing step of Arrow. Arrow offers several advantages: a) it doesn’t require access to training data; b) it routes differently in every layer and token, increasing the overall model expressivity, and c) it is compute efficient since it requires no further training and SVD decomposition can be computed efficiently for low-rank matrices (Elhage et al., 2021; Nakatsukasa, 2019). 4.1.2. S UPERVISED TASK ROUTING When generalizing to a new task, we can learn the optimal routing given the task data D∗. This setting is similar to previous task generalization works (Ponti et al., 2023; Cac- cia et al., 2023; Huang et al., 2024). We compare results in this supervised setting to both Poly (Ponti et al., 2023) and LoraHub (Huang et al., 2024). Poly Routing treats the distribution over experts at each layer as an |L|-dimensional parameter that is learned by minimizing the cross-entropy on the new task data D∗. It 4Towards Modular LMs by Building and Reusing a Library of LoRAs optimizes the merging coefficients of LoRAs for the new task, i.e. A∗ = P|L| i=1 wiAi and B∗ = P|L| i=1 wiBi. Here p(·|h∗, x) = (w1, . . . , wn) is the (input-independent) learn- able routing distribution for a given layer. LoraHub Routing (Huang et al., 2024) is similar to Poly with the exception that a) it resorts to gradient-free optimiza- tion to learn routing coefficients and b) it doesn’t fine-tune the experts’ parameters, making it less expressive thanPoly. π-tuning Routing uses Fisher Information to create an embedding for each task-specific expert. In the fine-tuning process, π-tuning first trains an expert for the next task, then it retrieves a subset of experts most similar to the target task’s expert using FIM embeddings. Finally, both the inter- polation coefficients and experts’ parameters are tuned on the target task’s data (Wu et al., 2023). 4.2. LoRA Composition Given a routing distribution w = p(· |h∗, x) obtained ei- ther using the previously presented zero-shot or supervised routing, we linearly combine adapters in the library, i.e. A∗ = P|L| i=1 wiAi, B∗ = P|L| i=1 wiBi and use the result- ing adapter to perform inference at every layer of the base LLM (Ponti et al., 2023; Huang et al., 2024). For 0-shot task generalization, we employ top-k routing, composing the k experts with the highest routing logits. 5. Experiments Our experimental evaluation aims to answer the following questions: 1) How does building a LoRA library compare to non-modular methods (e.g. full fine-tuning)? 2) How large is the gap between privately trained libraries (similar to online hubs) and libraries which assume access to multi- task data? 3) To what extent does routing facilitate reusing a library of LoRA adapters? Multi-Task DatasetWe train expert modules on 256 tasks from the original Flan v2 dataset (Longpre et al., 2023). We exclude the SNI tasks (> 1000 tasks) (Wang et al., 2022b) from training for computational reasons. We reserved 12 SNI tasks for downstream out-of-domain evaluation. Simi- larly to Wang et al. (2023), we sub-sampled 10,000 exam- ples per task to ensure computational feasibility. Within these samples, 1,000 are allocated for validation and early- stopping. We will release our dataset for reproducibility. Evaluation For our supervised adaptation study, we use 12 held-out SNI tasks, each corresponding to a different SNI category. We threshold the number of training ex- amples to 10,000 examples per task. We evaluate perfor- mance with Rouge-L scores (Lin & Hovy, 2003). For zero- shot evaluation, we mainly use ten tasks widespread in the literature, including 1) common-sense reasoning: Wino- Grande (Sakaguchi et al., 2021), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020); 2) question answering: boolQ (Clark et al., 2019), OpenbookQA (Mihaylov et al., 2018), ARC-easy and hard (Clark et al., 2018); 3) cod- ing: (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021); 4) general-purpose reasoning: BBH. (Suzgun et al., 2022)2 We remove overlaps between the evaluation tasks and the Flan multi-task training set (boolQ, ARC, Wino- Grande, HellaSwag, OpenbookQA and PIQA). We also include zero-shot results on the 12 held-out SNI tasks in the appendix. Models and TrainingThis work focuses on augmenting LLMs with a library of adapters to transform them into mod- ular architectures. Our primary focus is on Phi-2 (Microsoft Research, 2023), a state-of-the-art model (as of March 2024) with 2.8 billion parameters, leading its class of models with parameter counts below 3 billion, according to the open leaderboard (Beeching et al., 2023). Additionally, we con- ducted experiments using the larger Mistral 7B (Jiang et al., 2023) model, given its widespread use in the community. For all models, we only patch attention layers with LoRA adapters. Unless stated otherwise, for all our multi-task training and single-task adaptation scenarios, we use LoRA rank of 4, dropout of 0.05 and learning rate of 1e-4. Unless specified, we set the number of clusters for MBC to 10, re- sulting in the best upstream validation loss and downstream performance for Phi-2, as demonstrated in Fig. 4. Methods We consider the following methods in both zero- shot and supervised scenarios (except for FullFT): • Base: the base model tested without any adaptation; • Shared: a single expert LoRA finetuned on the joint training set of all tasks (256 tasks unless stated other- wise) on top of the base model with multi-task learning; • FullFT: like Shared but the full model is finetuned. We adopt the following naming convention for the models using a library of experts: <library>–<routing>. For the library type, we consider Poly, MHR, Private and MBC libraries described in Sec. 3. For MBC, we match the total amount of compute, meaning that we use 40% of the training steps to compute the LoRA clustering and the other 60% to compute the final cluster adapters. For routing, we use µ, TP, CM and Arrow in the zero-shot scenario and Poly and LoraHub3 for the supervised scenario, described in Sec. 4. 2We test on a subset of 1000 randomly sampled examples to reduce evaluation costs. 3For LoraHub, we match the amount of compute used by SGD. Assuming the backward pass is twice the compute of a forward pass, and since nevergrad (NG; Rapin & Teytaud, 2018) only does forward passes, to match the compute of 5 SGD training epochs, we perform 30 epochs of NG with 1/2 of the training data used by SGD methods. 5Towards Modular LMs by Building and Reusing a Library of LoRAs Library Route |L| PIQA BOOLQ WG HSWAG ARC E ARC C HE OQA BBH MBPP Acc. Phi-2 (2.8B) Base - - 79.2 82.7 75.7 72.5 77.5 52.9 45.1 49.8 48.0 56.0 63.8 FullFT - - 80.3 80.8 77.0 73.2 83.5 57.9 50.0 48.0 47.7 57.2 65.6 Shared - 1 80.4 82.4 76.6 73.4 83.2 55.8 46.3 50.4 48.4 58.4 65.5 Poly µ 8 80.6 82.3 76.7 71.7 82.7 55.3 48.2 50.4 49.8 59.1 65.7 MHR µ 8 80.1 83.0 77.1 70.4 83.2 55.5 46.3 53.4 52.0 58.0 65.9 Private µ 256 79.5 83.2 76.0 73.1 81.4 54.5 43.9 47.8 48.5 59.9 64.8 Private ↗↗ 256 80.2 84.3 77.6 72.6 84.2 56.4 50.6 52.2 47.7 59.9 66.6 MBC µ 10 80.3 85.1 77.3 73.1 84.3 57.7 48.8 50.2 51.6 62.3 67.1 MBC ↗↗ 10 79.9 84.7 77.7 72.9 84.8 57.9 51.8 50.2 52.2 62.3 67.4 Mistral (7B) Base - - 81.1 82.2 66.5 78.8 68.9 49.6 28.0 44.6 47.9 47.5 59.5 Shared - 1 50.4 84.6 68.6 79.5 84.8 60.0 24.4 50.4 49.2 47.5 63.1 Private µ 256 82.1 82.7 67.2 79.6 78.7 54.8 29.9 45.2 49.0 49.4 61.9 Private ↗↗ 256 82.8 86.6 66.6 81.1 85.7 60.8 30.5 50.6 49.5 49.4 64.4 MBC µ 10 83.0 87.6 68.5 80.8 86.2 60.9 28.7 48.6 51.5 50.2 64.6 MBC ↗↗ 10 82.8 87.3 70.6 80.9 84.5 59.6 28.0 52.8 45.5 47.1 63.9 Table 1.Downstream zero-shot results for Phi-2 and Mistral backbones. |L| denotes the library size. For comparison with other routing baselines, see Fig. 3. Figure 3.Comparison of routing approaches with both Private and MBC libraries. Left & Middle. Downstream zero-shot performance on two backbones; Arrow outperforms other routing approaches in the case of private libraries, while in the case of MBC libraries, routing is less important. Right. Upstream performance on the held-out sets of each of the 256 training tasks. Arrow nearly matches Oracle routing (which uses information about the task identity) in the case of Private library and noticeably improves for MBC. 5.1. Zero-Shot Results In the zero-shot scenario, downstream tasks are evaluated without further fine-tuning. Tab. 1 presents the mean down- stream accuracy for 10 held-out tasks. First, we analyze Phi-2 results. We observe that MHR-µ achieves strong zero- shot performance, competitive with Shared and FullFT, in line with the results of Caccia et al. (2023). Interest- ingly, training one adapter per task and then taking the average, Private-µ, still achieves gains w.r.t.Base, albeit falling short of multi-task training ( FullFT and Shared), highlighting the competitiveness of uniform ( µ) adapter routing (Chronopoulou et al., 2023a). Comparing the perfor- mance of our proposed MBC approach for library construc- tion (MBC-µ) to previous approaches, we notice a sizable bump in performance of 1.2% absolute accuracy points over the strongest baseline (MHR). Similarly, when studying the zero-shot performance of Phi-2 on 12 SNI tasks in Tab. 5 we observe that MBC-µ strongly outperforms other baselines. Importantly, both Shared and FullFT methods, as well as Poly and MHR libraries assume simultaneous access to the full dataset of all tasks. In contrast, Private and MBC libraries can be trained in an embarrassingly parallel man- ner and therefore do not require any distributed training infrastructure (Li et al., 2022). Next, we analyze whether more informed routing strategies can improve performance beyond the µ−routing. The full results are reported in Figure 3 (Left & Middle). We see that TP, CM and Arrow routing improve the performance over µ routing for the Private Phi-2 library, gaining 0.9%, 0.9% and 1.8% points respectively. This highlights the importance of routing for larger libraries. Notably, Arrow (66.6%) can surpass the performance of FullFT (65.5%) when applied to the Private library. On the MBC library, TP routing decreases performance when 6Towards Modular LMs by Building and Reusing a Library of LoRAs 100 101 102 # of Clusters 65 66 67Downstream Acc. (%) 100 101 102 # of Clusters 0.9 0.8 0.7 Upstream Log Lik. Figure 4.Phi-2 zero-shot accuracy on the 10 held-out tasks (left) and validation log-likelihood on the training tasks ( right) as a function of the number of MBC clusters. compared to uniform routing, while MBC-↗↗ improves over MBC-µ by 0.3% points and proves itself as a more robust routing method for bothPrivate and MBC libraries. Overall, MBC-↗↗ improves 3.6 points over the base model and 1.8% absolute over FullFT. For Mistral, we find a similar trend withMBC libraries achiev- ing the best performance. Arrow routing results in a 2.5% increase in average performance over µ routing when used with the Private library (Private-↗↗ vs. Private-µ). Arrow is able to narrow the performance gap with MBC, without requiring simultaneous data access across tasks. We do not see any gains from using other routing methods for 10 experts in the MBC library in this case. We make simi- lar observations analyzing 0-shot SNI-12 results presented in Table 5, where Private-↗↗ attains notable gains of 10 Rouge-L points over Private-µ while MBC-µ strongly out- performs all other baselines. MBC Analysis Overall, MBC enhances the performance of the library across all our results. To investigate this further, we compare different clustering techniques. First, we com- pare to clusters obtained by randomly selecting examples (RandomExamples). This is equivalent to randomly parti- tioning the joint multi-task dataset. Then, we compare to clusters obtained by randomly choosing tasks from the entire set of training tasks (RandomTask). Finally, we cluster task embeddings, which are obtained by forwarding task-specific examples through the model and averaging their represen- tation at the model’s penultimate layer (Embeddings). For all these methods, we set the number of clusters to 10. The results are shown in Table 2. RandomTask surpasses RandomExamples by 1.6%, which indicates that grouping tasks rather than task examples is crucial for positive trans- fer. Embeddings underperforms MBC and supports our ob- servation that the cosine similarity between the weights of privately-trained LoRA correlates better than using repre- sentation similarity for 0-shot generalization. Additionally, we also report average pairwise cluster “similarity” (as mea- sured by the cosine similarity of the LoRA weights for each cluster) and observe a tendency that expert clusters with Clustering Mean Acc. Similarity RandExamples∗-µ 64.8 0.82 RandTask∗-µ 66.4 0.58 RandTask-µ 66.4 0.58 Embeddings∗-µ 66.1 0.37 MBC∗-µ 66.7 0.37 MBC-µ 67.1 0.27 Table 2.Ablation of task clustering: RandTask clusters tasks ran- domly, RandExamples clusters examples randomly, Embeddings clusters examples based on their embedding similarity. ‘*’ denotes one epoch of training to save computation. We also report average cosine similarity between cluster adapters. lower similarity, i.e. higher diversity, tend to result in higher performance. We conjecture that this stems from different clusters contributing distinct features to the joint model; however, we leave further investigation in this direction to future work (Jolicoeur-Martineau et al., 2023). 5.2. Upstream Performance We further assess the efficacy of Arrow routing by looking at the upstream in-distribution performance, measured as the average of the Rouge-L on the validation sets of the 256 training tasks. Within this setting, we can compute the performance of the Oracle routing, which selects for each task the corresponding expert. In Fig. 3 (Right) we report the results for Arrow and µ routing with both MBC and Private libraries. For both libraries, ↗↗ increases performance w.r.t. µ and almost matches Oracle performance in the Private setting. This demonstrates Arrow’s ability to correctly select the most relevant modules from a large library of experts. 5.3. Supervised Adaptation In Table 3, we present the supervised adaptation results for Phi-2 on the full (100% of training data) and limited (10% of training data) data regimes. The detailed per-task performance as well as the adaptation results for the Mis- tral model are presented in Table 8 and 7. First, for all models (Phi-2, Mistral) we observe a notable performance boost coming from using Private and MBC libraries com- pared to No Library, which optimizes a LoRA for each downstream task by starting from a random initialization, and Shared, which starts from the multi-task trained LoRA solution. Secondly, similarly to zero-shot results, we ob- serve that MBC can boost the performance with both Poly and µ routing: for Phi-2 the performance of MBC-µ tops Private-µ. Additionally, we see that randomly grouping tasks RandomTask-Poly outperforms the non-library base- lines but does not quite match MBC-based clustering for all the models. The low performance of LoraHub can be attributed to the fact that LoraHub does not fine-tune the LoRA experts’ weights but only their routing coefficients 7Towards Modular LMs by Building and Reusing a Library of LoRAs Method 100% Data 10% Data Base 22.2 22.2 No Library 75.5 53.9 Shared 75.8 56.4 Poly 73.4 61.7 MHR 74.8 64.5 π-tuning 76.7 64.6 Private-µ 76.9 62.5 RandTask-Poly 76.7 67.6 MBC-µ 78.8 67.0 MBC-Poly 78.8 68.2 Table 3.Supervised adaptation results on 12 SNI held-out tasks for Phi-2 obtained in the full (100% of training data) and limited (10% of the training data) data settings. (due to gradient free optimization). Refer to App. 10.2 for more insights onto this point. Finally, MBC-µ performs simi- larly to MBC-Poly, echoing results in (Caccia et al., 2023). 5.4. Summary of Results Mirroring the questions at the start of this section, we list our main takeaway messages below: 1. When appropriately routed, independently trained ex- perts (Private-↗↗) can match and surpass the zero- shot performance of full fine-tuning (for Phi-2) and shared tuning (for Mistral 7B). This is a rather sur- prising result given that experts are independently trained and routing is learned post-hoc. These results show promise for building collaboratively and asyn- chronously trained LMs. 2. If data sharing is possible, then clustering tasks by their similarity with MBC constitutes a very effective strategy. In this case, simply averaging the LoRA adapters ob- tained through MBC (MBC-µ) is sufficient compared to more sophisticated routing. Our zero-shot and super- vised adaptation results underscore the superiority of task-based over example-based clustering. 3. Arrow appears to be a very performant zero-shot rout- ing strategy while requiring minimal information about the trained LoRAs and none about the training data. For supervised adaptation, training both adapters and the routing coefficients appears to be crucial. Over- all, if routing seems beneficial for large libraries of adapters, the gains for smaller libraries are diminish- ing. This appears to stand in contrast with sparse MoE models, where (non-uniform) routing is crucial (Jiang et al., 2024). This may be due to the linearity of LoRA experts, which stands in contrast with MLP experts in sparse MoEs (Fedus et al., 2022); however, we leave this investigation for future work. Our main finding is that adapter parameters are suitable both to inform task clustering, and thus guide library building, and to route new inputs, thus facilitating library reuse. 6. Related Work Multi-task learninginvolves training on a joint set of all tasks (Caruana, 1997), potentially leading to performance degradation due to task interference (Zhao et al., 2018). An extensive literature studies how to partition learnable param- eters into shared and task-specific ones (Ding et al., 2023; Strezoski et al., 2019; Bragman et al., 2019; Zaremoodi et al., 2018; Wallingford et al., 2022; Fifty et al., 2021). We operate in the parameter-efficient multi-task learning setting (Ponti et al., 2023; Vu et al., 2021; Chronopoulou et al., 2023a; Pfeiffer et al., 2021). Vu et al. (2021) train one prefix adapter (Li & Eisner, 2019) per task and learn to re-use them for other tasks based on the adapter similarities. MBC can be seen as an extension of this approach where we cluster tasks based on their weight similarity to ensure more transfer during multi-task pre-training. Mixture of experts(MoEs), when coupled with sparse rout- ing, are notable for augmenting model capacity with mini- mal computational overhead (Fedus et al., 2022). Among the most important differences in this work: i) adapter ex- perts are not trained during base model pre-training, ii) they are parameter-efficient and iii) they are tailored to specific tasks instead of being opaque computation units at the token level whose specialization is not easily interpretable (Jiang et al., 2024). Regarding ii), Wang et al. (2022a); Zadouri et al. (2023); Muqeeth et al. (2023) employs routing each ex- ample to a set of experts, showcasing enhanced performance on unseen tasks. Gupta et al. (2022) trains a separate router for each task and picks a router from a similar task based on domain knowledge. Ye et al. (2022) proposes task-level MoEs that treat a collection of transformer layers as experts and a router chooses from these experts dynamically. Recent work by Caccia et al. (2023); Ponti et al. (2023); Ostapenko et al. (2023) investigate the effectiveness of densely routed adapter experts trained end-to-end with an expert library for MTL fine-tuning. For expert aggregation, we employ parameter-space weighted averaging (Wortsman et al., 2022; Zhang et al., 2023a; Ram´e et al., 2023) with weights induced by a learned router, a technique akin to those in previous works (Ostapenko et al., 2023; Zadouri et al., 2023). Sev- eral recent works have also proposed techniques for learning how to route queries to specialized pretrained open-source LLMs (Lu et al., 2023; Shnitzer et al., 2023). Model ensemblingtechniques aim to enhance model ro- bustness and generalization by integrating multiple dis- tinct models (Frankle et al., 2020; Wortsman et al., 2022; Ram´e et al., 2023; Jin et al., 2022; Matena & Raffel, 2022; Chronopoulou et al., 2023b; Yang et al., 2023). Parameter space averaging of independent models serves as an efficient 8Towards Modular LMs by Building and Reusing a Library of LoRAs ensembling method for full models (Ilharco et al., 2022; Ainsworth et al., 2022; Jin et al., 2022) and adapters (Zhang et al., 2023a; Yadav et al., 2024), requiring only a single forward pass through the model, unlike output space ensem- bling (Dietterich, 2000; Breiman, 1996), that requires many forward passes. Efficient output ensembling techniques that can be applied in conjunction with our work are in (Wen et al., 2020). Similarly, Pfeiffer et al. (2021) proposes en- sembling bottleneck style adapters with the subsequent fine- tuning step. Tam et al. (2023) presents a merging framework called MaTs using the conjugate gradient method. Yadav et al. (2024) proposes Ties-Merging to mitigate interference due to redundant parameter values. Daheim et al. (2024) merge models by reducing their individual gradient mis- match with an ideal joint model, weighting their parameters with normalized Fisher Information. Data Clustering for LMs have been proposed to im- prove performance and decrease task interference (Fifty et al., 2021; Gururangan et al., 2023; Gou et al., 2023). These methods include clustering using similarities com- puted by tf-idf and neural embeddings, K-means clustering with balanced linear assignment, and soft clustering with GMMs (Gross et al., 2017; Chronopoulou et al., 2023a; 2021; Gururangan et al., 2023; Duan et al., 2021; Caron et al., 2018). Recent work by Zhou et al. (2022) observes the potential of adapter parameters as effective task embed- dings for clustering purposes, a concept we leverage in this work. A similar observation, but regarding task gradients, has been made by Vu et al. (2020b). Building libraries of composable expertshas been envi- sioned in several previous works (Pfeiffer et al., 2021; Wu et al., 2023; Huang et al., 2023; Shah et al., 2023; Xun Wu, 2024). Beck et al. (2021); Poth et al. (2023) orchestrated a framework for assembling diverse adapters, offering flex- ibility in both training and inference. Most related to this work, Huang et al. (2023) build LoRAHub, a library of task-specific LoRAs that can be combined for few-shot generalization. Pfeiffer et al. (2021) introduce a two-stage learning algorithm that leverages knowledge from multiple tasks. They first learn task-specific experts and then com- bine the experts in a separate knowledge composition step. Xun Wu (2024) introduces a learnable gating function to combine multiple LoRAs, called Mixture of LoRA Experts (MoLE). Wu et al. (2023) presents π-tuning for vision, lan- guage, and vision-language few-shot tasks. π-tuning trains task-specific experts and then uses task embedding based on the diagonal of the Fisher information matrix to retrieve the top-k most similar tasks to a target task. We extend and complement these works by i) proposing novel methods to build a library, and ii) proposing techniques for zero-shot post-hoc routing independently trained adapters. Related to ii), in a concurrent work, Muqeeth et al. (2024) learns a sigmoid gate for each expert, which is later used as expert prototype for zero-shot transfer. Notably, this method is applicable to the same setting as Arrow, and generalizes beyond linear adapters. However, in contrast to Arrow, ob- taining the expert prototypes requires additional training after the experts are learned. 7. Conclusions and Future Work We investigate how to build and reuse a library of adapters “end-to-end”. We show the potential of reusing indepen- dently (or partially independently) trained adapters with a zero-shot routing strategy. Overall, we strategically in- vestigate the modular augmentation of smaller (language) models, offering a promising direction for research that prioritizes efficiency, flexibility, and performance. The current investigation focuses on LoRA adapters. For future work, we are excited by the exploration of a het- erogeneous “universe” of adapters—including soft and hard prompts (Lester et al., 2021; Wen et al., 2023), MLPs (Houlsby et al., 2019), etc.—and combinations thereof. Whether our approach can result in encouraging re- sults at a greater scale (both in terms of data and model size) remains open to further investigation. Using the proposed routing strategy for modular continual learning (Ostapenko et al., 2021; Ermis et al., 2022; Wang et al., 2022c) is an- other promising direction for future work, especially given the fact that the Arrow router is local to each expert. In prin- ciple, it may be less susceptible to catastrophic forgetting as no gradient-based training is required to incorporate new experts into the library. 8. Broader Impact This work sheds light on different ways of extending the capabilities of language models by surrounding them with a universe of lightweight adapters that can be trained on conventional hardware. Allowing the reuse of adapters might enable systems that are trained in a collaborative and distributed fashion and that use less total energy, with positive ramifications for the environment, but still attain the performance of vanilla systems. Further, this might allow users with smaller computational resources to more easily use and customize LLMs. There are also many potential societal consequences of improving LLMs, some being less desirable and even undesirable, but none of which we feel must be specifically highlighted here. 9. Summary of Contributions • OO led the effort on library compression and adapta- tion baselines. conceptualized and implemented MBC clustering, designed and implemented various experi- ments, and contributed to paper writing and codebase. 9Towards Modular LMs by Building and Reusing a Library of LoRAs • ZS worked on 0-shot and supervised adaptation, de- signed the task predictor routing, implemented the π- tuning baseline, and contributed to the codebase and writing. • EP, LCh, NLR were involved in the project after its start, and contributed to the general vision and to proof- writing. • MP maintained and optimized the code, and prepared the code release. • LCa led the efforts on 0-shot routing; designed, con- ceptualized, and implemented Arrow routing; imple- mented the CM baseline and contributed to the code- base. • AS led the project and conceived its idea, worked on the codebase, data generation and evaluations, and wrote the paper. References Ainsworth, S. K., Hayase, J., and Srinivasa, S. Git re-basin: Merging models modulo permutation symmetries. arXiv preprint arXiv:2209.04836, 2022. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Beck, T., Bohlender, B., Viehmann, C., Hane, V ., Adamson, Y ., Khuri, J., Brossmann, J., Pfeiffer, J., and Gurevych, I. Adapterhub playground: Simple and flexible few-shot learning with adapters. arXiv preprint arXiv:2108.08103, 2021. Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open_llm_leaderboard, 2023. Belofsky, J. Token-level adaptation of lora adapters for downstream task generalization, 2023. Bisk, Y ., Zellers, R., Gao, J., Choi, Y ., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, pp. 7432–7439, 2020. Bragman, F. J., Tanno, R., Ourselin, S., Alexander, D. C., and Cardoso, J. Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution ker- nels. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pp. 1385–1394, 2019. Breiman, L. Bagging predictors. Machine learning, 24: 123–140, 1996. Caccia, L., Ponti, E., Su, Z., Pereira, M., Roux, N. L., and Sordoni, A. Multi-head adapter routing for cross-task generalization, 2023. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp. 132–149, 2018. Caruana, R. Multitask learning. Machine learning, 28: 41–75, 1997. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, Z., Shen, Y ., Ding, M., Chen, Z., Zhao, H., Learned- Miller, E., and Gan, C. Mod-squad: Designing mixture of experts as modular multi-task learners, 2022. Chronopoulou, A., Peters, M. E., and Dodge, J. Efficient hierarchical domain adaptation for pretrained language models. arXiv preprint arXiv:2112.08786, 2021. Chronopoulou, A., Peters, M. E., Fraser, A., and Dodge, J. Adaptersoup: Weight averaging to improve general- ization of pretrained language models. arXiv preprint arXiv:2302.07027, 2023a. Chronopoulou, A., Pfeiffer, J., Maynez, J., Wang, X., Ruder, S., and Agrawal, P. Language and task arithmetic with parameter-efficient layers for zero-shot summarization. arXiv preprint arXiv:2311.09344, 2023b. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Daheim, N., M ¨ollenhoff, T., Ponti, E., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gra- dient matching. In The Twelfth International Confer- ence on Learning Representations, 2024. URL https: //openreview.net/forum?id=D7KJmfEDQP. Dietterich, T. G. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pp. 1–15. Springer, 2000. Ding, C., Lu, Z., Wang, S., Cheng, R., and Boddeti, V . N. Mitigating task interference in multi-task learning via explicit task routing with non-learnable primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7756–7765, 2023. 10Towards Modular LMs by Building and Reusing a Library of LoRAs Duan, Z., Zhang, H., Wang, C., Wang, Z., Chen, B., and Zhou, M. Enslm: Ensemble language model for data di- versity by semantic clustering. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2954–2967, 2021. EleutherAI. Multiple-choice normaliza- tion. https://blog.eleuther.ai/ multiple-choice-normalization/, 2021. Ac- cessed: 2024-05-12. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T., et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021. Ermis, B., Zappella, G., Wistuba, M., Rawal, A., and Ar- chambeau, C. Memory efficient continual learning with transformers. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Pro- cessing Systems , 2022. URL https://openreview. net/forum?id=U07d1Y-x2E. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and ef- ficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. URL http://jmlr.org/papers/ v23/21-0998.html. Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., and Finn, C. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:27503–27516, 2021. Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning, pp. 3259– 3269. PMLR, 2020. Gou, Y ., Liu, Z., Chen, K., Hong, L., Xu, H., Li, A., Yeung, D.-Y ., Kwok, J. T., and Zhang, Y . Mixture of cluster- conditional lora experts for vision-language instruction tuning. arXiv preprint arXiv:2312.12379, 2023. Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of experts for large scale weakly supervised vision. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6865–6873, 2017. Gupta, S., Mukherjee, S., Subudhi, K., Gonzalez, E., Jose, D., Awadallah, A. H., and Gao, J. Sparsely activated mixture-of-experts are robust multi-task learners. arXiv preprint arXiv:2204.07689, 2022. Gururangan, S., Li, M., Lewis, M., Shi, W., Althoff, T., Smith, N. A., and Zettlemoyer, L. Scaling expert lan- guage models with unsupervised domain discovery.arXiv preprint arXiv:2303.14177, 2023. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790– 2799, 2019. URL http://proceedings.mlr.press/ v97/houlsby19a/houlsby19a.pdf. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adapta- tion of large language models. In International Confer- ence on Learning Representations, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Huang, C., Liu, Q., Lin, B. Y ., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269 , 2023. Huang, C., Liu, Q., Lin, B. Y ., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing mod- els with task arithmetic.arXiv preprint arXiv:2212.04089, 2022. Jang, J., Kim, S., Ye, S., Kim, D., Logeswaran, L., Lee, M., Lee, K., and Seo, M. Exploring the benefits of training expert language models over instruction tuning, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Data- less knowledge fusion by merging weights of language models. arXiv preprint arXiv:2212.09849, 2022. Jolicoeur-Martineau, A., Gervais, E., Fatras, K., Zhang, Y ., and Lacoste-Julien, S. Population parameter averaging (papa), 2023. 11Towards Modular LMs by Building and Reusing a Library of LoRAs Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Hen- derson, J. Parameter-efficient multi-task fine-tuning for Transformers via shared hypernetworks. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pp. 565–576, August 2021. URL https://aclanthology. org/2021.acl-long.47. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning, 2021. URL https://arxiv.org/pdf/2104.08691.pdf. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language mod- els. arXiv preprint arXiv:2208.03306, 2022. Li, X. L. and Eisner, J. Specializing word embeddings (for parsing) by information bottleneck. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP), pp. 2744–2754, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1276. URL https://www.aclweb. org/anthology/D19-1276. Li, X. L. and Liang, P. Prefix-tuning: Optimizing con- tinuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pp. 4582–4597, Online, August 2021. Asso- ciation for Computational Linguistics. doi: 10.18653/ v1/2021.acl-long.353. URL https://aclanthology. org/2021.acl-long.353. Lin, C.-Y . and Hovy, E. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 human language technology conference of the North American chapter of the association for computa- tional linguistics, pp. 150–157, 2003. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-efficient fine- tuning is better and cheaper than in-context learning, 2022. URL https://arxiv.org/abs/2205.05638. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effec- tive instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Lu, K., Yuan, H., Lin, R., Lin, J., Yuan, Z., Zhou, C., and Zhou, J. Routing to the expert: Efficient reward- guided ensemble of large language models.arXiv preprint arXiv:2311.08692, 2023. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y ., Paul, S., and Bossan, B. Peft: State-of-the-art parameter- efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022. Matena, M. S. and Raffel, C. A. Merging models with fisher- weighted averaging. Advances in Neural Information Processing Systems, 35:17703–17716, 2022. Microsoft Research. Phi-2: The Surprising Power of Small Language Models, 2023. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Mireshghallah, F., Taram, M., Vepakomma, P., Singh, A., Raskar, R., and Esmaeilzadeh, H. Privacy in deep learn- ing: A survey. arXiv preprint arXiv:2004.12254, 2020. Muqeeth, M., Liu, H., and Raffel, C. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023. Muqeeth, M., Liu, H., Liu, Y ., and Raffel, C. Learning to route among specialized experts for zero-shot generaliza- tion. arXiv preprint arXiv: 2402.05859, 2024. Nakatsukasa, Y . The low-rank eigenvalue problem.arXiv preprint arXiv:1905.11490, 2019. Ostapenko, O., Rodriguez, P., Caccia, M., and Charlin, L. Continual learning via local mod- ule composition. Advances in Neural Informa- tion Processing Systems , 34, 2021. URL https: //proceedings.neurips.cc/paper/2021/file/ fe5e7cb609bdbe6d62449d61849c38b0-Paper.pdf. Ostapenko, O., Caccia, L., Su, Z., Le Roux, N., Charlin, L., and Sordoni, A. A case study of instruction tuning with mixture of parameter-efficient experts. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. Pfeiffer, J., Kamath, A., R¨uckl´e, A., Cho, K., and Gurevych, I. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Com- putational Linguistics, pp. 487–503, April 2021. URL https://aclanthology.org/2021.eacl-main.39. Pfeiffer, J., Ruder, S., Vuli´c, I., and Ponti, E. M. Modular deep learning. arXiv preprint arXiv:2302.11529, 2023. URL https://arxiv.org/pdf/2302.11529.pdf. 12Towards Modular LMs by Building and Reusing a Library of LoRAs Ponti, E. M., Sordoni, A., Bengio, Y ., and Reddy, S. Com- bining parameter-efficient modules for task-level gen- eralisation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computa- tional Linguistics, pp. 687–702, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.eacl-main.49. Poth, C., Sterz, H., Paul, I., Purkayastha, S., Engl ¨ander, L., Imhof, T., Vuli ´c, I., Ruder, S., Gurevych, I., and Pfeiffer, J. Adapters: A unified library for parameter- efficient and modular transfer learning. arXiv preprint arXiv:2311.11077, 2023. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21: 1–67, 2020. URL https://www.jmlr.org/papers/ volume21/20-074/20-074.pdf. Ram´e, A., Ahuja, K., Zhang, J., Cord, M., Bottou, L., and Lopez-Paz, D. Model ratatouille: Recycling diverse models for out-of-distribution generalization. In Inter- national Conference on Machine Learning, pp. 28656– 28679. PMLR, 2023. Rapin, J. and Teytaud, O. Nevergrad - A gradient- free optimization platform. https://GitHub.com/ FacebookResearch/Nevergrad, 2018. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106, 2021. Shah, V ., Ruiz, N., Cole, F., Lu, E., Lazebnik, S., Li, Y ., and Jampani, V . Ziplora: Any subject in any style by effec- tively merging loras. arXiv preprint arXiv:2311.13600, 2023. Shnitzer, T., Ou, A., Silva, M., Soule, K., Sun, Y ., Solomon, J., Thompson, N., and Yurochkin, M. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789, 2023. Strezoski, G., Noord, N. v., and Worring, M. Many task learning with task routing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1375–1384, 2019. Suzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay, Y ., Chung, H. W., Chowdhery, A., Le, Q. V ., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Tam, D., Bansal, M., and Raffel, C. Merging by matching models in task subspaces. arXiv preprint arXiv:2312.04339, 2023. Vu, T., Wang, T., Munkhdalai, T., Sordoni, A., Trischler, A., Mattarella-Micke, A., Maji, S., and Iyyer, M. Ex- ploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 7882–7926, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.635. URL https://aclanthology.org/ 2020.emnlp-main.635. Vu, T., Wang, T., Munkhdalai, T., Sordoni, A., Trischler, A., Mattarella-Micke, A., Maji, S., and Iyyer, M. Exploring and predicting transferability across nlp tasks. arXiv preprint arXiv:2005.00770, 2020b. Vu, T., Lester, B., Constant, N., Al-Rfou, R., and Cer, D. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. Wallingford, M., Li, H., Achille, A., Ravichandran, A., Fowlkes, C., Bhotika, R., and Soatto, S. Task adaptive parameter sharing for multi-task learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7561–7570, 2022. Wang, Y ., Agarwal, S., Mukherjee, S., Liu, X., Gao, J., Awadallah, A. H., and Gao, J. Adamix: Mixture-of- adaptations for parameter-efficient model tuning. arXiv preprint arXiv:2205.12410, 2022a. Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y ., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022b. Wang, Y ., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023. Wang, Z., Tsvetkov, Y ., Firat, O., and Cao, Y . Gradient vac- cine: Investigating and improving multi-task optimization in massively multilingual models. In International Con- ference on Learning Representations, 2021. URL https: //openreview.net/forum?id=F1vEjWK-lH_. Wang, Z., Zhang, Z., Lee, C.-Y ., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V ., Dy, J., and Pfister, T. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139–149, 2022c. 13Towards Modular LMs by Building and Reusing a Library of LoRAs Wen, Y ., Tran, D., and Ba, J. Batchensemble: An alterna- tive approach to efficient ensemble and lifelong learning, 2020. Wen, Y ., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., and Goldstein, T. Hard prompts made easy: Gradient- based discrete optimization for prompt tuning and discov- ery. arXiv preprint arXiv:2302.03668, 2023. Wortsman, M., Ilharco, G., Gadre, S. Y ., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y ., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned mod- els improves accuracy without increasing inference time. In International Conference on Machine Learning , pp. 23965–23998. PMLR, 2022. Wu, C., Wang, T., Ge, Y ., Lu, Z., Zhou, R., Shan, Y ., and Luo, P. π-tuning: Transferring multimodal foundation models with optimal multi-task interpolation. In Inter- national Conference on Machine Learning, pp. 37713– 37727. PMLR, 2023. Xun Wu, Shaohan Huang, F. W. Mole: Mixture of lora ex- perts. In International Conference on Learning Represen- tations, ICLR 2024, 2024. URL https://openreview. net/forum?id=uWvKBCYh4S. Yadav, P., Tam, D., Choshen, L., Raffel, C. A., and Bansal, M. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. Yang, E., Wang, Z., Shen, L., Liu, S., Guo, G., Wang, X., and Tao, D. Adamerging: Adaptive model merging for multi-task learning. arXiv preprint arXiv:2310.02575, 2023. Ye, Q., Zha, J., and Ren, X. Eliciting and understanding cross-task skills with task-level mixture-of-experts. arXiv preprint arXiv:2205.12701, 2022. Zadouri, T., ¨Ust¨un, A., Ahmadian, A., Ermis ¸, B., Locatelli, A., and Hooker, S. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. arXiv preprint arXiv:2309.05444, 2023. Zaremoodi, P., Buntine, W., and Haffari, G. Adaptive knowledge sharing in multi-task learning: Improving low- resource neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computa- tional Linguistics (Volume 2: Short Papers), pp. 656–661, 2018. Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, J., Chen, S., Liu, J., and He, J. Composing parameter- efficient modules with arithmetic operations. arXiv preprint arXiv:2306.14870, 2023a. Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y . Llama-adapter: Efficient fine- tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023b. Zhao, X., Li, H., Shen, X., Liang, X., and Wu, Y . A modu- lation module for multi-task learning with applications in image retrieval. In Proceedings of the European Confer- ence on Computer Vision (ECCV), pp. 401–416, 2018. Zhou, W., Xu, C., and McAuley, J. Efficiently tuned parameters are task embeddings. arXiv preprint arXiv:2210.11705, 2022. 14Towards Modular LMs by Building and Reusing a Library of LoRAs Library L piqa boolq wgrande hswag arcE arcC HE oqa bbh mbpp Acc . StableLM (3B) Base - - 78.2 73.1 66.6 73.7 59.6 41.5 18.3 37.6 34.7 32.3 51.6 Shared - 1 79.4 80.3 68.0 71.3 74.7 42.1 11.6 38.0 38.3 21.0 52.5 Private µ 100 79.4 76.8 67.3 74.4 72.4 44.0 16.5 42.6 37.0 34.6 54.5 Private ↗↗ 100 80.1 72.1 70.8 74.8 73.4 45.3 16.5 43.6 36.1 33.5 54.6 MBC µ 10 80.4 80.4 68.2 74.7 76.7 47.4 14.6 43.0 35.4 36.2 55.7 MBC ↗↗ 10 80.5 79.0 68.2 73.6 75.2 46.4 13.4 43.0 32.0 27.6 53.9 Table 4.Out-of-distribution zero-shot results: Accuracy on held-out tasks for StableLM. The best results are underlined. 10. Appendix 10.1. Analyzing∥ABT v∥2 for in-distribution and out-of-distribution samples In this section, we analyze whether the motivation behind Arrow routing holds in practice. Recall that at each layer, Arrow routing initializes prototypes in the linear router for expert i with the unit vector vi maximizing ∥ABT v∥2. Concretely, we hypothesize that for a hidden activation h computed from x ∈ Di, we have ∥AiBT i v∥2 > ∥AjBT j v∥2, for experts i, j. In other words, the norm of the linearly transformed prototype will be higher under the expert belonging to the same task as the input h. To test this hypothesis, we run the following experiment. Let hl denote the input to the expert at layer l, and (ABT )i l denote the linear transformation of expert i at layer l. We first sample 5000 examples from the multitask dataset. Then, for a given input x ∈ Di at each layer l, we compute both ∥(ABT )i l · hl∥2 and ∥(ABT )j l · hl∥2 where j is another randomly sampled expert such that i ̸= j. We then compute the average norm ratio r across all layers, i.e. r = LX l 1 L ∥(ABT )i l · hi l∥2 ∥(ABT )j l · hi l∥2 . Note that the random expert j is sampled at every layer, and the output of the in-distribution expert is propagated to the next layer. As such, r > 1 indicates that on average, the in-distribution expert produces a higher norm output, which would validate the use of the norm-maximizing initialization that Arrow routing uses. In figure 5, we see that for all the points considered, this ratio is positive, indicating that in-distribution experts tend to be maximize the norm of the linearly transformed input. 1.000 1.002 1.004 1.006 1.008 1.010 1.012 1.014 1.016 average norm ratio r 0 50 100 150 200 250 300probability density Norm Ratio r of in-distribution over out-of-distribution experts Figure 5.Histogram of the ratios r computed over 5000 samples. 10.2. Few-shot adaptation We apply some of the proposed methods to a data scarce setting with up to only 0.5% of the original training data per task (approx. 40 examples per task). We show the results in Table 6. Even in this setting gradient based method MBC-Poly considerably outperforms LoraHub, where the LoraHub is given compute equivalent to training gradient based methods 15Towards Modular LMs by Building and Reusing a Library of LoRAs Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) Base - 4.0 3.3 26.4 3.5 16.2 32.5 35.2 62.5 54.2 12.8 8.2 7.6 22.2 Shared 1 38.3 17.9 36.4 11.5 77.2 39.4 45.8 84.5 40.7 21.5 34.3 24.1 39.3 Private-µ 256 10.6 16.1 35.6 9.6 64.8 58.2 42.6 72.2 61.7 17.5 25.1 24.0 36.6 Private-↗↗ 256 20.4 18.8 31.8 10.5 76.3 36.4 46.8 84.2 41.8 19.2 33.4 28.7 37.4 MBC-µ 10 31.8 26.9 33.9 12.7 77.6 77.9 47.2 86.0 49.2 22.4 37.0 29.8 44.4 MBC-↗↗ 10 32.6 15.9 31.3 7.6 79.6 36.6 41.7 80.2 33.1 21.5 32.0 28.5 36.7 Mistral (7B) Base - 13.7 10.7 31.8 5.6 37.4 22.0 35.6 49.1 58.6 13.7 22.2 14.2 26.4 Shared 1 50.8 18.0 37.5 8.8 67.4 80.0 54.1 81.9 59.6 30.0 32.0 27.0 45.6 Private-µ 256 30.1 17.2 10.2 7.7 70.4 37.7 38.6 63.0 63.0 20.7 25.4 23.2 36.4 Private-↗↗ 256 38.5 23.7 43.7 12.7 78.0 76.7 54.6 83.3 57.9 25.2 35.4 33.4 46.9 MBC-µ 10 54.0 26.1 46.4 15.0 80.8 80.1 46.0 82.7 66.5 28.1 46.1 36.0 50.6 MBC-↗↗ 10 38.1 24.3 35.9 12.8 85.5 77.1 43.3 82.1 57.7 29.0 33.9 31.2 45.9 Table 5.Out-of-distribution zero-shot results on 12 held-out SNI tasksfor library built for the Phi-2 and Mistral base models. Applying ↗↗ routing to the Private libraries results in performance improvements over the µ routing for both models, with a notable improvement of over 10 Rouge-L points in case of Mistral. It is worth noticing that µ routing performed better than ↗↗ in case of MBC library for both models. We note that ↗↗ only selects top-4 experts for routing, whereas µ averages full libraries. Best results are underlined. on full dataset. Additionally, we observe that MBC-PolyZ, a method similar to MBC-Poly that only updates the routings and not the expert’s weights, performs similarly to LoraHub. Interestingly, when data amount is lowered, the perfromance of MBC-PolyZ is reduced by a relatively smaller margin than MBC-Poly which can be explained by a smaller amount of updated parameters. Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Full data MBC-LoraHub 10 41.5 21.9 37.4 17.5 78.1 68.3 48.0 82.0 62.6 21.2 33.5 31.1 45.3 MBC-Poly 10 96.9 84.4 67.2 53.9 96.4 97.8 60.2 87.9 91.3 29.4 81.7 99.7 78.9 MBC-PolyZ 10 37.7 27.9 36.2 12.6 75.9 74.4 48.7 81.3 58.9 22.5 36.1 31.1 45.3 10% MBC-Poly 10 89.6 53.3 64.5 44.5 93.5 98.5 58.5 75.7 87.3 27.2 65.6 66.8 68.8 MBC-PolyZ 10 34.3 27.5 36.2 12.4 76.5 74.3 47.5 86.2 57.9 22.7 35.3 31.4 45.2 5% MBC-Poly 10 87.0 43.0 61.3 41.7 92.0 95.2 55.3 77.3 89.0 25.4 59.1 47.8 64.5 MBC-PolyZ 10 32.6 28.6 36.0 13.0 76.4 73.9 47.3 86.3 57.7 22.7 36.6 31.0 45.2 0.5% MBC-Poly 10 49.7 30.4 43.7 20.3 77.3 78.4 48.2 86.5 72.2 23.2 43.0 29.1 50.2 MBC-PolyZ 10 32.0 27.4 34.3 12.6 77.6 78.1 47.2 86.5 53.2 22.2 37.0 29.1 44.8 Table 6.Rouge-L score for Phi-2 model after adaptation with different portions of data per task ranging from full dataset down to 10% and 5% of data per task. Note, MBC-PolyZ only tunes the routing weights, whereas MBC-Poly trains both the routing weights and the experts parameters. 11. Implementation details and hyperparameters In this section we provide some technical details about the experiments conducted in this paper. Training hyperparameters. For all LoRA experts trained in this paper we employ LoRA rank of 4, LoRA dropout probability of 0.05, LoRA α of 16, and a learning rate of 1e-4 with a learning rate warm-up and annealing phases. We experimented with only patching fully connected layers (FC), only attention layers + attention output projection (ATT+O) or both (BOTH). For the preliminary experiments in Figure 2 we modify only the MLP (FC) layers of the transformer (.*fc[12].*). We found that patching FC layers severely underperform ATT+O layers. Patching BOTH gives marginal gains over ATT+O while significantly increasing computation cost and memory usage due to the wide projection (4 * hidden 16Towards Modular LMs by Building and Reusing a Library of LoRAs Method L SNI Tasks Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) No Library - 93.2 74.2 64.9 51.4 95.9 96.2 59.3 81.4 90.5 26.9 73 99.1 75.5 Shared 1 93.1 73.4 65.0 48.9 96.0 95.9 58.4 86.8 91.2 29.0 73.4 98.4 75.8 MHR 1 94.5 66.9 63.0 47.8 94.9 95.6 59.5 86.6 91.0 27.9 70.7 98.2 74.8 Poly 10 92.1 66.4 63.0 45.9 94.9 96.4 56.0 85.3 90.2 27.6 70.0 93.2 73.4 Private-µ 256 93.6 78.1 65.0 50.7 94.8 97.8 59.7 87.9 90.7 28.1 76.4 99.7 76.9 MBC-µ 10 96.4 83.2 67.6 53.5 96.2 98.0 60.5 88.2 90.7 29.8 82.3 99.5 78.8 MBC-LoraHub 10 41.5 21.9 37.4 17.5 78.1 68.3 48.0 82.0 62.6 21.2 33.5 31.1 45.3 RandTask-Poly 10 96.4 77.1 66.5 48.6 96.7 98.9 59.9 85.1 90.7 28.6 73.9 97.5 76.7 MBC-Poly 10 96.9 84.4 67.2 53.9 96.4 97.8 60.2 87.9 91.3 29.4 81.7 99.7 78.9 Mistral (7B) No Library - 97.6 88.3 68.9 59.9 98.8 98.8 62.9 87.3 91.8 37.5 80.5 100 81.0 Shared 1 95.8 87.4 69.9 52.7 98.7 99.2 63.5 87.6 91.6 37.2 78.1 100 80.1 Private-µ 256 98.5 87.2 70.6 54.1 98.3 99.1 64.0 89.1 92.0 37.3 81.0 100 80.9 MBC-µ 10 98.1 84.8 70.1 54.2 98.7 95.7 62.8 82.9 92.0 38.2 82.1 99.5 79.9 MBC-LoRAHub 10 47.8 23.1 45.9 14.0 81.4 79.6 49.7 84.9 69.6 28.6 42.2 34.5 50.1 RandTask-Poly 10 98.4 86.9 69.3 53.8 96.1 93.0 64.7 84.5 92.1 39.3 80.1 99.5 79.8 MBC-Poly 10 98.7 88.7 69.2 56.1 97.4 99.5 64.1 82.2 92.2 38.7 81.1 99.0 80.6 Table 7.Supervised adaptation results (100% training data per task): Rouge-L on 12 held-out SNI for Phi-2 and Mistral 7B models for different libraries. LoraHub follows the original implementation and optimizes the weighting coefficients for the adapters in the library with a non-gradient based optimizer. Best results are underlined. Method L SNI Tasks (10%) Rouge-L 202 304 614 613 362 242 1728 1557 035 1356 039 1153 Phi-2 (2.8B) No Library - 71.5 36.1 53.6 36.9 80.0 85.5 46.7 62.3 84.0 21.7 41.3 27.2 53.9 Shared 1 76.8 35.5 55.5 39.4 82.6 89.3 47.6 61.8 86.0 23.3 47.9 31.2 56.4 MHR 1 83.6 45.1 58.2 40.0 91.3 94.3 54.0 84.1 85.7 25.7 58.0 54.2 64.5 Poly 1 74.4 38.3 57.8 39.5 82.5 92.2 50.8 85.1 85.1 25.5 54.8 54.1 61.7 Private-µ 256 81.7 41.2 60.6 40.4 89.8 96.0 49.6 75.1 87.1 23.8 57.2 47.9 62.5 MBC-µ 10 86.2 52.3 64.3 43.8 93.8 97.3 53.3 75.0 87.5 26.3 61.1 63.8 67.0 MBC-LoraHub 10 43.6 22.2 36.5 13.5 77.0 68.8 45.5 82.2 63.2 21.2 34.6 27.6 44.7 RandTask-Poly 10 87.9 51.0 63.5 41.4 94.1 95.8 55.6 79.6 89.0 27.1 61.1 65.3 67.6 MBC-Poly 10 88.9 52.0 64.4 45.6 94.3 96.9 56.7 75.2 87.5 27.1 64.4 66.0 68.2 Mistral (7B) No Library - 91.7 66.8 66.2 47.8 95.2 98.3 59.5 69.9 90.7 33.9 65.8 68.1 71.2 Shared 1 94.6 64.9 65.1 45.3 90.7 91.0 60.3 82.7 89.6 33.4 66.3 91.3 72.9 Private-µ 256 89.0 64.3 66.0 47.2 94.7 98.8 59.4 84.1 90.6 33.5 67.6 92.9 74.0 MBC-µ 10 94.2 62.6 66.3 47.9 95.9 96.6 59.9 83.6 90.7 33.7 69.3 92.8 74.5 MBC-LoRAHub 10 47.7 23.2 47.0 15.4 85.8 72.4 49.0 81.9 71.7 27.3 27.7 30.6 48.3 RandTask-Poly 10 95.0 64.8 66.0 48.8 94.3 92.1 59.6 87.1 90.6 34.4 66.4 92.4 74.3 MBC-Poly 10 95.1 66.3 66.0 48.3 96.4 98.4 60.2 84.9 90.5 33.7 67.8 92.7 75.0 Table 8.Supervised few-shot adaptation results (10% training data per task): Rouge-L on 12 held-out SNI for Phi-2 and Mistral 7B models for different libraries. LoraHub follows the original implementation and optimizes the weighting coefficients for the adapters in the library with a non-gradient based optimizer. Best results are underlined. 17Towards Modular LMs by Building and Reusing a Library of LoRAs size) of the first FC layer in the transformer residual block. Therefore, for the rest of the experiments, we modified attention layers + attention output projection (e.g. .*Wqkv.* |.*out proj.* for Phi-2). Downstream zero-shot results.All library-bases downstream zero-shot results are reported using top-4 routing with temperature 1. Unless stated otherwise, for all MBC libraries we use 10 experts. Additionally, in our implementation of the downstream evaluation we append an EOS token to the target options to mark the end of a sentence. We use token-length normalized scores for selecting continuations for multiple-choice tasks evaluation (EleutherAI, 2021). Adaptation experiments.For the adaptation experiments we also use the learning rate of 1e-4, with the same learning rate schedule as stated above. For both MHR and Poly adaptation, we tune both the experts and the routing weights. 18Towards Modular LMs by Building and Reusing a Library of LoRAs c0 ”ropes background new situation answer”, ”ropes prompt bottom no hint”, ”ropes plain background situation”, ”ropes new situation background answer”, ”ropes given background situation”, ”ropes prompt bottom hint beginning”, ”ropes prompt beginning”, ”ropes read background situation”, ”ropes plain bottom hint”, ”ropes plain no background”, ”ropes prompt mix”, ”ropes background situation middle” c1 ”glue sst2 2 0 0”, ”adversarial qa droberta generate question”, ”true case”, ”stream qed”, ”huggingface xsum”, ”cot esnli”, ”cot gsm8k”, ”trec 1 0 0”, ”yelp polarity reviews 0 2 0”, ”lambada 1 0 0”, ”glue cola 2 0 0”, ”ag news subset 1 0 0”, ”gem dart 1 1 0”, ”math dataset algebra linear 1d 1 0 0”, ”cnn dailymail 3 4 0”, ”wiki hop original explain relation”, ”dbpedia 14 given list what category does the paragraph belong to”, ”gem wiki lingua english en 1 1 0”, ”fix punct”, ”imdb reviews plain text 1 0 0”, ”race middle Write a multi choice question for the following article”, ”gigaword 1 2 0”, ”dbpedia 14 given a list of category what does the title belong to”, ”gem web nlg en 1 1 0”, ”word segment”, ”race high Write a multi choice question for the following article”, ”wmt16 translate de en 1 0 0”, ”cot ecqa”, ”aeslc 1 0 0”, ”dream generate first utterance”, ”wmt16 translate fi en 1 0 0”, ”dream answer to dialogue”, ”para crawl enes”, ”adversarial qa dbert generate question”, ”race middle Write a multi choice question options given ”, ”wmt14 translate fr en 1 0 0” c2 ”adversarial qa dbidaf question context answer”, ”super glue record 1 0 2”, ”wiki hop original generate object”, ”adversarial qa droberta tell what it is”, ”dbpe- dia 14 given a choice of categories ”, ”wiki hop original choose best object affirmative 3”, ”quac 1 0 0”, ”wiki hop original choose best object interrogative 1”, ”wiki hop original choose best object affirmative 1”, ”adversarial qa dbert answer the following q”, ”wiki hop original choose best object interrogative 2”, ”adversarial qa droberta question context answer”, ”squad v2 0 3 0 0”, ”wiki hop original generate subject”, ”wiki bio guess person”, ”adversarial qa dbidaf answer the following q”, ”adversarial qa droberta answer the following q”, ”adversarial qa dbert tell what it is”, ”race high Write a multi choice question options given ”, ”wiki hop original choose best object affirmative 2”, ”wiki hop original generate subject and object”, ”drop 2 0 0”, ”adversarial qa dbert question context answer”, ”adversarial qa dbidaf tell what it is” c3 ”wiqa what might be the first step of the process”, ”wiqa what is the final step of the following process”, ”wmt16 translate ro en 1 0 0”, ”wiqa what might be the last step of the process”, ”wiki bio key content”, ”gem common gen 1 1 0”, ”duorc SelfRC build story around qa”, ”app reviews generate review”, ”wiki bio what content”, ”wiki bio who”, ”gem e2e nlg 1 1 0”, ”cot esnli ii”, ”wmt16 translate tr en 1 0 0”, ”wiqa what is the missing first step”, ”wiki bio comprehension”, ”coqa 1 0 0”, ”duorc ParaphraseRC build story around qa”, ”multi news 1 0 0” c4 ”wiki qa found on google”, ”app reviews categorize rating using review”, ”race middle Is this the right answer”, ”super glue cb 1 0 2”, ”wiki qa Topic Prediction Answer Only”, ”wiki qa Direct Answer to Question”, ”super glue wsc fixed 1 0 2”, ”cot gsm8k ii”, ”unified qa science inst”, ”race high Is this the right answer”, ”cot strategyqa”, ”cot ecqa ii”, ”quarel do not use”, ”wiki qa exercise”, ”wiki qa automatic system”, ”cot creak ii”, ”quarel heres a story”, ”quarel choose between”, ”stream qed ii”, ”wiki qa Topic Prediction Question Only”, ”glue qnli 2 0 0”, ”cot sensemaking ii”, ”super glue copa 1 0 2”, ”social i qa Generate the question from the answer”, ”social i qa Show choices and generate index”, ”quarel testing students”, ”wiki qa Topic Prediction Question and Answer Pair”, ”wiki qa Decide good answer”, ”wiki qa Jeopardy style”, ”wiki qa Generate Question from Topic”, ”definite pronoun resolution 1 1 0”, ”wiqa effect with label answer”, ”glue wnli 2 0 0”, ”cot qasc”, ”cot strategyqa ii”, ”quarel logic test”, ”stream aqua ii” c5 ”quoref Context Contains Answer”, ”duorc SelfRC generate question by answer”, ”quoref Find Answer”, ”duorc ParaphraseRC movie director”, ”duorc ParaphraseRC answer question”, ”quoref Found Context Online”, ”quoref Read And Extract ”, ”duorc ParaphraseRC title generation”, ”duorc ParaphraseRC decide worth it”, ”quoref What Is The Answer”, ”duorc ParaphraseRC generate question”, ”quoref Guess Title For Context”, ”quoref Answer Test”, ”duorc SelfRC question answering”, ”duorc SelfRC title generation”, ”duorc ParaphraseRC generate question by answer”, ”duorc ParaphraseRC extract answer”, ”duorc SelfRC answer question”, ”duorc SelfRC decide worth it”, ”duorc ParaphraseRC question answering”, ”quoref Answer Question Given Context”, ”duorc SelfRC extract answer”, ”quoref Guess Answer”, ”quoref Answer Friend Question”, ”duorc SelfRC movie director”, ”duorc SelfRC generate question”, ”quoref Given Context Answer Question” c6 ”super glue rte 1 0 2”, ”cot sensemaking”, ”super glue wic 1 0 2”, ”cos e v1 11 rationale”, ”anli r3 0 1 0”, ”dream generate last utterance”, ”paws wiki 1 1 0”, ”cos e v1 11 generate explanation given text”, ”cot creak”, ”stream aqua”, ”snli 1 1 0”, ”cos e v1 11 i think”, ”glue qqp 2 0 0”, ”cos e v1 11 explain why human”, ”anli r2 0 1 0”, ”anli r1 0 1 0”, ”glue stsb 2 0 0”, ”cos e v1 11 aligned with common sense”, ”glue mnli 2 0 0”, ”so- cial i qa I was wondering”, ”cosmos qa 1 0 0”, ”glue mrpc 2 0 0”, ”social i qa Generate answer” c7 ”dream read the following conversation and answer the question”, ”app reviews convert to star rating”, ”cos e v1 11 question option description text”, ”social i qa Show choices and generate answer”, ”quartz answer question based on”, ”sciq Direct Question Closed Book ”, ”qasc qa with separated facts 3”, ”quartz given the fact answer the q”, ”quartz answer question below”, ”kilt tasks hotpotqa final exam”, ”sciq Multiple Choice”, ”wiqa does the supposed perturbation have an effect”, ”cos e v1 11 question description option text”, ”wiki qa Is This True ”, ”quartz use info from question paragraph”, ”sciq Direct Question”, ”qasc qa with separated facts 2”, ”wiqa which of the following is the supposed perturbation”, ”app reviews convert to rating”, ”cos e v1 11 question option description id”, ”wiqa effect with string answer”, ”qasc qa with separated facts 5”, ”dream baseline”, ”quartz having read above passage”, ”cos e v1 11 question description option id”, ”qasc qa with separated facts 1”, ”cos e v1 11 description question option text”, ”qasc qa with combined facts 1”, ”qasc is correct 1”, ”cos e v1 11 description question option id”, ”so- cial i qa Check if a random answer is valid or not”, ”sciq Multiple Choice Closed Book ”, ”quartz use info from paragraph question”, ”qasc is correct 2”, ”qasc qa with separated facts 4”, ”quartz read passage below choose”, ”quartz paragraph question plain concat”, ”sciq Multiple Choice Question First” c8 ”race middle Read the article and answer the question no option ”, ”race high Select the best answer”, ”quail description context question answer id”, ”quail context question description text”, ”race high Read the article and answer the question no option ”, ”race high Select the best answer no instructions ”, ”quail context description question answer id”, ”race high Taking a test”, ”super glue multirc 1 0 2”, ”race middle Select the best answer”, ”quail context question description answer id”, ”quail description context question answer text”, ”quail context question answer description text”, ”race high Select the best answer generate span ”, ”race middle Select the best answer generate span ”, ”quail context question answer description id”, ”quail context description question answer text”, ”quail context description question text”, ”quail context question description answer text”, ”quail description context question text”, ”race middle Taking a test”, ”quail no prompt id”, ”quail no prompt text”, ”race middle Select the best answer no instructions ” c9 ”natural questions open 1 0 0”, ”web questions whats the answer”, ”web questions question answer”, ”dbpedia 14 pick one category for the following text”, ”kilt tasks hotpotqa combining facts”, ”web questions short general knowledge q”, ”kilt tasks hotpotqa straighforward qa”, ”adversar- ial qa dbidaf generate question”, ”adversarial qa droberta based on”, ”web questions get the answer”, ”kilt tasks hotpotqa complex question”, ”web questions potential correct answer”, ”trivia qa rc 1 1 0”, ”kilt tasks hotpotqa formulate”, ”adversarial qa dbert based on”, ”adversarial qa dbidaf based on”, ”squad v1 1 3 0 0” Table 9.Task names for each of the 10 clusters obtained by applying MBC clustering to Phi-2 private library with 256 experts, with each expert trained for 2 epochs. 19Towards Modular LMs by Building and Reusing a Library of LoRAs c0 ”adversarial qa dbert generate question”, ”adversarial qa dbidaf generate question”, ”adversarial qa droberta generate question”, ”app reviews generate review”, ”cot creak”, ”cot esnli”, ”cot esnli ii”, ”dream generate first utterance”, ”dream generate last utterance”, ”duorc ParaphraseRC title generation”, ”duorc SelfRC title generation”, ”fix punct”, ”gem common gen 1 1 0”, ”gem dart 1 1 0”, ”gigaword 1 2 0”, ”huggingface xsum”, ”lam- bada 1 0 0”, ”race high Write a multi choice question for the following article”, ”race high Write a multi choice question options given ”, ”race middle Write a multi choice question for the following article”, ”race middle Write a multi choice question options given ”, ”stream aqua”, ”stream qed”, ”wiqa what is the missing first step”, ”wmt16 translate fi en 1 0 0”, ”wmt16 translate ro en 1 0 0”, ”yelp polarity reviews 0 2 0” c1 ”ag news subset 1 0 0”, ”app reviews convert to rating”, ”app reviews convert to star rating”, ”cot creak ii”, ”cot ecqa ii”, ”cot gsm8k ii”, ”cot sensemaking ii”, ”cot strategyqa”, ”dbpedia 14 given a choice of categories ”, ”dbpedia 14 given a list of category what does the title belong to”, ”dbpedia 14 given list what category does the paragraph belong to”, ”glue mnli 2 0 0”, ”glue qnli 2 0 0”, ”glue qqp 2 0 0”, ”glue stsb 2 0 0”, ”glue wnli 2 0 0”, ”kilt tasks hotpotqa complex question”, ”paws wiki 1 1 0”, ”qasc is correct 1”, ”qasc is correct 2”, ”snli 1 1 0”, ”so- cial i qa Check if a random answer is valid or not”, ”social i qa Generate answer”, ”social i qa Generate the question from the answer”, ”so- cial i qa I was wondering”, ”squad v1 1 3 0 0”, ”squad v2 0 3 0 0”, ”stream qed ii”, ”super glue multirc 1 0 2”, ”super glue rte 1 0 2”, ”super glue wic 1 0 2”, ”super glue wsc fixed 1 0 2”, ”trec 1 0 0”, ”wiki bio guess person”, ”wiki qa Is This True ” c2 ”app reviews categorize rating using review”, ”cos e v1 11 question option description text”, ”cot qasc”, ”cot strategyqa ii”, ”dbpe- dia 14 pick one category for the following text”, ”definite pronoun resolution 1 1 0”, ”kilt tasks hotpotqa final exam”, ”math dataset algebra linear 1d 1 0 0”, ”qasc qa with separated facts 4”, ”quarel do not use”, ”quoref Context Contains Answer”, ”race high Is this the right answer”, ”race middle Is this the right answer”, ”sciq Direct Question”, ”sciq Multiple Choice”, ”sciq Multiple Choice Closed Book ”, ”sciq Multiple Choice Question First”, ”social i qa Show choices and generate index”, ”stream aqua ii”, ”super glue cb 1 0 2”, ”super glue copa 1 0 2”, ”uni- fied qa science inst”, ”wiki qa Decide good answer”, ”wiki qa Direct Answer to Question”, ”wiki qa Generate Question from Topic”, ”wiki qa Jeopardy style”, ”wiki qa Topic Prediction Answer Only”, ”wiki qa Topic Prediction Question Only”, ”wiki qa Topic Prediction Question and Answer Pair”, ”wiki qa automatic system”, ”wiki qa exercise”, ”wiki qa found on google” c3 ”adversarial qa dbert answer the following q”, ”adversarial qa dbert based on”, ”adversarial qa dbert question context answer”, ”adversar- ial qa dbert tell what it is”, ”adversarial qa dbidaf answer the following q”, ”adversarial qa dbidaf based on”, ”adversarial qa dbidaf question context answer”, ”adversarial qa dbidaf tell what it is”, ”adversarial qa droberta answer the following q”, ”adversarial qa droberta based on”, ”adversar- ial qa droberta question context answer”, ”adversarial qa droberta tell what it is”, ”cos e v1 11 aligned with common sense”, ”cos e v1 11 explain why human”, ”cos e v1 11 generate explanation given text”, ”cos e v1 11 i think”, ”cos e v1 11 rationale”, ”drop 2 0 0”, ”duorc ParaphraseRC generate question by answer”, ”duorc SelfRC generate question by answer”, ”kilt tasks hotpotqa combining facts”, ”kilt tasks hotpotqa formulate”, ”kilt tasks hotpotqa straighforward qa”, ”natural questions open 1 0 0”, ”trivia qa rc 1 1 0”, ”web questions get the answer”, ”web questions potential correct answer”, ”web questions question answer”, ”web questions short general knowledge q”, ”web questions whats the answer” c4 ”duorc ParaphraseRC answer question”, ”duorc ParaphraseRC decide worth it”, ”duorc ParaphraseRC extract answer”, ”duorc ParaphraseRC generate question”, ”duorc ParaphraseRC movie director”, ”duorc ParaphraseRC question answering”, ”duorc SelfRC answer question”, ”duorc SelfRC decide worth it”, ”duorc SelfRC extract answer”, ”duorc SelfRC generate question”, ”duorc SelfRC movie director”, ”duorc SelfRC question answering”, ”quac 1 0 0”, ”quoref Answer Friend Question”, ”quoref Answer Test”, ”quoref Find Answer”, ”quoref Found Context Online”, ”quoref Given Context Answer Question”, ”quoref Guess Answer”, ”quoref Guess Title For Context”, ”quoref Read And Extract ”, ”quoref What Is The Answer” c5 ”cos e v1 11 description question option id”, ”cos e v1 11 question description option id”, ”dream baseline”, ”dream read the following conversation and answer the question”, ”quail context description question answer id”, ”quail context description question answer text”, ”quail context description question text”, ”quail context question answer description id”, ”quail context question answer description text”, ”quail context question description answer id”, ”quail context question description answer text”, ”quail context question description text”, ”quail description context question answer id”, ”quail description context question answer text”, ”quail description context question text”, ”quail no prompt id”, ”quail no prompt text”, ”race high Read the article and answer the question no option ”, ”race high Select the best answer”, ”race high Select the best answer generate span ”, ”race high Select the best answer no instructions ”, ”race high Taking a test”, ”race middle Read the article and answer the question no option ”, ”race middle Select the best answer”, ”race middle Select the best answer generate span ”, ”race middle Select the best answer no instructions ”, ”race middle Taking a test” c6 ”cos e v1 11 description question option text”, ”cos e v1 11 question description option text”, ”cos e v1 11 question option description id”, ”qasc qa with combined facts 1”, ”qasc qa with separated facts 1”, ”qasc qa with separated facts 2”, ”qasc qa with separated facts 3”, ”qasc qa with separated facts 5”, ”quarel choose between”, ”quarel heres a story”, ”quarel logic test”, ”quarel testing students”, ”quartz answer question based on”, ”quartz answer question below”, ”quartz given the fact answer the q”, ”quartz having read above passage”, ”quartz paragraph question plain concat”, ”quartz read passage below choose”, ”quartz use info from paragraph question”, ”quartz use info from question paragraph”, ”quoref Answer Question Given Context”, ”ropes background new situation answer”, ”ropes background situation middle”, ”ropes given background situation”, ”ropes new situation background answer”, ”ropes plain background situation”, ”ropes plain bottom hint”, ”ropes plain no background”, ”ropes prompt beginning”, ”ropes prompt bottom hint beginning”, ”ropes prompt bottom no hint”, ”ropes prompt mix”, ”ropes read background situation”, ”sciq Direct Question Closed Book ”, ”so- cial i qa Show choices and generate answer”, ”wiqa does the supposed perturbation have an effect”, ”wiqa effect with label answer”, ”wiqa effect with string answer”, ”wiqa which of the following is the supposed perturbation” c7 ”aeslc 1 0 0”, ”cnn dailymail 3 4 0”, ”coqa 1 0 0”, ”cot gsm8k”, ”dream answer to dialogue”, ”duorc ParaphraseRC build story around qa”, ”duorc SelfRC build story around qa”, ”gem e2e nlg 1 1 0”, ”gem web nlg en 1 1 0”, ”gem wiki lingua english en 1 1 0”, ”multi news 1 0 0”, ”wiki bio comprehension”, ”wiki bio key content”, ”wiki bio what content”, ”wiki bio who”, ”wiqa what is the final step of the following process”, ”wiqa what might be the first step of the process”, ”wiqa what might be the last step of the process”, ”wmt16 translate tr en 1 0 0” c8 ”anli r1 0 1 0”, ”anli r2 0 1 0”, ”anli r3 0 1 0”, ”cosmos qa 1 0 0”, ”cot ecqa”, ”cot sensemaking”, ”glue cola 2 0 0”, ”glue mrpc 2 0 0”, ”glue sst2 2 0 0”, ”imdb reviews plain text 1 0 0”, ”para crawl enes”, ”super glue record 1 0 2”, ”true case”, ”wmt14 translate fr en 1 0 0”, ”wmt16 translate de en 1 0 0”, ”word segment” c9 ”wiki hop original choose best object affirmative 1”, ”wiki hop original choose best object affirmative 2”, ”wiki hop original choose best object affirmative 3”, ”wiki hop original choose best object interrogative 1”, ”wiki hop original choose best object interrogative 2”, ”wiki hop original explain relation”, ”wiki hop original generate object”, ”wiki hop original generate subject”, ”wiki hop original generate subject and object” Table 10.Task names for each of the 10 clusters obtained by applying MBC clustering to Mistral 7B private library with 256 experts. 20",
      "meta_data": {
        "arxiv_id": "2405.11157v1",
        "authors": [
          "Oleksiy Ostapenko",
          "Zhan Su",
          "Edoardo Maria Ponti",
          "Laurent Charlin",
          "Nicolas Le Roux",
          "Matheus Pereira",
          "Lucas Caccia",
          "Alessandro Sordoni"
        ],
        "published_date": "2024-05-18T03:02:23Z",
        "pdf_url": "https://arxiv.org/pdf/2405.11157v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the problem of reusing trained parameter-efficient adapters (LoRAs) to improve Large Language Model (LLM) performance on new tasks. The key contributions include: i) studying the creation of LoRA-based modular multi-task LLMs where experts are trained independently and the router is post-hoc; ii) proposing Model-Based Clustering (MBC) to build a library of adapters by clustering tasks based on the similarity of their LoRA parameters, which optimizes for transfer across multi-task datasets; and iii) introducing Arrow, a novel zero-shot routing mechanism that dynamically selects relevant adapters for new inputs without retraining, by leveraging the SVD decomposition of LoRA parameters. The paper demonstrates that MBC-based adapters and Arrow routing lead to superior generalization, matching or outperforming traditional joint training.",
        "methodology": "The methodology focuses on building and reusing a library of LoRA adapters. For building the library, several approaches are considered: training private adapters (one per task), training a single shared adapter, and using Poly/MHR adapters (K 'basis' adapters as linear combinations). The core contribution for library building is Model-Based Clustering (MBC), a two-stage process: first, private LoRAs are trained for a fixed number of steps; then, tasks are clustered using k-means based on the cosine similarity of their flattened LoRA parameters, and finally, one adapter is trained per cluster using the combined datasets. For reusing the library, routing strategies include zero-shot methods (uniform 'µ Routing', 'TP Routing' with a task predictor, 'CM Routing' based on centroid matching, and the novel 'Arrow Routing') and supervised adaptation methods ('Poly Routing', 'LoraHub Routing', and 'π-tuning Routing'). Arrow Routing estimates a layer-specific routing matrix by applying SVD to the LoRA parameters (AiBTi) to find expert prototypes (first right singular vector), then routes per token and per layer by computing the absolute dot product alignment between hidden states and prototypes. Selected adapters are combined via linear weighting.",
        "experimental_setup": "Experiments were conducted on Phi-2 (2.8B parameters) and Mistral 7B LLMs, with LoRA adapters patching only attention layers (rank 4, dropout 0.05, learning rate 1e-4). The multi-task dataset for library building consisted of 256 tasks from Flan v2, with 10,000 examples per task (1,000 for validation). Overlaps with evaluation tasks were removed. For zero-shot evaluation, 10 held-out tasks were used, including common-sense reasoning (WinoGrande, HellaSwag, PIQA), question answering (BoolQ, OpenbookQA, ARC-easy/hard), coding (HumanEval, MBPP), and general reasoning (BBH). Supervised adaptation was evaluated on 12 held-out SuperNatural Instructions (SNI) tasks with up to 10,000 examples each. Performance was measured using average accuracy for zero-shot tasks and Rouge-L scores for supervised adaptation. MBC typically used 10 clusters, with 40% of training steps for clustering and 60% for training cluster adapters. Baselines included Base (no adaptation), Shared (single multi-task LoRA), FullFT (full model finetuning), Poly, MHR, Private, and various routing strategies.",
        "limitations": "The current investigation is primarily focused on LoRA adapters, limiting its direct applicability to other adapter types. The paper notes that Poly/MHR adapters are not clearly reusable for zero-shot generalization as their 'latent skills' do not correspond to specific tasks. It's observed that while Arrow routing narrows the performance gap with MBC, MBC sometimes performs better. For smaller libraries, the gains from sophisticated routing strategies diminish. The linearity of LoRA experts is suggested as a potential reason for differences in routing importance compared to sparse Mixture-of-Experts models with MLP experts, an area for further investigation. The scalability of the proposed approach to even greater data and model sizes remains an open question.",
        "future_research_directions": "Future work includes extending the approach to a heterogeneous 'universe' of adapters beyond LoRA, such as soft and hard prompts, MLPs, and combinations thereof. Further investigation is needed to understand why greater diversity in expert clusters (lower similarity) tends to yield higher performance. The research also aims to explore the diminishing gains of routing for smaller libraries and its implications for the linear nature of LoRA experts versus non-linear MLP experts in sparse Mixture-of-Experts models. Finally, applying the proposed Arrow routing strategy to modular continual learning is a promising direction, potentially offering robustness against catastrophic forgetting due to its local, gradient-free nature."
      }
    },
    {
      "title": "Spectral Adapter: Fine-Tuning in Spectral Space",
      "abstract": "Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for\npretrained deep neural networks have captured widespread interest. In this\nwork, we study the enhancement of current PEFT methods by incorporating the\nspectral information of pretrained weight matrices into the fine-tuning\nprocedure. We investigate two spectral adaptation mechanisms, namely additive\ntuning and orthogonal rotation of the top singular vectors, both are done via\nfirst carrying out Singular Value Decomposition (SVD) of pretrained weights and\nthen fine-tuning the top spectral space. We provide a theoretical analysis of\nspectral fine-tuning and show that our approach improves the rank capacity of\nlow-rank adapters given a fixed trainable parameter budget. We show through\nextensive experiments that the proposed fine-tuning model enables better\nparameter efficiency and tuning performance as well as benefits multi-adapter\nfusion.",
      "full_text": "Spectral Adapter: Fine-Tuning in Spectral Space Fangzhao Zhang Electrical Engineering Stanford University zfzhao@stanford.edu Mert Pilanci Electrical Engineering Stanford University pilanci@stanford.edu Abstract Recent developments in Parameter-Efficient Fine-Tuning (PEFT) methods for pre- trained deep neural networks have captured widespread interest. In this work, we study the enhancement of current PEFT methods by incorporating the spectral information of pretrained weight matrices into the fine-tuning procedure. We investigate two spectral adaptation mechanisms, namely additive tuning and or- thogonal rotation of the top singular vectors, both are done via first carrying out Singular Value Decomposition (SVD) of pretrained weights and then fine-tuning the top spectral space. We provide a theoretical analysis of spectral fine-tuning and show that our approach improves the rank capacity of low-rank adapters given a fixed trainable parameter budget. We show through extensive experiments that the proposed fine-tuning model enables better parameter efficiency and tun- ing performance as well as benefits multi-adapter fusion. Code is released at https://github.com/pilancilab/spectral_adapter. 1 Introduction Size of language and vision model undergoes a drastic explosion in recent days and results in billions of parameters up to date. While fine-tuning has been used a lot for adapting pretrained large models to various downstream tasks, fine-tuning tasks become increasingly hard with current size of pretrained models due to the huge demand of computing resource. Meanwhile, exchange and storing of fine- tuned models are also expensive given their enormous size. To alleviate these rising problems for fine-tuning large pretrained models, a recent line of research has digged into the Parameter-Efficient Fine-Tuning (PEFT) model family and harnessed great attention. A high-level philosophy behind those PEFT methods is to train a reduced number of parameters compared to full fine-tuning, which instantly saves computing resource and enables light-weight fine-tuned model exchange. Among all PEFT methods, Low-Rank Adaptation (LoRA) [ 20] model is a huge success attributed to its simplicity and effectiveness. Specifically, LoRA proposes to tune an additive trainable low-rank matrix and brings zero inference latency after merging the adapter into pretrained model weights. Since its emergence, numerous variants of LoRA have been developed. For instance, AdaLoRA [65], IncreLoRA [62], and DyLoRA [ 54] propose to dynamically adjust LoRA rank distribution for improving tuning efficiency, QLoRA [10] combines LoRA with model quantization to further save computing resource, LoRA+ [ 16] and PrecLoRA [ 61] study the optimization landscape of LoRA training, and more recent variant DoRA [32] decomposes pretrained weights into magnitude and direction components and applies LoRA for direction tuning, see Apppendix A for a more comprehensive review of different LoRA variants. Other PEFT methods such as Orthogonal Fine- Tuning (OFT) proposes to multiply pretrained weights by tunable orthogonal matrices for preservation of hypersphere energy between pretrained neurons. Though these different PEFT methods focus on improving fine-tuning efficiency with reduced parameters, rare attention has been paid to utilize pretrained model weights’ information beyond its magnitude in the fine-tuning procedure. Prior research in statistical machine learning such as [36] has studied the Empirical Spectral Distribu- tion (ESD) of deep models’ weight matrices and found that the ESDs for larger model weights are arXiv:2405.13952v2  [cs.LG]  4 Nov 2024Figure 1: Training loss of fine-tuning Llama3 8B model with Orca Math dataset [38] and evaluation score on GSM8K benchmark [7]. We follow experimental setup in [53], see Appendix F.1 for details. All methods except full fine-tuning maintain approximately 0.23% trainable parameters. usually more structured and contain indicative information to distinguish between different training stages. More recent work such as [3] investigates the \"dark matter\" effect of bottom spectral space of model weights and recognizes its critical role in attention sink phenomenon observed in [57]. Both work contributes to decrypting spectral information of model weights and sheds light on building insightful understanding of the connection between weight matrices’ spectral information and model performance. In this work, we explore further the value of model weights’ spectral pattern and unravel its effectiveness in enhancing fine-tuning tasks. We showcase via extensive empirical observation that integration of spectral information of pretrained model weights improves current PEFT methods’ parameter efficiency, tuning effect, and arises as a natural solution to multi-adapter fusion problems. Moreover, the suggested fine-tuning model maintains better practicality compared to prior spectral tuning models, which will be investigated further below. Though any technique for weight fine-tuning can be directly applied to fine-tune singular vector matrices of pretrained model weights, we investigate two specific forms of such extension, namely additive tuning and orthogonal rotating the top singular vector space, which we address as Spectral AdapterA and Spectral AdapterR respectively in later content. The spectral adaptation mechanisms being considered are formally depicted in Section 2. As a warmup, to show that incorporating spectral information is indeed helpful, Figure 1 displays the training loss of fine-tuning Llama3 8B model on HuggingFace Orca Math dataset and validation score on GSM8K benchmark, from which it can be clearly observed that Spectral AdapterA performs superior to recent variants of PEFT methods and behaves closest to full fine-tuning, here we follow experimental setup in [53], see Appendix F.1 for details and more investigation. In below, we first introduce the fine-tuning model being studied in Section 2 and we then provide some theoretic insights in Section 3. After that, we detail the advantage of our spectral adapter in enhancing fine-tuning result, improving model’s parameter efficiency, and helping with multi-adapter fusion as well as address any concern with respect to practicality issues in Section 4. Conclusion and future work is discussed in Section 5. For sake of page limitation, literature review is deferred to Appendix A. To summarize, the proposed spectral adaptation mechanism demonstrates the first attempt to fine-tune spectral space of pretrained model weights in a parameter-efficient and storage-economic way which improves current PEFT methods from aspects involving tuning results, parameter efficiency, and multi-adapter fusion. We hope this work serves as a building block and motivates further and deeper insightful investigation for exploring spectral structure of pretrained model weights, which becomes increasingly meaningful especially in current large model regime. 2 Spectral Adapter: Incorporating Spectral Information into Fine-Tuning Motivated by the intrinsic low-rank of weight shifts in fine-tuning procedure studied in [1], LoRA [20] proposes to add a low-rank factorized trainable matrix to pretrained model weights and tune only these additive parameters for downstream task adaptation, which usually injects far fewer trainable parameters compared to full fine-tuning and results in light-weight tuned adapters. LoRA serves as an outstanding representative of PEFT family and is now widely-used for different fine-tuning tasks. 2Figure 2: Compared to LoRA which proposes to add low-rank trainable matrices to pretrained weights, we study two types of spectral adapters: Spectral AdapterA considers additively tuning the top columns of singular vector matrices and Spectral AdapterR considers orthogonally rotating the top columns of singular vector matrices. Inspired by the parameter efficiency of LoRA and the close connection between matrix rank and its spectral representation, here we study two spectral fine-tuning mechanisms, both are completed via first carrying out Singular Value Decomposition (SVD) of pretrained model weights and then fine- tuning the top columns of singular vector matrices obtained via the SVD. More precisely, consider a pretrained weight matrix with its spectral representation of form W =USV T , we define additive spectral adapter as Spectral AdapterA(W) ∶=[U1 +AU U2]S[V1 +AV V2], and correspondingly the rotational version Spectral AdapterR(W) ∶=[U1RU U2]S[V1RV V2], where U1, V1 denote the top- r columns of U and V and U2, V2 denote the rest of the columns. A =(AU , AV ) consists of trainable matrices of shape same as (U1, V1) and R =(RU , RV ) consists of two trainable orthogonal matrices of shape r by r such that RT U RU =RT V RV =I. As we show in later sections, the orthogonality constraint is efficiently handled with the Cayley parameterization, see Section 4.3 for details. The proposed fine-tuning model architecture can be visualized from Figure 2. Here Spectral AdapterA more resembles LoRA as it is of additive form while Spectral AdapterR more resembles prior Orthogonal Fine-Tuning (OFT) method which we compare further in Section 4. To ensure zero initialization as often done for PEFT methods, we initialize AU and AV both at zero. For rotational spectral adapter, we initialize RU and RV as identity matrices. A more thorough literature review suggests that prior work considering tuning model weights’ spectral representation (FSGAN[ 47], SVDiff [ 15]) has been proposed for alleviating overfitting when fine-tuning different vision models. These methods only look at tuning the singular values of flattened CNN weights and thus have fixed amount of trainable parameters. Moreover, these methods require storing all U, Sand V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks. Contrarily, we consider incorporating spectral information in generic fine-tuning procedure for different layers (flattened CNN weights, dense linear weights, etc.) and our method enables flexible parameter budget choices by varying values of r. Methodology-wise, we consider tuning the top-r columns of U and V by additive and rotational tuning, both requiring only these top columns to be stored additionally and the left part can be merged into a single weight matrix. See Section 4.4 for more investigation on practicality of the proposed method. 3 Theoretical Insights After introducing the model architecture of spectral adapter we consider, the main question now remains whether tuning the spectral representation of pretrained weights is indeed an improvement over existing PEFT methods. Before we step into our empirical observations, we first provide some theoretical insights for the proposed spectral adaptation mechanism. In this section, we show advantage of our spectral adapter method compared to LoRA from two theoretic perspectives by 3analyzing both the rank capacity of the adapters (Section 3.1) and the subspace alignment of pretrained weight matrices (Section 3.2). Specifically, we will see that Spectral AdapterA has larger rank capacity than LoRA adapter, which indicates the tuned weight has more adaptation freedom and thus is more desirable. Moreover, the dominant spectral direction of pretrained weight matrix identifies more ideal neuron alignment under the setting we consider in Section 3.2, which justifies the robustness of tuning top singular vectors in our spectral adapter. In Appendix D, we show that Spectral AdapterA is approximately equivalent to DoRA [32] for vector-form weights. 3.1 Adapter Rank Capacity For any pretrained weight matrixW, suppose that the adapter is given by the parameterizationfθ(W) where θ represents trainable weights. For instance with LoRA adapter, fθ(W) =W +ABT , where θ ={A, B} is trainable. We define the rank capacity of an adapter fθ(W) as follows: R(fθ; W) ∶= max θ rank(fθ(W))−min θ rank(fθ(W)), which describes the range of matrix ranks the tuned weight can achieve given a specific adapter form. Then, the following lemma shows that Spectral AdapterA has twice the rank capacity of LoRA adapter under an equal number of trainable parameters. Lemma 3.1. Suppose that W ∈Rn×m is an arbitrary full row-rank matrix and n ≤m without loss of generality. Consider rank-r LoRA and rank-r additive spectral adapter, which have an equal number of trainable parameters. We have R(LoRA; W) =r, R(Spectral AdapterA; W) =2r. See Appendix B for proof. Therefore when pretrained model weight matrix is close to full row-rank, as what has been observed in [20], Spectral AdapterA has nearly double rank capacity compared to LoRA adapter. Furthermore, some prior work explicitly imposes low-rank constraint when training original NNs [50, 43, 66, 22, 68, 24, 9]. Using LoRA adapter to fine-tune such pretrained model weights would destroy their rank constraints while applying spectral adapter preserves the constraints. Next we proceed to show that top spectral space of pretrained weight matrices is more aligned with ideal neuron direction under a simple setting via subspace decomposition analysis of pretrained model weights. This observation corroborates our choice of tuning top singular vectors in our proposed spectral adaptation mechanism. Empirically, we observe that tuning top directions performs superior to tuning bottom ones, see Appendix F.3 and F.5.1 for related experiments. 3.2 Weight Subspace Alignment Figure 3: Top singu- lar vector of pretrained weight recognizes more ideal neuron direction. Il- lustration plot for Section 3.2. Consider two-layer ReLU network with m hidden nodes and univariate output. For squared loss objective, we can write out the training problem explicitly as min W(1),W(2) ∥(XW (1))+W(2) −y∥2 2 +β(∥W(1)∥2 F +∥W(2)∥2 2), where X ∈ Rn×d is the data matrix, (W(1) ∈ Rd×m, W(2) ∈ Rm) are first and second layer weights respectively and y ∈Rn is the label vector. For better visualization, we take d = 3. Consider the case that all data points lie on xy−plane, which mimics the usual observation that data points occupy a low-dimensional manifold. Then we can decompose each first layer neuron W(1) j ∈ Rd into W(1) j = wj1 +wj2 where wj1 ∈ R(X), wj2 ⊥ R(X). With simple algebra, for non-zero weight decay which is often the default setting for current deep learning optimizers, one can derive wj2 =0 and thus W(1) j =wj1 ∈R(X). Therefore all optimal neurons lie also in xy−plane. However, due to optimization errors, some of the trained neurons might be slightly deviated from xy−plane, as illustrated in Figure 3, where ui indicates pretrained neuron directions, though most of them lie in xy−plane, some might deviate (i.e., u4). u⋆ indicates the top singular vector direction of pretrained weight W(1) which here recognizes the xy−plane orientation, and thus fine-tuning u⋆ is noiseless and is expected to be more robust. 44 Empirical Results: The Impact of Spectral Information We experiment our proposed spectral adapter with fine-tuning large language models and diffusion models and compare against various recent PEFT methods. From language model experiments, we observe that Spectral Adapter A performs superior to various PEFT baselines and harnesses higher scores on different benchmarks, which again verifies the effectiveness of incorporating spectral information into the fine-tuning procedure, see Section 4.1 for details. For diffusion model experiments, we will see that the advantage of spectral adapter comes in two-fold: Spectral AdapterA offers a natural solution to existing problems in multi-adapter fusion procedure and Spectral AdapterR manifests finer-grained parameter budgets as well as better parameter efficiency, see Section 4.2 and 4.3 respectively. For a fair comparison with all baselines, we use their official implementation and follow hyperparameter setting in their original reports as long as available. See each individual section for corresponding experimental details. All experiments are done with NVIDIA RTX A6000 GPU. 4.1 Language Model Fine-Tuning: Enhancing Fine-Tuning Results with Spectral Adapter A For large language model experiments, we present experimental results for fine-tuning DeBERTaV3- base model (185M) and Mistral model (7B) on GLUE and GSM8K tasks respectively. Our Spectral AdapterA method achieves superior tuning results compared to various recent PEFT methods in most experiments. DeBERTaV3-base Experiment. Table 1 shows fine-tuning results of DeBERTaV3-base model on GLUE benchmarks with various PEFT methods. For a fair comparison, we use official implemen- tations for LoRA, DoRA, OFT and AdaLoRA in HuggingFace PEFT library, with hyperparameter setting for LoRA [20] and AdaLoRA [65] following their original reports. We use same hyperpa- rameter setting as LoRA for DoRA and follow the setting used in BOFT [33], a variant of OFT, for OFT experiments. We abbreviate Spectral AdapterA as SpectralA for presentation simplicity and we tune hyperparameters for Spectral AdapterA. See Appendix F.2 for hyperparameter details and F.3 for loss/validation plot comparison. We fine-tune all q, k, vmatrices in attention layers. Our Spectral AdapterA achieves highest average score and best scores for most tasks with fewest trainable parameters. Method # Param GLUE MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg. LoRAr=24 0.72% 88.87 95.06 87.00 65.84 91.87 91.45 81.22 90.43 86.47 DoRAr=24 0.73% 88.91 95.29 88.72 65.84 92.01 91.51 80.14 90.10 86.57 OFTr=4 0.72% 89.16 95.06 87.74 66.75 93.28 91.33 78.70 89.72 86.47 AdaLoRAr=24 1.07% 89.44 94.95 89.70 63.06 93.17 91.48 83.75 91.22 87.10 SpectralA r=24 0.72% 89.79 95.75 90.19 69.44 93.35 91.65 83.39 90.64 88.03 Table 1: Accuracy comparison of fine-tuning DeBERTaV3-base with various PEFT methods on GLUE benchmarks. SpectralA is abbreviation for Spectral AdapterA. See Section 4.1 for experimental details. Mistral 7B Experiment. We experiment our Spectral Adapter A with Mistral 7B model [23] fine-tuned for GSM8K task [ 7]. Since all baseline model reports include no fine- tuning tasks with the Mistral family, we use official implementations of all baseline meth- ods for comparison and we fix learning rate to be 2.5e − 5 for all methods following [ 51]. Method #Param GSM8K Pre-Trained − 37.91 ±1.34 LoRAr=8 0.16% 44.81 ±1.37 DoRAr=8 0.17% 43.82 ±1.37 SpectralA r=8 0.16% 49.73 ±1.38 Table 2: Accuracy comparison of fine-tuning Mis- tral 7B model with different PEFT methods on GSM8K benchmark. See Section 4.1 for experi- mental details. We take r = 8 for LoRA, DoRA and Spectral AdapterA to maintain approximately same num- ber of trainable parameters for all methods. Ta- ble 2 presents the accuracy comparison where SpectralA stands for Spectral Adapter A. From the result, we observe that our Spectral AdapterA scores higher than both LoRA and DoRA by a large margin and increases the pretrained model baseline significantly, which verifies the effective- ness of the proposed spectral adaptation mecha- nism. See Appendix F.4 for more about experi- mental details. Note for a different learning rate, DoRA performs better than LoRA while still worse than our method, see also Appendix F.4 for details. 54.2 Diffusion Model Fusion: Improving Multi-Object Fine-Tuning with Spectral Adapter A Figure 4: Distributing different concept tunings along different spectral space helps with identity preservation in multi-adapter fusion, see Section 4.2 for details. Multi-adapter fusion is a current bottleneck in diffusion model fine-tuning tasks with LoRA adapters. Simply adding different LoRA adapters tuned for distinct objects will result in problems involving identity loss and concept binding [12]. To tackle this toughness, different methods emerge such as Gradient Fusion [12] and Orthogonal Adaptation [42]. Specifically, Orthogonal Adaptation method proposes to fix LoRA parameter B to have orthogonal basis and train A solely. Experiments there show that merging LoRA weights with such orthogonal basis helps preserving individual object characteristics compared to its non-orthogonal counterpart. In Orthogonal Adaptation [ 42], the authors maintain B by manually keeping large orthogonal matrices for different layer sizes and sample r columns from corresponding orthogonal matrix to form B for each LoRA adapter. With knowledge from random matrix theory, such sampled matrices are likely to have orthogonal basis. Notably, our Spectral AdapterA naturally operates on orthogonal singular vectors and thus introduces an elegant solution to multi-adapter fusion problems by distributing different concept tunings along different columns of singular vector matrices, which maps to wireless communications where the signals are distributed over non-overlapping frequencies. A subtlety here lies in the choice of column space for different fine-tuning tasks: (1) Sample-based methods can be adopted if data privacy is considered and different tuning tasks are done independently. In Appendix F.5, we show that tuning top columns manifests better generation quality compared to both tuning bottom columns and sampling random orthogonal basis as what has been done in Orthogonal Adaptation [42]. Thus there is a trade-off between high-quality generation and concept collapsing, i.e., sampling from top singular vectors is more encouraged while column overlapping between concepts happens more often compared to sampling from the whole set. (2) On the other hand, if fine-tuning tasks are not isolated and can collaborate on the column scheduling, then more deliberate tuning scheduling can be adopted, for example in a two-concept tuning task with r =4, the first concept can allocate first to fourth columns and the second concept then claims fifth to eighth columns. Figure 4 demonstrates steps for the same method for three-concept tuning task. Since we expect fine-tuned weights to stay close to original weights, though both row space and column space are tuned in spectral adapter, this adaptation mechanism approximates orthogonal-basis tuning for different objects and thus we expect it helps improving identity preservation for multi-adapter fusion. In this section, we investigate this effect via extensive diffusion model experiments. Our experiments follow [42] and build on [12] which studies multi-LoRA fusion. We experiment with multi-object tuning and face generation tasks. Due to space limitation, we present some multi-object tuning results below and we leave the rest to Appendix F.5. For all tasks, we compare against baselines including Gradient Fusion [12], Orthogonal Adaptation [42], and FedAvg [37]. We start with a simple review for these baseline methods. Baseline Review To merge different LoRA adapters, say we have a set of LoRA parameters{∆θ1, . . . ,∆θn} where ∆θi = AiBT i and pretrained parameter θ0, FedAvg [ 37] proposes to merge them in to a single parameter by taking a weighted average as θmerged =θ0 +∑i λi∆θi, where λi is the weight attached to parameter ∆θi and is usually taken to satisfy ∑i λi = 1, i.e., θmerged is a convex combination of individual adapters. Gradient Fusion [12] instead considers solving an auxiliary optimization problem of form θmerged =argminθ ∑n i=1 ∥(θ0 +∆θi)Xi −θXi∥2 F where Xi represents the input activation of the i-th concept. Orthogonal Adaptation [42] follows FedAvg method and replaces original LoRA 6Figure 5: Generation results of Chilloutmix diffusion model [8] with different fused adapters tuned on three custom animal concepts. See Section 4.2 for details. parameters with orthogonal-based LoRA adapters. For our method, to merge different spectral adapters, let θ0 = U0S0V T 0 denote the spectral representation of pretrained model weight. Given a set of spectral adapters {(Ui, Vi), . . . ,(Un, Vn)} with zero-padding to make the shape the same as (U0, V0), we follow FedAvg and compute θmerged = (U0 +∑i λiUi)S0(V0 +∑i λiVi)T . In the following experiments, we take λi =1/n as in [42] for all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion. Notably, all FedAvg, Orthogonal Adaptation, and our Spectral AdapterA fusion can be done approximately instantly while Gradient Fusion usually takes around 10 ∼ 15 minutes for solving its auxiliary optimization problems for all concept adapters. Multi-Object Generation We follow default training setting in [ 12] and fine-tune the Chilloutmix diffusion model [ 8] on three custom animal concepts, see original animals in \"reference\" in Figure 5. For better spatial alignment, we adopt T2I-Adapter [39] with sketch condition and we set guidance equal to one, see also \"reference\" in Figure 5 for the sketch condition being used. LoRA rank r =8 is adopted. For baseline comparisons, we use original code for Gradient Fusion [ 12] and Orthogonal Adaptation [42]. We adapt code of Gradient Fusion for FedAvg method since there is no official implementation available. Custom animal name is replaced with special token < Vanimal> for fine-tuning. For our Spectral AdapterA, we follow the method depicted in Figure 4 and tune first, second, and third top eighth columns of singular vector matrices for different animal concepts. Figure 5 shows the generation results with different methods for selected prompts. Notably, baseline methods sometimes fail to capture the custom animal concepts while Spectral AdapterA recognizes all custom animals and generates visually satisfactory images. For better measurement, we also compute the alignment scores for each generated image with both reference images and prompt texts. It can be witnessed that our method achieves better alignment scores compared to baselines. See Appendix F.7 for details on alignment score computation. 4.3 Diffusion Model Expressiveness: Improving Parameter Efficiency with Spectral AdapterR Spectral AdapterR is closely connected to prior Orthogonal Fine-Tuning (OFT ) [45] method which proposes to multiply the pretrained model weights by trainable orthogonal matrices in the fine- tuning procedure. Motivation behind OFT is to preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. Unlike OFT which orthogonally rotates neurons, Spectral Adapter R multiplies the top- r columns of singular vector space U and V by orthogonal trainable matrices. For our implementation, several options are available for maintaining a trainable orthogonal matrix such as adding an orthogonality penalty in the objective function considered in [65] or via Cayley parameterization considered in [ 45]. We follow [ 45] and adopt Cayley parameterization which is supported by Pytorch [44]. Specifically, the orthogonal matrix R is 7constructed via R =(I +Q)(I −Q)−1 with a skew-symmetric matrix Q maintained as (A −AT )/2 where A is our trainable parameter. Compared to adding an auxiliary orthogonality penalty, this parametrization is exact and thus the SVD form is preserved after tuning with Spectral AdapterR and can be adopted directly for subsequent fine-tuning tasks, which we state formally as a lemma below: Lemma 4.1. With the Cayley parametrization, Spectral AdapterR is an exact rotation operation and thus preserves the structure of the SVD of the fine-tuned weight. Subsequent fine-tunings can be applied consequently without recomputing the SVD each time. See Appendix C for the proof of above lemma. Unlike LoRA which requires number of trainable parameters to scale with weight size, when tuning top-r columns of U an V , Spectral AdapterR only requires two trainable matrices of size r ×r and thus can be more parameter-efficient especially for large pretrained weight. For common weight size such as W ∈ R1024×1024, LoRA with only r = 1 introduces same number of trainable parameters as Spectral AdapterR with r =32. For a thorough analysis on parameter efficiency improvement brought by Spectral AdapterR, we here also compare with different variants of LoRA which are proposed for trainable parameter savings. We review all baselines in detail below. Baseline Review We compare our Spectral Adapter R with LoRA [ 20], SVDiff [ 15], LiDB [ 48], OFT [ 45], and VeRA [25]. Though the other methods are proposed for vision model tuning, VeRA is originally proposed for LLM tuning and we extend it here to diffusion model tuning due to its parameter efficiency. Consider a pretrained weight W ∈Rn×n, SVDiff originally proposes to tune all singular values of flattened CNN weights, here we extend it to tune all singular values of text encoder and U-Net weights for our comparison, thus trainable parameter attached to W will be of size n and is nonadjustable. LiDB stands for Lightweight Dreambooth and proposes to cut down trainable parameter budget by introducing auxiliary frozen matrixAaux ∈Rn×a and Baux ∈Rb×n, then it mimics LoRA but uses AauxABT Baux in replace of ABT with trainable (A ∈ Ra×r, B∈ Rb×r). Thus with a, b< n, LiDB requires (a +b)r < 2nr trainable parameters. In below, we use a = 50, b= 100 as default in [48]. OFT multiplies the weight matrix by a trainable orthogonal matrix via Cayley parametrization discussed above, thus its complete version requires n2 trainable parameters. For parameter efficiency, OFT proposes to use block-diagonal trainable matrix with all diagonal blocks being orthogonal. Thus with r diagonal blocks, the number of trainable parameter will be r ×(n/r)2. Method Granularity #Param Auxiliary Param LoRA / ∞ 2nr∝n noSVDiff / 1 n∝n noLiDB / ∞ (a+b)r∝r yes OFT / #factors ofn1 (n/r)2 ∝nr no VeRA / ∞ n+r∝n yes Spectral AdapterR , n 2r2 ∝r no 1 Ceiling operation is ignored for this count. Table 3: Baseline methods comparison for parameter effi- ciency. Granularity indicates number of trainable parameter budgets available. See Section 4.3 for details. Further reduction of trainable parame- ter is achieved via sharing the diagonal blocks, which demands only (n/r)2 parameters. In below comparison, we use this shared block-diagonal version for best parameter efficiency of OFT. VeRA proposes to use ΛaAΛbBT in replace of ABT where Λa and Λb are diagonal matrices of size n ×n and r ×r respectively. Thus the total num- ber of trainable parameters by VeRA is (n +r) ∝n. Table 3 compares dif- ferent properties across all methods, where n represents weight size and r represents rank for all methods except for OFT, where r denotes number of diagonal blocks. Parameter Efficiency We fine-tune the Chilloumix diffusion model [8] with various PEFT methods on custom vase concept and present the generation results for prompt \"a <Vvase> on a table\" in Figure 6 for various trainable parameter budgets, where grey dash denotes that the corresponding parameter budget is unobtainable with a given adapter no matter how the hyperparameter is chosen and empty entry without grey dash 8Figure 6: Generation results for prompt “a <Vvase> on a table” after fine-tuning Chilloutmix diffusion model [8] on custom vase images with different PEFT methods. See Section 4.3 for details. represents that there is a way to achieve the corresponding parameter budget though the generation result is skipped for better visualization. We follow default LoRA implementation in [12] for LoRA baseline and adjust it for all other methods. From Figure 6, it can be observed that LoRA, OFT, and LiDB start to generate vase close to custom vase with at least 200k trainable parameters. SVDiff and VeRA are unable to generate ideal vase images even if scaled to large parameter budget. On the contrary, Spectral AdapterR starts to recognize the custom vase concept with only 20k trainable parameters and has finer-grained parameter choices compared to other methods, i.e., notably Spectral AdapterR can have as few as1k parameters while other methods start with at least tens of thousands of trainable parameters. In a word, Spectral AdapterR enjoys finer-grained parameter budget choices and manifests better visual quality with fewer parameters, thus achieves enhanced parameter efficiency compared to various other PEFT methods. Figure 7: Generation results for prompt “a yellow <Vchair>” after fine-tuning Chilloutmix diffusion model [8] on custom chair images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Section 4.3 for details. Figure 7 above presents generation results of Chilloutmix diffusion model [8] tuned on custom chair concept with different methods under various parameter budgets. The prompt used is \"a yellow <Vchair>\". See \"reference\" in Figure 7 for original chair images. From the generation results, it can be observed that LoRA generates reasonable chairs for all rank r =1, 2, 3 though it already induces 273k parameters even if rank is set to 1. OFT and VeRA start to recognize custom chair with >100k parameters. SVDiff has a single fixed trainable parameter budget of size around 100k. LiDB forms a competitive candidate and generates satisfactory images with smallest trainable parameter budget among all baseline methods. However, our Spectral AdapterR still generates images better aligned to 9reference images with as few as 20k trainable parameters and has finer-grained parameter budget choices compared to LiDB. See Appendix F.6 for hyperparameter setting and Appendix F.7 for alignment score computation details. 4.4 Final Note: A Closer Look at SVD Cost Figure 8: Runtime and GPU storage cost plot. See Section 4.4 for details. To alleviate the concerns with respect to online training cost and show that our pro- posed method is very practical, we provide runtime and GPU storage cost bar plot in Figure 8, which shows runtime and GPU storage cost for LoRA and for our Spec- tral AdapterA when used for fine-tuning diffusion model in Section 4.2 and Mistral 7B model in Section 4.1. Here we adopt rank r = 8 for both LoRA and Spectral AdapterA. It can be observed that our Spec- tral Adapter A introduces negligible run- time and storage overhead for current large model size. Modern numerical tools such as randomized SVD [13] can also be exploited for further runtime reduction and the SVD procedure can be paral- lelized when multiple machines are available. See Appendix E for further investigation. 5 Conclusion and Limitations In this work, we investigate the incorporation of spectral information of pretrained model weights into current PEFT models by introducing a spectral adaptation mechanism which updates only the top singular vectors of pretrained weights. We investigate the additive and rotational variants of such spectral adaptation mechanism. Theoretically, we show the motivation of tuning top singular vectors by comparing the rank capacity of different fine-tuning models and carrying out weight decomposition of pretrained model layers. Empirically, we verify the superiority of our proposed spectral adaptation method compared to various recent PEFT methods from different aspects via extensive experiments. To our best knowledge, this is the first work considering incorporating spectral information as a practical generic paradigm for fine-tuning tasks and enhances fine-tuning results, parameter efficiency, as well as benefits multi-adapter fusion of existing PEFT methods. For future work, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Other PEFT methods such as AdaLoRA [65] can also be dynamically combined with spectral adaptation. A limitation of the current work remains in the choice of tuning top spectral space. Though its validity has been theoretically verified under simple settings, further investigation on tuning different columns of singular vector matrices is critical to understanding the role of spectral information in fine-tuning procedure. Besides, fine-tuning spectral representation of different components, i.e., only the attention layer, of current large models is also worth studying. Moreover, the time consumption of singular value decomposition procedure increases as model grows larger and thus faster singular value decomposition method also benefits. 106 Acknowledgement This work was supported in part by the National Science Foundation (NSF) under Grant DMS- 2134248; in part by the NSF CAREER Award under Grant CCF-2236829; in part by the U.S. Army Research Office Early Career Award under Grant W911NF-21-1-0242; and in part by the Office of Naval Research under Grant N00014-24-1-2164. References [1] A. Aghajanyan, L. Zettlemoyer, and S. Gupta. Intrinsic dimensionality explains the effectiveness of language model fine-tuning, 2020. [2] A. Asai, M. Salehi, M. E. Peters, and H. Hajishirzi. Attempt: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts, 2022. [3] N. Cancedda. Spectral filters, dark signals, and attention sinks, 2024. [4] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen. One-for-all: Generalized lora for parameter- efficient fine-tuning, 2023. [5] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-Tur. Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention. arXiv preprint arXiv:2205.03720, 2022. [6] A. Chronopoulou, M. E. Peters, A. Fraser, and J. Dodge. Adaptersoup: Weight averaging to improve generalization of pretrained language models, 2023. [7] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. [8] C. M. Creator. Chilloutmix diffusion model. https://civitai.com/models/6424/chilloutmix. [9] M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learning, 2014. [10] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023. [11] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter, 2022. [12] Y . Gu, X. Wang, J. Z. Wu, Y . Shi, C. Yunpeng, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, Y . Ge, S. Ying, and M. Z. Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. [13] N. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010. [14] K. Hambardzumyan, H. Khachatrian, and J. May. Warp: Word-level adversarial reprogramming, 2021. [15] L. Han, Y . Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning, 2023. [16] S. Hayou, N. Ghosh, and B. Yu. Lora+: Efficient low rank adaptation of large models, 2024. [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig. Towards a unified view of parameter-efficient transfer learning, 2022. [18] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao. Mera: Merging pretrained adapters for few-shot learning. arXiv preprint arXiv:2308.15982, 2023. [19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. At- tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp, 2019. 11[20] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. [21] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin. Lorahub: Efficient cross-task generalization via dynamic lora composition, 2024. [22] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014. [23] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. [24] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers, 2022. [25] D. J. Kopiczko, T. Blankevoort, and Y . M. Asano. Vera: Vector-based random matrix adaptation, 2024. [26] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Zhao, Y . Wu, B. Li, et al. Conditional adapters: Parameter-efficient transfer learning with fast inference. Advances in Neural Information Processing Systems, 36, 2024. [27] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning, 2021. [28] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation, 2021. [29] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and T. Zhao. Loftq: Lora-fine- tuning-aware quantization for large language models, 2023. [30] Z. Lin, A. Madotto, and P. Fung. Exploring versatile generative language model via parameter- efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020. [31] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications, 2023. [32] S.-Y . Liu, C.-Y . Wang, H. Yin, P. Molchanov, Y .-C. F. Wang, K.-T. Cheng, and M.-H. Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [33] W. Liu, Z. Qiu, Y . Feng, Y . Xiu, Y . Xue, L. Yu, H. Feng, Z. Liu, J. Heo, S. Peng, Y . Wen, M. J. Black, A. Weller, and B. Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization, 2023. [34] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too, 2023. [35] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks, 2021. [36] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning, 2018. [37] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication- efficient learning of deep networks from decentralized data, 2023. [38] A. Mitra, H. Khanpour, C. Rosset, and A. Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. [39] C. Mou, X. Wang, L. Xie, Y . Wu, J. Zhang, Z. Qi, Y . Shan, and X. Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023. [40] mrm8488. Lora finetune deberta-v3 huggingface blog, 2021. Available at https://huggingface.co/mrm8488/deberta-v3-small-finetuned-mnli/commits/main. [41] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 12[42] R. Po, G. Yang, K. Aberman, and G. Wetzstein. Orthogonal adaptation for modular customiza- tion of diffusion models, 2023. [43] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. A. Yarmohammadi, and S. Khudanpur. Semi- orthogonal low-rank matrix factorization for deep neural networks. In Interspeech, 2018. [44] pytorch group. Pytorch orthogonal parameterization method implementation, 2023. [45] Z. Qiu, W. Liu, H. Feng, Y . Xue, Y . Feng, Z. Liu, D. Zhang, A. Weller, and B. Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning, 2023. [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. [47] E. Robb, W.-S. Chu, A. Kumar, and J.-B. Huang. Few-shot adaptation of generative adversarial networks, 2020. [48] N. Ruiz, Y . Li, V . Jampani, W. Wei, T. Hou, Y . Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [49] A. Rücklé, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers, and I. Gurevych. Adapter- drop: On the efficiency of adapters in transformers, 2021. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] H. Skogström. Lora finetune mistral 7b valohai blog, 2024. https://valohai.com/blog/finetune- mistral/. [52] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao. Parameter efficient multi-task model fusion with partial linearization, 2023. [53] K. Turgutlu. Answer.ai qdora report, 2024. https://www.answer.ai/posts/2024-04-26-fsdp-qdora- llama3.html. [54] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation, 2023. [55] T. Vu, B. Lester, N. Constant, R. Al-Rfou, and D. Cer. Spot: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021. [56] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim. Multitask prompt tuning enables parameter-efficient transfer learning, 2023. [57] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks, 2024. [58] L. Xu, H. Xie, S.-Z. J. Qin, X. Tao, and F. L. Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment, 2023. [59] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang, and Q. Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models, 2023. [60] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison. Bayesian low-rank adaptation for large language models, 2024. [61] F. Zhang and M. Pilanci. Riemannian preconditioned lora for fine-tuning foundation models, 2024. [62] F. F. Zhang, L. Li, J.-C. Chen, Z. Jiang, B. Wang, and Y . Qian. Increlora: Incremental parameter allocation method for parameter-efficient fine-tuning. ArXiv, abs/2308.12043, 2023. 13[63] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning, 2023. [64] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and B. Zhuang. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning, 2023. [65] Q. Zhang, M. Chen, A. Bukharin, N. Karampatziakis, P. He, Y . Cheng, W. Chen, and T. Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, 2023. [66] Y . Zhang, E. Chuangsuwanich, and J. Glass. Extracting deep neural network bottleneck features using low-rank matrix factorization. In2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 185–189. IEEE, 2014. [67] H. Zhao, H. Tan, and H. Mei. Tiny-attention adapter: Contexts are more important than the number of parameters, 2022. [68] Y . Zhao, J. Li, and Y . Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5005–5009. IEEE, 2016. [69] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li. Counter-interference adapter for multilingual machine translation, 2021. [70] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. 14Appendix A Prior Work Here we provide an overview of recent PEFT methods. Dating back to 2019, Houlsby et al. [ 19] develop the idea of parameter-efficient fine-tuning and introduce Adapter model, which injects trainable components between pretrained model layers, though the number of trainable parameters has been reduced due to the small size of adapters, this method incurs inference latency and is thus not desirable. Later improvement of Adapter fine-tuning focuses on improving inference latency [49, 26], fusing multiple adapters [6, 41, 18], modifying adapter model architecture [67], introducing parallelism [17, 69], and creating task-specific and layer-specific adapter [ 35, 30]. Another line of fine-tuning is prompt-tuning [27] which usually adds the trainable components into the prompt. Variants of prompt-tuning involve WARP [14], prefix-tuning [28], P-tuning [34], and ATTEMPT [2] which consider injecting different forms of trainable components. Multitask prompt-tuning is considered in [55, 56]. The more relevant PEFT methods to our spectral adaptation mechanism involves LoRA [20] and OFT [45], which inspires our Spectral AdapterA and Spectral AdapterR respectively. LoRA originates from the observation that model fine-tuning is intrinsically low-rank [1]. Variants of LoRA involve different methods proposing dynamic allocation of LoRA rank budgets [54, 62, 65, 5]. LoRA has been combined with model pruning [64] and quantization [10, 59, 29]. Some other variants further cut down the trainable parameter budget or activation storage by modifying LoRA model [25, 11, 63]. DoRA [32] fixes LoRA’s low-rank limitation by decomposing pretrained model weights and isolating their magnitudes. Laplace-LoRA [ 60] incorporates Bayesian inference into LoRA parameters to improve calibration. LoRAHub [21], MOELoRA [31], and L-LoRA [52] consider multitask LoRA. Delta-LoRA [70] updates pretrained weights simultaneously from information of LoRA parameters. GLoRA [4] generalizes LoRA by introducing a prompt module. Another line of variants focuses on analyzing the optimization scheme of LoRA model [ 61, 16]. OFT studies the multiplicative fine-tuning and its variant BOFT [33] improves OFT by utilizing butterfly parametrization for better information delivery efficiency. [58] offers a comprehensive review of recent development of PEFT methods. B Rank Capacity Proof Proof. Consider weight matrix W ∈ Rn×m with n ≤ m of full row rank. For LoRA parameter A ∈Rm×r, B∈Rn×r with n ≥r, final weight matrix W +ABT has rank in [n −r, n]. With Spectral AdapterA parameters AS ∈ Rm×r, BS ∈ Rn×r where n ≥ 2r. Let Xr denote the first r columns of any matrix X and X−r denote the rest columns, final weight matrix ((Ur +AS)Sr(Vr +BS)T )+ U−rS−rV T −r has rank in [n−2r, n]. Therefore, R(LoRA; W) =r and R(Spectral AdapterA; W) = 2r can be derived trivially. C Cayley Parameterization Proof Proof. With any trainable square matrix A, we set Q = (A −AT )/2 and thus Q = −QT and Q is skew-symmetric thereby. Now we show that for any skew-symmetric Q, (I +Q)(I −Q)−1 is orthogonal. Let O =(I +Q)(I −Q)−1, then OT O =((I +Q)(I −Q)−1)T (I +Q)(I −Q)−1 =(I −QT )−1(I +QT )(I +Q)(I −Q)−1 by Q skew-symmetric, =(I +Q)−1(I −Q)(I +Q)(I −Q)−1 since (I −Q) and (I +Q) have same eigen-basis and are commutable, =I, which shows that the Cayley parametrization is exact and no re-SVD is needed for orthogonality preservation. 15D Connection to DoRA In DoRA [32], the authors observe that plain LoRA method tends to either increase or decrease the magnitude and direction updates proportionally and thus lacks ability to make slight direction change together with large magnitude change, to come across this limitation, the authors propose to decompose pretrained model weights into magnitude and direction and update them separately. The magnitude is replaced with a trainable scalar and the direction is updated with original LoRA method. Experiments in [32] show that such decomposition helps improve effectiveness of LoRA significantly. Here we show that our Spectral AdapterA is closely connected to the weight decomposition trick used in DoRA when pretrained model weight is of vector form. We note that in DoRA, after the weight decomposition, each column becomes unit-length while in Spectral AdapterA, we also operates on matrices with unit-length columns. Specifically, consider a pretrained model weight w0 ∈Rn×1, then DoRA becomes w =w w0 +ba ∥w0 +ba∥2 , where w is a trainable scalar initialized at ∥w0∥2. band a are trainable parameters of size n ×1 and 1 ×1 respectively, with ba =0 at initialization. Comparably, Spectral AdapterA becomes w =( w0 ∥w0∥2 +a′)∥w0∥2(1 +b′), with trainable vectora′ ∈Rn×1 and trainable scalarb′ both initialized at zero. We can thus equivalently view ∥w0∥2(1 +b′) as a single trainable scalar initialized at ∥w0∥2, which then plays the role of magnitude adapter as w in DoRA. a′ is adopted for directional adaptation since it directly operates on the normalized base vector. E Cost Investigation (More Detailed) Here we address the potential concern about the overhead of our proposed spectral adaptation mechanism. Firstly, we note that spectral adapter introduces similar number of trainable parameters and can be merged into original model weights, thus it is lightweight for sharing and introduces no additional inference latency, which preserves the strengths of additive fine-tuning methods. Therefore, the major overhead concern exists in the runtime and GPU storage overhead during online training. Note our method involves only matrix multiplication in the forward procedure and thus should run as quick as LoRA. Though the SVD procedure can bring additional runtime overhead, it needs to be done only once for a single model and can be reused for later fine-tuning on various downstream tasks. Besides, modern numerical tools such as randomized SVD [ 13] can also be exploited and the SVD procedure can be parallelized when multiple machines are available. As for GPU storage, unlike SVDiff [15] where all SVD components are required for training procedure thus introducing significant GPU storage burden, our method requires only the top spectral space to be stored additionally and consumes similar GPU storage to LoRA for relatively small tuning ranks (which is usually the case). F Supplemental Materials for Experiments F.1 Experimental Setup for Figure 1 For Figure 1 experiments, we follow QDoRA [53] experimental setup for fine-tuning Llama3 8B model, where all k_proj, q_proj, v_proj, up_proj, down_proj, and gate_proj weights are tuned. We adopt the same data processing method and train on 10K Orca Math data (shuffled) as in [53]. We fix learning rate as 1e −5 for all methods as in QDoRA and train for one epoch with batch size 8. r =8 is adopted for LoRA, DoRA, AdaLoRA, and Spectral AdapterA while for OFT, we set number of diagonal blocks to be 800 to maintain similar amount of trainable parameters. LoRA alpha is set to be 16 following DoRA [32] convention and AdaLoRA hyperparameter is set following what has been used for MNLI benchmark in the original AdaLoRA report [65] with regularization set to 1e −3 which we find works better. For evaluation, we test on GSM8K [7] benchmark for exact matching. For more comparisons, Figure 9 provides training loss for smaller rank r = 4 (oft_r = 1600) and larger rank r =64 (oft_r =95). All settings are the same except that LoRA alpha is always kept as 16Figure 9: More experiments with Llama3 8B model with different number of trainable parameters. In the left plot, the training loss of LoRA and DoRA overlaps. See Appendix F.1 for details. twice as rank number. From Figure 9 we can observe that though increasing trainable parameters closes the gap between different tuning methods, our spectral adapter method is always superior to other PEFT methods and stays closest to full fine-tuning. F.2 Hyperparameter Setting for DeBERTaV3-base Experiment (Section 4.1) Dataset learning rate batch size #epochs optimizer weight decay MNLI 1e −4 32 1 AdamW 0.01 RTE 3e −4 32 10 AdamW 0.01 QNLI 1e −4 32 1 AdamW 0.01 MRPC 7e −4 32 13 AdamW 0.01 QQP 1e −4 32 10 AdamW 0.01 SST-2 1e −4 32 5 AdamW 0.01 CoLA 3e −4 32 8 AdamW 0.01 STS-B 5e −4 32 30 AdamW 0.01 Table 4: Hyperparameters for DeBERTaV3-base model fine-tuning with Spectral AdapterA in Section 4.1 Table 4 shows the hyperparameter setting for our Spectral AdapterA used for fine-tuning DeBERTaV3- base model in Section 4.1. We set number of diagonal blocks to be 4 and enable block sharing for OFT to maintain similar amount of trainable parameters. F.3 More About DeBERTaV3-base Experiment Left plot in Figure 10 presents the training loss and validation score comparisons of LoRA, SVDiff and our Spectral AdapterA for fine-tuning DeBERTaV3-base model on CoLA benchmark. We set learning rates for both LoRA and Spectral AdapterA as what has been used in popular public blog [40] for LoRA fine-tuning with DeBERTaV3-base model, which is not tuned in favor of our method. For SVDiff, since it is originally proposed for vision model tuning, we extend it to this experiment by tuning all singular values of pretrained weights. We find the same learning rate leads to poor fine-tuning results with SVDiff, we thus pick the best learning rate among [1e −3, 1e −4, 1e −5] according to validation performance and set learning rate to be 1e −3. We use r = 8 for LoRA and Spectral AdapterA. From Figure 10, it can be observed that Spectral AdapterA achieves better training and validation performance compared to both LoRA and SVDiff. Interestingly, in LoRA [20], the authors provide a correlation analysis between the LoRA additive component △W = ABT and original pretrained weight matrix W (see Section H.3 in [ 20]), and they find that the additive component does not contain the top singular directions of W. The authors therefore conclude that the learned LoRA component amplifies \"task-specific\" directions which are not emphasized in the pretrained weight matrix. Naively, this seems to suggest that tuning top singular subspace of pretrained weights is not ideal and one should identify the desired \"task-specific\" directions to improve LoRA. Here we show that this is not the case and fine-tuning top directions provides a significant improvement to LoRA. In the right plot of Figure 10 above, we experiment 17Figure 10: Left plot presents training loss and validation results for fine-tuning DeBERTaV3-base model with LoRA, SVDiff, and Spectral AdapterA on CoLA benchmark. Right plot compares the same statistics between LoRA and spectral adapter with top ranks and bottom ranks tuned respectively. tuning the top eighth rank and the bottom eighth rank of singular vector space in our Spectral AdapterA, which we present as \"Spectral Top\" and \"Spectral Bottom\" respectively. Remarkably, \"Spectral Top\" converges faster and scores higher than LoRA, which is then superior to \"Spectral Bottom\". This result unravels the fact that tuning different part of spectral space brings different tuning effect and tuning the top columns of singular vector space improves LoRA tuning significantly. See Section 3 for more theoretic insights. F.4 Hyperparameter Setting for Mistral 7B Experiment (Section 4.1) Method lr lora alpha batch size #epochs lora dropout weight decay LoRA 2.5e −5 16 4 2 0.05 0.01 DoRA 2.5e −5 16 4 2 0.05 0.01 Spectral AdapterA 2.5e −5 - 4 2 - 0.01 Table 5: Hyperparameters for Mistral 7B model fine-tuning task in Section 4.1 Table 5 shows training hyperparameter setting for fine-tuning Mistral 7B model in Section 4.1. We train with bfloat16 precision and fine-tune all q_proj, k_proj, v_proj, o_proj, and gate_proj weights. We evaluate with lm-evaluation-harness [47]. Table 6 shows accuracy comparison of different tuning methods with learning rate 1e −5. Our Spectral AdapterA still exceeds both LoRA and DoRA. F.5 Supplemental Materials for Multi-Adapter Fusion Experiment (Section 4.2) F.5.1 Comparison of Single Object Generation We present more experimental results to show that Spectral AdapterA with top ranks tuned behaves at least as good as LoRA with same parameter budget and is better than Orthogonal Adaptation [42], which is likely due to that Orthogonal Adaptation fixes LoRA parameter B and thus has limited expressiveness. We also show that tuning bottom ranks in spectral adapter behaves worse than all other methods. Figure 11 shows generation results for custom toy concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) generate inaccurate happy-face octopus, sad-face octopus, and green tortoise. Figure 12 shows generation results for custom animal concept tuning, where Orthogonal Adaptation and Spectral AdapterA (bottom) sometimes miss first dog concept. Method #Param GSM8K Pre-Trained − 38.82 LoRAr=8 0.16% 43.29 ±1.36 DoRAr=8 0.17% 43.52 ±1.37 SpectralA r=8 0.16% 46.47 ±1.37 Table 6: Supplemental experiments of fine-tuning Mistral 7B model with different PEFT methods with a different learning rate on GSM8K benchmark. See Section F.4 for experimental details. 18Figure 11: Generation results for single toy concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. F.5.2 More Multi-Adapter Fusion Generation Results Here we present more results for multi-adapter fusion generation. Figure 13 shows generation results for multi-object generation for custom toy concepts and Figure 14 presents generation results for multi-character generation for three computer scientists. See below for experimental details. Multi-Object Generation. As in Section 4.2, we fine-tune Chilloutmix diffusion model [8] on four custom toy concepts, see \"reference\" in Figure 13 for original toy images. We use r =8 for all methods and tune first, second, third, and fourth top eighth columns of singular vector space of pretrained weights for first, second, third, and fourth toys in our Spectral AdapterA. We follow all default experimental settings in [ 12] and tune all embedding layer, U-Net, and text-encoder. For better spatial alignment, we employ T2I-Adapter with sketch condition listed in \"reference\" in Figure 13. We randomly select three scenes and prompt fused-adapters for the results, see \"prompts\" in Figure 13 for individual prompt being used. From Figure 13, it can be observed that FedAvg and Orthogonal Adaptation generate unsatisfactory happy-face octopus and green tortoise toys. On the contrary, our spectral adapter generates high-quality images similar to Gradient Fusion while saving 19Figure 12: Generation results for single animal concept tuning with LoRA, Orthogonal Adaptation, and Spectral AdapterA with top and bottom ranks tuned respectively. Figure 13: Generation results of Chilloutmix diffusion model [8] tuned on four custom toy concepts with different fused adapters. See Appendix F.5.2 for details. much more time. Multi-Character Generation. We also experiment fine-tuning Chilloutmix diffusion model [ 8] with photos of three computer scientists Yoshua Bengio, Yann LeCun, and Geoffrey Hinton. As in multi-object generation, we use r = 8 for all methods and tune first, second, and third top eighth columns of singular vector space of pretrained weights for Bengio, Lecun, and Hinton in our Spectral AdapterA. We use T2I-Adapter [ 39] with keypose condition. See \"reference\" in Figure 14 for scientists’ photos and keypose condition being used. Figure 14 shows generation results for prompt 20\"<Vbengio> and <Vlecun> and <Vhinton>, standing near a lake, 4K, high quality, high resolution\" with different fused adapters, from which it can be observed that our spectral adapter generates picture of most consistent styles across characters and renders all scientists’ faces clearly. Figure 14: Generation results of Chilloutmix diffusion model [8] tuned on photos of three computer scientists with different fused adapters. See Appendix F.5.2 for details. F.6 Supplemental Materials for Parameter Efficiency Experiment (Section 4.3) In this section, we present more tuning results with various parameter budgets for parameter efficiency experiment studied in Section 4.3, see Section 4.3 for baseline method explanation. Table 7 shows the learning rates used for each baseline method and Table 8 shows learning rates used for our method, the rest experimental settings are default as in [12]. Method text encoder lr unet lr LoRA 1e −5 1e −4 VeRA (r =1) 1e −3 1e −4 VeRA (r =1024, 4096) 5e −3 1e −4 OFTA 1e −5 1e −4 LiDB 5e −4 1e −4 SVDiff 1e −3 1e −4 Table 7: Hyperparameters for baseline methods for diffusion model fine-tuning task in Section 4.3 Method vase chair table text unet text unet text unet Spectral AdapterR (r =2, 40) 1e −3 1e −2 1e −2 1e −2 1e −3 1e −2 Spectral AdapterR (r =4) 5e −3 5e −3 1e −3 1e −2 Spectral AdapterR (r =8) 5e −4 5e −2 1e −3 1e −2 1e −3 1e −2 Spectral AdapterR (r =16) 1e −2 1e −3 1e −3 1e −2 Spectral AdapterR (r =24) 1e −4 1e −2 1e −3 1e −3 1e −4 1e −2 Spectral AdapterR (r =32) 1e −4 5e −2 Table 8: Hyperparameters for Spectral AdapterR for diffusion model fine-tuning task in Section 4.3 Figure 15 shows generation results of Chilloutmix diffusion model [8] fine-tuned on custom table concept with different methods under various parameter budgets. The prompt used is “a <Vtable>”. LoRA generates acceptable images for all rank r =1, 2, 3 though it starts with 273k parameters even if rank is set to 1. OFT generates desirable images only for parameter budget > 400k. VeRA and LiDB start to generate reasonable images with >300k trainable parameters and SVDiff has only a single fixed parameter budget. Meanwhile, our Spectral AdapterR recognizes the shape of custom table with as few as 6k parameters and produces ideal images since 100k parameters. See Appendix F.7 for alignment score computation details. 21Figure 15: Generation results for prompt “a <Vtable>” after fine-tuning Chilloutmix diffusion model [8] on custom table images with different PEFT methods. Spectral R is abbreviation for Spectral AdapterR. See Appendix F.6 for details. F.7 Alignment Score Computation For better quantitative measurement, we compute alignment scores for our Figure 5,6,7,15 results. Specifically, we first compute CLIP [46] embedding for all generated/reference images and prompt texts, then we compute the cosine similarity between generated images’ embedding and reference images’ embedding to serve as their alignment score. Likewise, text score stands for cosine similarity between generated images’ embeddings and their corresponding prompt texts’ embeddings. Intuition here is that if an image is close to another image (or text), their CLIP vectors are expected to stay close as well. For Figure 5 alignment score computation, we crop each generated image vertically into three columns, then we compute their alignment scores to each corresponding reference animal, we finally take the mean of these three scores. For Figure 6, 7, 15 scores, we compute average score over three random trials, with each trial consisting of 8 generated images. 22",
      "meta_data": {
        "arxiv_id": "2405.13952v2",
        "authors": [
          "Fangzhao Zhang",
          "Mert Pilanci"
        ],
        "published_date": "2024-05-22T19:36:55Z",
        "pdf_url": "https://arxiv.org/pdf/2405.13952v2.pdf",
        "github_url": "https://github.com/pilancilab/spectral_adapter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Spectral Adapter, a Parameter-Efficient Fine-Tuning (PEFT) method that incorporates spectral information of pretrained weight matrices. It proposes two mechanisms: additive tuning (Spectral AdapterA) and orthogonal rotation (Spectral AdapterR) of top singular vectors. The key findings include demonstrating that spectral fine-tuning improves the rank capacity of low-rank adapters given a fixed trainable parameter budget, enhancing parameter efficiency, tuning performance, and benefiting multi-adapter fusion. This work is the first to fine-tune spectral space in a parameter-efficient and storage-economic way, offering a practical generic paradigm for fine-tuning tasks.",
        "methodology": "The core methodology involves performing Singular Value Decomposition (SVD) of pretrained weight matrices (W=USV^T) and then fine-tuning the top-r columns of the singular vector matrices (U and V). Spectral AdapterA additively tunes these top columns, similar to LoRA, defined as W_tuned = [U1+AU U2]S[V1+AV V2]. Spectral AdapterR orthogonally rotates the top columns, resembling Orthogonal Fine-Tuning (OFT), defined as W_tuned = [U1RU U2]S[V1RV V2]. The orthogonality constraint for Spectral AdapterR is efficiently maintained using Cayley parameterization. Trainable parameters AU, AV are initialized to zero, while RU, RV are initialized to identity matrices. Theoretical analyses show that Spectral AdapterA has double the rank capacity of LoRA and that tuning top singular vectors is robust due to better subspace alignment.",
        "experimental_setup": "The proposed spectral adapter was evaluated on large language models (LLMs) and diffusion models. For LLMs, DeBERTaV3-base (185M) was fine-tuned on GLUE benchmarks, Mistral 7B on GSM8K, and Llama3 8B on Orca Math dataset. For diffusion models, Chilloutmix diffusion model was fine-tuned for multi-object tuning, face generation, and custom concept generation (vase, chair, table). Baselines included LoRA, DoRA, OFT, AdaLoRA, SVDiff, LiDB, VeRA, Gradient Fusion, Orthogonal Adaptation, and FedAvg. Hyperparameters for baselines followed their original reports or official implementations, and Spectral AdapterA/R hyperparameters were tuned. Fine-tuning involved q, k, v matrices for LLMs and embedding layers, U-Net, and text-encoder for diffusion models. Evaluation metrics included accuracy (GLUE, GSM8K), training loss, validation scores, visual quality of generated images, and alignment scores computed using CLIP embeddings' cosine similarity. Experiments were conducted on NVIDIA RTX A6000 GPUs.",
        "limitations": "One limitation lies in the current choice of tuning only the top spectral space. Although its validity is theoretically verified under simple settings, further investigation into tuning different columns of singular vector matrices is deemed critical for a deeper understanding of spectral information's role in fine-tuning. Another constraint is the increasing time consumption of the Singular Value Decomposition (SVD) procedure as model sizes grow, indicating a need for faster SVD methods. Additionally, the paper suggests that fine-tuning the spectral representation of only specific components (e.g., just the attention layer) of large models warrants further study.",
        "future_research_directions": "Future research directions include a more in-depth investigation into tuning different columns of singular vector matrices beyond just the top ones, to gain a comprehensive understanding of the role of spectral information in fine-tuning. Exploring the fine-tuning of spectral representations of specific components within large models, such as only the attention layer, is also suggested. The paper also recommends dynamically combining spectral adaptation with other existing PEFT methods, such as AdaLoRA. Furthermore, to address the computational overhead, research into faster Singular Value Decomposition (SVD) methods, potentially utilizing techniques like randomized SVD, is encouraged as model sizes continue to increase.",
        "experimental_code": "import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy # Used in merge_spectraloft_into_weight\n\n# Helper function for Cayley parameterization, defined within SpectralLinearLayer_OFT but also needed for merging.\ndef cayley(data: torch.Tensor) -> torch.Tensor:\n    r, _ = data.shape\n    skew = 0.5 * (data - data.T)\n    I = torch.eye(r, device=data.device)\n    Q = torch.mm(I - skew, torch.inverse(I + skew))\n    return Q\n\n# From adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/models/edlora.py\nclass SpectralLinearLayer_OFT(nn.Module):\n    def __init__(self, name, original_module, rank=4, alpha=1, top=True, idx=0, revised_r=-1):\n        super().__init__()\n        self.name = name\n        if original_module.__class__.__name__ == 'Conv2d':\n            self.conv = True\n            in_channels, out_channels = original_module.in_channels, original_module.out_channels\n        else:\n            self.conv = False\n            in_channels, out_channels = original_module.in_features, original_module.out_features\n        W = original_module.weight.data.view(out_channels, in_channels)\n        U, S, V = torch.svd(W)\n        self.U = torch.nn.Parameter(U, requires_grad=False)\n        self.S = torch.nn.Parameter(S, requires_grad=False)\n        self.V = torch.nn.Parameter(V, requires_grad=False)\n        self.spectral_A = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True) # For U rotation\n        self.spectral_B = torch.nn.Parameter(torch.zeros(revised_r,revised_r), requires_grad=True) # For V rotation\n        self.spectral_C = torch.nn.Parameter(torch.ones(revised_r), requires_grad=True) # For S scaling\n        original_module.forward = self.forward\n        self.original_module = original_module\n        self.top = top\n        self.idx = idx\n        assert revised_r>0\n        self.rank = revised_r # The actual rank used for adaptation\n\n    def forward(self, hidden_states):\n        if self.top:\n            pad_U = self.U.clone()\n            # Apply orthogonal rotation to U's top-r columns using Cayley parametrization\n            pad_U[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.U[:,self.idx*self.rank:(self.idx+1)*self.rank]@cayley(self.spectral_A)\n            pad_S = self.S.clone()\n            # Apply scaling to S's top-r values\n            pad_S[self.idx*self.rank:(self.idx+1)*self.rank] = self.S[self.idx*self.rank:(self.idx+1)*self.rank]*self.spectral_C\n            pad_V = self.V.clone()\n            # Apply orthogonal rotation to V's top-r columns using Cayley parametrization\n            pad_V[:,self.idx*self.rank:(self.idx+1)*self.rank] = self.V[:,self.idx*self.rank:(self.idx+1)*self.rank]@cayley(self.spectral_B)\n        else:\n            raise Exception('Only top-r tuning is implemented.')\n        pad_W = pad_U@pad_S.diag()@pad_V.T\n        if self.conv : # Not implemented for Conv2d\n            raise Exception('Conv2d adaptation not implemented in this snippet.')\n        else:\n            return F.linear(hidden_states, pad_W, bias=self.original_module.bias)\n\n# From adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/pipelines/trainer_edlora.py\n# Excerpt from EDLoRATrainer.set_finetune_cfg illustrating instantiation of SpectralLinearLayer_OFT\n# (This code would be executed during model setup):\n# For text_encoder linear layers:\n# lora_module = SpectralLinearLayer_OFT(name + '.' + child_name, child_module, **text_encoder_cfg['lora_cfg'], idx=0, revised_r=text_encoder_cfg['lora_cfg']['rank'])\n# self.text_encoder_lora.append(lora_module)\n# params_list.extend(list(lora_module.parameters()))\n\n# For unet linear/conv1x1 layers:\n# lora_module = SpectralLinearLayer_OFT(name + '.' + child_name, child_module, **unet_cfg['lora_cfg'], idx=0, revised_r=unet_cfg['lora_cfg']['rank'])\n# self.unet_lora.append(lora_module)\n# params_list.extend(list(lora_module.parameters()))\n\n\n# From adapter_efficiency/mix_spectral/Mix-of-Show/mixofshow/utils/convert_edlora_to_diffusers.py\ndef merge_spectraloft_into_weight(original_state_dict, lora_state_dict, model_type, alpha, top=True, idx=0):\n    # Assumes 'cayley' function is available in scope.\n\n    # ... (name parsing logic to find spectral_A_name, spectral_B_name, spectral_C_name, U_name, S_name, V_name)\n    # The full implementation would dynamically resolve these names based on the original_layer_name (k)\n    # For brevity, a simplified example of how the merging occurs:\n\n    new_state_dict = copy.deepcopy(original_state_dict)\n    load_cnt = 0\n    for k in new_state_dict.keys(): # Iterate over keys of the original model's state_dict\n        # Assume spectral_A_name, spectral_B_name, etc. are resolved for the current k\n        # For example, if k is 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight'\n        # then spectral_A_name would be 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.spectral_A'\n\n        # This simplified check demonstrates the logic; actual code uses a more robust naming convention.\n        if 'spectral_B' in lora_state_dict.keys(): # check if any spectral_B parameter exists in the lora_state_dict for current layer type\n            # Example placeholder for dynamically resolved names from a lora_state_dict matching 'k'\n            # In a real scenario, this would involve parsing `k` to find the corresponding lora_state_dict keys\n            spectral_A_params_key = k.replace('.weight', '.spectral_A') # This is a placeholder, actual logic is more complex\n            spectral_B_params_key = k.replace('.weight', '.spectral_B') # This is a placeholder\n            spectral_C_params_key = k.replace('.weight', '.spectral_C') # This is a placeholder\n            U_params_key = k.replace('.weight', '.U') # This is a placeholder\n            S_params_key = k.replace('.weight', '.S') # This is a placeholder\n            V_params_key = k.replace('.weight', '.V') # This is a placeholder\n            \n            if (spectral_A_params_key in lora_state_dict and spectral_B_params_key in lora_state_dict and \n                spectral_C_params_key in lora_state_dict and U_params_key in lora_state_dict and \n                S_params_key in lora_state_dict and V_params_key in lora_state_dict):\n\n                load_cnt += 1\n                original_params = new_state_dict[k]\n                spectral_A_params = lora_state_dict[spectral_A_params_key].to(original_params.device)\n                spectral_B_params = lora_state_dict[spectral_B_params_key].to(original_params.device)\n                spectral_C_params = lora_state_dict[spectral_C_params_key].to(original_params.device)\n                U_params = lora_state_dict[U_params_key].to(original_params.device)\n                S_params = lora_state_dict[S_params_key].to(original_params.device)\n                V_params = lora_state_dict[V_params_key].to(original_params.device)\n                r = spectral_A_params.shape[0] # Rank of the adapter\n\n                if top:\n                    pad_U = U_params.clone() \n                    # Apply scaled Cayley rotation to U's top-r columns\n                    pad_U[:,idx*r:(idx+1)*r] = U_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_A_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))\n                    pad_V = V_params.clone()\n                    # Apply scaled Cayley rotation to V's top-r columns\n                    pad_V[:,idx*r:(idx+1)*r] = V_params[:,idx*r:(idx+1)*r]@(alpha*(cayley(spectral_B_params)-torch.eye(r).to(spectral_A_params.device))+torch.eye(r).to(spectral_A_params.device))\n                    pad_S = S_params.clone()\n                    # Apply scaled additive adjustment to S's top-r values\n                    pad_S[idx*r:(idx+1)*r] = S_params[idx*r:(idx+1)*r]*(alpha*(spectral_C_params-torch.ones(r).to(spectral_A_params.device))+torch.ones(r).to(spectral_A_params.device))\n                else:\n                    raise Exception('Only top-r tuning is implemented.')\n                \n                # Reconstruct the tuned weight matrix\n                spectral_param = pad_U@pad_S.diag()@pad_V.T\n                new_state_dict[k] = spectral_param # Update the main weight for the layer\n    return new_state_dict",
        "experimental_info": "The core methodology described, focusing on 'Spectral AdapterR', involves: \n- **Singular Value Decomposition (SVD)**: The pretrained weight matrices (W) are factorized into U, S, V^T (W = USV^T) at initialization within the `SpectralLinearLayer_OFT` module. U, S, and V are stored as non-trainable parameters.\n- **Fine-tuning Strategy**: The method tunes the top-`r` columns of the singular vector matrices (U and V) through orthogonal rotations and scales the corresponding top-`r` singular values (S).\n- **Orthogonality Constraint**: For the orthogonal rotations applied to U and V, the orthogonality constraint is maintained using Cayley parameterization. This is implemented via the `cayley` function.\n- **Trainable Parameters**: Three sets of parameters are introduced per adapted layer:\n    - `spectral_A` (rank `r` x `r` matrix): Learned parameters that, when transformed by the Cayley function, produce the orthogonal rotation matrix `RU` for the U matrix.\n    - `spectral_B` (rank `r` x `r` matrix): Learned parameters that, when transformed by the Cayley function, produce the orthogonal rotation matrix `RV` for the V matrix.\n    - `spectral_C` (rank `r` vector): Learned parameters that directly scale the singular values in S.\n- **Parameter Initialization**:\n    - `spectral_A` and `spectral_B` are initialized to zero matrices. When passed through the Cayley transformation, these result in initial identity rotation matrices (RU = I, RV = I), as specified in the method.\n    - `spectral_C` is initialized to a vector of ones. This means the singular values are initially scaled by 1.\n- **Rank (`r`)**: The number of top singular vectors/values to be fine-tuned (`top-r`) is specified by the `revised_r` argument passed to `SpectralLinearLayer_OFT`, which is typically configured via `lora_cfg['rank']` in the `finetune_cfg` (e.g., a common default or configured value is `8`).\n- **Alpha Parameter (`alpha`)**: An `alpha` scaling factor is applied during the merging process to control the strength of the adaptation. It scales the deviation from the identity transformation for rotations (`alpha*(cayley(params)-I) + I`) and from the initial scaling of one for singular values (`S_original * (alpha*(C_params-ones) + ones)`).\n- **Adapted Modules**: The `SpectralLinearLayer_OFT` module replaces `Linear` layers and `Conv2d` layers with (1,1) kernel size in both the text encoder and the UNet model."
      }
    },
    {
      "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers",
      "abstract": "Adapting large-scale pretrained language models to downstream tasks via\nfine-tuning is the standard method for achieving state-of-the-art performance\non NLP benchmarks. However, fine-tuning all weights of models with millions or\nbillions of parameters is sample-inefficient, unstable in low-resource\nsettings, and wasteful as it requires storing a separate copy of the model for\neach task. Recent work has developed parameter-efficient fine-tuning methods,\nbut these approaches either still require a relatively large number of\nparameters or underperform standard fine-tuning. In this work, we propose\nCompacter, a method for fine-tuning large-scale language models with a better\ntrade-off between task performance and the number of trainable parameters than\nprior work. Compacter accomplishes this by building on top of ideas from\nadapters, low-rank optimization, and parameterized hypercomplex multiplication\nlayers. Specifically, Compacter inserts task-specific weight matrices into a\npretrained model's weights, which are computed efficiently as a sum of\nKronecker products between shared \"slow\" weights and \"fast\" rank-one matrices\ndefined per Compacter layer. By only training 0.047% of a pretrained model's\nparameters, Compacter performs on par with standard fine-tuning on GLUE and\noutperforms standard fine-tuning on SuperGLUE and low-resource settings. Our\ncode is publicly available at~\\url{https://github.com/rabeehk/compacter}.",
      "full_text": "COMPACTER : Efﬁcient Low-Rank Hypercomplex Adapter Layers Rabeeh Karimi Mahabadi EPFL University, Idiap Research Institute rabeeh.karimi@idiap.ch James Henderson Idiap Research Institute james.henderson@idiap.ch Sebastian Ruder DeepMind ruder@google.com Abstract Adapting large-scale pretrained language models to downstream tasks via ﬁne-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, ﬁne-tuning all weights of models with millions or billions of parameters is sample-inefﬁcient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developedparameter-efﬁcient ﬁne-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard ﬁne-tuning. In this work, we propose COMPACTER , a method for ﬁne-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work.COMPACTER accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Speciﬁcally, COMPACTER inserts task-speciﬁc weight matrices into a pretrained model’s weights, which are computed efﬁciently as a sum of Kronecker products be- tween shared “slow” weights and “fast” rank-one matrices deﬁned perCOMPACTER layer. By only training0.047% of a pretrained model’s parameters,COMPACTER performs on par with standard ﬁne-tuning on GLUE and outperforms standard ﬁne-tuning on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter. 1 Introduction State-of-the-art pretrained language models (PLMs) in natural language processing (NLP) have used heavily over-parameterized representations consisting of hundreds of millions or billions of parameters to achieve success on a wide range of With four parameters I can ﬁt an elephant, and with ﬁve I can make him wiggle his trunk. John von Neumann NLP benchmarks [2, 3, 4]. These models are generally applied to downstream tasks via ﬁne-tuning [5], which requires updating all parameters and storing one copy of the ﬁne-tuned model per task. This causes substantial storage and deployment costs and hinders the applicability of large-scale PLMs to real-world applications. Additionally, ﬁne-tuning of over-parameterized models on low-resource datasets has been shown to be subject to instabilities and may lead to poor performance [6, 7]. Inspired by John von Neumann’s quotation, we ask, given that we have already learned general-purpose language representations via a PLM (i.e. we have ﬁt our elephant), how many more parameters 35th Conference on Neural Information Processing Systems (NeurIPS 2021) arXiv:2106.04647v2  [cs.CL]  27 Nov 20210.01 0.10 1.00 10.00 100.00 Percentage of the Trained Parameters Per Task (Relative to T5) 76 78 80 82 84 86 88GLUE Score T5Adapter Pfeiffer-Adapter AdapterDrop PromptTuning Intrinsic-SAID BitFit PHM-AdapterCompacter Compacter++ Adapter-LowRank Figure 1: The average score on GLUE (y axis), percentage of trainable parameters per task (x axis, in log scale), and memory footprint (size of the circles) of different methods. Feed forward down projection Nonlinearity Adapter Layer Multi-head attention Adapter +  Transformer Layer Layer norm Feed forward Adapter +  Layer norm Feed forward  up projection +  Figure 2: Left: Adapter integration in a pretrained transformer model. Right: Adapter architecture. Follow- ing Houlsby et al. [1], we include adapters after the attention and feed- forward modules. During training, we only update layer normalizations and adapters (shown in yellow), while the pretrained model is ﬁxed. do we need to reach state-of-the-art performance on standard NLP tasks. Speciﬁcally, we aim to develop practical, memory-efﬁcient methods that train a minimum set of parameters while achieving performance on par or better than full ﬁne-tuning for state-of-the-art NLP models. Recent literature has introducedparameter-efﬁcient ﬁne-tuning methods. These approaches generally keep the pretrained model’s parameters ﬁxed and introduce a set of trainable parameters per task, trading off the number of trainable parameters with task performance. At one end of the spectrum, prompts, i.e. natural language descriptions of a task, together with demonstrations have been used to achieve reasonable performancewithout any parameter updates on some benchmarks [8] but their performance generally lags behind ﬁne-tuned models. They also require huge models to work well but choosing good prompts becomes harder with larger model sizes [9]. Soft prompt methods treat prompts as trainable continuous parameters, which are prepended to the inputs at the input layer or intermediate layers [10, 11, 12]. Such methods, however, often require large models to achieve good performance and are very sensitive to initialization and unstable during training. The theoretically motivated low-rank methods train a small number of parameters that lie in a low-dimensional subspace using random projections [13, 14]. However, storing the random projection matrices causes substantial memory overhead and leads to slow training times. At the other end of the spectrum, adapter methods [1, 15] that insert trainable transformations at different layers of the pretrained model require more parameters than the aforementioned approaches but are more memory-efﬁcient and obtain performance comparable to full ﬁne-tuning [1, 16]. In this work, we proposeCOMPACTER , a method for ﬁne-tuning large-scale language models with an excellent trade-off between the number of trainable parameters, task performance, and memory footprint, compared to existing methods (see Figure 1).COMPACTER builds on ideas from adapters [1], low-rank methods [13], as well as recent hypercomplex multiplication layers [17]. Similar to adapters, COMPACTER inserts task-speciﬁc weight matrices into a pretrained model’s weights. Each COMPACTER weight matrix is computed as the sum of Kronecker products between shared “slow” weights and “fast” rank-one matrices deﬁned per COMPACTER layer (see Figure 3). As a result, COMPACTER achieves a parameter complexity ofO( k+ d)compared to O( kd)for regular adapters, where the adapters are of sizek×d. In practice, COMPACTER trains 0.047% of a PLM’s parameters. On the standard GLUE [ 18] and SuperGLUE [ 19] benchmarks, COMPACTER outperforms other parameter-efﬁcient ﬁne-tuning methods and obtains performance on par or better than full ﬁne-tuning. On low-resource settings, COMPACTER outperforms standard ﬁne-tuning. In summary, we make the following contributions: 1) We propose COMPACTER (Compact Adapter) layers, a parameter-efﬁcient method to adapt large-scale language models.2) We show that COMPACTER obtains strong empirical performance on GLUE and SuperGLUE.3) We demonstrate that 2COMPACTER outperforms ﬁne-tuning in low-resource settings.4) We provide a parameter complexity analysis of COMPACTER , showing that it requires dramatically fewer parameters than adapters and ﬁne-tuning. 5) We provide a systematic evaluation of recent parameter-efﬁcient ﬁne-tuning methods in terms of training time and memory consumption. We release our code to facilitate future work. 2 Background We start by introducing the required background on the Kronecker product and adapter layers [1, 15]. 2.1 Kronecker Product The Kronecker product between matrixA∈Rm×f and B∈Rp×q, denoted by A⊗B∈Rmp×fq , is mathematically deﬁned as: A⊗B=   a11B ··· a1f B ... ... ... am1B ··· amf B  , (1) where aij shows the element in theith row and jth column of A. 2.2 Adapter Layers Recent work has shown that ﬁne-tuningall parameters of a language model can lead to a sub-optimal solution, particularly for low-resource datasets [6]. As an alternative, Rebufﬁ et al.[15] and Houlsby et al. [1] propose to transfer a model to new tasks by inserting small task-speciﬁc modules called adapter layers within the layers of a pretrained model, as depicted in Figure 2. They then only train adapters and layer normalizations, while the remaining parameters of the pretrained model remain ﬁxed. This approach allows pretrained language models to efﬁciently adapt to new tasks. Each layer of a transformer model is composed of two primary modules: a) an attention block, and b) a feed-forward block. Both modules are followed by a skip connection. As shown in Figure 2, Houlsby et al. [1] suggest to insert an adapter layer after each of these blocks before the skip connection. Adapters are bottleneck architectures. By keeping the output dimension similar to their input, they cause no change to the structure or parameters of the original model. The adapter layerAl for layer lconsists of a down-projection,Dl∈Rk×d, GeLU non-linearity [20], and up-projectionUl∈Rd×k, where k is the input dimension, anddis the bottleneck dimension for the adapter layer. Adapters are deﬁned as: Al(x)= Ul(GeLU(Dl(x)))+x, (2) where xis the input hidden state. 3 Method In this section, we present COMPACTER , a compact and efﬁcient way to adapt large-scale PLMs. Problem formulation We consider the general problem of ﬁne-tuning large-scale language models, where we are given the training dataD={(xi,yi)}P i=1 with P samples. We assume we are also given a large-scale pretrained language modelfθ(.) parameterized by θthat computes the output for input xi. Our goal is to ﬁne-tunefθ(.) efﬁciently to enable the model to adapt to new tasks. 3.1 Compact and Efﬁcient Adapter Layers In this section, we introduce an efﬁcient version of adapter layers, building on top of recent advances in parameterized hypercomplex multiplication layers(PHM) [17]. To the best of our knowledge, we are the ﬁrst to exploit PHM layers for efﬁcient ﬁne-tuning of large-scale transformer models. The PHM layer has a similar form as a fully-connected layer, which converts an inputx∈Rk to an outputy∈Rd: y=Wx+b, (3) 3+  Parameter size of  Parameter size of  Shared parameters size Size of  Size of  Independent rank one weights Independent rank one weights Shared  weight  Shared weight  +  Figure 3: Illustration of generating weights of two differentCOMPACTER layers: W1 ∈Rd×k (ﬁrst row) and W2 ∈Rd×k (second row). We generate W1 and W2 using Wj = ∑n i=1 Ai⊗Bi j =∑n i=1 Ai⊗(sijtij⊤ ) (5), by computing the sum of Kronecker products ofshared matrices Aiand adapter-speciﬁc matrices Bj i, with i∈{1,...,n}and adapter index j∈{1,2}. We generate eachBj i by multiplying independent rank one weights. In this examplen=2, d=6, and k=8. where W∈Rk×d. The key difference is that in a PHM layer,W is learned as a sum of Kronecker products. Assume that kand dare both divisible by a user-deﬁned hyperparametern∈Z>0 . Then, the matrix Win (3) is computed as the sum ofnKronecker products as follows: W= n∑ i=1 Ai⊗Bi, (4) where Ai∈Rn×n and Bi∈R k n ×d n . The PHM layer has a parameter complexity ofO(kd n ), reducing parameters by at most 1 n [17] (see §4). 3.2 Beyond Hypercomplex Adapters Prior work indicates that some of the information captured in pretrained models can be ignored for transfer [21, 22]. Similarly, redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important [1]. In addition, sharing adapters across layers leads to a comparatively small drop of performance for some tasks [23]. Motivated by these insights, we propose the following two extensions to make hypercomplex adapters more efﬁcient. Sharing information across adapters Sharing all adapter parameters across layers is overall too restrictive and is not able to perform on par with ﬁne-tuning or using regular adapters [ 23]; however, our decomposition of adapters intoAiand Bimatrices as in Eq.(4) allows us to be more ﬂexible. Consequently, we divide our adaptation weights intoshared parameters that capture general information useful for adapting to the target task and adapter-speciﬁc parameters that focus on capturing information relevant for adapting each individual layer. Speciﬁcally, we deﬁneAias shared parameters that are common across all adapter layers whileBiare adapter-speciﬁc parameters. Low-rank parameterization Low-rank methods [13, 14] have demonstrated that strong performance can be achieved by optimizing a task in a low-rank subspace. Similarly, we hypothesize that a model can also be effectively adapted by learning transformations in a low-rank subspace. To this end, we propose to parameterize Bi∈R k n ×d n as a low-rank matrix, which is the product of two low-rank weights si∈R k n ×r and ti∈Rr×d n , where ris the rank of the matrix.1 Putting both extensions together, we propose thelow-rank parameterized hypercomplex multiplication layer (LPHM): W= n∑ i=1 Ai⊗Bi= n∑ i=1 Ai⊗(sit⊤ i ). (5) In general, we set r= 1so that Biis a rank-one matrix. Depending on the complexity of the target task, rcan be set to a higher value.2 Figure 3 illustrates our method. Overall, the LPHM layer reduces 1We do not factorizeAi as they are small, shared between all layers, and factorization hurts performance. 2If factors are over-parameterized, COMPACTER can be used forovercomplete knowledge distillation [24]. 4complexity further to O(k+ d) (see §4). The LPHM layer can also be seen as leveraging “slow” weights Aithat are shared across adapters and capture general information and “fast” weightsBi that learn adapter-speciﬁc information for adaptation of each individual layer [25]. COMPACTER Based on the above formulation, we introduce COMPACTER layers, which replace the down-projection and up-projection layers in adapters as follows: Al(x)= LPHMUl (GeLU(LPHMDl (x)))+x, where the up-projection weights LPHMUl are computed as in (5), replacing the layer Ul in (2). Similarly, down-projection weightsLPHMDl replace the layer Dl. While the two adapters in each layer of a transformer have their ownsiand tirank-one weights, we share theAiacross all layers and positions of the adapter layers. 4 Parameter Efﬁciency In this section, we compare the number of parameters of COMPACTER with adapters. Adapters parameters In the standard setting, two adapters are added per layer of a transformer model [1]. Each adapter layer consists of2kdparameters for the down and up-projection matrices (Ul, Dl) respectively wherekis the size of the input dimension anddis the adapter’s bottleneck dimension. The total number of parameters for adapters for a transformer model withLlayers of both an encoder and a decoder is, therefore,2L(2kd), which scales linearly with all three variables. PHM-A DAPTER parameters In the conventional PHM layer [17], as depicted in Eq.(4), parameters of Ai∈Rn×n and Bi∈R k n ×d n deﬁne the degree of freedom forWas n(kd n2 +n2)= kd n +n3. With the mild condition thatkd>n4, then kd n dominates and the overall parameter size of the PHM layer in(4) is O(kd n ). This condition is satisﬁed for typical values for adapters, PHM layers, and large-scale PLMs such as T5-large, with hidden sizek=1024, adapter hidden sized∈{24,32,48,96}, and n=2,4,8,12. Hence, the PHM layer offers a parameter reduction of almost1 n compared to standard fully-connected layers, which areO(kd).3 Similarly, employing PHM layers for modeling down and up-projection matrices offers a parameter reduction of almost 1 n. Each adapter with a PHM layer has in total 2(kd n + n3) parameters. For a Transformer model withLlayers, the total number of parameters ofPHM-A DAPTER is 4L(kd n +n3). COMPACTER parameters COMPACTER shares the trained weight matrices{Ai}n i=1 in (5) consisting of n3 parameters across all layers. COMPACTER also has two rank-one weights for each adapter, si,tiin (5) consisting of k n + d n parameters, resulting in a total of2n( k n + d n) parameters for down and up-projection weights. Therefore, the total number of parameters ofCOMPACTER is 4L(k+d)+n3 for a transformer withLlayers in the encoder and decoder. In settings with a large number of layers, the dominant term is 4L(k+ d). Therefore, with a mild condition that 4L(k+d)>n3, COMPACTER has a complexity ofO(k+d), which is far more efﬁcient compared to adapters’O(kd) and PHM-A DAPTER ’sO(kd n ) complexity respectively. In settings where nis large, the number of parameters for shared weight matrices{Ai}n i=1 for all layers remain constant in COMPACTER with a total ofn3 parameters while this scales linearly with the number of layersLfor PHM and adapter layers. As an example, in the T5BASE model with 222M parameters [3], COMPACTER only learns 0.047% of the parameters, and maintains comparable performance tofull ﬁne-tuning. 5 Experiments Datasets Following Raffel et al.[3], we evaluate the performance of the methods on the GLUE [18] and SUPERGLUE [19] benchmarks. These benchmarks cover multiple tasks of paraphrase detection (MRPC, QQP), sentiment classiﬁcation (SST-2), natural language inference (MNLI, RTE, QNLI, CB), linguistic acceptability (CoLA), question-answering (MultiRC, ReCoRD, BoolQ), word sense disambiguation (WiC), and sentence completion (COPA).4 As the original test sets are not publicly 3Even for smaller models where the n4 term dominates, we observe a substantial reduction of parameters compared to adapters. 4Following Devlin et al.[2], Raffel et al.[3], as a common practice, we do not experiment with WNLI [26] due to its adversarial nature with respect to the training set. 5available, we follow Zhang et al.[27] and split off 1k samples from the training set that we use for validation, while we use the original validation data as the test set. For datasets with fewer than 10k samples (RTE, MRPC, STS-B, CoLA, COPA, WiC, CB, BoolQ, MultiRC), we divide the original validation set in half, using one half for validation and the other for testing. Experimental details We use the state-of-the-art encoder-decoder T5 model [3] as the underlying model for all methods in our experiments. For computational efﬁciency, we report all results on T5BASE models (12 encoder and decoder layers and 222M parameters). We use its HuggingFace PyTorch implementation [28]. We ﬁne-tune all methods for 3 epochs on large datasets and 20 epochs for low-resource datasets ofGLUE (MRPC, CoLA, STS-B, RTE, BoolQ, CB, COPA, WiC) to allow the models to converge [27]. For all adapter-based methods, we experiment with adapters of bottleneck size of {96,48,24}. We save a checkpoint every epoch for all models and report the results for the hyper-parameters performing the best on the validation set for each task. For the PHM layers, we use the PyTorch implementation of Le et al.[29]. We include low-level details in Appendix A. For our methods, we experiment withn={4,8,12}and report the model performing the best. We include the results for all values ofnin Appendix B. Following Mahabadi et al. [30], we freeze the output layer of the pretrained model for all tasks across all methods.5 We show the results with ﬁne-tuning the output layer in Appendix C. Following Houlsby et al. [1], we update the layer normalization parameters for all methods where applicable.6 5.1 Baselines We compare against several recently proposedparameter-efﬁcient ﬁne-tuning methods: T5BASE We compare our method to the standard practice of ﬁne-tuning T5, where we ﬁne-tune all parameters of the model on each individual task. ADAPTER We compare to a strong adapter baseline [1], which adds adapters for each task after the feed-forward and attention modules in each transformer block of T5. PFEIFFER -ADAPTER Pfeiffer et al.[31] propose a more efﬁcient adapter variant, which keeps only one of the adapters in each layer for better training efﬁciency. We experimented with keeping either adapter and found keeping the adapter after the self-attention module in each layer to perform the best. ADAPTER -LOWRANK We parameterize each adapter’s weight as a product of two rank-one weights. PROMPT TUNING Prompt tuning [12] is the successor variant of Li and Liang[10], which prepends a randomly initialized continuous prompt to the input (PROMPT TUNING -R). We also compare to a variant, which initializes prompts using token embeddings of the pretrained language model’s vocabulary (PROMPT TUNING -T) [12]. INTRINSIC -SAID The Structure Aware Intrinsic Dimension [14] ﬁne-tunes the model by reparame- terizing the parameters in a lower-dimensional subspaceθd′ (d′≪D): θD i =θD i,0+λiPθd′−m i where parameter θD i,0 are the pretrained model’s parameters andP∈Rd′−m →RD is a random linear projec- tion via the Fastfood transform [32]. They then consider the total number of weight matrices in the PLM, m, and attribute a weight to each of them, resulting inλ∈Rm in total by tradingmparameters from the low dimensional spaceθd′ ∈Rd′ . Then, the total trainable parameters areθd′−m∈Rd′−m and λ. ADAPTER DROP We apply the method of Rücklé et al.[23], which drops the adapters from lower transformer layers for a better training efﬁciency to T5 with ADAPTER . Consequently, we drop adapters from the ﬁrst ﬁve layers of both the encoder and the decoder in T5BASE. BITFIT Cai et al. [33] propose to freeze the weights and only train the biases. By not storing intermediate activations, this method enables substantial memory savings. Ravfogel et al.[34] study a similar method for PLMs that ﬁne-tunes only the biases and the ﬁnal output layer.7 5This is much more efﬁcient as the output layer includes 11.1% of the parameters of T5 BASE. Tasks are formulated in a text-to-text format so the model can be applied to them without learning a new output layer [3]. We note that this is in contrast to the original adapter setting, which used an encoder-only masked PLM [1]. 6For BITFIT, we only update the biases. For PROMPT TUNING , the entire model is frozen. 7Note that in the HuggingFace T5 implementation, the biases in layer normalizations, linear layers, the output layer and self-attention layers are removed. We re-introduce these biases for BITFIT. 6Table 1: Performance of all models on the GLUE tasks. For each method, we report the total number of parameters across all tasks and the number of parameters that are trained for each task as a multiple and proportion of T5BASE model [3]. For MNLI, we report accuracy on the matched validation set. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation coefﬁcients. For CoLA, we report Matthews correlation. For all other tasks, we report accuracy. Bold fonts indicate the best results. For the results with†, due to insatiability during training, we restarted experiments with 6 random seeds and report the best. For INTRINSIC -SAID, d′is set to20K. Method #Totalparams Trainedparams /per taskCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg Baselines T5BASE 8.0×1 100% 61.7694.61 90.20/93.06 91.63/88.8489.68/89.9786.78 93.01 71.9486.50 ADAPTER 1.065 0.832%64.0293.81 85.29/89.73 90.18/87.20 90.73/91.02 86.49 93.21 71.9485.78PFEIFFER-ADAPTER 1.032 0.427%62.9 93.46 86.76/90.85 90.14/87.15 91.13/91.34 86.26 93.3076.26 86.32ADAPTERDROP 1.038 0.494%62.7 93.58 86.27/90.60 90.2/87.2591.37/91.6186.27 93.23 71.2285.85ADAPTER-LOWRANK 1.004 0.073%59.19 93.69 88.24/91.49 90.23/87.01 90.8/91.33 85.8 92.9 73.3885.82 PROMPTTUNING-R 1.003 0.034%0.47† 87.61 68.14/81.05 88.93/85.55 90.25/90.59 46.83† 92.33 54.6871.49PROMPTTUNING-T 1.003 0.034%10.59 90.94 68.14/81.05 89.69/86.14 89.84/90.21 81.46 92.75 54.6875.95 INTRINSIC-SAID 1.001 0.009%58.69 94.15 88.24/91.78 90.28/87.13 90.06/90.45 85.2393.3970.50 85.45BITFIT 1.010 0.126%58.16 94.15 86.76/90.53 90.06/86.99 90.88/91.26 85.31 92.99 67.6384.97 Our Proposed Methods PHM-ADAPTER(n=12) 1.013 0.179%57.3594.50 91.67/93.86 90.25/87.0590.45/90.8485.97 92.92 75.5486.40 COMPACTER(n=4) 1.004 0.073%63.7593.00 89.22/92.31 90.23/87.03 90.31/90.74 85.61 92.8877.70 86.62 COMPACTER++(n=4) 1.002 0.047%61.27 93.81 90.69/93.33 90.17/86.9390.46/90.9385.71 93.0874.82 86.47 5.2 Our Methods PHM-A DAPTER We learn the weights of adapters using PHM layers as in(4). To our knowledge, we are the ﬁrst who exploit the idea of PHM [17] for efﬁcientﬁne-tuning of large-scale language models. COMPACTER We learn adapter weights using LPHM layers as described in(5). We also explore a variant where we only keep theCOMPACTER layer after the feed-forward layer in each transformer block (COMPACTER ++).8 5.3 Results on the GLUE Benchmark Table 1 shows the results onGLUE with T5BASE (see Appendix E for results on T5SMALL). COMPACTER and COMPACTER ++ outperform all previous parameter-efﬁcient methods and perform on par with full ﬁne-tuning while only training 0.07% and 0.047% of parameters respectively. We now discuss the different methods in detail. Adapter-based methods For ADAPTER , not ﬁne-tuning the classiﬁer hurts the performance substantially (85.78 versus 86.48; cf. Appendix C).PFEIFFER -ADAPTER , which adds adapters only after the self-attention module outperforms the standard ADAPTER while being more parameter- efﬁcient. ADAPTER DROP obtains lower performance than ﬁne-tuning, demonstrating that adapting the lower layers of an encoder-decoder T5 model is important for its performance. Additionally, ADAPTER -LOWRANK is not expressive enough to perform well on this benchmark. Prompt tuning and BitFit For PROMPT TUNING , we observe high sensitivity to initialization and learning rate, as also conﬁrmed in [10]. We experimented with multiple random seeds but performance lags behind ﬁne-tuning substantially, in particular on low-resource datasets. This can be explained by the low ﬂexibility of such methods as all the information needs to be contained in the preﬁxes. As a result, the method only allows limited interaction with the rest of the model and good performance requires very large models [12]. In addition, increasing the sequence length leads to memory overhead (see §5.5) and the number of prompt tokens is limited by the number of tokens that can ﬁt in the model’s maximum input length, which makes such methods less ﬂexible and unsuitable for dealing with large contexts. Similarly, BITFIT performs worse than ﬁne-tuning, especially on low-resource datasets. Intrinsic-SAID Interestingly, the average performance ofINTRINSIC -SAID , which ﬁne-tunes only 0.009% of a model’s parameters is only 1.05 points below the ﬁne-tuning baseline. However, this method has two practical drawbacks: a) storing the random projection matrices results in a substantial 8We found this to slightly outperform keeping the COMPACTER layer after the self-attention layer instead. 7Table 2: Performance of all methods on the SUPER GLUE tasks. For each method, we report the total number of parameters across all tasks and the percentage of parameters that are trained for each task as a multiple and proportion of T5BASE model [3]. For CB, we report accuracy and F1. For MultiRC, we report F1 over all answer-options (F1a) and exact match of each question’s set of answers (EM) [19]. For ReCoRD, we report F1 and EM scores. For all other tasks, we report accuracy. For INTRINSIC -SAID, d′is set to20K. Bold fonts indicate the best results in each block. Method #Total params Trained params / per task BoolQ CB COPA MultiRC ReCoRD WiC Avg Baselines T5BASE 6.0×1 100% 81.10 85.71/78.2152.0 68.71/47.0 74.26/73.33 70.22 70.06 ADAPTER 1.049 0.832% 82.39 85.71/73.52 52.0 72.75/53.41 74.55/73.58 67.0870.55 PFEIFFER-ADAPTER 1.024 0.427% 82.45 85.71/75.63 54.0 72.53/51.76 74.69/73.70 68.6571.01 ADAPTERDROP 1.028 0.494% 82.26 85.71/75.63 42.0 72.92/53.3074.68/73.70 68.3469.84 ADAPTER-LOWRANK 1.003 0.073% 80.31 78.57/55.37 54.0 72.58/51.98 74.77/73.87 64.5867.34 PROMPTTUNING-R 1.002 0.034% 61.71 67.86/46.99 48.0 59.23/16.33 75.27/74.36 48.9055.41 PROMPTTUNING-T 1.002 0.034% 61.71 67.86/46.89 52.0 57.66/19.4475.37/74.4148.90 56.03 INTRINSIC-SAID 1.001 0.009% 78.72 75.00/51.83 54.0 69.98/52.78 74.86/73.91 65.8366.32 BITFIT 1.008 0.126% 79.57 78.57/54.4056.0 70.73/48.57 74.64/73.64 69.5967.30 Our Proposed Methods PHM-ADAPTER(n=4) 1.013 0.240% 80.31 85.71/73.52 44.0 71.99/51.65 74.62/73.6067.40 69.20 COMPACTER(n=12) 1.003 0.073% 78.59 96.43/87.4448.0 70.80/49.67 74.49/73.54 65.2071.57 COMPACTER++(n=12) 1.002 0.048% 78.84 92.86/84.9652.0 70.68/50.99 74.55/73.5068.03 71.82 memory overhead; b) it is very slow to train (see §5.5). Despite this, INTRINSIC -SAID provides insights regarding the effectiveness of low-rank optimization of pretrained language models [ 14], which motivates the development of parameter-efﬁcient methods such as COMPACTER . COMPACTER For our proposed methods, we observe ﬁne-tuning the output layer for both PHM-A DAPTER and COMPACTER ++ does not provide much performance difference (see Appendix C). PHM-A DAPTER reduces the parameters ofADAPTER from 0.83% to 0.179% (withn=12), being 4.64×more parameter-efﬁcient. COMPACTER reduces the number of parameters to the remarkable rate of 0.073% while obtaining comparable results to full ﬁne-tuning. By removing theCOMPACTER layer after self-attention, COMPACTER ++ obtains similar performance, while reducing the parameters to 0.047%. Adaptation without updating the layer normalization can be a promising direction to reduce the parameters further, for instance by building on recent advances in normalization-free models [35], which we leave to future work. 5.4 Results on the S UPER GLUE Benchmark Table 2 shows the performance of the methods onSUPER GLUE [19]. We include the results for all values of nin Appendix D. We observe a similar pattern as onGLUE in Table 1.COMPACTER and COMPACTER ++ perform substantially better compared to other parameter-efﬁcient ﬁne-tuning methods and even outperform full ﬁne-tuning while only training 0.073% and 0.048% of the parameters. 5.5 Efﬁciency Evaluation In this section, we compare the efﬁciency of our proposed methods with various recently proposed parameter-compact ﬁne-tuning methods under the same computation budget. To this end, we train all methods for 1 epoch on the MNLI dataset. For each method, we select the largest batch size that ﬁts a ﬁxed budget of the GPU memory (24 GB). For all adapter-based methods, we ﬁx the adapter size to 24. ForPROMPT TUNING , we set the number of preﬁx tokens to 100. ForINTRINSIC -SAID , we set d′= 1400. Finally, we set n= 4. In Table 3, we report the percentage of trained parameters per task, training time per epoch, and memory usage of each method. Moreover, Figure 1 shows the trade-off between quantitative performance, percentage of trained parameters, and memory footprint. Our approaches have several attractive properties. Based on our analysis in Table 1,COMPACTER and COMPACTER ++ obtain the best combination of highGLUE score averaged across all tasks, plus a sub- stantially lower number of parameters (0.073% and 0.047% respectively). In addition toCOMPACTER ++ 8Table 3: Percentage of trained parameters per task, average peak memory and training time for all methods. ∆% is the relative difference with respect tofull ﬁne-tuning (T5BASE). Lower is better. Method Trained params/ per task Memory (MB) ∆% Time/Epoch (min) ∆% T5BASE 100% 167.99 — 42.13 — ADAPTER 0.832% 124.02 -35.45% 31.81 -24.50% PFEIFFER -ADAPTER 0.427% 118.4 -41.88% 28.19 -33.09% ADAPTER DROP 0.494% 119.41 -40.68% 28.08 -33.35% ADAPTER -LOWRANK 0.073% 123.8 -35.69% 32.71 -22.36% PROMPT TUNING 0.034% 222.27 24.42% 44.54 5.72% INTRINSIC -SAID 0.009% 285.40 41.14% 144.01 241.82% BITFIT 0.126% 102.31 -64.20% 27.36 -35.06% PHM-A DAPTER 0.179% 123.93 -35.55% 35.55 -15.62% COMPACTER 0.073% 123.91 -35.57% 36.48 -13.41% COMPACTER ++ 0.047% 118.35 -41.94% 30.96 -26.51% performing well, its memory requirement is the second best among all methods, reducing memory usage by -41.94% compared to T5BASE. COMPACTER and COMPACTER ++ also speed up training substantially, by -13.41% and -26.51% relative to T5BASE. On the other hand,BITFIT, by not storing intermediate activations, has the lowest memory requirement (-64.2% relative to T5BASE) and is the fastest (-35.06% relative to T5BASE) at the cost of lower quantitative performance (1.53 points lower; see Table 1). Methods relying on pruning adapters, i.e., PFEIFFER -ADAPTER and ADAPTER DROP reduce the memory overhead and improve training time. However, their number of parameters is almost an order of magnitude more compared to COMPACTER ++, with 9.1 ×and 10.5×more parameters respectively. Moreover, although, PFEIFFER -ADAPTER performs on par with full ﬁne-tuning with a slight degradation (Table 1),ADAPTER DROP obtains a lower performance (-0.65 less on average across all tasks.). We note that dropping adapters from transformer layers is a general technique and could be applied toCOMPACTER for improving efﬁciency even further, which we leave to future work. Similarly, althoughADAPTER -LOWRANK reduces the memory overhead and improves the training time, it obtains a lower performance (Table 1) (-0.68 less on average across all tasks.). At the other end of the spectrum, INTRINSIC -SAID and PROMPT TUNING methods have the lowest number of parameters. However, they both come with high memory overhead (41.14% and 24.42% relative to full ﬁne-tuning (T5BASE) respectively), are slowest to train, and their performance substantially lags behind full ﬁne-tuning (see Table 1). For PROMPT TUNING , high memory costs are due to the fact that the computational complexity of self-attention, which requires storing the full attention matrix for gradient computation, scales quadratically with the sequence length [36]. For INTRINSIC -SAID , the high memory requirement is due to storing large random projection matrices, which limits the application of INTRINSIC -SAID for ﬁne-tuning large-scale PLMs. Moreover, computing projections via FastFood transform, although theoretically possible inO(Dlogd′) [32], is slow in practice even with a CUDA implementation. For pretrained language models with a large number of parameters, allocating random projections for the full parameter space is intractable. While using Fastfood transform partially ameliorates this issue by reducing the memory usage fromO(Dd′) to O(D), the memory issue with such methods remains unresolved. Overall, given the size of large-scale transformer models with millions and billions of parameters, such as T5 [3], efﬁcient memory usage is of paramount importance for practical applications.COMPACTER and COMPACTER ++ offer a great trade-off in terms of performance, memory usage, and training time. With regard to our inspiration of von Neumann’s quotation, we thus ﬁnd that only a comparatively small number of additional parameters are necessary for the practical and efﬁcient adaptation of PLMs. 5.6 Low-resource Fine-tuning COMPACTER ++ has substantially fewer parameters compared to T5BASE. In this section, we investigate whether this could helpCOMPACTER ++ to generalize better in resource-limited settings. We subsample each dataset of GLUE for varying sizes in the range{100,500,1000,2000,4000}. Figure 4 shows the 90 1000 2000 3000 4000 # Samples per task 76 78 80 82 84Average scores on GLUE T5BASE Compacter++ Figure 4: Results on GLUE for the various number of training samples per task (100,500,1000,2000,4000). We show mean and standard deviation across 5 seeds. results. COMPACTER ++ substantially improves the results in the low-resource setting, indicating more effective ﬁne-tuning in this regime. 6 Related Work Adapters Adapters have recently emerged as a new paradigm for ﬁne-tuning pretrained language models [1]. In another line of work, Üstün et al. [37] proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks [38], where they generate adapter parameters conditioned on trained input language embeddings. This, however, leads to a large number of additional parameters compared to the base model. Contemporaneously, Mahabadi et al. [30] use a single compact hypernetwork allowing to generate adapter weights efﬁciently conditioned on multiple tasks and layers of a transformer model. Pilault et al.[39] also proposed a task-conditioned transformer for multi-task learning which is less parameter-efﬁcient. The aforementioned work is complementary to COMPACTER , and one could potentially combineCOMPACTER with contextual parameter generation to generate adapter modules. Compared to Mahabadi et al.[30], COMPACTER ++ reduces the parameters by 6.2×. Hypercomplex representations Deep learning advances in the hypercomplex domain are in a nascent stage, and most work is fairly recent [40, 41, 42, 43, 44]. Replacing matrix multiplications in standard networks with Hamilton products that have fewer degrees of freedom offers up to a 4×saving of param- eter size in a single multiplication operation [42, 44]. Very recently, Zhang et al.[17] extend such meth- ods in a way that they could reduce the parameters of a fully connected layer under a mild condition to 1/n, where nis a user-speciﬁed parameter. To the best of our knowledge, there is no previous work that attempts to leverage the hypercomplex space for efﬁcient ﬁne-tuning of large-scale language models. Other parameter-efﬁcient models Li et al. [13] and Aghajanyan et al. [14] study training models in a low-dimensional randomly oriented subspace instead of their original parameter space. Another recent line of work has shown that pretrained models such as BERT are redundant in their capacity, allowing for signiﬁcant sparsiﬁcation without much degradation in end metrics [45, 46, 47]. Such methods, however, remain not well supported by current hardware and often perform worse compared to dedicated efﬁcient architectures [48]. 7 Conclusion We have proposedCOMPACTER , a light-weight ﬁne-tuning method for large-scale language models. COMPACTER generates weights by summing Kronecker products between shared “slow” weights and “fast” rank-one matrices, speciﬁc to each COMPACTER layer. Leveraging this formulation, COMPACTER reduces the number of parameters in adapters substantially from O(kd) to O(k+d). Through extensive experiments, we demonstrate that despite learning 2127.66×fewer parameters than standard ﬁne-tuning, COMPACTER obtains comparable or better performance in a full-data setting and outperforms ﬁne-tuning in data-limited scenarios. 10Acknowledgements We are grateful to Dani Yogatama for feedback on a draft of this manuscript. The authors would like to thank Tuan Le for his assistance in reproducing the results of Zhang et al.[17]. We would like to also thank Armen Aghajanyan for his assistance to reproduce the results of his work [14]. We thank Jue Wang for his comments on an earlier version of this paper. The authors are grateful to Brian Lester, Rami Al-Rfou, Noah Constant, and Mostafa Dehghani for their assistance. Rabeeh Karimi Mahabadi was supported by the Swiss National Science Foundation under the project Learning Representations of Abstraction for Opinion Summarization (LAOS), grant number “FNS-30216”. References [1] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for nlp. In ICML, 2019. [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. InNAACL, 2019. [3] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. JMLR, 2020. [4] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [5] Jeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁcation. In ACL, 2018. [6] Matthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained representations to diverse tasks. InRepL4NLP, 2019. [7] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. arXiv preprint arXiv:2002.06305, 2020. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. InNeurIPS, 2020. [9] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True Few-Shot Learning with Language Models. arXiv preprint arXiv:2105.11447, 2021. URL http://arxiv.org/abs/2105.11447. [10] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. ACL, 2021. [11] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial reprogramming. ACL, 2021. [12] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. EMNLP, 2021. [13] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. InICLR, 2018. [14] Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the effectiveness of language model ﬁne-tuning.ACL, 2021. 11[15] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Efﬁcient parametrization of multi-domain deep neural networks. InCVPR, 2018. [16] Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efﬁcient transfer learning. InEMNLP Findings, 2020. [17] Aston Zhang, Yi Tay, SHUAI Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In ICLR, 2021. [18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2019. [19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: a stickier benchmark for general-purpose language understanding systems. In NeurIPS, 2019. [20] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [21] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting Few-sample BERT Fine-tuning. InICLR, 2021. [22] Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, and Sebastian Ruder. Rethinking Embedding Coupling in Pre-trained Language Models. InICLR, 2021. [23] Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. AdapterDrop: On the Efﬁciency of Adapters in Transformers.EMNLP, 2021. [24] Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In ICML, 2018. [25] Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: An Alternative Approach to Efﬁcient Ensemble and Lifelong Learning. InICLR, 2020. [26] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In KR, 2012. [27] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few-sample bert ﬁne-tuning. In ICLR, 2021. [28] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP: System Demonstrations, 2020. [29] Tuan Le, Marco Bertolini, Frank Noé, and Djork-Arné Clevert. Parameterized hypercomplex graph neural networks for graph classiﬁcation.ICANN, 2021. [30] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter- efﬁcient multi-task ﬁne-tuning for transformers via shared hypernetworks. InACL, 2021. [31] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rück ´le, Cho Kyunghyun, and Iryna Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. InEACL, 2021. [32] Quoc Le, Tamás Sarlós, and Alex Smola. Fastfood-approximating kernel expansions in loglinear time. In ICML, 2013. [33] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efﬁcient on-device learning. NeurIPS, 2020. [34] Shauli Ravfogel, Elad Ben-Zaken, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning for transformer-based masked languagemodels.arXiv preprint arXiv:2106.10199. 12[35] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. ICML, 2021. [36] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [37] Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. Udapter: Language adaptation for truly universal dependency parsing. InEMNLP, 2020. [38] Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell. Contextual parameter generation for universal neural machine translation. InEMNLP, 2018. [39] Jonathan Pilault, Amine El hattami, and Christopher Pal. Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters & less data. InICLR, 2021. [40] Chase J Gaudet and Anthony S Maida. Deep quaternion networks. In IJCNN, 2018. [41] Titouan Parcollet, Ying Zhang, Mohamed Morchid, Chiheb Trabelsi, Georges Linarès, Renato de Mori, and Yoshua Bengio. Quaternion convolutional neural networks for end-to-end automatic speech recognition. In Interspeech, 2018. [42] Titouan Parcollet, Mirco Ravanelli, Mohamed Morchid, Georges Linarès, Chiheb Trabelsi, Renato De Mori, and Yoshua Bengio. Quaternion recurrent neural networks. InICLR, 2018. [43] Xuanyu Zhu, Yi Xu, Hongteng Xu, and Changjian Chen. Quaternion convolutional neural networks. In ECCV, 2018. [44] Yi Tay, Aston Zhang, Anh Tuan Luu, Jinfeng Rao, Shuai Zhang, Shuohang Wang, Jie Fu, and Siu Cheung Hui. Lightweight and efﬁcient neural natural language processing with quaternion networks. In ACL, 2019. [45] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks.NeurIPS, 2020. [46] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When BERT Plays the Lottery, All Tickets Are Winning. In EMNLP, 2020. [47] Shrey Desai, Hongyuan Zhan, and Ahmed Aly. Evaluating lottery tickets under distributional shifts. In DeepLo, 2019. [48] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? arXiv preprint arXiv:2003.03033, 2020. [49] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. TACL, 2019. [50] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013. [51] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In IWP, 2005. [52] Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. InSemEval, 2017. [53] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. InNAACL, 2018. [54] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. InEMNLP, 2016. [55] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, 2005. 13[56] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second pascal recognising textual entailment challenge. Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. [57] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, 2007. [58] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The ﬁfth pascal recognizing textual entailment challenge. InTAC, 2009. [59] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alterna- tives: An evaluation of commonsense causal reasoning. InAAAI Spring Symposium Series, 2011. [60] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. Inproceedings of Sinn und Bedeutung, 2019. [61] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACL, 2018. [62] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018. [63] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In NAACL, 2019. [64] Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. InNAACL, 2019. [65] Karin Kipper Schuler. Verbnet: A broad-coverage, comprehensive verb lexicon.PhD Thesis, 2005. [66] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 1995. [67] Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davison, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, François Lagunas, Lysandre Debut, Morgan Funtowicz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cistac, Victor Muštar, Jeff Boudier, and Anna Tordjmann. Datasets. GitHub. Note: https://github.com/huggingface/datasets, 2020. 14Table 4: Selected learning rates for all methods. Method Learning rate T5BASE 3e−4 ADAPTER 3e−4 PFEIFFER -ADAPTER 3e−4 ADAPTER DROP 3e−4 ADAPTER -LOWRANK 3e−3 PROMPT TUNING -R 3e−2 PROMPT TUNING -T 3e−1 INTRINSIC -SAID 3e−2 BITFIT 3e−4 PHM-A DAPTER 3e−3 COMPACTER 3e−3 COMPACTER ++ 3e−3 Table 5: Selected learning rates for all methods, when we also ﬁne-tune the output layer. Method Learning rate ADAPTER 3e−3 PFEIFFER -ADAPTER 3e−4 ADAPTER DROP 3e−4 ADAPTER -LOWRANK 3e−3 BITFIT 3e−4 PHM-A DAPTER 3e−3 COMPACTER 3e−3 COMPACTER ++ 3e−3 A Experimental Details Datasets We run all experiments on the standardGLUE benchmark [18] with Creative Commons license (CC BY 4.0) and the SUPER GLUE benchmark [19]. These benchmark consist of multiple datasets: CoLA [49], SST-2 [50], MRPC [51], QQP9, STS-B [52], MNLI [53], QNLI [54], and RTE, which is a combination of data from RTE1 [ 55], RTE2 [56], RTE3 [57], RTE5 [58], COPA [59], CB [60], MultiRC [ 61], ReCoRD [ 62], BoolQ [ 63], and WiC [ 64] where sentences are selected from VerbNet [65], WordNet [66], and Wiktionary. We download all datasets from the HuggingFace Datasets library [67]. Low-resource ﬁne-tuning For the experiment conducted in §5.6, we set the number of epochs to 1000, 200, 100, 50, 25, for datasets subsampled to size 100, 500, 1000, 2000, and 4000 respectively. Based on our results, this is sufﬁcient to allow the models to converge. We save a checkpoint every 250 steps for all models and report the results for the hyper-parameters performing the best on the validation set for each task. Data pre-processing: Following Raffel et al.[3], we cast all datasets into a sequence-to-sequence format. We recast STS-B as a 21-class classiﬁcation task by rounding its target scores to their nearest increment of 0.2. Computing infrastructure: We run the experiments in Table 1, 2, 9, and 3 on one NVIDIA GEFORCE RTX 3090, and experiments in §5.6 on one GEFORCE GTX 1080 T I GPU. Training hyper-parameters: For the experiments onGLUE , we set the maximum sequence length to 128 and batch size to 100. Following Raffel et al.[3], we use maximum sequence length of 256 for the tasks in SUPER GLUE , and for ReCoRD, we set it to 512. We used batch size of 32 forSUPER GLUE , and for ReCoRD, we set it to 16 due to the GPU memory limit. For results in §5.6, we set the batch size to 40 to match the lower GPU memory ofGEFORCE GTX 1080 T I GPU. For setting the learning rates, we trained all methods with 3e−5, 3e−4, 3e−3, 3e−2, and 3e−1 and use the learning rate performing the best on the validation set for each method. Table 4 shows the ﬁnal selected learning rate for each method reported in Table 1. For the method variants where we also ﬁne-tune the ﬁnal output layer (Table 7), we report the selected learning rate in Table 5. We train all models with the AdamW optimizer from the HuggingFace library [28] with default hyper-parameters ofβ1 =0.9, β2 =0.999, ϵ=1e−8. We set warm-up steps to 500 for all methods in Table 1 and 7. We set the warm-up steps to 0 for all methods in Table 2 and 9, which based on our experiments, improved the results for all methods. 9https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 15Table 6: Performance of all methods on the tasks in GLUE for different values of hyper-parameters. For each method, we report the total number of parameters across all tasks and the number of parameters that are trained for each task as a multiple and proportion of T5BASE model [3]. For MNLI, we report accuracy on the matched validation set. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation coefﬁcients. For CoLA, we report Matthews correlation. For all other tasks, we report accuracy. Bold fonts indicate the best results in each block. Method #Totalparams Trainedparams /per taskCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg INTRINSIC-SAID (d′=0.4K) 1.001 0.0002%0.0 92.55 78.43/85.62 90.25/87.19 90.43/90.66 69.93 89.31 58.9975.76INTRINSIC-SAID (d′=1.4K) 1.001 0.0006%52.40 93.3589.22/92.4190.44/87.31 89.86/90.23 82.01 93.12 67.6384.36INTRINSIC-SAID (d′=2.5K) 1.001 0.0011%45.78 93.92 89.22/92.2090.43/87.3790.32/90.90 82.86 93.17 64.0383.65INTRINSIC-SAID (d′=10K) 1.001 0.0045%56.13 93.58 88.73/91.99 90.34/87.1890.63/90.9984.84 93.3671.22 85.36INTRINSIC-SAID (d′=20K) 1.001 0.0090%58.69 94.1588.24/91.78 90.28/87.13 90.06/90.4585.23 93.3970.50 85.45 PHM-ADAPTER(n=4) 1.018 0.239%59.21 93.69 87.25/90.91 90.23/86.99 90.55/90.73 85.9393.0469.78 85.30PHM-ADAPTER(n=8) 1.011 0.160%61.8493.58 91.18/93.5790.25/87.08 90.74/91.0785.74 92.93 70.5086.23PHM-ADAPTER(n=12) 1.013 0.179%57.3594.50 91.67/93.8690.25/87.05 90.45/90.8485.9792.9275.54 86.40 COMPACTER(n=4) 1.004 0.073%63.7593.00 89.22/92.3190.23/87.0390.31/90.74 85.61 92.8877.70 86.62COMPACTER(n=8) 1.004 0.073%61.7893.8190.20/93.1090.23/87.0390.16/90.4485.7893.08 74.1086.34COMPACTER(n=12) 1.004 0.073%61.38 93.6991.18/93.7190.11/86.8890.53/90.9885.7693.1270.50 86.17 COMPACTER++(n=4) 1.002 0.047%61.27 93.81 90.69/93.33 90.17/86.9390.46/90.93 85.71 93.08 74.8286.47COMPACTER++(n=8) 1.002 0.047%62.79 92.55 88.24/91.95 90.16/86.94 90.43/90.78 85.36 92.82 73.3885.95COMPACTER++(n=12) 1.002 0.048%63.01 93.92 91.18/93.75 90.23/87.0190.40/90.65 85.46 92.88 71.2286.34 B Impact of Hyper-parameters In this section, we study the impact of hyper-parameters for each method reported in Table 1. We report the results in Table 6. Impact of dimension (d′) on INTRINSIC -SAID Increasing the dimensiond′for INTRINSIC -SAID method often improves results. Though, as discussed in [14], d′is task-dependent so needs to be tuned for every new dataset to achieve optimal performance. Impact of non PHM-A DAPTER : Table 6 shows the results for varying values ofn= {4,8,12}. We experiment with adapters of bottleneck sized∈{24,48,96}. For the T5BASE model with k= 768, the condition kd>n 4 discussed in §4 is partially satisﬁed for d= 24and n= 4,8 and fully satisﬁed for d∈{48,96}and n= 4,8,12. Note that this condition is satisﬁed for larger versions of the T5 model, i.e., T5-large (770 million parameters,k=1024), T5-3B (2.8 billion parameters,k=1024), and T5-11B (11 billion parameters,k=1024) with adapter hidden size d∈{24,32,48,96}and n=2,4,8,12. Due to the huge computational costs of training these models, we could not run experiments on such a large scale. Nevertheless, we observe substantial parameter reduction using PHM-A DAPTER . In Table 6, we report the number of parameters ford= 24for all methods. Compared to ADAPTER , PHM-A DAPTER with n=8 reduces the parameters substantially by 5.2×. Impact of non COMPACTER : For COMPACTER and COMPACTER ++, we observe that the number of trainable parameters is almost constant across different values ofn. This is due to the fact that the number of trainable parameters in layernorms (LN) and biases (B) in each LPHM layer make up a high proportion of parameters for our methods. For instance forn= 4, for COMPACTER with 0.073% of trainable parameters, LN and B make up 28.49% and 23.51% respectively of its trainable parameters; for COMPACTER ++ with 0.047% of trainable parameters, LN and B make up 44.01% and 18.15% respectively of its parameters; while forPHM-A DAPTER with 0.239% of trainable parameters, LN and B make up only 8.63% and 7.12% respectively of its parameters. Consequently, simply removing biases from adapters, and exploring ideas of training language models without layer normalizations [35] can be promising directions on reducing parameters further, which we leave to future work. COMPACTER has more than an order of magnitude fewer parameters compared toADAPTER , with a parameter reduction at a remarkable rate of 11.4×. COMPACTER ++ even reduces the parameters further by 17.7×in total. 16C Results with Fine-tuning the Output Layer Table 7 shows the results for the methods in Table 1 with ﬁne-tuning the output layer. The parameters of the output layer dominate the parameters of each method and thus reduce the relative parameter savings. The standard adapter obtains the largest improvement in performance when ﬁne-tuning the output layer compared to the results in Table 1. In contrast, our proposed methods perform well with or without ﬁne-tuning the output layer. Table 7: Performance of all methods on the tasks in GLUE, where the output layer is tuned. For each method, we report the total number of parameters across all tasks and the percentage of parameters that are trained for each task as a multiple and proportion of T5BASE model [3]. For MNLI, we report accuracy on the matched validation set. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation coefﬁcients. For CoLA, we report Matthews correlation. For all other tasks, we report accuracy. Bold fonts indicate the best results in each block. Method #Totalparams Trainedparams /per taskCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg Baselines ADAPTER 1.065 11.89% 61.8094.1588.24/91.6790.27/87.05 91.51/91.7186.02 92.6476.26 86.48PFEIFFER-ADAPTER 1.032 11.49% 64.7693.58 87.75/91.58 90.16/87.17 91.21/91.50 86.1693.3073.38 86.41ADAPTERDROP 1.038 11.56% 61.67 93.69 84.80/89.20 90.14/87.17 90.92/91.3486.24 93.23 73.3885.62ADAPTER-LOWRANK 1.004 11.13% 62.82 93.81 88.73/91.99 90.34/87.19 90.51/90.58 85.81 92.93 74.8286.32BITFIT 1.010 11.19% 57.1394.15 89.71/92.7890.07/87.02 90.91/91.22 85.34 93.06 68.3585.43 Our Proposed Methods PHM-ADAPTER(n=4) 1.017 11.30% 62.79 93.5889.22/92.4190.23/87.01 90.61/90.8186.06 92.9575.54 86.47PHM-ADAPTER(n=8) 1.011 11.22% 61.2494.3888.73/91.99 90.28/87.08 90.53/90.98 85.9493.0373.38 86.14PHM-ADAPTER(n=12) 1.013 11.24% 65.2593.69 88.73/92.0490.34/87.16 90.75/90.8985.74 92.92 72.6686.38 COMPACTER(n=4) 1.004 11.13% 61.2793.58 88.24/91.67 90.25/87.0890.67/91.02 85.8292.92 73.3885.99COMPACTER(n=8) 1.004 11.13% 60.3193.8189.71/92.63 90.23/87.02 90.49/90.85 85.1993.0871.94 85.93COMPACTER(n=12) 1.004 11.13% 59.25 93.1291.18/93.75 90.31/87.1690.37/90.82 85.33 92.9775.54 86.35 COMPACTER++(n=4) 1.002 11.11% 64.28 94.2790.20/92.9690.23/87.04 90.27/90.61 85.80 92.9773.38 86.55COMPACTER++(n=8) 1.002 11.11% 63.78 93.5890.20/93.0190.19/87.02 90.12/90.56 85.57 92.84 70.5086.12COMPACTER++(n=12) 1.002 11.11% 62.05 93.23 87.75/91.58 90.19/86.97 90.08/90.48 85.52 92.7579.86 86.41 D Results on S UPER GLUE Table 8 shows the performance of our proposed methods onSUPER GLUE for different values ofn. We include the learning rate obtaining the best validation performance for all methods reported in Table 2 in Table 11. Table 8: Performance of our proposed methods on the tasks inSUPER GLUE for different values of n. For each method, we report the total number of parameters across all tasks and the percentage of parameters that are trained for each task as a multiple and proportion of T5BASE model [3]. For CB, we report accuracy and F1. For MultiRC, we report F1 over all answer-options (F1a) and exact match of each question’s set of answers (EM) [19]. For ReCoRD, we report F1 and EM scores. For all other tasks, we report accuracy. Bold fonts indicate the best results in each block. Method #Total params Trained params / per task BoolQ CB COPA MultiRC ReCoRD WiC Avg PHM-ADAPTER(n=4) 1.013 0.24% 80.31 85.71/73.5244.0 71.99/51.65 74.62/73.6067.40 69.20 PHM-ADAPTER(n=8) 1.008 0.160% 79.39 82.14/69.87 44.0 71.49/50.77 74.46/73.48 67.7168.15 PHM-ADAPTER(n=12) 1.009 0.179% 79.33 78.57/75.4352.0 70.48/50.66 74.14/73.1468.65 69.16 COMPACTER(n=4) 1.003 0.073% 79.88 89.29/82.51 42.0 71.87/51.98 74.64/73.5965.83 70.18 COMPACTER(n=8) 1.003 0.073% 79.57 85.71/80.0656.0 70.75/49.67 74.56/73.5770.85 71.19 COMPACTER(n=12) 1.003 0.073% 78.59 96.43/87.4448.0 70.80/49.67 74.49/73.54 65.2071.57 COMPACTER++(n=4) 1.002 0.047% 79.94 85.71/80.06 50.0 72.16/50.33 74.63/73.6068.34 70.53 COMPACTER++(n=8) 1.002 0.047% 78.23 82.14/70.87 48.0 71.61/51.43 74.62/73.6467.71 68.69 COMPACTER++(n=12) 1.002 0.048% 78.84 92.86/84.96 52.0 70.68/50.99 74.55/73.50 68.0371.82 17E Impact of Model Size Table 9 shows the results of methods using T5SMALL (60M parameters) on GLUE . For all adapter-based methods, we experiment with adapters of bottleneck size of {16,32,64}. For our methods, we experiment with n={4,8,16}. All parameter-efﬁcient ﬁne-tuning methods are performing worse than full ﬁne-tuning with this small model size. This is in contrast to the results of Table 1 and 2, where some parameter-efﬁcient ﬁne-tuning methods were able to perform on par or outperform full ﬁne-tuning with the larger model size of T5BASE (222M parameters). Among all methods, adapters, and our proposed methods perform the best. We report the learning rate performing the best on the validation set of each method in Table 10. Table 9: Performance of all methods on the tasks in GLUE. For each method, we report the total number of parameters across all tasks and the percentage of parameters that are trained for each task as a multiple and proportion of T5SMALL model [3]. For MNLI, we report accuracy on the matched validation set. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation coefﬁcients. For CoLA, we report Matthews correlation. For all other tasks, we report accuracy. Bold fonts indicate the best results in each block. We repeat the experiments marked with ∗multiple times for different seeds, but they were not successful. Method #Totalparams Trainedparams /per taskCoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE Avg Baselines T5SMALL 8×1 100% 46.90 91.7487.25/90.9790.07/86.6888.75/89.2082.20 90.59 65.4782.71ADAPTER 1.054 0.698% 36.88 90.8388.73/91.9388.09/84.06 88.98/89.34 80.50 89.75 62.5981.06ADAPTERDROP 1.009 0.139% 34.73 89.91 83.33/88.36 87.96/83.89 88.73/88.80 79.33 89.86 61.8779.71PFEIFFER-ADAPTER 1.027 0.363% 38.86 90.48 85.78/89.90 87.82/84.2689.24/89.5680.63 89.84 57.5580.36ADAPTER-LOWRANK 1.005 0.090% 40.55 90.60 84.80/89.20 88.01/83.98 88.04/88.27 79.92 89.95 61.1580.41PROMPTTUNING-R 1.007 0.085% 0.0∗ 86.35 68.14/81.05 87.48/83.91 87.35/87.87 76.27 88.49 50.3672.48PROMPTTUNING-T 1.007 0.085% 0.0∗ 79.59 71.08/82.18 87.76/83.55 87.48/87.76 74.65 89.02 57.5572.78BITFIT 1.015 0.190% 25.59 90.48 84.80/89.42 88.01/83.77 87.58/87.89 78.15 88.94 63.3178.90INTRINSIC-SAID 1.003 0.033% 0.0∗ 90.25 84.80/89.05 88.07/84.00 87.81/88.08 79.02 89.90 52.5275.77 Our Proposed Methods PHM-ADAPTER(n=4) 1.015 0.216% 40.08 90.6086.27/90.21 88.26/84.25 89.56/89.88 80.73 90.1060.43 80.94PHM-ADAPTER(n=8) 1.011 0.170% 37.85 90.48 82.84/87.72 88.08/84.07 89.07/89.46 80.68 89.64 61.8780.16PHM-ADAPTER(n=16) 1.031 0.414% 36.2790.8383.82/88.34 88.03/84.02 87.94/88.44 80.04 89.95 58.9979.70 COMPACTER(n=4) 1.005 0.090% 44.6589.45 84.80/89.20 88.00/83.96 88.19/88.47 79.54 89.66 64.0380.90COMPACTER(n=8) 1.005 0.091% 42.90 89.56 84.31/89.12 88.01/83.95 88.51/88.79 79.60 89.6866.19 80.97COMPACTER(n=16) 1.006 0.097% 40.12 89.22 85.29/89.86 88.08/84.06 89.28/89.60 79.87 89.71 59.7180.44 COMPACTER++(n=4) 1.003 0.059% 39.89 90.37 84.31/89.26 88.04/83.99 88.69/88.98 79.45 89.05 63.3180.49COMPACTER++(n=8) 1.003 0.059% 34.98 90.37 83.82/88.50 88.02/83.99 88.87/89.30 79.39 89.57 64.0380.08COMPACTER++(n=16) 1.003 0.065% 37.54 89.79 85.78/89.90 88.01/83.96 88.93/89.30 79.35 89.40 64.7580.61 Table 10: Selected learning rates for all methods with T5SMALL Method Learning rate T5SMALL 3e−4 ADAPTER 3e−3 PFEIFFER -ADAPTER 3e−4 ADAPTER DROP 3e−3 ADAPTER -LOWRANK 3e−3 PROMPT TUNING -R 3e−2 PROMPT TUNING -T 3e−1 INTRINSIC -SAID 3e−2 BITFIT 3e−3 PHM-A DAPTER 3e−3 COMPACTER 3e−3 COMPACTER ++ 3e−3 Table 11: Selected learning rates for all methods with T5BASE on SUPER GLUE. Method Learning rate T5BASE 3e−4 ADAPTER 3e−4 PFEIFFER -ADAPTER 3e−4 ADAPTER DROP 3e−4 ADAPTER -LOWRANK 3e−3 PROMPT TUNING -R 3e−2 PROMPT TUNING -T 3e−1 BITFIT 3e−4 INTRINSIC -SAID 3e−3 PHM-A DAPTER 3e−3 COMPACTER 3e−3 COMPACTER ++ 3e−3 18",
      "meta_data": {
        "arxiv_id": "2106.04647v2",
        "authors": [
          "Rabeeh Karimi Mahabadi",
          "James Henderson",
          "Sebastian Ruder"
        ],
        "published_date": "2021-06-08T19:17:04Z",
        "pdf_url": "https://arxiv.org/pdf/2106.04647v2.pdf",
        "github_url": "https://github.com/rabeehk/compacter"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes COMPACTER (Compact Adapter) layers, a parameter-efficient method for fine-tuning large-scale language models. COMPACTER builds on ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication (PHM) layers. It achieves an excellent trade-off between task performance and the number of trainable parameters by computing task-specific weight matrices as a sum of Kronecker products between shared 'slow' weights and 'fast' rank-one matrices defined per COMPACTER layer. This approach reduces parameter complexity from O(kd) for regular adapters to O(k+d). COMPACTER trains only 0.047% of a pretrained model's parameters (e.g., T5BASE) while performing on par with standard fine-tuning on GLUE and outperforming it on SuperGLUE and in low-resource settings. The work also provides a parameter complexity analysis and a systematic evaluation of various parameter-efficient fine-tuning methods.",
        "methodology": "COMPACTER integrates into a pretrained Transformer model by inserting adapter layers after the attention and feed-forward modules. Unlike standard adapters, COMPACTER replaces the down-projection (Dl) and up-projection (Ul) layers with Low-rank Parameterized Hypercomplex Multiplication (LPHM) layers. An LPHM layer's weight matrix (W) is computed as a sum of 'n' Kronecker products: W = SUM(Ai ⊗ Bi). Here, Ai are 'slow' weights shared across all adapter layers, capturing general information. Bi are adapter-specific 'fast' weights, which are parameterized as low-rank matrices (specifically, rank-one matrices where Bi = si * ti^T, with si and ti being rank-one weights). This allows for flexible sharing and significant parameter reduction. Only the adapter layers and layer normalizations are trained, while the pretrained model's parameters are fixed. A variant, COMPACTER++, keeps the COMPACTER layer only after the feed-forward layer in each transformer block.",
        "experimental_setup": "The underlying model used is the state-of-the-art encoder-decoder T5BASE (222M parameters) from HuggingFace PyTorch implementation, with some evaluations also on T5SMALL (60M parameters). Performance is evaluated on the GLUE and SUPERGLUE benchmarks, which cover various NLP tasks like paraphrase detection, sentiment classification, natural language inference, and question-answering. For validation, 1k samples are split from the training set, and original validation data is used as the test set. For datasets with less than 10k samples, the original validation set is halved for validation and testing. Models are fine-tuned for 3 epochs on large datasets and 20 epochs on low-resource datasets. Adapter bottleneck sizes are {96, 48, 24}, and COMPACTER's 'n' hyperparameter is set to {4, 8, 12}. The AdamW optimizer is used, with learning rates tuned from {3e-5, 3e-4, 3e-3, 3e-2, 3e-1}. The output layer of the pretrained model is frozen by default. Efficiency is evaluated by training for 1 epoch on the MNLI dataset, measuring trained parameters, average peak memory usage (fixed GPU budget of 24 GB), and training time per epoch.",
        "limitations": "The work notes that sharing all adapter parameters across layers, as in some prior methods, is too restrictive and underperforms. Prompt tuning methods exhibit high sensitivity to initialization and learning rate, often lagging behind fine-tuning, and require very large models to perform well, with increasing sequence length leading to memory overhead. INTRINSIC-SAID suffers from substantial memory overhead due to storing large random projection matrices and is very slow to train, making it intractable for large PLMs despite theoretical efficiency. For smaller T5 models (T5SMALL), parameter-efficient fine-tuning methods, including COMPACTER, perform worse than full fine-tuning. Additionally, a significant proportion of trainable parameters in COMPACTER and COMPACTER++ still come from layer normalizations and biases within the LPHM layers.",
        "future_research_directions": "Potential future research directions include combining COMPACTER with contextual parameter generation to create adapter modules, exploring adaptation without updating layer normalization by leveraging recent advances in normalization-free models, and applying general techniques like AdapterDrop to COMPACTER for further efficiency improvements. The authors also suggest investigating higher rank 'r' values for the Bi matrices in LPHM beyond r=1 for more complex tasks or overcomplete knowledge distillation, and further reducing parameters by exploring methods to remove biases from adapters.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom typing import Union, Optional\nimport torch.nn.functional as F\n\nfrom .inits import glorot_uniform, glorot_normal\nfrom .kronecker import kronecker_product, kronecker_product_einsum_batched\n\ndef matvec_product(W: torch.Tensor, x: torch.Tensor,\n                       bias: Optional[torch.Tensor],\n                       phm_rule: Union[torch.Tensor],\n                       kronecker_prod=False) -> torch.Tensor:\n    \"\"\"\n    Functional method to compute the generalized matrix-vector product based on the paper\n    \"Parameterization of Hypercomplex Multiplications (2020)\"\n    https://openreview.net/forum?id=rcQdycl0zyk\n    y = Hx + b , where W is generated through the sum of kronecker products from the Parameterlist W, i.e.\n    W is a an order-3 tensor of size (phm_dim, in_features, out_features)\n    x has shape (batch_size, phm_dim*in_features)\n    phm_rule is an order-3 tensor of shape (phm_dim, phm_dim, phm_dim)\n    H = sum_{i=0}^{d} mul_rule \\otimes W[i], where \\otimes is the kronecker product\n    \"\"\"\n    if kronecker_prod:\n       H = kronecker_product(phm_rule, W).sum(0)\n    else: \n       H = kronecker_product_einsum_batched(phm_rule, W).sum(0)\n\n    y = torch.matmul(input=x, other=H)\n    if bias is not None:\n        y += bias\n    return y\n\n\nclass PHMLinear(torch.nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 phm_dim: int,\n                 phm_rule: Union[None, torch.Tensor] = None,\n                 bias: bool = True,\n                 w_init: str = \"phm\",\n                 c_init: str = \"random\",\n                 learn_phm: bool = True,\n                 shared_phm_rule=False,\n                 factorized_phm=False,\n                 shared_W_phm=False,\n                 factorized_phm_rule=False,\n                 phm_rank = 1,\n                 phm_init_range=0.0001,\n                 kronecker_prod=False) -> None:\n        super(PHMLinear, self).__init__()\n        assert w_init in [\"phm\", \"glorot-normal\", \"glorot-uniform\", \"normal\"]\n        assert c_init in [\"normal\", \"uniform\"]\n        assert in_features % phm_dim == 0, f\"Argument `in_features`={in_features} is not divisble be `phm_dim`{phm_dim}\"\n        assert out_features % phm_dim == 0, f\"Argument `out_features`={out_features} is not divisble be `phm_dim`{phm_dim}\"\n        self.in_features = in_features\n        self.out_features = out_features\n        self.learn_phm = learn_phm\n        self.phm_dim = phm_dim\n        self._in_feats_per_axis = in_features // phm_dim\n        self._out_feats_per_axis = out_features // phm_dim\n        self.phm_rank = phm_rank\n        self.phm_init_range = phm_init_range\n        self.kronecker_prod=kronecker_prod\n        self.shared_phm_rule = shared_phm_rule\n        self.factorized_phm_rule = factorized_phm_rule \n        if not self.shared_phm_rule:\n            if self.factorized_phm_rule:\n                self.phm_rule_left = nn.Parameter(torch.FloatTensor(phm_dim, phm_dim, 1),\n                       requires_grad=learn_phm)\n                self.phm_rule_right = nn.Parameter(torch.FloatTensor(phm_dim, 1, phm_dim),\n                       requires_grad=learn_phm)\n            else:\n                self.phm_rule = nn.Parameter(torch.FloatTensor(phm_dim, phm_dim, phm_dim), \n                       requires_grad=learn_phm)\n        self.bias_flag = bias\n        self.w_init = w_init\n        self.c_init = c_init\n        self.shared_W_phm = shared_W_phm \n        self.factorized_phm = factorized_phm\n        if not self.shared_W_phm:\n            if self.factorized_phm:\n                self.W_left = nn.Parameter(torch.Tensor(size=(phm_dim, self._in_feats_per_axis, self.phm_rank)),\n                              requires_grad=True)\n                self.W_right = nn.Parameter(torch.Tensor(size=(phm_dim, self.phm_rank, self._out_feats_per_axis)),\n                              requires_grad=True)\n            else:\n                self.W = nn.Parameter(torch.Tensor(size=(phm_dim, self._in_feats_per_axis, self._out_feats_per_axis)),\n                              requires_grad=True)\n        if self.bias_flag:\n            self.b = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\"b\", None)\n        self.reset_parameters()\n\n    def init_W(self):\n        if self.w_init == \"glorot-normal\":\n            if self.factorized_phm:\n                for i in range(self.phm_dim):\n                    self.W_left.data[i] = glorot_normal(self.W_left.data[i])\n                    self.W_right.data[i] = glorot_normal(self.W_right.data[i])\n            else:\n                for i in range(self.phm_dim):\n                    self.W.data[i] = glorot_normal(self.W.data[i])\n        elif self.w_init == \"glorot-uniform\":\n            if self.factorized_phm:\n                for i in range(self.phm_dim):\n                    self.W_left.data[i] = glorot_uniform(self.W_left.data[i])\n                    self.W_right.data[i] = glorot_uniform(self.W_right.data[i])\n            else:\n                for i in range(self.phm_dim):\n                    self.W.data[i] = glorot_uniform(self.W.data[i])\n        elif self.w_init == \"normal\":\n            if self.factorized_phm:\n                for i in range(self.phm_dim):\n                    self.W_left.data[i].normal_(mean=0, std=self.phm_init_range)\n                    self.W_right.data[i].normal_(mean=0, std=self.phm_init_range)\n            else:\n                for i in range(self.phm_dim):\n                    self.W.data[i].normal_(mean=0, std=self.phm_init_range)\n        else:\n            raise ValueError\n\n    def reset_parameters(self):\n        if not self.shared_W_phm:\n           self.init_W()\n\n        if self.bias_flag:\n            self.b.data = torch.zeros_like(self.b.data)\n\n        if not self.shared_phm_rule:\n            if self.factorized_phm_rule:\n                if self.c_init == \"uniform\":\n                   self.phm_rule_left.data.uniform_(-0.01, 0.01)\n                   self.phm_rule_right.data.uniform_(-0.01, 0.01)\n                elif self.c_init == \"normal\":\n                   self.phm_rule_left.data.normal_(std=0.01)\n                   self.phm_rule_right.data.normal_(std=0.01)\n                else:\n                   raise NotImplementedError\n            else:\n                if self.c_init == \"uniform\":\n                   self.phm_rule.data.uniform_(-0.01, 0.01)\n                elif self.c_init == \"normal\":\n                   self.phm_rule.data.normal_(mean=0, std=0.01)\n                else:\n                   raise NotImplementedError\n\n    def set_phm_rule(self, phm_rule=None, phm_rule_left=None, phm_rule_right=None):\n        \"\"\"If factorized_phm_rules is set, phm_rule is a tuple, showing the left and right\n        phm rules, and if this is not set, this is showing  the phm_rule.\"\"\"\n        if self.factorized_phm_rule:\n            self.phm_rule_left = phm_rule_left\n            self.phm_rule_right = phm_rule_right \n        else:\n            self.phm_rule = phm_rule \n \n    def set_W(self, W=None, W_left=None, W_right=None):\n        if self.factorized_phm:\n            self.W_left = W_left\n            self.W_right = W_right\n        else:\n            self.W = W\n\n    def forward(self, x: torch.Tensor, phm_rule: Union[None, nn.ParameterList] = None) -> torch.Tensor:\n        if self.factorized_phm:\n           W = torch.bmm(self.W_left, self.W_right)\n        if self.factorized_phm_rule:\n           phm_rule = torch.bmm(self.phm_rule_left, self.phm_rule_right)\n        return matvec_product(\n               W=W if self.factorized_phm else self.W,\n               x=x,\n               bias=self.b,\n               phm_rule=phm_rule if self.factorized_phm_rule else self.phm_rule, \n               kronecker_prod=self.kronecker_prod)\n",
        "experimental_info": "{\n  \"train_task_adapters\": true,\n  \"add_adapter_in_feed_forward\": true,\n  \"add_adapter_in_self_attention\": true,\n  \"hypercomplex_adapters\": true,\n  \"hypercomplex_division\": 8,\n  \"learn_phm\": true,\n  \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n  \"shared_phm_rule\": true,\n  \"factorized_phm\": true,\n  \"shared_W_phm\": false,\n  \"factorized_phm_rule\": false,\n  \"phm_c_init\": \"normal\",\n  \"phm_rank\": 1,\n  \"phm_init_range\": 0.01,\n  \"add_layer_norm_before_adapter\": false,\n  \"add_layer_norm_after_adapter\": true,\n  \"non_linearity\": \"swish\",\n  \"task_reduction_factor\": 16,\n  \"unfreeze_lm_head\": false,\n  \"unfreeze_layer_norms\": true\n}"
      }
    },
    {
      "title": "The Expressive Power of Low-Rank Adaptation",
      "abstract": "Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that\nleverages low-rank adaptation of weight matrices, has emerged as a prevalent\ntechnique for fine-tuning pre-trained models such as large language models and\ndiffusion models. Despite its huge success in practice, the theoretical\nunderpinnings of LoRA have largely remained unexplored. This paper takes the\nfirst step to bridge this gap by theoretically analyzing the expressive power\nof LoRA. We prove that, for fully connected neural networks, LoRA can adapt any\nmodel $f$ to accurately represent any smaller target model $\\overline{f}$ if\nLoRA-rank $\\geq(\\text{width of }f) \\times \\frac{\\text{depth of\n}\\overline{f}}{\\text{depth of }f}$. We also quantify the approximation error\nwhen LoRA-rank is lower than the threshold. For Transformer networks, we show\nany model can be adapted to a target model of the same size with\nrank-$(\\frac{\\text{embedding size}}{2})$ LoRA adapters.",
      "full_text": "Published as a conference paper at ICLR 2024 THE EXPRESSIVE POWER OF LOW-RANK ADAPTATION Yuchen Zeng Department of Computer Science University of Wisconsin-Madison yzeng58@wisc.edu Kangwook Lee Department of Electrical and Computer Engineering University of Wisconsin-Madison kangwook.lee@wisc.edu ABSTRACT Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent tech- nique for fine-tuning pre-trained models such as large language models and diffu- sion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f if LoRA-rank ≥ (width of f) × depth of f depth of f , under a mild assumption. We quantify the approximation error when the LoRA- rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank- (embedding size 2 ) LoRA adapters. Our study reveals numerous theoretical insights on hyperparameter tun- ing and algorithm development for LoRA, all of which are empirically validated. 1 I NTRODUCTION Recent foundation models, such as large language models (OpenAI, 2023; Liu et al., 2019; Touvron et al., 2023), have achieved remarkable success in a wide range of applications. Due to their sub- stantial size, the standard full fine-tuning approach—where all the model’s parameters are updated for specialized tasks—is becoming increasingly difficult and inefficient. This leads to the growing popularity of parameter-efficient fine-tuning approaches (Hu et al., 2022a; Liu et al., 2022b; Ben Za- ken et al., 2022; Hu et al., 2022b). Instead of updating all parameters, these approaches selectively update smaller subsets of weights or introduce lightweight adapters, thereby greatly decreasing the computational and storage costs. The most dominant approach along this line is Low-Rank Adaptation (LoRA) (Hu et al., 2022a), which employs lightweight low-rank adapters to pre-trained weight matrices. Far from merely en- hancing computational efficiency, empirical evidence has shown that LoRA can match or even ex- ceed the performance of full fine-tuning (Hu et al., 2022a). To date, LoRA has been widely used and achieved considerable success in adapting large language models (Hu et al., 2022a; Dinh et al., 2022b) and image generation models (Ryu, 2023; Fan et al., 2023) for various downstream tasks. Despite the empirical success of LoRA, little is known in theory about how it works. A notable exception (Malladi et al., 2023) showed that LoRA finetuning is approximately equivalent to full fine-tuning in the lazy regime. However, many theoretical questions remain open, such as: What is the minimum rank of the LoRA adapters required to adapt a (pre-trained) model f to match the functionality of the target model f? How does the model architecture (i.e., depth, width) affect the minimal rank? If the adapter rank is lower than this threshold, what is the resulting approximation error? Answering such questions will provide important theoretical insights into when and why LoRA achieves effective adaptation. Our Contributions. In this paper, we present the first set of theoretical results that character- ize the expressive power of Low-Rank Adaptation (LoRA) for Fully Connected Neural Networks (FNN) and Transformer Networks (TFN). In particular, we identify the necessary LoRA-rank for adapting a frozen model to exactly match a target model. For FNN cases, we also establish the required LoRA-rank for closely approximating the target model when a small approximation error is allowed. Our work focuses solely on the expressive power of the model with low-rank adapters, i.e., we show that under which conditions, effective low-rank adapters exist for the given adaptation task. This excludes other aspects such as optimization and generalization. We now present the essence of our main theoretical findings in the following informal statement. 1 arXiv:2310.17513v3  [cs.LG]  18 Mar 2024Published as a conference paper at ICLR 2024 Theorem 1 (Informal). Let f be a target FNN and f0 be an arbitrary frozen FNN. Under mild conditions on ranks and network architectures, there exist low-rank adapters such that a low-rank adapted version of f0 is exactly equal to f. We present the detailed formulations of Theorem 1 under two scenarios: (i) applying a uni- form rank across all LoRA adapters, as detailed in Theorem 3 and the specialized instance Corollary 4 for randomly drawn frozen and target models; and (ii) allowing different ranks ap- plied to each LoRA adapter, as described in Theorem 6. To the best of our knowledge, this is the first known theoretical results on the expressive power of LoRA. While this informal theorem is for exact approximation, we also derive the approximation bounds as well, i.e., we characterize the approximation error between the finetuned model and the target model as a function of the LoRA- rank, as provided in Theorem 5 for the uniform LoRA-rank scenario and Theorem 6 for general cases. Furthermore, within the same framework, we investigate the expressive power of tuning the final layers for randomly generated frozen models, as described in Lemma 4. This result allows us to contrast LoRA and final layer tuning, thereby providing insights for future algorithm development. We summarize our main findings on TFN in the following informal theorem. Theorem 2 (Informal). Let f be the target TFN and f0 be the frozen TFN. Under mild conditions on ranks and network architectures, there exist low-rank adapters for attention weight matrices such that a low-rank adapted version of f0 is exactly equal to f. The formal statement of Theorem 2 is provided in Theorem 7, with a specialized version in Corol- lary 10 tailored for randomly generated frozen and target models. In Sec. 5 and G, we perform experiments on both synthetic and real datasets to substantiate our theoretical results, demonstrating the practical applicability of our theoretical findings in algorithm development and hyperparameter tuning. 1.1 R ELATED WORKS Expressive Power of Neural Networks Theoretical study of the expressive power of unfrozen neural networks has progressed since the first universal approximation theorem (Hornik et al., 1989), showing that sufficient network width and depth can guarantee function approximation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017). Many recent studies obtained sim- ilar results for deep neural networks with modern twists such as ReLU activations and Transformer networks (Yun et al., 2020a; Raghu et al., 2017; Telgarsky, 2016; 2015; Bietti & Bach, 2021; Oy- mak et al., 2023; Lee et al., 2017; Shen & Zhang, 2020; Likhosherstov et al., 2021; Hsu et al., 2021; Park et al., 2021; Yun et al., 2020b; Giannou et al., 2023b). Metrics like Vapnik-Chervonenkis and Rademacher complexities (Vapnik & Chervonenkis, 2015; Bartlett & Mendelson, 2001) assess clas- sification capacity. However, these theories cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods. Expressive Power of Adaptation Methods In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the ex- pressive power of adaptation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target net- work that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method. A more detailed discussion of related works is provided in Sec. B. 1.2 N OTATIONS Define [N] := {1, 2, . . . , N}. Let the operators ∧ and ∨ denote the minimum function and the maximum function, respectively. We useI to represent the identity matrix. 2Published as a conference paper at ICLR 2024 For a sequence ofL matrices (Wl)L l=1, we simplify the product of these matricesWLWL−1 ··· W1 as QL l=1 Wl, with matrices multiplied in descending order from WL to W1. When m > n, we define Pn i=m ai = 0 and Qn i=m ai = 1 for scalars (ai)n i=m, and Pn i=m Wi = O and Qn i=m Wi = I for square matrices (Wi)n i=m. Singular Value Decomposition (SVD) of the matrix W can be expressed as W = UDV ⊤, where U, V ∈ RD×D are orthonormal matrices and D ∈ RD×D is a diagonal matrix. The singular values, sorted in descending sequence, are represented on the diagonal of D, denoted as σ1(W) ≥ σ2(W) ≥ ··· ≥σD(W) ≥ 0, where σd(W) denotes the d-th largest singular value for all d ∈ [D]. When d > D, σd(W) is defined as zero. The best rank- r approximation (in the Frobenius norm or the 2-norm) of W is Pr i=1 σiuivT i , where ui and vi are the i-th column of U and V , respectively (Eckart & Young, 1936; Mirsky, 1960). We denote this best rank- r approximation by LRr(W), where LR is a shorthand for “Low-Rank”. When r ≥ rank(W), it is clear that LRr(W) = W. Occasionally, the subscript r may be omitted to indicate a general low-rank approximation without specifying the rank. 2 W ARM UP : E XPRESSIVE POWER OF LINEAR MODELS WITH LORA Before delving into the expressive power of LoRA for FNN and TFN, we begin by investigating the simplest scenario: both the target model f and the frozen model f0 are linear, i.e., Target Model f(x) = Wx, Frozen Model f0(x) = WL ··· W1x = \u0010QL l=1 Wl \u0011 x. This problem serves as a simplified version of approximating a target FNN, where the target modelf has a single layer, the frozen modelf0 has L layers, all bias vectors in both two models are zero, and the activation functions are linear. Throughout this paper, for the sake of simplicity, we will assume that both models have the same number of neurons in each layer, i.e., W, W1, . . . ,WL ∈ RD×D. Nevertheless, our results are readily extendable to situations where the frozen model is wider than the target model, which is a more natural setting as the frozen models are often overparameterized to ensure high capacity and good performance across diverse tasks in practice. See the discussion in Sec. H for more details. The objective here is to incorporate low-rank adapters into the frozen model so that the adapted model can effectively approximate the target model. Unless otherwise specified, we always consider a uniform LoRA-rank for all low-rank adapters throughout this paper. For a given LoRA-rank R ∈ [D], we apply LoRA adapters ∆W1, . . . ,∆WL to the frozen model, and the adapted model can be represented as Adapted Model f(x) = (WL + ∆WL) ··· (W1 + ∆W1)x, where rank(∆Wl) ≤ R for all l ∈ [L]. Since the frozen model and adpated model are all linear, we can focus on quantifying the discrepancy between the linear coefficients, i.e.,QL l=1(Wl+∆Wl)−W. In the subsequent lemma, we establish the minimal achievable norm, and identify the smallest LoRA-rank required for the adapted model to exactly represent the target model, i.e., f = f, under a non-singularity assumption. We will demonstrate in Sec. 3.3 that this non-singularity assumption is mild, as it can be satisfied even by randomly generated weight matrices. Lemma 1. Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rank R ∈ [D], assume that all the weight matrices of the frozen model (Wl)L l=1, andQL l=1 Wl + LRr(E) are non-singular for all r ≤ R(L − 1). Then, we have the following: min ∆Wl:rank(∆Wl)≤R \r\r\rQL l=1(Wl + ∆Wl) − W \r\r\r 2 = σRL+1(E). Thus, when R ≥ ⌈RE L ⌉, the optimal solution satisfies QL l=1(Wl + ∆Wl) = W, implying f = f. Proof Sketch. We start the proof by noting that the distance between the adapted and target models\r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = \r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! −   W − LY l=1 Wl !\r\r\r\r\r 2 . 3Published as a conference paper at ICLR 2024 The remaining proof aims to minimize the right-hand side under the constraint rank(∆Wl) ≤ R for all l ∈ [L]. The basic idea here is to match QL l=1(Wl + ∆Wl) − QL l=1 Wl with the best rank-r approximation of W − QL l=1 Wl. The key steps to solve this problem are as follows. 1. Demonstrate that QL l=1(Wl + ∆Wl) − QL l=1 Wl can be decomposed into L terms: QL l=1(Wl + ∆Wl) − QL l=1 Wl = PL l=1 \u0010QL i=l+1 Wi \u0011 ∆Wl \u0010Ql−1 i=1(Wi + ∆Wi) \u0011 . Since rank(∆Wl) ≤ R, it follows that rank \u0010QL l=1(Wl + ∆Wl) − QL l=1 Wl \u0011 ≤ RL. 2. Consider the rank- RL approximation of W − QL l=1 Wl. Decompose this low-rank approxima- tion into L terms PL l=1 El such that rank(El) ≤ R, where El’s will be determined later. 3. To match QL l=1(Wl + ∆ Wl) − QL l=1 Wl with the rank- RL approximation of W −QL l=1 Wl, we let \u0010QL i=l+1 Wi \u0011 ∆Wl \u0010Ql−1 i=1(Wi + ∆Wi) \u0011 = El by choosing ∆Wl = \u0010QL i=l+1 Wi \u0011−1 El \u0010Ql−1 i=1(Wi + ∆Wi) \u0011−1 . 4. Select appropriate (El)L l=1 such that Wi + ∆Wi are invertible for i ∈ [L]. The complete proof and the explicit construction of optimal LoRA adapters, are detailed in Sec. D. In fact, this lemma delivers a crucial insight. When we consider L = 1 and R = D, the lemma becomes strikingly similar to the Eckart–Young–Mirsky theorem (Eckart & Young, 1936; Mirsky, 1960). However, there is a significant difference from the classical theorem on the optimal low-rank approximation, which involves a single target matrix and a single matrix as an optimization variable. Our lemma demonstrates that a comparable result can be achieved for a “product of matrices,” where each matrix is optimized subject to a low-rank constraint. That being said, even though each matrix is constrained by a low rank, the “effective rank” is the sum of these low ranks, i.e., in this scenario, is LR. Consequently, once the low-rank adapters are optimally configured, one can make the product equal to the best rank LR-approximation of the target matrix. This can be viewed as an extension of the matrix approximation theorem to a product of matrices, each subject to low-rank constraints. Our main theoretical results on the expressive power of LoRA, which we will present in the subsequent sections, will build upon this core matrix approximation result. 3 E XPRESSIVE POWER OF FNN S WITH LORA 3.1 P ROBLEM SETTING We use FNNL,D(·; (Wl)L l=1, (bl)L l=1) to denote a L-layer width- D fully connected ReLU neural network with weight matrices Wl ∈ RD×D and biases bl ∈ RD, where l ∈ [L]. The target FNN f and frozen FNN f0 can be represented as follows: Target FNN f := FNNL,D(·; (Wl)L l=1, (bl)L l=1), Frozen FNN f0 := FNNL,D(·; (Wl)L l=1, (bl)L l=1), where Wl ∈ RD×D and bl ∈ RD represent the weight matrix and bias vector for the l-th layer of the target model f, respectively. Likewise, Wl ∈ RD×D, bl ∈ RD are those for f0, for layer l ∈ [L]. Given a specified LoRA-rank R ∈ [D], we adapt the frozen FNN f0 into a new model f via LoRA. The adapted model f is defined as Adapted FNN f := FNNL,D(·; (Wl + ∆Wl)L l=1, (bbl)L l=1), where the weight matrix for the low-rank adapter∆Wl ∈ RD×D satisfies specified rank constraints, updated bias vector bbl ∈ RD for l ∈ [L]1. As noted in Sec. 2, it is common for the pretrained model to be larger than necessary. Therefore, we focus on a setting where the frozen model is deeper than the target model, i.e.,L ≥ L. Furthermore, in this section, we let the input space X ∈RD×D be bounded. 1We consider the case where the bias parameters can also be updated, as suggested by Hu et al. (2022a). Experiments investigating the impact of updating bias parameters are presented in Sec. G.5. 4Published as a conference paper at ICLR 2024 3.2 O NE-LAYER RELU FNN A PPROXIMATION We start with investigating the expressive power of LoRA on one-layer FNN. In this setting, our aim is to identify LoRA adapters (∆Wl)L l=1 and bias vectors (bbl)L l=1 such that the adapted model ReLU((WL + ∆WL) · ReLU((WL−1 + ∆WL−1) · ReLU(··· ) + bbL−1) + bbL) closely approximates the target one-layer ReLU FNN model ReLU(W1 · +b1). This differs from the setting described in Sec. 2, where a multi-layer FNN with linear activation functions and zero biases was used to approximate a one-layer FNN with the same properties. In the current setting, we introduce non-linearity through the use of ReLU activation functions in the frozen model and also take biases into account. Consequently, to generalize the findings to this new setting, addressing the introduced non-linearity due to the ReLU activation functions in the frozen model is the main challenge. We employ the following two steps to extend the results in Sec. 2 to the current setting. 1. (Linearization) We eliminate the nonlinearity in the first L −1 layers of the adapted model, mak- ing it equivalent to a one-layer ReLU FNN. This can be readily achieved by choosing sufficiently large bias vectors for the first L − 1 layers to ensure that all ReLUs in these layers are activated. This technique of eliminating non-linearity is inspired by Giannou et al. (2023a). 2. (Weight Matrix Alignment) We update the bias vectors of the last layer bbL to align with that of the target model f, and apply the linear model approximation results (i.e., Lemma 1) to identify the low-rank adapters that match the weight matrix f. Following the steps above, we arrive at the subsequent lemma, which demonstrates that any one- layer FNN can be closely approximated by a multi-layer FNN finetuned via LoRA. The complete proof is provided in Sec. E.1. Lemma 2. Define error matrixE := W1 −QL l=1 Wl, with its rank represented byRE = rank(E). Consider a LoRA-rank R ∈ [D]. Assume that the weight matrices W1, . . . ,WL ∈ RD×D andQL l=1 Wl + LRr(E) for all r ≤ R(L − 1) are non-singular. Let x be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that the expected squared error can be bounded as E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 RL+1(E). Moreover, whenR ≥ ⌈RE L ⌉, we have f(x) = f(x) for all x ∈ X. 3.3 M ULTI -LAYER RELU FNN A PPROXIMATION We now generalize our discussion to the approximation of multi-layer ReLU FNNs. The key strategy for extending the results to approximating multi-layer ReLU FNNs under LoRA is model partition, inspired from Giannou et al. (2023a). To elucidate this, we start with a specific example. Example 1. Consider the case where L = 2 and L = 4. We view a two-layer target model f as a composition of two one-layer ReLU FNNs. Accordingly, we partition the four-layer adapted modelf into two submodels, each consisting of two layers. For each layer in the target model, we utilize two corresponding layers in the frozen/adapted model for approximation. This problem then simplifies into a one-layer FNN approximation problem, which has already been addressed in Lemma 2. Based on this example, we introduce a ordered partitionP = {P1, . . . , PL} to partition the layers in the adapted model f, where SL i=1 Pi = [L]. Each element Pi ∈ Pconsists of consecutive integers. Given a partition P, each element Pi specifies that the layers with indexl ∈ Pi in the adapted model will be used to approximate the i-th layer in the target model. Example 1, which uses every two layers in the adapted model to approximate each layer in the target model, can be considered as a partition represented as {{1, 2}, {3, 4}}. Similarly, we extend this simple uniform partition into general cases for L-layer target FNN and L-layer frozen FNN: Pu = \b Pu 1 , . . . , Pu L \t := \b {1, . . . , M}, {M + 1, . . . ,2M}, . . . , \b (L − 1)M + 1, . . . , L \t\t , 5Published as a conference paper at ICLR 2024 where M := ⌊L/L⌋. The uniform partition indicates that every M layers in the adapted model are employed to approximate each layer in the target model. We useQ l∈Pi Wl to denote the product of the weight matrices from the layers l ∈ Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, Q l∈Pu 1 Wl = QM l=1 Wl = WM ··· W1. We first extend Lemma 2 to multi-layer FNN approximation setting using this uniform partition. Uniform Model Partition. Given a specified LoRA-rank R ∈ [D], to derive our results, we intro- duce a mild non-singularity assumption on the weight matrices of the target model and frozen model for the feasibility of our analysis. This assumption is mild, supported by Lemma 3 that even weight matrices initialized at random can meet this requirement. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R ∈ [D], the weight matrices of the frozen model (Wl)L l=1 and matrices \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) are non-singular for all r ≤ R(M − 1) and i ∈ [L]. Lemma 3. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds ∀R ∈ [D]. Given this assumption, here we present our first main result, which shows that any frozen FNN can be adapted to exactly approximate the target FNN via LoRA. Theorem 3. Under Assumption 1, if LoRA-rank R ≥ ⌈maxi∈[L] rank(Wi −Q l∈Pu i Wl)/M⌉, then there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that the low-rank adapted modelf can exactly approximate the target modelf, i.e., f(x) = f(x), ∀x ∈ X. Moreover, combining Lemma 3 and Theorem 3 gives the following corollary. Corollary 4. Assume that the elements of (Wl)L l=1, (Wl)L l=1 are independently drawn from ar- bitrary continuous distributions. When R ≥ D/M, with probability 1, there exists rank- R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that low-rank adapted model f can exactly approximate the target model f on X, i.e., f(x) = f(x), ∀x ∈ X. To understand the implications of this corollary, let us considerL ≫ L. In this scenario, the required LoRA-rank is sufficiently small such that the dimension of the rank-R matrix is approximately2RD. This corollary suggests that with2RDL ≥ 2D2L/M ≈ 2D2L learnable parameters, even a random FNN can be adapted into the target model f. It is noteworthy that the total number of parameters of the target model is D2L. This indicates that even though the learnable parameters under LoRA finetuning appear to be highly constrained (low-rank constrained learnable parameters distributed across many layers), the effective expressive power of LoRA is nearly optimal up to a constant factor of 2. Our discovery provides the first theoretical insights into the practical success of LoRA. Furthermore, Theorem 3 indicates that if the model f is ‘close’ to f such that maxi∈[L] rank(Wi −Q l∈Pu i Wl) is small, the number of learnable parameters used by LoRA can be lower than D2L. Meanwhile, when the employed LoRA-rank is lower than the critical threshold, the following theo- rem provides an upper bound for the approximation error. Theorem 5. Define the approximation error of i-th layer as Ei = σRM+1(Wi −Q l∈Pu i Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Under Assumption 1, there exists rank- R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD such that for input x ∈ Xwith Exx⊤ = Σ, E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Theorem 5 provides an upper bound on the approximation error for the adapted model. This bound is influenced by several factors: (i) magnitude of the target model’s parameters and the input, which 6Published as a conference paper at ICLR 2024 is captured by β and \r\rWk \r\r F, (ii) the rank of the adapter R and the discrepancy between the frozen model and the target model (Wi −Q l∈Pu i Wl)L i=1, both of which contribute to the termEi, (iii) the depth of the frozen model L, reflected in M and consequenly Ei. All the proofs of the results derived for uniform partition are provided in Sec. E.2. General Model Partition. We note that employing this uniform partition strategy for approxi- mating the target model may not always yield optimal results. To illustrate this, we revisit the case considered by Example 1, where L = 2 and L = 4. Consider a scenario where the first layer of the frozen model has been pretrained to match the first layer of the target model. In this case, we can use just the first layer in f to approximate the first layer in f, and a zero LoRA-rank is sufficient for the exact representation of the first layer. The remaining three layers in f can then be used to approximate the second layer in f. Compared to uniform partition, this partition leverages more layers to approximate the second layer in f, allowing us to achieve the desired performance with a lower LoRA-rank, as per Lemma 2. This suggests that our approximation error bounds could be further optimized by considering partitioning schemes tailored to specific scenarios. We now extend our results to a more general setting, where we do not assume a uniform parti- tion. Concurrently, recent research by Zhang et al. (2023) has shown that the application of varying LoRA-ranks leads to improved results. Consequently, we permit each layer in the frozen model to utilize adapters with different LoRA-ranks. The rank of the LoRA adapter associated with the l-th layer in the frozen model is denoted by Rl, where l ∈ [L]. This result relies on Assumption 2, an analog of Assumption 1, but revised to include a general model partition. More details, including the proofs, are provided in Sec. E.3. Theorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If P l∈Pi Rl ≥ rank(Wi − Q l∈Pi Wl) for all i ∈ [L], there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = σP l∈Pi Rl+1(Wi −Q l∈Pi Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Then, there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that for any input x ∈ Xwith Exx⊤ = Σ, the approximation error can be bounded as E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Comparison to Tuning Final Layers. Updating the final layers and keeping the initial layers frozen (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014; Rahimi & Recht, 2007) is another popular model adaptation method. However, unlike LoRA, which can adapt even randomly generated networks to match a target model, empirical studies (Kornblith et al., 2019) suggest that the effectiveness of final layers tuning heavily depends on the quality of the initial layers. This indicates that merely tuning the final layers of randomly generated networks may not yield desirable performance. The following lemma rigorously supports this assertion, demonstrating that regardless of how the final layers are tuned, it is impossible to adapt a randomly generated model into even a one-layer FNN, a model of very low complexity. Lemma 4. Let D ≥ 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl)L l=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L − 1 layers, f ̸= f. In Corollary 4, we demonstrate that LoRA can adapt any randomly generated models to match the target model, using at most twice the number of learnable parameters as the target model. However, this lemma reveals that final layers tuning, even with L − 1 times the learnable parameters of the target model, cannot achieve performance comparable to LoRA. In other words, LoRA requires at most 2RDL ≤ 2D2 learnable parameters to achieve an exact approximation, while final layers 7Published as a conference paper at ICLR 2024 tuning fails to approximate the target model even with (L − 1)D2 learnable parameters. Therefore, when L ≥ 3, LoRA can deliver strictly superior performance than final layers tuning with the same or fewer parameters. This provides insights into the empirical observation that LoRA outperforms final layers tuning (Kaplun et al., 2023; Ding et al., 2023). 4 E XPRESSIVE POWER OF TRANSFORMER NETWORKS WITH LORA 4.1 P ROBLEM SETTING Transformer network, denoted as TFNL,D, is a composition of L Transformer blocks and an output layer, parameterized by weight Wo ∈ RD×D. Each transformer block comprises a H-head self- attention layer, parameterized by weight ((Wh Ol, Wh V l, Wh Kl, Wh Ql)H h=1)L l=1, followed by a token- wise feedforward layer, parameterized by weight (W1l, W2l)L l=1 and bias (b1l, b2l)L l=1. We assume that all weight matrices have a dimension of D × D, while the bias vectors are of dimension D. We employ the same formulations of transformer blocks as Yun et al. (2020a), with one exception: we exclude skip connections for analytical feasibility. As before, we use · (e.g., W1l) to represent the corresponding parameters for the target model, and ∆· (e.g., ∆Wh Ol) to represent the corre- sponding low-rank update. For TFN cases,we consider scenarios where both the frozen model and the target model have L Transformer blocks. For an explicit formulation, please refer to Sec. F.2. 4.2 M AIN RESULTS ON TRANSFORMER NETWORKS We now present our main findings on TFNs. The first result relies on a non-singularity assumption (Assumption 4) tailored for TFN. This assumption is mild, and models with randomly generated weights can satisfy its criteria (Lemma 14). Further details are deferred to Sec. F.2. The following theorem shows that adding LoRA adapters primarily to the self-attention layers en- ables the adapted model f to exactly approximate the target modelf. This finding is consistent with a recent observation made by Hu et al. (2022a), which indicates that a good performance can be achieved by adapting only the attention layers when applying LoRA to TFNs. Theorem 7. Consider a given LoRA-rank R ∈ [D]. Let Assumption 4 hold. Let Gi be the rank- based functionality gap to i-th transformer block (i ∈ [L]) or output layer ( i = L + 1) defined in (23). If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, then there exists low-rank adapters with rank lower than R ∈ [D] ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo with other low-rank adapters set toO, and updated bias vectors (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates target model f, i.e., f(X) = f(X). Proof Sketch. The primary challenge for extending our analysis to TFNs, similar to FNN cases, is the nonlinearity introduced by softmax and ReLU. To manage this, we segment a sequence of transformer blocks based on the softmax and ReLU functions. Specifically, we align the output of attention scores before the softmax is applied, and then match the output of the first feedforward layer before ReLU is applied. The complete proof of Theorem 7 and results for randomly generated models can be found in Sec. F.2. Meanwhile, our results here are specifically for TFNs with multi-head attention layers. For TFNs with single-head attention layers, the construction of LoRA adapters differs due to the absence of Wh Oi. Since the results are similar, we defer the problem setting and results for TFNs with single-head attention layers to Sec. F.1. 5 E XPERIMENTS Recall that all our theoretical statements are based on our construction of the LoRA adapters pre- sented in their corresponding proofs. To validate these results, here we empirically examine the relationship between approximation error and rank by integrating the LoRA adapters, which are constructed with the uniform partition in our proof, into the frozen model. 8Published as a conference paper at ICLR 2024 Validation of Our LoRA Adapter Construction. We employ the Mean Squared Error (MSE) to assess the approximation error, comparing the MSE of the LoRA adapter as derived from the gradient update method with that from our construction. We consider linear models and FNNs with model dimension D = 16. For linear model cases, we set L = 1, L= 2, while for FNN cases, we set L = 2, L= 4. We include two variants of the frozen model for fine-tuning: one with randomly initialized parameters (Random) and another pretrained on the target distribution (Pretrained). 4 8 12 16 0.0 0.2 Random 4 8 12 16 Pretrained Gradient Update Our Construction Rank MSE (a) Linear model approximation. 4 8 12 16 0.00 0.01 0.02 Random 4 8 12 16 Pretrained Gradient Update Our Construction Rank MSE (b) FNN approximation. Figure 1: Approximation error (mea- sured by MSE) versus LoRA-rank. Our results for linear model approximation and FNN ap- proximation via LoRA are depicted in Fig. 1a and 1b, re- spectively. Firstly, we observe that the MSE of both two cases is close to zero when R ≥ D L/L = 8 , which cor- roborates our claims. Meanwhile, a comparison between the left and right columns of Fig. 1a suggests that pre- training can further reduce the required rank to achieve near-zero approximation error. Furthermore, the curves of our construction align well with those of the gradient update method in linear model approximation cases, con- firming the optimality claimed in Lemma 1. However, for FNN approximation cases, the gradient update method outperforms our construction in the small rank region. We conjecture that the suboptimality of our construction for this multi-layer FNN case could arise from unnecessar- ily matching the intermediate outputs of the frozen model with those of the target model during adapter construc- tion. Additionally, the uniform partition could also be one contributing factor. Findings Empirical Observation Theoretical Insights For a fixed downstream task, larger models require a lower LoRA-rank to achieve the desired performance. Sec. G.9 Lemma 1, 2, and Theo- rem 5, 6 When the frozen model is closer to the target model, a lower LoRA-rank is sufficient to attain the desired per- formance. Sec. G.9 and 6-th footnote in Hu et al. (2022a) Lemma 1, 2, and Theo- rem 5, 6, 7 LoRA outperforms final layers tuning if the quality of shared representation is not good. Sec. G.4 and observations by Kaplun et al. (2023) and Ding et al. (2023) Lemma 4 In addition to applying low-rank updates to weight matri- ces, it is crucial to also update the bias. Sec. G.5 and 2-nd footnote in Hu et al. (2022a) Proofs in Sec. 3.2 and E.1 Tuning attention weights is sufficient for achieving good performance on TFNs. Sec. 4.2 in Hu et al. (2022a) Theorem 7 Current optimization algorithms for LoRA training might be suboptimal. Fig. 4, 5, and 9 — Table 1: Summary of our findings, supported by empirical evidence and theoretical results. Detailed Experimental Setup and Additional Experiments in the Appendix. Further experi- ment details and a series of additional experiments, including simulations on FNNs and TFNs at different depths, evaluation of classification tasks, empirical comparison between LoRA and the final layers tuning, investigation of the importance of updatable bias, LoRA’s generalization and optimization properties, and experiments on GLUE benchmark (Wang et al., 2018), are provided in Sec. G. Table 1 summarizes all the empirical findings and aligns them with theoretical insights. 6 C ONCLUSIONS This work pioneers the theoretical analysis of LoRA fine-tuning’s expressive capabilities in FNNs and TFNs, offering novel insights into how rank, model depth, and proximity to the target model influence LoRA’s effectiveness. Our theoretical findings are validated by empirical evidence. Future work includes quantifying approximation errors for TFNs when the LoRA-ranks are lower than required and refining LoRA adapter update algorithms based on our construction of LoRA adapters. 9Published as a conference paper at ICLR 2024 ACKNOWLEDGEMENT This work was supported by NSF Award DMS-2023239, NSF/Intel Partnership on Machine Learn- ing for Wireless Networking Program under Grant No. CNS-2003129, and a grant by FuriosaAI. We extend our heartfelt gratitude to Angeliki Giannou, Kartik Sreenivasan, Tuan Dinh, Jy-yong Sohn, Jingpeng Liu, and anonymous reviewers for their insightful comments that significantly enhanced the quality of our paper. REPRODUCIBILITY STATEMENT The code for all experiments reported in this paper is publicly accessible. For the purpose of re- producibility, the code can be found at the following anonymized GitHub repository: https: //github.com/UW-Madison-Lee-Lab/Expressive_Power_of_LoRA . REFERENCES Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. Ekin Aky ¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. In International Conference on Learning Representations (ICLR), 2023. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Prov- able in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023. Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. In Computational Learning Theory (COLT), volume 2111, pp. 224–240, 2001. Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine- tuning for Transformer-based masked language-models. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1–9, 2022. Yoshua Bengio and Olivier Delalleau. On the expressive power of deep architectures. InAlgorithmic Learning Theory, pp. 18–36, 2011. Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. 2021. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report, 2005. Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014. Pin-Yu Chen. Model reprogramming: Resource-efficient cross-domain machine learning. arXiv preprint arXiv:2202.10629, 2022. George Cybenko. Approximation by superpositions of a sigmoidal function.Mathematics of control, signals and systems, 2:303–314, 1989. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 10Published as a conference paper at ICLR 2024 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. In North American Chapter of the Asso- ciation for Computational Linguistics (NAACL), pp. 4171–4186, 2019. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. Parameter-efficient fine- tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220–235, 2023. Tuan Dinh, Daewon Seo, Zhixu Du, Liang Shang, and Kangwook Lee. Improved input reprogram- ming for GAN conditioning. arXiv preprint arXiv:2201.02692, 2022a. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. LIFT: Language-interfaced fine-tuning for non- language machine learning tasks. Advances in Neural Information Processing Systems (NeurIPS), 35:11763–11784, 2022b. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In Inter- national Conference on Machine Learning (ICML), pp. 647–655, 2014. Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations (ICLR) , 2021. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.Psychome- trika, 1936. Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Annual Conference on Learning Theory, volume 49, pp. 907–940, 2016. Gamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial Reprogramming of neural networks. In International Conference on Learning Representations (ICLR), 2019. Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate condi- tionally from unconditional generative models. In International Conference on Learning Repre- sentations (ICLR), 2018. Matthias Englert and Ranko Lazic. Adversarial Reprogramming revisited. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 28588–28600, 2022. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. Angeliki Giannou, Shashank Rajput, and Dimitris Papailiopoulos. The expressive power of tuning only the Norm layers. arXiv preprint arXiv:2302.07937, 2023a. Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. Looped Transformers as programmable computers. In International Conference on Machine Learning (ICML), volume 202, pp. 11398–11442, 2023b. Michael Gira, Ruisu Zhang, and Kangwook Lee. Debiasing pre-trained language models via effi- cient fine-tuning. In Workshop on Language Technology for Equality, Diversity and Inclusion, pp. 59–69, 2022. Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference on Learning Representations, 2017. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations , 2021. 11Published as a conference paper at ICLR 2024 Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni- versal approximators. Neural Networks, 2:359–366, 1989. Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-Gkaragkounis. On the approximation power of two-layer networks of random ReLUs. InConference on Learning Theory, volume 134, pp. 2423–2461, 2021. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con- ference on Learning Representations (ICLR), 2022a. Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and Maosong Sun. Sparse structure search for delta tuning. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pp. 9853–9865, 2022b. Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs cross- entropy in classification tasks. In International Conference on Learning Representations, 2021. Arthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Convergence and gen- eralization in neural networks. Advances in neural information processing systems, 31, 2018. Gal Kaplun, Andrey Gurevich, Tal Swisa, Mazor David, Shai Shalev-Shwartz, and Eran Malach. SubTuning: Efficient finetuning for multi-task learning. arXiv preprint arXiv:2302.06354, 2023. Kenji Kawaguchi. Deep learning without poor local minima. Advances in neural information pro- cessing systems, 29, 2016. Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better ImageNet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. Thomas Laurent and James von Brecht. Deep linear networks with arbitrary loss: All local minima are global. In Proceedings of the 35th International Conference on Machine Learning, volume 80, pp. 2902–2907, 2018. Holden Lee, Rong Ge, Tengyu Ma, Andrej Risteski, and Sanjeev Arora. On the ability of neural nets to express distributions. In Conference on Learning Theory, pp. 1271–1296, 2017. Kangwook Lee, Changho Suh, and Kannan Ramchandran. Reprogramming GANs via input noise design. In Machine Learning and Knowledge Discovery in Databases - European Conference, (ECML PKDD), volume 12458, pp. 256–271, 2020. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Empirical Methods in Natural Language Processing (EMNLP), pp. 3045–3059, 2021. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. InAs- sociation for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP), pp. 4582–4597, 2021. Shiyu Liang and R. Srikant. Why deep neural networks for function approximation? InInternational Conference on Learning Representations (ICLR), 2017. Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. 2021. Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over- parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85–116, 2022a. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems (NeurIPS) , volume 35, pp. 1950–1965, 2022b. 12Published as a conference paper at ICLR 2024 Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint arXiv:1702.08580, 2017. Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. Advances in neural information processing systems, 30, 2017. Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. A kernel-based view of language model fine-tuning. In International Conference on Machine Learning , pp. 23610–23641, 2023. Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. Journal of Machine Learning Research, 17(81):1–32, 2016. Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of mathematics, 1960. OpenAI. GPT-4 technical report, 2023. Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. In International Conference on Machine Learning (ICML), 2023. Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation. In International Conference on Learning Representations (ICLR), 2021. Jorge P´erez, Javier Marinkovi ´c, and Pablo Barcel ´o. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. Aleksandar Petrov, Philip HS Torr, and Adel Bibi. When do prompting and prefix-tuning work? A theory of capabilities and limitations. arXiv preprint arXiv:2310.19698, 2023. Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the expressive power of deep neural networks. In International Conference on Machine Learning (ICML), pp. 2847–2854, 2017. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NeurIPS), volume 3, pp. 5, 2007. Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning. https://github. com/cloneofsimo/lora, 2023. Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam- ics of learning in deep linear neural networks. 2014. Daewon Seo, Hongyi Wang, Dimitris Papailiopoulos, and Kangwook Lee. Empirical study on the effective VC dimension of low-rank neural networks. In ICML Workshop on Overparameteriza- tion: Pitfalls & Opportunities, 2021. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: An astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 806–813, 2014. Haizhao Shen, ZuoweiYang and Shijun Zhang. Deep network approximation characterized by num- ber of neurons. Communications in Computational Physics, (5):1768–1811, 2020. Matus Telgarsky. Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101, 2015. Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory , pp. 1517–1539, 2016. 13Published as a conference paper at ICLR 2024 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ´elien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. In Advances in neural information processing systems (NeurIPS) , volume 33, pp. 7852–7862, 2020. Rasul Tutunov, Antoine Grosnit, Juliusz Ziomek, Jun Wang, and Haitham Bou-Ammar. Why can large language models generate correct chain-of-thoughts? arXiv preprint arXiv:2310.13571 , 2023. Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. In Measures of Complexity: Festschrift for Alexey Chervonenkis, pp. 11–30. 2015. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo ˜ao Sacramento, Alexander Mordv- intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353–355, 2018. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. arXiv preprint arXiv:2303.07895, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2022. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are Transformers universal approximators of sequence-to-sequence functions? In International Con- ference on Learning Representations (ICLR), 2020a. Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and San- jiv Kumar. O(n) connections are expressive enough: Universal approximability of sparse trans- formers. 33:13783–13794, 2020b. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Inter- national Conference on Learning Representations, 2023. 14Published as a conference paper at ICLR 2024 Appendix This appendix encompasses more discussions, experiments, and proofs of the results presented in the main body. Given the extensive use of notations in our paper, we begin by presenting a list of common notations in Sec. A for the reader’s convenience. We then delve into a more detailed discussion of related works in Sec. B. Following this, we present the proofs of results from the main body and auxiliary results in Sec. C, D, E, and F. Specifically, we provide additional results for TFN with single-head attention layers, and TFN with multi-head attention layers under random model cases in Sec. F. Further experimental details and interesting experiment findings are provided in Sec. G. Finally, we discuss how to extend our results to cases with varying model dimensions in Sec. H, while this work primarily focuses on instances where both the target model and the frozen model possess the same model width D. More potential future works are outlined in Sec. I. A List of Common Notations 17 B Expanded Related Works 18 C Proofs Related to Linear Algebra 19 C.1 Common Matrix Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Non-Singularity of Randomly Generated Matrices . . . . . . . . . . . . . . . . . . 19 D Proofs for Linear Model Approximation 20 E Proofs for FNN Approximation 25 E.1 Approximating One-Layer ReLU FNN via LoRA . . . . . . . . . . . . . . . . . . 25 E.2 Approximating Multi-Layer ReLU FNN via LoRA with Uniform Model Parition . 27 E.3 Approximating Multi-Layer ReLU FNN via LoRA with General Model Parition . . 31 E.4 Approximating Multi-Layer ReLU FNN via Final Layers Tuning . . . . . . . . . . 32 F Proofs for TFN Approximation 33 F.1 Approximating Transformer Network with Single-Head Attention Layers . . . . . 33 F.2 Approximating Transformer Network with Multi-Head Attention Layers . . . . . . 36 G Experiments 39 G.1 Additional Details of Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . 40 G.2 Additional Details on Gradient Update Method . . . . . . . . . . . . . . . . . . . 40 G.3 Validation of Our LoRA Adapter Construction . . . . . . . . . . . . . . . . . . . . 40 G.3.1 FNN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 G.3.2 TFN Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 G.4 Comparison to Tuning Final Layers . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.5 Benefits of Tuning Biases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.6 Training Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 G.7 Generalization Performances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.8 Evaluation on Classification Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . 43 G.9 Evaluation on Real Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 15Published as a conference paper at ICLR 2024 H Extension to Cases with Different Model Dimensions 45 I Extended Future Works 46 16Published as a conference paper at ICLR 2024 A L IST OF COMMON NOTATIONS We first give a list of common notations that are used in the main body and appendix for reference. • f: LoRA-adapted model. • f: target model. • f0: frozen/pretrained model. • R: rank of LoRA adapters. • D: dimensionality of the model, representing the number of neurons in each layer for FNNs and the embedding size for TFNs. • L: depth of the (frozen) model, representing the number of layers for FNNs and the number of transformer blocks for TFNs. • N: sequence length of the input for TFNs. • x: input. • x: random input. • X: matrix input. • X: input space. • Σ: Exx⊤. • W: a weight matrix associated with (frozen) model. Subscripts and superscripts may be added for specificity. • b: a bias vector associated with the (frozen) model. Subscripts may be added for specificity. • zl: the output of the first l layers in the (frozen) FNN. • Zl: the output of the first l transformer blocks in a (frozen) TFN. • W: a weight matrix associated with the target model. Subscripts and superscripts may be added for specificity. • b: a bias vector associated with the target model. Subscripts may be added for specificity. • zl: the intermediate output of the first l layers in target FNN given the random input x. • Zl: the output of the first l transformer blocks in a target TFN. • L: depth of the target model, representing the number of layers for FNNs and the number of transformer blocks for TFNs. • ∆W: the weight matrix of a LoRA adapter. • bb: a bias vector associated with the LoRA-adapted model. • bzl: the output of the first l layers in the LoRA-adapted model given the random input x. • bZl: the output of the first l transformer blocks in the LoRA-adapted model. • M: the ratio of the depth of the frozen model to that of the target model, i.e., L/L. • P: partition P = {P1, . . . , PL}, each element Pi specifies that the layers with index l ∈ Pi in the adapted model will be used to approximate the i-th layer in the target model. • Pi: the i-th element in partition P. • Pu: uniform partition Pu := {{1, . . . , M}, {M + 1, . . . ,2M}, . . . , \b (L − 1)M + 1, . . . , L \t }. The uniform partition indicates that everyM layers in the adapted model are employed to approx- imate each layer in the target model. • Pu i : the i-th element in uniform partition Pu. • ID: the D ×D identity matrix. When the context permits, the subscript D of ID may be omitted, simplifying the notation to I. • Ia:b,D: a diagonal matrix where the diagonal entries from theath to bth position are set to 1, while all remaining entries are 0s. 17Published as a conference paper at ICLR 2024 • σd(·): the d-th largest singular value for the given square matrix. Whend is greater than the width of the matrix, σd(·) = 0. • LRr(·): best rank-r approximation of a square matrix in Frobenuis norm and spectral norm. The subscript r may be omitted to indicate a general low-rank approximation without specifying the rank. • Q l∈Pi Wl: product of the weight matrices from the layers l ∈ Pi, with the later layer positioned to the left and the earlier layer to the right in the matrix product. For example, Q l∈Pu 1 Wl = QM l=1 Wl = WM ··· W1. B E XPANDED RELATED WORKS Expressive Power of Fully Connected Neural Networks The theoretical exploration of the ex- pressive power of unfrozen fully connected neural networks has advanced since the introduction of the first universal approximation theorem (Hornik et al., 1989; Cybenko, 1989). Subsequent studies have demonstrated the benefits of depth, asserting that sufficient depth can ensure function approxi- mation (Bengio & Delalleau, 2011; Eldan & Shamir, 2016; Liang & Srikant, 2017; Telgarsky, 2016; 2015). There are also works that have examined the expressive power of FNN from a view of width (Lu et al., 2017; Park et al., 2021; Bietti & Bach, 2021) and the number of neurons (Shen & Zhang, 2020). While these results assume that weight matrices can be arbitrarily adjusted for optimal performance, Hsu et al. (2021) examined the expressive power of randomly generated two- layer FNNs. Our work shares similarities with this direction, as we also delve into scenarios with randomly generated models. Beyond characterizing expressive power by approximation error, alter- native metrics have been proposed. Metrics such as Vapnik-Chervonenkis (Vapnik & Chervonenkis, 2015; Seo et al., 2021) and Rademacher complexities (Bartlett & Mendelson, 2001) are utilized to assess classification capacity. Furthermore, Raghu et al. (2017) introduced a novel metric that cap- tures the structural properties of an FNN, and Lee et al. (2017) investigated the ability of FNNs to express distributions. Expressive Power of Transformers As TFNs have grown increasingly popular, a few studies have been conducted to investigate their expressive power. Yun et al. (2020a) established the universal approximation theorem for TFNs in approximating sequence-to-sequence functions. Likhosherstov et al. (2021) characterized the self-attention layer as a matrix and demonstrated that this matrix can approximate any sparse matrices. Beyond approximation, further research has delved into other facets of TFNs’ expressive power. For instance, Giannou et al. (2023b) found that looped transform- ers can emulate an instruction-set computer, while P´erez et al. (2019) demonstrated that TFNs attain Turing completeness when operating with infinite precision. However, all these theories above cannot fully explain the performance of frozen neural networks as they generally cannot factor in pre-trained model parameters and adaptation methods. Expressive Power of Adaptation Methods Our work focuses on investigating the expressive power of adaptation methods. In stark contrast to the flourishing research on the expressive power of neural networks, there exists a limited number of works investigating the expressive power of adap- tation methods. A notable exception is Giannou et al. (2023a), investigating the expressive power of normalization parameter fine-tuning. They demonstrate that fine-tuning the normalization layers alone can adapt a randomly initialized ReLU network to match any target network that is O(width) times smaller. We borrow some proof techniques from this work, including techniques for extending results from linear neural networks to ReLU neural networks. In another recent work (Englert & Lazic, 2022), the authors show that neural reprogramming (Elsayed et al., 2019; Engel et al., 2018; Lee et al., 2020; Dinh et al., 2022a; Chen, 2022), a technique that modifies only the inputs while keeping the pretrained network frozen, can adapt any random two-layer ReLU network to achieve arbitrarily high accuracy on a Bernoulli data model over hypercube vertices. Oymak et al. (2023) explores prompt-tuning within a one-layer attention architecture, revealing that the model result- ing from prompt tuning (Lester et al., 2021) is more expressive than the naive self-attention model. Petrov et al. (2023) shows that prompt-tuning and prefix tuning (Li & Liang, 2021) are strictly less expressive than full fine-tuning. Despite these early attempts, no existing study has yet explored the expressive power of LoRA, the current leading adaptation method. 18Published as a conference paper at ICLR 2024 Other Theoretical Analysis of Adaptation Methods Lots of efforts have been taken to theoret- ically analyze other properties of adaptation methods such as generalization. Maurer et al. (2016) provides the generalization bounds for transfer learning, particularly for final layers tuning, demon- strating that the estimation error reduces as the pretrained task diversity and the number of samples for the target task increase. Tripuraneni et al. (2020) further refines this bound by studying the effect of the number of samples in the pre-trained tasks. Interestingly, the estimation error of the final layers tuning provided in Tripuraneni et al. (2020) heavily depends on the quality of the shared rep- resentation. This insight aligns with our finding on final layers tuning (Lemma 4), which implies that tuning the final layers fails to adapt an L-layer randomly generated FNN to approximate any one-layer target FNN if the first layer remains frozen. This failure is attributed to the poor qual- ity of the shared random representation. Du et al. (2021) further investigates final layers tuning in few-shot cases, i.e., when there are only a few samples for the target task. A recent study by Malladi et al. (2023), which examined LoRA and full fine-tuning through the lens of the Neural Tangent Kernel (Jacot et al., 2018), suggested that if the kernel view describes full fine-tuning, then LoRA approximates full fine-tuning. However, their theoretical analysis of LoRA is based on linear models, thus limiting its applicability. In contrast, our study considers a more general setting. With the rapid advancement of large language models, new adaptation methods such as in-context learning (Brown et al., 2020), prefix tuning, and prompt-tuning (Lester et al., 2021) are gaining increasing attention. A particular focus of research is the exploration of the theoretical underpinnings of in-context learning (Aky¨urek et al., 2023; Bai et al., 2023; Wies et al., 2023; Xie et al., 2022; von Oswald et al., 2022; Ahn et al., 2023). Aky¨urek et al. (2023) demonstrates that transformer-based in- context learners implicitly implement standard learning algorithms, while Bai et al. (2023) presents a similar finding and posits that in-context learning performs algorithm selection like a statistician. Wies et al. (2023) delves into the analysis of the sample complexity of in-context learning. Other works find that in-context learning is equivalent to gradient descent (von Oswald et al., 2022; Ahn et al., 2023), and Bayesian inference (Xie et al., 2022). Beyond in-context learning, a recent research by Tutunov et al. (2023) developed a theoretical framework elucidating how LLMs can accurately generate chain-of-thought reasoning (Wei et al., 2022). C P ROOFS RELATED TO LINEAR ALGEBRA In this section, we present a collection of commonly used matrix inequalities and the basic properties of randomly generated matrices. C.1 C OMMON MATRIX INEQUALITIES Here, we present some commonly used basic properties for matrix multiplication including rank computation, norm inequalities, as well as key results involving the trace and Frobenius norm of matrices for reference: rank(AB) ≤ rank(A) ∧ rank(B); ∥Ax∥2 ≤ ∥A∥2 ∥x∥2 ; (1) Ex⊤Ax = tr(ACov(x)) + (Ex)⊤A(Ex) = tr(AExx⊤); tr(AB) = tr(BA); tr(AB) ≤ tr(A)tr(B); ∥A∥F = q tr(AA⊤); ∥A∥F = tr(A) for symmetric A; ∥A∥F = sX i σ2 i (A). C.2 N ON-SINGULARITY OF RANDOMLY GENERATED MATRICES Although the non-singularity of randomly generated matrices is already established, we include a proof for completeness. 19Published as a conference paper at ICLR 2024 To facilitate the proof, we introduce a lemma which states that if a polynomial is non-zero, then the set of roots corresponding to a zero value of the polynomial has a Lebesgue measure of zero. Lemma 5 (Caron & Traynor (2005)). Let p(x) be a polynomial of degree d, x ∈ Rn. If p is not the zero polynomial, then the set S := {x ∈ Rn | p(x) = 0} is of Lebesgue measure zero. We note that the determinant of a matrix can be viewed as a polynomial function of its vectorized version. Based on this insight, we proceed with our proof. Lemma 6. Let X ∈ RD×D be a random matrix that follows arbitrary continuous distribution with support having non-zero Lebesgue measure on RD×D. Then, X is non-singular with probability 1. Proof of Lemma 6. The result is a direct consequence of Lemma 5. Let x = vec(X). Then, x is a random vector following arbitrary continuous distribution with a support having non-zero Lebesgue measure on RD×D. First, we establish the relationship: P(det(X) = 0) = P(p(x) = 0) for some polynomial function p. We denote the support of random vector x by X ⊂RD2 , and the probability density function (PDF) of x by q. Then, P(p(x) = 0) = Z X 1 {p(x) = 0}q(x)dx = Z X∩{x:p(x)=0} q(x)dx. By Lemma 5, the Lebesgue measure of {x : p(x) = 0} is zero. Hence, Z X∩{x:p(x)=0} q(x)dx = 0. By combining all the equations above, we conclude that P(det(X) = 0) = 0 , which implies X is non-singular with probability 1. D P ROOFS FOR LINEAR MODEL APPROXIMATION In this section, we present the results and corresponding proofs for the linear model approximation problem introduced in Sec. 2. The deep linear model is a common technique in theoretical deep learning research, which offers valuable insights into deep nonlinear models, and has been employed in many notable studies, including those by Saxe et al. (2014); Kawaguchi (2016); Lu & Kawaguchi (2017); Hardt & Ma (2017) and Laurent & von Brecht (2018). We employ this toy model as a preliminary model, which serves as a foundation for extending our results to nonlinear models (i.e., FNN and TFN). We first provide a slightly more detailed version of Lemma 1 along with its proof. Then, we present a variant of it that allows for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 7. Lemma 7. [Detailed version of Lemma 1] Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank(E). For a given LoRA-rankR ∈ [D], assume that all the weight matrices of the frozen model (Wl)L l=1, and QL l=1 Wl + LRr(E) are non-singular for all r ≤ R(L − 1). Then, the approximation error min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = σRL+1   W − LY l=1 Wl ! | {z } Error matrix E , and the optimal solution to the matrix approximation problem satisfies QL l=1(Wl + ∆Wl) =QL l=1 Wl + LRRL∧RE(E). Therefore, when R ≥ ⌈RE L ⌉, we have QL l=1(Wl + ∆Wl) = W, implying f ≡ f. 20Published as a conference paper at ICLR 2024 Proof of Lemma 7. Our goal is to find matrices ∆W1, . . . ,∆WL of rank R or lower such that the product of the adapted matrices approximates the target matrix well, i.e., we aim to solve the following constrained optimization problem: min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 . By subtracting QL l=1 Wl from both terms, the constrain optimization problem becomes min ∆Wl:rank(∆Wl)≤R \r\r\r\r\r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! | {z } :=A −   W − LY l=1 Wl ! | {z } :=E \r\r\r\r\r\r\r\r\r 2 . (2) To perform analysis on (2), we start with the analysis of A as follows: A = LY l=1 (∆Wl + Wl) − LY l=1 Wl = ∆WL L−1Y l=1 (∆Wl + Wl) + WL L−1Y l=1 (∆Wl + Wl) − LY l=1 Wl. Here, we have separated the first term in the productQL l=1(∆Wl + Wl), breaking it into two parts: one involving ∆WL and the other WL. We can further expand the part involving WL: A =∆WL L−1Y l=1 (∆Wl + Wl) + WL   ∆WL−1 L−2Y l=1 (∆Wl + Wl) + WL−1 L−2Y l=1 (∆Wl + Wl) ! − LY l=1 Wl. At this point, it becomes clear that this expression can be iteratively decomposed. Following this pattern, we can express A as: A =∆WL L−1Y l=1 (∆Wl + Wl) + WL∆WL−1 L−2Y l=1 (∆Wl + Wl) (3) + . . .+ ( LY l=2 Wl)(∆W1 + W1) − LY l=1 Wl = LX l=1 \" ( LY i=l+1 Wi)∆Wl( l−1Y i=1 (Wi + ∆Wi)) # | {z } :=Al . In this final form, A is decomposed as A = PL l=1 Al. It is important to note that rank(Al) ≤ rank(∆Wl) ≤ R. Consequently, rank(A) ≤ PL l=1 rank(Al) ≤ RL. Then, the optimization problem (2) can be relaxed into a low-rank approximation problem (2) ≥ min A:rank(A)≤RL ∥A − E∥2 , (4) where the optimal solution is A = LRRL∧RE(E) := E′. Therefore, if we can identify rank- R or lower matrices (∆Wl)L l=1 such that LY l=1 (Wl + ∆Wl) − LY l=1 Wl | {z } :=A = LRRL∧RE(W − LY l=1 Wl) | {z } :=E′ , (5) 21Published as a conference paper at ICLR 2024 then we effectively solve the matrix approximation problem as defined in (2). Moreover, it is straightforward to verify that (5) directly implies all statements in this lemma. Therefore, our re- maining proof focuses on proving (5). Denote RE′ = RL ∧ RE. To derive the explicit form of E′, we first refer to the SVD of E as E = UDV ⊤, where U and V are orthonormal matrices and the first RE diagonal entries of D are non-zero, with all remaining entries being zero. Based on this, E′ is expressed as E′ = UDI1:RL,DV ⊤. Having already derived the decomposition A = PL l=1 Al, we next aim to decompose E′ as E′ =PL l=1 E′Ql, where Q1, . . . ,QL ∈ RD×D. The goal now shifts to identifying ∆Wl, Ql such that Al = E′Ql for each l ∈ [L]. Achieving this would complete the proof of (5). Therefore, our goal becomes finding ∆W1, . . . ,∆WL with rank(∆Wl) ≤ R for all l ∈ [L] such that Al = ( LY i=l+1 Wi)∆Wl( l−1Y i=1 (Wi + ∆Wi)) = E′Ql, for all l ∈ [L]. (6) One sufficient condition for achieving (6) is that the decomposed matrices Q1, QL and low-rank adapters ∆W1, . . . ,∆WL meet the following conditions: LX l=1 E′Ql = E′, (7) ∆Wl = ( LY i=l+1 Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi))−1, for all l ∈ [L] (8) rank(∆Wl) ≤ R, for all l ∈ [L], (9) rank(Wl + ∆Wl) = D, for all l ∈ [L − 1]. (10) Here (7) describes the decomposition ofE′, (8) provides one simple solution to (6) when (10) holds, and (9) is the rank constraint on the low-rank adapter. In particular, the (10) is used to ensure the invertibility of Ql i=1(Wi + ∆Wi) for l ∈ [L − 1]. This condition is not necessary for l = L as the inverse of WL + ∆WL is not required for computing any low-rank adapters. We will show that the matrices (Ql)L l=1 defined by Ql = V I(R(l−1)+1)∧RE′:Rl∧RE′,DV ⊤, for all l ∈ [L], (11) and ∆Wl defined by (8) for alll ∈ [L] satisfies the all four conditions (7), (8), (9), and (10). We note that the definition of (Ql)L l=1 clearly satisfies condition (7). For the remaining conditions, namely (8), (9), (10), we proceed the proof by induction. When l = 1. We begin by examining the three conditions (8), (9) and (10) under the base case l = 1. We first determine Q1 and ∆W1 based on (11) and (8): ∆W1 = ( LY i=2 Wi)−1E′Q1, Q1 = I1:R,D. (12) By the choice of ∆W1, we satisfy the condition (8). Moreover, it directly follows that rank(∆W1) ≤ rank(Q1) = R, thereby fulfilling the rank constraint in (9). 22Published as a conference paper at ICLR 2024 Therefore, we just need to prove that W1 + ∆W1 is full-rank, as required by condition (10). To compute rank(W1 + ∆W1), we proceed as follows: rank(W1 + ∆W1) (12) = rank( W1 + ( LY i=2 Wi)−1E′Q1) (Substituting for ∆W1) = rank(( LY i=1 Wi) + E′Q1) (Left multiplying with invertible ( LY i=2 Wi)−1) = rank(( LY i=1 Wi) + LRR∧RE′ (E)). (Simplifying) Given the assumption that QL l=1 Wl + LRr(E) is full rank for all r ≤ R(L − 1), rank(W1 + ∆W1) = rank((QL i=1 Wi) + LRR∧RE′ (E)) = D, satisfying the last condition (10). When l >1. Consider l = 2, . . . , L. We assume that for i ∈ [l − 1], we have determined matrices Qi and ∆Wi based on (11) and (8), respectively, and we assume that they satisfy the conditions (8), (9), and (10). First, under the induction assumption that Wi + ∆Wi is invertible for all i ∈ [l − 1], to achieve Al = E′Ql, we set ∆Wl based on (8). This definition ensures rank(∆Wl) ≤ rank(Ql) = R, thereby satisfying the condition (9). To prove thatWl +∆Wl is full-rank (condition (10)), we focus on computing rank(Wl + ∆Wl). We proceed as follows: rank(Wl + ∆Wl) (8) = rank(Wl + ( LY i=l+1 Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi)−1)) (Substituting for ∆Wl) = rank(ID + ( LY i=l Wi)−1E′Ql( l−1Y i=1 (Wi + ∆Wi))−1) (Left multiplying invertible W−1 l ) = rank \u0010l−1Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Right multiplying invertible l−1Y i=1 (Wi + ∆Wi)) = rank \u0010 (Wl−1 + ∆Wl−1) l−2Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Rearranging terms) (8) = rank \u0010 (Wl−1 + ( LY i=l Wi)−1E′Ql−1( l−2Y i=1 (Wi + ∆Wi))−1) l−2Y i=1 (Wi + ∆Wi) + ( LY i=l Wi)−1E′Ql \u0011 (Substituting for ∆Wl−1) = rank \u0010 ( LY i=l−1 Wi + E′Ql−1( l−2Y i=1 (Wi + ∆Wi))−1) l−2Y i=1 (Wi + ∆Wi) + E′Ql \u0011 (Left multiplying LY i=l Wi) = rank   ( LY i=l−1 Wi l−2Y i=1 (Wi + ∆Wi) + E′Ql−1 + E′Ql ! (Rearranging terms) = ··· 23Published as a conference paper at ICLR 2024 = rank( LY i=1 Wi + E′( lX i=1 Qi)) (Taking similar steps) = rank( LY i=1 Wi + LRRl∧RE′ (E)). (Simplifying) By the assumption that QL l=1 Wl + LRr(E) is full-rank for r ≤ R(L − 1) and consequently, rank(Wl + ∆Wl) = rank(QL i=1 Wi + LRRl∧RE′ (E)) = D, satisfying the last condition (10). Conclusion of Inductive Proof. Thus, by induction, we show that the definitions of(∆Wl)L l=1 in (8) and (Ql)L l=1 in (11) ensure that Al = E′Ql for all l ∈ [L]. Summing over l from 1 to L satisfies condition (5), thereby completing the proof. The following lemma extends the results to a more general setting where different LoRA-ranks can be employed across layers. Lemma 8. Define error matrix E := W − QL l=1 Wl, and denote its rank by RE = rank( E). For a sequence of LoRA-ranks for all layers (Rl)L l=1, assume that all the weight matrices of the frozen model (Wl)L l=1, and QL l=1 Wl + LRr(E) are non-singular for all r ≤ PL−1 l=1 Rl. Then, the approximation error min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 = σPL l=1 Rl+1   W − LY l=1 Wl ! | {z } Error matrix E , and the optimal solution to the matrix approximation problem satisfies QL l=1(Wl + ∆Wl) =QL l=1 Wl +LR(PL l=1 Rl)∧RE(E). Therefore, whenPL l=1 Rl ≥ RE, we have QL l=1(Wl +∆Wl) = W, implying f ≡ f. Proof of Lemma 8. The proof follows the same steps of Lemma 7 with only minor modifications. In the current setting, we target the following constrained optimization problem: min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r LY l=1 (Wl + ∆Wl) − W \r\r\r\r\r 2 , where we allow each LoRA adapter ∆Wl can possess different LoRA-ranks Rl, i.e., rank(∆Wl) ≤ Rl, l ∈ [L]. Subtracting QL l=1 Wl from both terms leads us to a similar constrained optimization problem as (2). The only distinction lies in the rank constraint: min ∆Wl:rank(∆Wl)≤Rl \r\r\r\r\r\r\r\r\r  LY l=1 (Wl + ∆Wl) − LY l=1 Wl ! | {z } :=A −   W − LY l=1 Wl ! | {z } :=E \r\r\r\r\r\r\r\r\r 2 . (13) Following the same steps, we decompose A into (3). Given that rank(Al) ≤ rank(∆Wl) ≤ Rl, we deduce that rank(A) ≤ PL l=1 rank(Al) ≤ PL l=1 Rl. Consequently, the optimization problem above can be eased into a low-rank approximation problem analogous to (4): (13) ≥ min A:rank(A)≤PL l=1 Rl ∥A − E∥2 , where the optimal solution is A = LR(PL l=1 Rl)∧RE(E) := E′. Therefore, if we can identify the LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl such that LY l=1 (Wl + ∆Wl) − LY l=1 Wl | {z } :=A = LR(PL l=1 Rl)∧RE(W − LY l=1 Wl) | {z } :=E′ , 24Published as a conference paper at ICLR 2024 the proof is completed. The remaining part of the proof adheres to the steps outlined in the proof of Lemma 7 deriving (5). The only difference is that we consider a different selection of (Ql)l = 1L that satisfies (9) here: Ql = V I(Pl−1 i=1 Ri)∧RE′:(Pl i=1 Ri)∧RE′,DV ⊤. Applying the same steps with this change yields the desired outcomes. This lemma illustrates that in linear cases, the total number of parameters needed to achieve an exact approximation is constant, regardless of LoRA-rank assignment. It suggests that applying a LoRA-rank of R per layer is equivalent to applying a LoRA-rank of RL at the final layer. As a result, fine-tuning only the last layer, which involves assigning a LoRA-rank of D to the last layer, is equivalent to implementing LoRA where each adapter is constrained to have a rank ofD/L. Both methods can achieve an exact approximation and maintain the same parameter efficiency. E P ROOFS FOR FNN A PPROXIMATION In this section, we provide the full proof for deriving the main results outlined in Sec. 3. For the sake of completeness, we restate our results from the main body before presenting the proof. E.1 A PPROXIMATING ONE-LAYER RELU FNN VIA LORA We first provide a slightly more detailed result on the one-layer ReLU FNN approximation (Lemma 9) along with its corresponding proof. Then, we present a variant of this lemma by al- lowing for different LoRA-ranks for each low-rank adapter. The proof for this variant involves only a minor modification of the proof for Lemma 9. Lemma 9 (Detailed version of Lemma 2). Define error matrix E := W1 −QL l=1 Wl, with its rank represented by RE = rank(E). Consider a LoRA-rank R ∈ [D]. Assume that the weight matrices W1, . . . ,WL ∈ RD×D and QL l=1 Wl + LRr(E) for all r ≤ R(L −1) are non-singular. Letx be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that for any input x ∈ X, f(x) − f(x) = ReLU    LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . Therefore, when R ≥ ⌈RE/L⌉, the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x ∈ X. Furthermore, let x be a random input sampled from a distribution with bounded support X and let Σ = Exx⊤. Then, the expected squared error is bounded as E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 RL∧RE+1(W1 − LY l=1 Wl). Proof of Lemma 9. This proof consists of three main steps: (i) linearize the first L − 1 layers of the adapted model f to reduce it to a single-layer FNN, (ii) align the weight matrices and bias vectors of this simplified f with those of the target model f, (iii) derive an upper bound of the error E \r\rf(x) − f(x) \r\r2 2 . Linearization. The main challenge here stems from the non-linearities introduced by the ReLU activation function. To remove the non-linearities in the firstL −1 layers of updated model f, since the input space X is bounded, we can set all the entries of bb1, . . . ,bbL−1 sufficiently large, thereby 25Published as a conference paper at ICLR 2024 activating all ReLUs in the first L − 1 layers of f. Consequently, we have f(x) = ReLU((WL + ∆WL)zL−1 + bbL) = ReLU \u0010 (WL + ∆WL)ReLU((WL−1 + ∆WL−1)zL−2 + bbL−1) + bbL \u0011 = ReLU \u0010 (WL + ∆WL)((WL−1 + ∆WL−1)zL−2 + bbL−1) + bbL \u0011 = ReLU \u0010 (WL + ∆WL)(WL−1 + ∆WL−1)zL−2 + (WL + ∆WL)bbL−1 + bbL \u0011 = ··· = ReLU  LY l=1 (Wl + ∆Wl)x + ( L−1X l=1 LY i=l+1 (Wi + ∆Wi)bbl) + bbL ! , which is equivalent to a single-layer ReLU neural network with weight matrix QL l=1(Wl + ∆Wl) and bias vector (PL−1 l=1 QL i=l+1(Wi + ∆Wi)bbl) + bbL. Parameter Alignment. To match the updated model f(x) and target model f(x), we proceed as follows. For weight matrix, Lemma 7 guarantees the existence of rank- R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D such that LY l=1 (Wl + ∆Wl) = LY l=1 Wl + LRRL∧RE(W − LY l=1 Wl). (14) For the bias vector, we setbbL = b1 −PL−1 l=1 QL i=l+1(Wi+∆Wi)bbl such that PL−1 l=1 QL i=l+1(Wi+ ∆Wi)bbl + bbL = b1. Therefore, we obtain f(x) − f(x) = ReLU    LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . Error Derivation. We compute the expected squared error as follows: E \r\rf(x) − f(x) \r\r2 2 ≤ E \r\r\r\r\r   LRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x \r\r\r\r\r 2 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\r\r\rLRRL∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) \r\r\r\r\r 2 2 E∥x∥2 2 = ∥Σ∥F σ2 RL∧RE+1(W1 − LY l=1 Wl). (By the definition of LRRL∧RE(·)) This completes the proof. Lemma 9 is extended to cases where different LoRA-ranks can be used for different low-rank adapters, as detailed in the following lemma. Lemma 10. Define error matrix E := W1 − QL l=1 Wl, and denote its rank by RE = rank(E). Consider a sequence of LoRA-ranks (Rl)L l=1. Assume that the weight matrices W1, . . . ,WL ∈ RD×D and QL l=1 Wl + LRr(E) for all r ≤ PL−1 l=1 Rl are non-singular. Then, there LoRA adapters (∆Wl)L l=1 satisfying the rank constraints rank(∆Wl) ≤ Rl for all l ∈ [L] and bias vectors bb1, . . . ,bbL ∈ RD such that for any input x ∈ X, f(x) − f(x) = ReLU    LR(PL l=1 Rl)∧RE(W1 − LY l=1 Wl) − (W1 − LY l=1 Wl) ! x ! . 26Published as a conference paper at ICLR 2024 Therefore, when PL l=1 Rl ≥ RE, the adapted model exactly approximates the target model, i.e., f(x) = f(x) for all x ∈ X. Furthermore, for a random inputx drawn from a distribution supported onX, and with Σ = Exx⊤, the expected squared error is bounded by: E \r\rf(x) − f(x) \r\r2 2 ≤ ∥Σ∥F σ2 (PL l=1 Rl)∧RE+1(W1 − LY l=1 Wl). Proof of Lemma 10. This proof closely adheres to the steps detailed in the proof of Lemma 9. The primary change implemented here is that, when we draw the analogy to (14), we apply Lemma 8 instead of Lemma 7. This results in LY l=1 (Wl + ∆Wl) = LY l=1 Wl + LR(PL l=1 Rl)∧RE(W − LY l=1 Wl). Utilizing the steps from the proof of Lemma 9 and integrating the modification specified above, we can establish the desired result. E.2 A PPROXIMATING MULTI -LAYER RELU FNN VIA LORA WITH UNIFORM MODEL PARITION In this part, we restate all the results considering uniform model partition from Sec. 3.3, along with their corresponding proofs, presented in the same order. Assumption 1 (Non-Singularity). For a fixed LoRA-rank R ∈ [D], the weight matrices of the frozen model (Wl)L l=1 and matrices \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) are non-singular for all r ≤ R(M − 1) and i ∈ [L]. Lemma 3. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 1 holds ∀R ∈ [D]. Proof of Lemma 3. We first use Lemma 6 to establish that W1, . . . ,WL, W1, . . . ,WL are non- singular with probability 1. The goal of the remaining proof is to demonstrate that \u0010Q l∈Pu i Wl \u0011 + LRr(Wi − Q l∈Pu i Wl) is full-rank with probability 1. In this proof, we use p· to denote the proba- bility density function, where the subscript indicates the associated random variable. Fix an arbitrary i ∈ [L] and r ∈ [R]. Then probability of the \u0010Q l∈Pu i Wl \u0011 + LRr \u0010 Wi − Q l∈Pu i Wl \u0011 being full-rank can be computed as P   det     Y l∈Pu i Wl   + LRr  Wi − Y l∈Pu i Wl     ̸= 0    = Z E P   det     Y l∈Pu i Wl   + LRr(E)   ̸= 0 \f\f\f\f\f\f Wi − Y l∈Pu i Wl = E   pWi−Q l∈Pu i Wl (E)dE. If the conditional random matrix \u0010Q l∈Pu i Wl \u0011 +LRr(E) | Wi −Q l∈Pu i Wl = E has a continuous distribution with support of non-zero Lebesgue measure on RD×D, then P   det     Y l∈Pu i Wl   + LRr(E)   ̸= 0 \f\f\f\f\f\f Wi − Y l∈Pu i Wl = E    = 1 ensuring \u0010Q l∈Pu i Wl \u0011 + LRr \u0010 Wi − Q l∈Pu i Wl \u0011 is full-rank with probability 1. 27Published as a conference paper at ICLR 2024 Consequently, the remaining part of the proof aims to show that the conditional random matrix\u0010Q l∈Pu i Wl \u0011 + LRr(E) | Wi − Q l∈Pu i Wl = E follows arbitrary continuous distribution with support having non-zero Lebesgue measure onRD×D. Denote W = Q l∈Pu i Wl. Now, consider the conditional distribution of Q l∈Pu i Wl | Wi − Q l∈Pu i Wl = E, which can be written as pW|Wi−W=E(W) = pWi (E + W). Since pWi is continuous with support of non-zero Lebesgue measure on RD×D, the same holds for Q l∈Pu i Wl | Wi − Q l∈Pu i Wl = E. Furthermore, adding a constant matrix LRr(E) to this conditional distribution preserves the desired properties, thus completing the proof. Theorem 3. Under Assumption 1, there exists rank-R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD when the rank of the low-rank adapter R ≥ ⌈maxi∈[L] rank(Wi − Q l∈Pu i Wl)/M⌉, the low-rank adapted model f can exactly approximate the target model f, i.e., f(x) = f(x) for all input x ∈ X. Proof of Theorem 3. The key to this proof lies in a simple idea: for each layer i ∈ [L] in the target model, we can update M layers (i.e., (i − 1)M + 1-th layer to iM-th layer) in the frozen model to approximate it as guaranteed by Lemma 9. Hence, all layers of the target model can be approximated by the adapted model. Model Decomposition. We partition the adapted model f into L sub-models, each defined as fi(·) = FNNL,D(·; (Wl + ∆Wl)l∈Pu i , (bbl)l∈Pu i ), i ∈ [L]. In a similar manner, we break down f into L sub-models, each is a one-layer FNN: fi(·) = FNN1,D(·; Wi, bi), i∈ [L]. We can then express f(x) and f(x) as compositions of their respective sub-models: f(·) = fL ◦ ···f1(·), f(·) = fL ◦ ···f1(·). To analyze the error E \r\rf(x) − f(x) \r\r 2 = E \r\rf(x) − f(x) \r\r 2, we consider the error caused by each submodel. Let eRi = rank(Wi − Q l∈Pu i Wl) denote the rank of the discrepancy between the target weight matrix and the frozen weight matrices, where i ∈ [L]. By Lemma 9, we can select ∆W1, . . . ,∆WL, bb1, . . . ,bbL such that fi(z) − fi(z) = ReLU    LRRL∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  z  , (15) E \r\rfi(z) − fi(z) \r\r2 2 ≤ \r\rEzz⊤\r\r F σ2 RL∧eRi+1(Wi − LY l=1 Wl). (16) Given these selected parameters, fi is functionally equivalent to a one-layer FNN: fi(z) = ReLU    LRRL∧eRi (Wi − Y l∈Pu i Wl) + Y l∈Pu i Wl  z  . Clearly, when R ≥ maxi⌈ eRi M ⌉, it follows that fi = gi for all i ∈ [L], which implies f = g. Corollary 4. Assume that the elements of matrices (Wl)L l=1, (Wl)L l=1 are independently drawn from arbitrary continuous distributions. When R ≥ D/M, there exists rank-R or lower matrices ∆W1, . . . ,∆WL ∈ RD×D and bias vectors bb1, . . . ,bbL ∈ RD such that low-rank adapted model f can functionally cover the target model f on X, i.e., f(x) = f(x) for all input x ∈ X, with probability 1. 28Published as a conference paper at ICLR 2024 Proof of Corollary 4. To prove the statement, we start by noting that combining Lemma 3 and The- orem 3 directly gives us f(x) = f(x) on X when R ≥ maxi∈[L]⌈rank(Wi − Q l∈Pu i Wl)/M⌉. Therefore, the only thing left is to show that rank(Wi − Q l∈Pu i Wl) = D for i ∈ [L] with prob- ability 1. In this proof, we use p· to denote the probability density function, where the subscript indicates the associated random variable. To establish this, consider the following probability expression: P   det  Wi − Y l∈Pu i Wl   ̸= 0    = Z P   det \u0000 Wi − W \u0001 ̸= 0 \f\f\f\f\f\f Y l∈Pu i Wl = W   pQ l∈Pu i Wl(W)dW. Since W is independent of Q l∈Pu i Wl, we have P   det \u0000 Wi − W \u0001 ̸= 0 \f\f\f\f\f\f Y l∈Pu i Wl = W    = P \b det \u0000 Wi − W \u0001 ̸= 0 \t Lemma 6 = = = = = 1. Therefore, we conclude that P n det \u0010 Wi − Q l∈Pu i Wl \u0011 ̸= 0 o = 1 , which completes the proof. Theorem 5. Define the approximation error of i-th layer as Ei = σRM+1(Wi −Q l∈Pu i Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Under Assumption 1, there exists rank- R or lower matrices (∆Wl)L l=1 with ∆Wl ∈ RD×D and bias vectors (bbl)L l=1 with bbl ∈ RD such that for input x ∈ Xwith Exx⊤ = Σ, E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Proof of Theorem 5. This proof is a continuation of the proof of Theorem 3. In this proof, we will consider a more general case, without enforcing any constraints on the rank of the adapters R. We use cWi to denote the corresponding weight matrix, i.e., cWi = LR RL∧eRi (W1 − Q l∈Pu i Wl) +Q l∈Pu i Wl. Error Decomposition. For submodel i = 2, . . . ,L, we calculate the expected error of the compo- sition of the first i sub-models, E∥bzi − zi∥2 = E \r\rfi(bzi−1) − fi(zi−1) \r\r 2 (17) = E \r\r(fi(bzi−1) − fi(zi−1)) + \u0000 fi(zi−1) − fi(zi−1) \u0001\r\r 2 (Rearranging terms) ≤ E∥fi(bzi−1) − fi(zi−1)∥2| {z } Ai + E \r\rfi(zi−1) − fi(zi−1) \r\r 2| {z } Bi . (Applying triangle inequality) Here Ai represents the error resulting from the discrepancy between the firsti −1 submodels, while Bi represents the error arising from the mismatch between the i-th submodel. 29Published as a conference paper at ICLR 2024 Computing Ai. We start by computing the error introduced by the first i − 1 submodels, denoted by Ai: Ai = E∥fi(bzi−1) − fi(zi−1)∥2 = E \r\r\rReLU(cWi(bzi−1 − zi−1)) \r\r\r 2 ≤ E \r\r\rcWi(bzi−1 − zi−1) \r\r\r 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\rcWi \r\r\r F E∥bzi−1 − zi−1∥2 . (18) Here, \r\r\rcWi \r\r\r F = \r\r\r\r\r\r Y l∈Pu i Wl + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F = \r\r\r\r\r\r Wi +   Y l∈Pu i Wl − Wi   + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F (Rearranging terms) ≤ \r\rWi \r\r F + \r\r\r\r\r\r   Y l∈Pu i Wl − Wi   + LRRM∧eRi (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r F (Applying triangle inequality) = \r\rWi \r\r F + vuuut DX j=RM∧eRi+1 σ2 j (Wi − Y l∈Pu i Wl) (19) (By the definition of Wi and LRRM∧eRi+1(·)) ≤ max k∈[L] ( \r\rWk \r\r F + Ei) := α. By combining (18) and (19), we get Ai ≤ max k∈[L] \u0000\r\rWk \r\r F + Ei \u0001 E∥bzi−1 − zi−1∥2 ≤ αE∥bzi−1 − zi−1∥2 . (20) Computing Bi. We proceed to compute the error associated with the i-th submodel, which we denote as Bi. It can be evaluated as follows: Bi = E \r\rfi(zi−1) − fi(zi−1) \r\r 2 (15) = E \r\r\r\r\r\r ReLU    LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  zi−1   \r\r\r\r\r\r 2 ≤ E \r\r\r\r\r\r  LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl)  zi−1 \r\r\r\r\r\r 2 (ReLU is 1-Lipschitz) (1) ≤ \r\r\r\r\r\r LRRM∧eRi (Wi − Y l∈Pu i Wl) − (Wi − Y l∈Pu i Wl) \r\r\r\r\r\r 2 E∥zi−1∥2 = σRM∧eRi+1(Wi − Y l∈Pu i Wl)E∥zi−1∥2 . We can further simplify E∥zi−1∥2 as : E∥zi−1∥2 = E \r\rReLU(Wi−1zi−2 + bi−1) \r\r 2 = E \r\rWi−1zi−2 + bi−1 \r\r 2 (ReLU is 1-Lipschitz) 30Published as a conference paper at ICLR 2024 ≤ \r\rWi−1 \r\r F E∥zi−2∥2 + \r\rbi−1 \r\r 2 (Applying triangle inequality and (1)) ≤ \r\rWi−1 \r\r F \u0000\r\rWi−2 \r\r F E∥zi−3∥2 + \r\rbi−2 \r\r 2 \u0001 + \r\rbi−1 \r\r 2 (Following the same steps) ≤ i−1Y j=1 \r\rWj \r\r F E∥x∥2 + i−1X j=1 i−1Y k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 (Repeating the same steps) = q ∥Σ∥F i−1Y j=1 \r\rWj \r\r F + i−1X j=1 i−1Y k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 ≤ β. Therefore, we obtain Bi ≤ βσRM∧eRi+1(Wi − Y l∈Pu i Wl). Error Composition. Having established upper bounds for Ai and Bi, we next evaluate the ex- pected error for the composition of the first i adapted submodels. E∥bzi − zi∥2 (17) ≤ Ai + Bi (20) ≤ αE∥bzi−1 − zi−1∥2 + Bi ≤ α(αE∥bzi−2 − zi−2∥2 + Bi−1) + Bi = α2E∥bzi−2 − zi−2∥2 + αBi−1 + Bi ≤ ··· ≤αi−1E∥bz1 − z1∥2 + iX k=2 αi−kBk. (21) To compute the overall approximation error of f, which is the composite of all submodels, we have E \r\rf(x) − f(x) \r\r 2 = E \r\rf(x) − f(x) \r\r 2 = E∥bzL − zL∥2 (21) ≤ αL−1E∥bz1 − z1∥2 + LX i=2 αL−iBi (16) ≤ αL−1βσRM∧eRi+1(Wi − Y l∈Pu i Wl) + β LX i=2 αL−iσRM∧eRi+1(Wi − Y l∈Pu i Wl) = β LX i=1 αL−iσRM∧eRi+1(Wi − Y l∈Pu i Wl) = β LX i=1 αL−iσRM+1(Wi − Y l∈Pu i Wl). Substituting α with maxk∈[L]( \r\rWk \r\r F + Ei) concludes the proof. E.3 A PPROXIMATING MULTI -LAYER RELU FNN VIA LORA WITH GENERAL MODEL PARITION Firstly, we provide the required non-singular assumption and the lemma demonstrating the mildness of this assumption for the general model partition cases after introducing necessary notations. Assumption 2. For the given LoRA-rank sequence(Rl)L l=1 and partition P, the weight matrices of the frozen model W1, . . . ,WL and \u0000Q l∈Pi Wl \u0001 + LRr(Wi − Qmax Pi−1 l=min Pi Wl) are non-singular for all r ≤ Pmax Pi−1 l=min Pi Rl and i ∈ [L]. Note that max Pi and min Pi here represent the maximum and minimum elements in the set Pi, respectively. Lemma 11. Let (Wl)L l=1, (Wl)L l=1 ∈ RD×D be matrices whose elements are drawn independently from arbitrary continuous distributions. Then, with probability 1, Assumption 2 holds for all R ∈ [D]. 31Published as a conference paper at ICLR 2024 Proof of Lemma 11. Following the same steps in the proof of Lemma 3 but replacing the uniform partition with the general partition completes the proof. We now restate Theorem 6 and provide its proof. Theorem 6. Consider a partition P for the frozen model. Let Assumption 2 hold. If P l∈Pi Rl ≥ rank(Wi − Q l∈Pi Wl) for all i ∈ [L], there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that the adapted model f can exactly approximate the target model. Moreover, define the approximation error of the i-th layer as Ei = σP l∈Pi Rl+1(Wi −Q l∈Pi Wl), and the magnitude of the parameters and the input as β := maxi∈[L] \u0010p ∥Σ∥F Qi j=1 \r\rWj \r\r F + Pi j=1 Qi−1 k=j+1 \r\rWk \r\r F \r\rbj \r\r 2 \u0011Wp ∥Σ∥F. Then, there exists LoRA adapters (∆Wl)L l=1 with rank(∆Wl) ≤ Rl and biases (bbl)L l=1 such that for any input x ∈ Xwith Exx⊤ = Σ, the approximation error can be bounded as E \r\rf(x) − f(x) \r\r 2 ≤ β LX i=1 max k∈[L] \u0000\r\rWk \r\r F + Ek \u0001L−i Ei. Proof of Theorem 6. This proof follows the same steps as the proofs of Theorem 3 and Theorem 5, substituting the uniform partition Pu with the general partition P and applying Lemma 10 in place of Lemma 2 to derive the desired outcome. E.4 A PPROXIMATING MULTI -LAYER RELU FNN VIA FINAL LAYERS TUNING We now aim to examine another commonly used model adaptation method, the final layers tuning, within the same theoretical framework. The main limitation of this method, as compared to LoRA, is that while LoRA can update all layers, the tuning of final layers keeps the initial layers frozen. Consequently, a clear limitation arises when the initial layers of the frozen model f are less discriminative than the target model f. That is, if there exist two input vectors x1, x2 ∈ RD×D such that the output of the initial layers of the frozen model f0 is the same, but the output of the target model f is different, then no matter how the final layers are tuned, it is impossible for the adapted model f to exactly approximate the target model f. To formalize this, we observe that for the first layer of the frozen model, the outputs of the inputs in the non-activation region are always zero. In other words, whenx1, x2 ∈ {x : W1x + b1 ≤ 0}, we have ReLU(W1x1+b1) = ReLU(W1x2+b1) = 0. Therefore, no matter how the subsequent layers are tuned, we still have f(x1) = f(x2). When we fix the first l −1 layers, the non-activation region becomes {x : W2(W1x + b1) + b2 ≤ 0}. Similarly, we define the non-active region of the first l layer in the frozen model as Il = n x : Ql i=1 Wix + Pl i=1 Ql j=i+1 Wjbi ≤ 0 o . Correspondingly, we define Il = n x : Ql i=1 Wix + Pl i=1 Ql j=i+1 Wjbi ≤ 0 o . The following lemma is provided based on these definitions. Lemma 12. If l ∈ [L − 1] such that Il \\ SL i=1 Ii ̸= ∅ and the weight matrices of the target model (Wi)L i=1 are non-singular, then for any tuning of the lastL − l layers, f ̸= f. Proof of Lemma 12. For the simplicity of the presentation, we let I = SL i=1 Ii to denote the non- activation region of the target model. Then, the condition Il \\ SL i=1 Ii ̸= ∅ can be written as Il \\ I ̸= ∅. Clearly, both I and Il are closed convex sets. Condition Il \\ I ̸= ∅. The condition Il \\ I ̸= ∅ indicates that there exists a region in Il where the ReLUs are deactivated in the l-th layer of the frozen model, but activated in the entire target model. Therefore, for any x1, x2 ∈ Il \\ I, we have f(x1) = f(x2) regardless of how the final l + 1 layers 32Published as a conference paper at ICLR 2024 are tuned. If these x1, x2 ∈ Il \\ I satisfies f(x1) ̸= f(x2), this proof is completed. The remaining proof is showing the existence of such x1, x2. Existence of x1, x2. Firstly, we show that there exists twox1, x2 ∈ Il \\ I such that x1 ̸= x2. Let x1 ∈ Il \\ I. Since Il is a closed set, there exists a sequence (zi)∞ i=1 where zi ∈ Il and zi ̸= x1 satisfying limi→∞ zi = x1. Note that at least one element zi must not belong to I, otherwise x1 would be in I due to the closed property of I, contradicting the selection of x1. Let x2 = zi. Therefore, we have two distinct x1, x2 ∈ Il \\ I with x1 ̸= x2. Then, given x1, x2 ∈ Il \\ I such that x1 ̸= x2, both x1, x2 activate all the ReLUs in the target model. Since x1, x2 ̸∈ I and the weight matrices of the target model (Wl)L l=1 all are non-singular, we have f(x1) − f(x2) = WL ··· W1(x1 − x2) ̸= 0, implying f(x1) ̸= f(x2). Meanwhile, since x1, x2 ∈ Il, the output of the initial l layers of the frozen model are equal, thus we have f(x1) = f(x2) no matter how we tune the last L − l layers. This completes the proof. The following lemma reduces the assumptions to the assumption of randomly generated mod- els. This assumption aligns with that of Corollary 4, thereby facilitating a more effective comparison between the expressive power of LoRA and the adaptation of the final layers. W1x + b1 = 0 W1x + b1 = 0 I1 ¯I1 Figure 2: An example of I1 and I1 when D = 2. Lemma 4. Let D ≥ 2 and f be a one-layer target FNN. Assume that the elements of weight matrices (Wl)L l=1 are independently drawn from arbitrary continuous distributions. With probability 1, for any tuning of the last L − 1 layers, f ̸= f. Proof of Lemma 4. If we can show that I1 \\ I1 ̸= ∅, by Lemma 12, we obtain the desired results. Therefore, the remaining proof aims to show that I1 \\ I1 ̸= ∅ with probability 1. Note that I1 \\I1 = ∅ holds only when W1 = W1 (not that this is necessary condition not sufficient condition), as demonstrated in Figure 2. However, since the elements of matrices W1 are indepen- dently drawn from arbitrary continuous distributions, we have P(W1 ̸= W1) = 1 for all l ∈ [L]. Therefore, I1 \\ I1 = ∅ holds with probability 1. By Lemma 12, we complete the proof. F P ROOFS FOR TFN A PPROXIMATION In this section, we not only provide the proof for the results outlined in Sec. 4, but also introduce the problem setting for TFNs with single-head attention layers and present the corresponding results. F.1 A PPROXIMATING TRANSFORMER NETWORK WITH SINGLE -HEAD ATTENTION LAYERS In this part, we outline the problem setting to investigate the expressive power of LoRA in TFNs that utilize single-head attention layers. The primary distinction between this setting and that of TFNs with multi-head attention layers lies in the weight matrices. Specifically, the Wh Ol matrices for combining different attention heads are absent in this case. Despite this difference, the derived results are consistent, albeit under slightly modified assumptions regarding the weight matrices and a different LoRA adaptation strategy. We start by introducing necessary notations. For an input matrix X ∈ RD×N , where D is the dimension of the token embeddings and N is the number of tokens, the l-th Transformer block using single-head self-attention can be expressed as: Attnl(Zl−1) = WV lZl−1 · softmax \u0000 (WKlZl−1)⊤WQlZl−1 \u0001 , Zl := W2l · ReLU(W1l · Attnl(Zl−1) + b1l1⊤ N ) + b2l1⊤ N , 33Published as a conference paper at ICLR 2024 where the weight matrices WKl, WQl, WV l, W1l, W2l ∈ RD×D, bias vectors b1l, b2l∈RD , Zl is the output of l-th transformer block, with Z0 = X. The output of the first L Transformer blocks are subsequently fed into the output layer. This produces the final output of the TFN, given by softmax(WoZL), where Wo ∈ RD×D represents the weight matrix of the output layer. For single-head self-attention layers, the target model f, frozen model f, and the adapted model f can be formally represented as: Target TFN g = TFNL,D \u0000 ·; \u0000 (WV l, WKl, WQl, W2l, W1l)L l=1, Wo \u0001 , (b1l, b2l)L l=1 \u0001 , Frozen TFN f0 = TFNL,D \u0000 ·; \u0000 (WV l, WKl, WQl, W2l, W1l)L l=1, Wo \u0001 , (b1l, b2l)L l=1 \u0001 , Adapted TFN f = TFNL,D \u0010 ·; \u0000 (WV l+ ∆WV l, WKl + ∆WKl, WQl + ∆WQl, W2l + ∆W2l, W1l + ∆W1l)L l=1, Wo + ∆Wo \u0001 , (bb1l, bb2l)L l=1 \u0011 . Here, WKl, WQl, WV lare the weight matrices for generating key, query, and values in the l- th transformer block of the target TFN; W1l, W2l and b1l, b2l serve as the weight matrices and bias vectors, respectively, for the feedforward layer in the same block; Wo is the weight ma- trix for the output layer. For the frozen TFN, the same roles are played by WKl, WQl, WV l, W1l, W2l, and b1l, b2l for all l ∈ [L] and Wo. For the adapted model, low-rank adapters ∆WKl, ∆WQl, ∆WV l, ∆W1l, ∆W2l, ∆Wo with a rank constraint R ∈ [D] are added to each weight matrix, and the bias vectors are updated to bb1l, bb2l for all l ∈ [L]. Given the problem setting outlined above, we give the non-singularity assumption for TFNs with single-head attention layers. Assumption 3 (Non-Singularity). All the weight matrices of both the target model and the frozen model, as well as the following matrices for all r ∈ [D], W⊤ KlWQl + LRr \u0010 W ⊤ KlWQl − W⊤ KlWQl \u0011 , where l = 1, WKlWQl + LRr \u0010 W−1⊤ 2,l−1W ⊤ 2,l−1W ⊤ KlWQlW2,l−1W−1 2,l−1 − WKlWQl \u0011 , for l ∈ [L] \\ {1}, W1lWV l+ LRr \u0000 W1lWV l− W1lWV l \u0001 , for l = 1, W1lWV l+ LRr \u0010 W1lWV lW2,l−1W−1 2,l−1 − W1lWV l \u0011 , for all l ∈ [L] \\ {1}, WoW2L + LRr(WoW2L − WoW2L), are non-singular. Lemma 13. Let the elements of all weight matrices in target model f and the frozen model f be independently sampled from continuous distributions. Then, Assumption 3 holds with probability 1. Proof of Lemma 13. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3. Theorem 8. Consider the rank of the adapter weight matrices R ∈ [D]. Let Assumption 3 hold. Define the rank-based functionality gap Gi to i-th transformer block ( i ∈ [L]) or output layer (i = L + 1) as Gi =    maxh \u0010 rank(W h⊤ Ki W h Qi − Wh⊤ Ki Wh Qi) \u0011 ∨ maxh \u0010 rank(W1iW h V i− W1iWh V i) \u0011 , i= 1, maxh \u0010 rank(W ⊤ 2,i−1W h⊤ Ki W h QiW2,i−1 − W⊤ 2,i−1Wh⊤ Ki Wh QiW2,i−1) \u0011 ∨maxh \u0010 rank(W1iW h V iW2,i−1 − W1iWh V iW2,i−1) \u0011 , 2 ≤ i ≤ L, rank(WoW2L − WoW2L), i = L + 1. If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, there exists rank- R or lower weight matrices for low-rank adapters (∆WKl, ∆WQl, ∆WV l, ∆W1l)L l=1, ∆W2L, ∆Wo with other low-rank adapters set to O, and updated bias vectors: (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates f, i.e., f(X) = f(X), with probability 1. 34Published as a conference paper at ICLR 2024 Proof of Theorem 8. Let Hl ∈ RD×N and Zl ∈ RD×N denote the intermediate and final outputs of the l-th transformer block in the target model f, respectively. Specifically, Hl represents the output from the first feedforward layer in the l-th transformer block. They are defined as Hl = ReLU   W1lWV lZl−1 · softmax \u0010 Z ⊤ l−1W ⊤ KlWQlZl−1 \u0011 + b1l1⊤ N ! , Zl = W2lHl + b2l1⊤ N , where l ∈ [L]. For the adapted model f, we introduce cHl and bZl to denote the corresponding intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively: cHl = ReLU   (W1l + ∆W1l)(WV l+ ∆WV l) · bZl−1 · softmax \u0010 bZ⊤ l−1(WKl + ∆WKl)⊤(WQl + ∆WQl) bZl−1 \u0011 + bb1l1⊤ N ! , bZl = (W2l + ∆W2l)cHl + bb2l1⊤ N , where l ∈ [L]. We note that Z0 = bZ0 = X. In this proof, we set ∆W2l = O for all l ∈ [L]. Our goal is to show that adding low-rank adapters to self-attention layers and the first feedforward layers in all transformer blocks enables the adapted model f to be functionally equivalent to the target model f of the same dimensions. We start by inductively constructing the adapter weight matrices(∆W1l, ∆WV l, ∆WKl, ∆WQl, bb1l, bb2l)L l=1 such that cHl = Hl for all l ∈ [L]. We then select the low-rank adapters for W2L and the Wo to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O. When l = 1. To achieve cHl with Hl for all X, the following conditions must be satisfied: Bias Vector:bb1l = b1l, Query and Key: (WKl + ∆WKl)⊤(WQl + ∆WQl) = W ⊤ KlWQl Value and First Feedforward Layer: (W1l + ∆W1l)(WV l+ ∆WV l) = W1lWV l. To achieve this, we set bb1l = b1l to achieve (24), and select rank- R or lower matrices ∆WKl, ∆WQl, ∆W1l, ∆WV las suggested by Lemma 7. This ensures cHl = Hl for l = 1. When l >1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l − 1, which is cHl−1 = Hl−1. This implies Hl−1 = W −1 2,l−1(Zl−1 − b2,l−11⊤ N ) = W−1 2,l−1( bZl−1 − bb2,l−11⊤ N ) = cHl−1. Using this assumption, we express bZl−1 in terms of Zl−1: bZl−1 = W2,l−1W −1 2,l−1(Zl−1 − b2,l−11⊤ N ) + bb2,l−11⊤ N . Let bb2,l−1 = W2,l−1W −1 2,l−1b2,l−1, then we have bZl−1 = W2,l−1W −1 2,l−1Zl−1. (22) To achieve cHl = Hl, we express both cHl and Hl in terms of Zl−1: Hl = ReLU \u0010 W1lWV l· Zl−1 · softmax \u0010 Z ⊤ l−1W ⊤ KlWQlZl−1 \u0011 + b1l1⊤ N \u0011 cHl = ReLU \u0010 (W1l + ∆W1l)(WV l+ ∆WV l) · bZl−1 35Published as a conference paper at ICLR 2024 · softmax \u0010 bZ⊤ l−1(WKl + ∆WKl)⊤(WQl + ∆WQl) bZl−1 \u0011 + bb1l1⊤ N \u0011 , (22) = ReLU   (W1l + ∆W1l)(WV l+ ∆WV l) · W2,l−1W −1 2,l−1Zl−1 · softmax \u0010 Z ⊤ l−1W −1⊤ 2,l−1W⊤ 2,l−1(WKl + ∆WKl)⊤ (WQl + ∆WQl)W2,l−1W −1 2,l−1Zl−1 \u0011 + bb1l1⊤ N ! . Therefore, we need to align the following three components: Bias Vector: bb1l = b1l, Query and Key: (WKl + ∆WKl)⊤(WQl + ∆WQl) = W−1⊤ 2,l−1W ⊤ 2,l−1W ⊤ KlWQlW2,l−1W−1 2,l−1, Value and First Feedforward Layer: (W1l + ∆W1l)(WV l+ ∆WV l) = W1lWV lW2,l−1W−1 2,l−1. By setting bb1l based on (26) and adjusting ∆WKl, ∆WQl, ∆W1l, ∆WV lbased on Lemma 7, we satisfy all three conditions above, thereby obtaining cHl = Hl for l ∈ [L] \\ {1}. Output Layer Analysis. By the induction method, we have established cHl = Hl for all l ∈ [L]. We will complete the proof by showing that f(X) = f(X) for all X ∈ X. The final output distribution of the target TFN f can be written as f(X) = softmax(WoZL) = softmax \u0000 Wo \u0000 W2LHL + b2L1⊤ N \u0001\u0001 . We can similarly formulate the final output distribution of the adapted model f : f(X) = softmax((Wo + ∆Wo) bZL) = softmax \u0010 (Wo + ∆Wo) \u0010 (W2L + ∆W2L)cHL + bb2L1⊤ N \u0011\u0011 , To align these two expressions, we select ∆W2L and ∆Wo based on Lemma 7, and let bb2L = (Wo + ∆Wo)−1Wob2L, where Wo + ∆Wo is invertible as shown in the proof of Lemma 7. Thus, the proof is complete. The following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 9. Assume that the elements of all the weight matrices of both the target TFN and the frozen TFN are independently drawn from arbitrary continuous distributions. If R ≥ ⌈ D 2 ⌉, adding low-rank adapters of rank at most R to weight matrices in (∆WKl, ∆WQl, ∆WV l, ∆W1l)L l=1, ∆W2L, ∆Wo and tuning the bias vectors, enables the adapted model f to exactly approximate the target modelf, i.e., f(X) = f(X) for all X ∈ RD×N . Proof of Corollary 9. By combining Lemma 13 and Theorem 8, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome. F.2 A PPROXIMATING TRANSFORMER NETWORK WITH MULTI -HEAD ATTENTION LAYERS In this section, we first provide the explicit formulation of TFN with multi-head attention layers. Consider an input matrix X ∈ RD×N , where D is the dimension of the token embeddings and N is the number of tokens. The output of the l-th transformer block is denoted as Zl, which can be computed as follows: Attnl(Zl−1) := HX h=1 Wh OlWh V lZl−1 · softmax \u0000 (Wh KlZl−1)⊤Wh QlZl−1 \u0001 , Zl := W2l · ReLU(W1l · Attnl(Zl−1) + b1l1⊤ N ) + b2l1⊤ N , 36Published as a conference paper at ICLR 2024 where we define Z0 = X. Here, H is the number of attention heads. The weight matrices for each head h ∈ [H] in the l-th transformer block are Wh Ol, Wh V l, Wh Kl, Wh Ql ∈ RD×D. The softmax operator softmax(·) is applied column-wise to the matrix. Further, W2l, W1l ∈ RD×D are the weight matrices and b1l, b2l ∈ RD are the bias vectors in the feedforward layers. A Transformer network, denoted asTFNL,D, is a composition ofL Transformer blocks, followed by an softmax output layer softmax(Wo ·), where Wo ∈ RD×D. The final output of the TFN is given by softmax(WoZL). To study the expressive power of LoRA within TFNs featuring multi-head attention layers, we next specify the parameters of the target model f, frozen model f0, and the adapted model f, each with L transformer blocks and a dimension D. To study the expressive power of LoRA within TFNs featuring multi-head attention lay- ers, we next specify the parameters of the target model f, frozen model f0, and the adapted model f, each with L transformer blocks and a dimension D. For ease of presentation, we drop the subscript in TFNL,D, referring to it simply as TFN. Given a specified rank R ∈ [D] for LoRA, these models are defined as follows: Target TFNf = TFN \u0010 ·; \u0010 ((W h Ol,W h V l,W h Kl,W h Ql)H h=1,W2l,W1l)L l=1,Wo \u0011 ,(b1l,b2l)L l=1 \u0011 , Frozen TFNf0 = TFN\u0000·; \u0000((Wh Ol,Wh V l,Wh Kl,Wh Ql)H h=1,W2l,W1l)L l=1,Wo \u0001,(b1l,b2l)L l=1 \u0001, Adapted TFNf = TFN \u0010 ·; \u0000((Wh Ol + ∆Wh Ol,Wh V l+ ∆Wh V l,Wh Kl + ∆Wh Kl,Wh Ql + ∆Wh Ql)H h=1, W2l + ∆W2l,W1l + ∆W1l)L l=1,Wo + ∆Wo \u0001,(bb1l,bb2l)L l=1 \u0011 , where the weight matrices ∈ RD×D, and the bias vectors ∈ RD. Moreover, the weight matrices of the low-rank adapters ∆Wh Ol, ∆Wh V l, ∆Wh Kl, ∆Wh Ql, ∆W2l, ∆W1l for all h ∈ [H] and l ∈ [L] are of rank R or lower. We next introduce non-singularity Assumption 4 for TFN with multi-head attention layers scenar- ios, which is then validated by Lemma 14. We then provide proof of our main results for TFNs — Theorem 7. Additionally, we introduce a supplementary theorem that amalgamates results for TFNs with both single-head and multi-head attention layers when the weight matrices are randomly initialized. This is articulated in Corollary 10. Assumption 4 (Non-Singularity). For a fixed R ∈ [D], all the weight matrices of both the target model and the frozen model and the following matrices for all r ∈ [R], Wh⊤ Kl Wh Ql + LRr \u0010 W h⊤ KlW h Ql −Wh⊤ Kl Wh Ql \u0011 , for allh ∈[H] andl = 1, Wh⊤ Kl Wh Ql + LRr \u0010 W−1⊤ 2,l−1W ⊤ 2,l−1W h⊤ KlW h QlW2,l−1W−1 2,l−1 −Wh⊤ Kl Wh Ql \u0011 , for allh ∈[H] andl ∈[L] \\ {1}, Wh OlWh V l+ LRr \u0010 W−1 1l W1lW h OlW h V l−Wh OlWh V l \u0011 , for allh ∈[H] andl = 1, Wh OlWh V l+ LRr \u0010 W−1 1l W1lW h OlW h V lW2,l−1W−1 2,l−1 −Wh OlWh V l \u0011 , for allh ∈[H] andl ∈[L] \\ {1}, WoW2L + LRr(WoW2L −WoW2L), are non-singular. Lemma 14. Let the elements of all weight matrices in the target model f and frozen model f0 be independently sampled from continuous distributions. Then, Assumption 4 holds with probability 1. Proof of Lemma 14. The results can be obtained by replicating the same steps outlined in the proof of Lemma 3. For the reader’s reference, we restate Theorem 7 here integrated with the explicit formulation of the rank-based functionality gap Gi. Theorem 7. Consider a given LoRA-rank R ∈ [D]. Let Assumption 4 hold. Define the rank-based functionality gap Gi to i-th transformer block (i ∈ [L]) or output layer (i = L + 1) as 37Published as a conference paper at ICLR 2024 Gi =   maxh \u0010 rank(W h⊤ KiW h Qi −Wh⊤ Ki Wh Qi) \u0011 ∨maxh \u0010 rank(W1iW h OiW h V i−W1iWh OiWh V i) \u0011 , i= 1, maxh \u0010 rank(W ⊤ 2,i−1W h⊤ KiW h QiW2,i−1 −W⊤2,i−1Wh⊤ Ki Wh QiW2,i−1) \u0011 ∨maxh \u0010 rank(W1iW h OiW h V iW2,i−1 −W1iWh OiWh V iW2,i−1) \u0011 , 2 ≤i ≤L, rank(WoW2L −WoW2L), i = L+ 1. (23) If R ≥ maxi∈[L+1]⌈Gi 2 ⌉, then there exists low-rank adapters with rank lower than R ∈ [D] ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo with other low-rank adapters set toO, and updated bias vectors (bb1l, bb2l)L l=1, such that for any X ∈ RD×N , the adapted model f exactly approximates target model f, i.e., f(X) = f(X). Proof of Theorem 7. The key idea of this proof is the same as the proof of Theorem 8: our first step is to ensure that, for each transformer block, the output from the first feedforward layer in the target model matches that in the adapted model. Once this is established, we select an appropriate output layer weight matrix to complete the proof. Similar to the proof of Theorem 8, we define Hl ∈ RD×N and Zl ∈ RD×N as the intermediate and final outputs of the l-th transformer block in the target model f, respectively. In particular, Hl corresponds to the output of the first feedforward layer in the l-th transformer block. They are formulated as Hl = ReLU   W1l  HX h=1 W h OlW h V l· Zl−1 · softmax \u0010 Z ⊤ l−1W h⊤ Kl W h QlZl−1 \u0011! + b1l1⊤ N ! , Zl = W2lHl + b2l1⊤ N . For the adapted model f, we introduce cHl and bZl accordingly to denote the intermediate output of the first feedforward layer and the final output of the l-th transformer block for the adapted model, respectively: cHl = ReLU   W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · bZl−1 · softmax \u0010 bZ⊤ l−1(Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) bZl−1 \u0011\u0011 + bb1l1⊤ N ! , bZl = W2l cHl + bb2l1⊤ N . Note that Z0 = bZ0 = X. We aim to demonstrate that adding low-rank adapters to the weight matrices allows the adapted TFN f to be functionally equivalent to the target TFN of identical dimen- sions. We will initiate our proof by inductively constructing the adapter weight matrices ((∆Wh Ol, ∆Wh V l, ∆Wh Kl, ∆Wh Ql)H h=1, bb1l, bb2l)L l=1 such that cHl = Hl for all l ∈ [L], and then select the ∆W2L and the low-rank adapter for the output layer ∆Wo to approximate the output of the target model. For unmentioned low-rank adapters, we set them as O. When l = 1. To achieve cHl with Hl for all X, we must satisfy the following conditions: Bias Vector:bb1l = b1l, (24) Query and Key: (Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) = W h⊤ Kl W h Ql, Value and Output Projection: (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) = W−1 1l W1lW h OlW h V l. To achieve this, we set bb1l = b1l to achieve (24), and select rank- R or lower matrices ∆Wh Kl, ∆Wh Ql, ∆Wh Ol, ∆Wh V lfor all h ∈ [H] as suggested by Lemma 7. This ensures cHl = Hl for l = 1. 38Published as a conference paper at ICLR 2024 When l >1. Now we focus on the cases where l = 2, . . . , L. Assume the induction hypothesis holds for l − 1, which is cHl−1 = Hl−1. Following the same steps in the proof of Theorem 8, we let bb2,l−1 = W2,l−1W −1 2,l−1b2,l−1, thereby obtaining, bZl−1 = W2,l−1W −1 2,l−1Zl−1. (25) To achieve cHl = Hl, we express both cHl and Hl in terms of Zl−1: Hl = ReLU \u0010 W1l \u0010 HX h=1 W h OlW h V l· Zl−1 · softmax \u0010 Z ⊤ l−1W h⊤ Kl W h QlZl−1 \u0011\u0011 + b1l1⊤ N \u0011 cHl = ReLU \u0010 W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · bZl−1 · softmax \u0010 bZ⊤ l−1(Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) bZl−1 \u0011\u0011 + bb1l1⊤ N \u0011 , (25) = ReLU   W1l \u0010 HX h=1 (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) · W2,l−1W −1 2,l−1Zl−1 · softmax \u0010 Z ⊤ l−1W −1⊤ 2,l−1W⊤ 2,l−1(Wh Kl + ∆Wh Kl)⊤ (Wh Ql + ∆Wh Ql)W2,l−1W −1 2,l−1Zl−1 \u0011\u0011 + bb1l1⊤ N ! . Therefore, we need to align the following three components: Bias Vector: bb1l = b1l, (26) Query and Key: (Wh Kl + ∆Wh Kl)⊤(Wh Ql + ∆Wh Ql) = W−1⊤ 2,l−1W ⊤ 2,l−1W h⊤ Kl W h QlW2,l−1W−1 2,l−1, Value and Output Projection: (Wh Ol + ∆Wh Ol)(Wh V l+ ∆Wh V l) = W−1 1l W1lW h OlW h V lW2,l−1W−1 2,l−1. By setting bb1l based on (26) and adjusting ∆Wh Kl, ∆Wh Ql, ∆Wh Ol, ∆Wh V lfor all h ∈ [H] based on Lemma 7, we satisfy all three conditions above, thereby obtaining cHl = Hl for l ∈ [L] \\ {1}. Output Layer Analysis. By applying the induction method, we have established cHl = Hl for all l ∈ [L]. Lastly, we choose the ∆Wo, ∆W2L and the bias vector bb2L using the same approach as in the proof of Theorem 8. This concludes the proof. The following corollary identifies the specific LoRA-rank required to achieve exact representation for random model cases in the current setting. Corollary 10. Assume that the elements of all the weight matrices of both the tar- get TFN and the frozen TFN are independently drawn from arbitrary continuous distribu- tions. If R ≥ ⌈D 2 ⌉, adding low-rank adapters of rank at most R to weight matrices in ((∆Wh Kl, ∆Wh Ql, ∆Wh V l, ∆Wh Ol)H h=1)L l=1, ∆W2L, ∆Wo and tuning the bias vectors, enables the adapted model f to exactly approximate the target model f, i.e., f(X) = f(X) for all X ∈ RD×N . Proof of Corollary 10. By combining Lemma 14 and Theorem 7, and following the same steps in the proof of Corollary 4 which yields maxi Gi = D, we can obtain the desired outcome. G E XPERIMENTS In this section, we perform experiments on both synthetic and real datasets to corroborate our the- oretical results. Firstly, we focus on validating the construction of the LoRA adapter in our proof. 39Published as a conference paper at ICLR 2024 Subsequently, we extend our experimental validation to encompass the effects of tuning final lay- ers and the significance of updatable bias. Additionally, we offer visual representations of training curves, assess the generalization performance of LoRA, and evaluate its efficacy on classification tasks. We also conduct experiments on real datasets to further support our theoretical insights in real-world scenarios. G.1 A DDITIONAL DETAILS OF EXPERIMENT SETUP We implement LoRA adapter∆W by reparameterizing it as ∆W = AB⊤, where A, B ∈ RD×R, and we use the same initialization scheme as proposed by Hu et al. (2022a). For experiments pre- sented in Sec. 5, G.3.1, G.3.2, G.4, and G.5, we consider two variants of frozen models: • (Random) The first method involves randomly generating all the weight matrices using the Xavier uniform distribution, which is the default weight initialization method used in PyTorch. • (Pretrained) The second method aims to simulate scenarios where the pretrained model is rel- atively closer to the target model. We achieve this by initially creating the target model and the frozen model in the same way as the first method and then performing full-rank updates on the frozen model via gradient descent to approximate the target model until the approximation error is reduced by 1/3. For other experiments on synthetic datasets, we default to the randomly parameterized frozen model unless specified otherwise. G.2 A DDITIONAL DETAILS ON GRADIENT UPDATE METHOD In our experiments, we utilize the Adam optimizer. We tune the learning rate∈ \b 10−2, 10−3, 10−4\t and the weight decay ∈ \b 0, 10−2, 10−3, 10−4\t . The optimal configuration is determined based on the validation loss on a set of 256 samples independently drawn from a standard normal distribution. We run 5,000 iterations for each hyperparameter setting, where at each step 256 fresh standard Gaussian samples are generated for loss and gradient computation. G.3 V ALIDATION OF OUR LORA A DAPTER CONSTRUCTION Recall that all our theoretical statements are based on our construction of the LoRA adapters pre- sented in their corresponding proofs. To validate these results, here we empirically examine the relationship between approximation error and rank by integrating the LoRA adapters, which are constructed with the uniform partition in our proof, into the frozen model. Furthermore, we evaluate the effectiveness of our constructed LoRA adapters by comparing their performance against adapters updated through gradient descent and optimized by Adam. All simulations are conducted five times using different seeds, and the reported values represent the median computed across different runs. G.3.1 FNN A PPROXIMATION 4 8 12 16 0.00 0.05 0.10 L =1, L =2 4 8 12 16 L =2, L =4 Gradient Update Our Construction Rank MSE (a) Frozen model is randomly generated. 4 8 12 16 0.00 0.01 0.02 L =1, L =2 Gradient Update Our Construction 4 8 12 16 L =2, L =4 Rank MSE (b) Frozen model is pretrained. Figure 3: Approximation error (measured by MSE) versus LoRA-rank on FNNs. In this experiment, we assess the effectiveness of our low-rank adapter construction for FNN ap- proximation, which is detailed in the proof of Theorem 5. 40Published as a conference paper at ICLR 2024 4 8 12 16 10−8 10−4 L =1, L =2 4 8 12 16 L =2, L =4 Gradient Update Our Construction Rank log(MSE) Figure 4: Log-scale MSE versus LoRA- rank on randomly initialized FNNs. Setup. We consider two scenarios: one with L = 1 and L = 2 and the other one with L = 2 and L = 4 . It should be noted that for both these cases, we have M = ⌊L/L⌋ = 2 here. We employ the gradient update method and the construction outlined in the proof of Theorem 5 to update the LoRA adapters. Results. Fig. 3 presents the results for FNN approxima- tion. Consistent with the implications drawn in Sec. 5, the y limit changes from Fig. 3a to Fig. 3b suggest that the pretrained frozen model results in less ap- proximation error. Additionally, we observe that our construction’s performance aligns closely with the gradient update method when the target model depth L = 1 . However, this alignment is not observed when L = 2 on low-rank region (i.e., R ≤ 4), This further underscores the limitation of our LoRA adapter construction, which inherently assumes that the intermediate outputs of the frozen model and the target model need to align. To facilitate a more effective comparison between our construction and the gradient update method in the higher-rank region (i.e., R ≥ 6), we present the curves on a logarithmic scale, as depicted in Fig. 4. While the gradient update appears to reach the optimal performance achieved by our LoRA construction in FNNs, a gap is still discernible when viewed on a logarithmic scale. The MSE of the gradient update method is approximately 10−4, while for our LoRA construction, it’s around 10−8 for a sufficiently large rank. G.3.2 TFN A PPROXIMATION We assess the effectiveness of our LoRA adapter construction in approximating TFN, as detailed in the proof of Theorem 7. Setup. We examine target model f and frozen model f, both featuring the same architecture with L transformer blocks, a single output layer, two attention heads, and embedding size D = 16. We focus on two scenarios: L = 1 and L = 2 . The weight matrices for the attention layers follow a standard Gaussian distribution, while those for the linear layers are initialized using the Xavier uniform distribution, which is PyTorch’s default scheme for linear layer initialization. 4 8 12 16 10−4 101 L =1 4 8 12 16 L =2 Gradient Update Our Construction Rank log(MSE) (a) Frozen model is randomly generated. 4 8 12 16 10−4 100 L =1 4 8 12 16 L =2 Gradient Update Our Construction Rank log(MSE) (b) Frozen model is pretrained. Figure 5: Approximation error (measured by MSE) versus LoRA-rank on TFNs. Results. The observations here align with those from the experiments of FNN approximation. We note that the gradient update method outperforms our approach when the rank is relatively small but lags behind as the rank increases. This advantage of the gradient update method at minimal ranks arises from the inherent complexity of TFNs, which allows for more flexible low-rank adapter con- struction. Meanwhile, the gradient update method’s performance does not significantly improve as the rank increases. This arises from the inherent complexity involved in optimizing TFNs. Nonethe- less, our results corroborate the claims made in Theorem 7, as the approximation error must be eradicated when the rank reaches ⌈D 2 ⌉ = 8. 41Published as a conference paper at ICLR 2024 2000 40000.00 0.05 0.10 Random 2000 4000 Pretrained LoRA Tuning Final Layers # Tunable Parameters MSE (a) Comparison between LoRA and tuning final layers. 5 10 15 0.00 0.05 0.10 Random 5 10 15 Pretrained Fixed Biases Updatable Biases # Tunable Parameters MSE (b) Comparison between LoRA with fixed biases and LoRA with updatable biases. Figure 6: Approximation error (measured by MSE) versus the number of tunable parameters when various methods are employed. The analyses are conducted on FNN models. G.4 C OMPARISON TO TUNING FINAL LAYERS Tuning or adding the final layers only is also a common adaptation method used in various domains, including computer vision (Chatfield et al., 2014; Donahue et al., 2014; Sharif Razavian et al., 2014), and natural language processing (Devlin et al., 2019; Gira et al., 2022). Recall that Corollary 4 and Lemma 12 demonstrate that tuning final layers does not perform as well as LoRA for randomly generated models, provided the LoRA-rank satisfies the rank constraints shown in Corollary 4. In this experiment, we aim to validate this assertion and compare the performance of tuning final layers and LoRA in more general scenarios, such as when the frozen model has been pretrained, and when the LoRA-rank is smaller than required. Setup. We consider FNN models withD = 16, L = 1, L= 8. In this experiment, we employ two baselines: LoRA and tuning final layers. The LoRA adapters and the final layers are updated using the gradient update method. Results. Figure 6a compares the MSE of LoRA and final layer tuning when the same number of tunable parameters are used. In the case of randomly generated models, we observe that final layer tuning yields a significantly higher MSE when using the same number of tunable parameters, corroborating our results in Lemma 12. However, when the frozen model has been pretrained, the performance of final layer tuning improves considerably, though it still falls short of LoRA. This aligns with conclusions drawn from previous theoretical studies such as Tripuraneni et al. (2020), which asserts that the performance of final layer tuning heavily depends on the quality of the shared representations. G.5 B ENEFITS OF TUNING BIASES In our proof, as detailed in Sec. 3.2 and E.1, the updatable biases in the FNN play a crucial role in eliminating the nonlinearity of ReLUs. In this experiment, we investigate the importance of updatable biases in ensuring the success of LoRA in FNN cases. Setup. We consider FNN models with parameters D = 16 , L = 1 , L= 2 , and examine the performance of LoRA both with and without biases tuning for adapting it to match the target FNN. The LoRA adapters and biases are updated using the gradient update method. Results. The performance of LoRA with and without updatable biases is presented in Figure 6b. We observe that in both random and pretrained model cases, LoRA with updatable biases outper- forms LoRA with fixed biases when the number of tunable parameters is relatively small. However, the performance gap is not significant and diminishes as the number of tunable parameters increases. This suggests that while tuning biases in conjunction with the low-rank adapters does enhance per- formance, the gain is not substantial. In other words, even without bias tuning, LoRA’s performance remains competitive. G.6 T RAINING CURVES 42Published as a conference paper at ICLR 2024 0 2000 4000 10−5 10−2 L = 1, L= 2 0 2000 4000 10−3 10−2 L = 2, L= 4 rank = 1 rank = 4 rank = 8 rank = 13 rank = 16 Epoch log(Train Loss) Figure 7: Training curves of LoRA with varying LoRA-ranks when D = 16. Although our theoretical study does not incorporate any training process, we present the training curves of the LoRA gradient update method to illuminate the optimiza- tion aspects of LoRA. Setup We depict the training curves of LoRA fine- tuning on randomly generated FNNs for R = 1, 4, 8, 13, 16. Unless stated otherwise, all settings strictly adhere to the FNN experiments described in Sec. 5. Results The training curves visualized in Fig. 7 reveal that models with smaller ranks (e.g., R=1,4) converge swiftly due to their limited search space, but they settle at a relatively high training loss. Medium rank models (e.g., R=8) converge more slowly. Highly overparameterized models (g, R=13,18) appear to converge faster, aligning with recent advancements in optimization theory, which suggest that overparameterized models are easier to optimize (Liu et al., 2022a). G.7 G ENERALIZATION PERFORMANCES While our theoretical study only establishes the upper bound of LoRA’s performance with infinite data samples, it does not consider LoRA’s generalization performance in practice. Although this is beyond the current scope of our paper, we empirically investigate LoRA’s generalization perfor- mance in this experiment. Setup. We include a training set of 400 samples for the cases whereL = 1, L= 2, and 800 training samples for the cases where L = 2, L= 4. We evaluate how well LoRA’s training performance transfers to the test set. 0 4 8 12 16 10−5 10−2 L = 1, L= 2 0 4 8 12 16 10−3 L = 2, L= 4 Train Test Rank MSE Figure 8: Assessment of LoRA’s gener- alization performance on FNNs. Results. Fig. 8 presents the training and test MSE ver- sus LoRA-ranks. However, no clear pattern is observed in the variation of the gap between the training and test MSE with respect to the LoRA-ranks. This could be due to Adam not precisely finding the minimum (see Fig. 4), potentially avoiding overfitting. To assess LoRA’s generalization performance, we fine- tuned the frozen model on the training set and reported the training and test MSE. We notice an increasing gen- eralization gap (test MSE - train MSE) as the LoRA rank increases – this is very evident with L=2, and less so with L=4. This is intuitive as larger LoRA ranks imply a larger hypothesis class (e.g., the Rademacher complexity), so it is expected. We defer a detailed analysis of LoRA’s generalization performance to future work but believe our simulation results provide a valuable starting point for further discussion and investigation. G.8 E VALUATION ON CLASSIFICATION TASKS Our theory and previous experiments all focus on regression cases. In this experiment, we consider binary and multi-class classification tasks to optimize the LoRA adapter vias cross-entropy and report the performance of LoRA using accuracy. Multi-class Classification. As shown in Fig. 9a, consistent with our theoretical results, our con- struction achieves 100% accuracy when R ≥ 8. The performance of gradient update is also similar to our observation when MSE is employed as the metric, particularly when MSE is plotted on a log- arithmic scale (Fig. 4). This observation echoes the findings of Hui & Belkin (2021), which indicate that optimizing MSE is fundamentally equivalent to optimizing cross-entropy. Binary Classification. We have conducted binary classification tasks. We use the same setup as before but add one more output layer ∈ R2×D which is a block diagonal matrix, with the first 8 43Published as a conference paper at ICLR 2024 0 4 8 12 16 0.5 1.0 L = 1, L= 2 Gradient Update Our Construction 0 4 8 12 16 L = 2, L= 4 Rank Accuracy (a) Multi-class classification tasks with 16 classes. 0 4 8 12 16 0.6 0.8 1.0 L = 1, L= 2 Gradient Update Our Construction 0 4 8 12 16 L = 2, L= 4 Rank Accuracy (b) Binary classification task. Figure 9: Accuracy versus the rank on classification tasks. The analyses are conducted on FNN models. elements in the first rows and the last 8 elements in the second row are 1 and all remaining elements are 0. We fix this output layer, optimize the cross entropy on the LoRA adapters, and report the test accuracy. As shown in Fig. 9b, we observe that in this binary classification scenario, even with a very low LoRA-rank R = 1, the accuracy has been significantly improved, comparable to the results achieved by higher ranks. In the region of higher ranks, our construction significantly outperforms the gradi- ent update method. The suboptimal performance of the gradient update method in this simulation suggests that, despite LoRA’s current impressive performance in practical applications, there is po- tential for further refinement. G.9 E VALUATION ON REAL DATASETS In our theoretical analysis, we demonstrate how the sizes of frozen models and the distance between the frozen and target models influence the necessary LoRA-ranks to achieve the desired performance (see Lemma 1, 2, and Theorem 5, 6, 7). Specifically, our results suggest that larger models require fewer LoRA-ranks to reach the desired performance. Similarly, when the frozen model is closer to the target model, a lower LoRA-rank is sufficient to achieve the same performance. We validate these theoretical insights through experiments on the GLUE benchmark (Wang et al., 2018). Setup Our experiments are conducted using Tesla V100-PCIE-16GB, NVIDIA A100-SXM4- 80GB, NVIDIA A100-SXM4-40GB, and NVIDIA L40 GPUs. For each run, a single GPU is uti- lized. Unless otherwise specified, all our settings align with those established by Hu et al. (2022a). Impact of Model Size on LoRA Rank In practice, most existing studies on LoRA use the same LoRA-rank for models of varying sizes. For instance, in the original LoRA paper (Hu et al., 2022a), Tables 9 and 10 demonstrate the use of the same LoRA-rank for RoBERTa-base (Liu et al., 2019), RoBERTa-large (Liu et al., 2019), and DeBERTa-XXL (He et al., 2021). Similarly, in the QLoRA paper (Dettmers et al., 2023), a LoRA-rank of 64 is set for different models ranging from 13B to 65B parameters (see their Appendix B.2). To validate our theoretical findings, we evaluated the per- formance of LoRA on models of different sizes, specifically RoBERTa-base with 110M parameters and RoBERTa-large with 340M parameters. The results are presented in Table 2. Initially, we observe that, in the absence of fine-tuning (LoRA-rank R = 0), there is no consistent trend – RoBERTa-base performs better on 3 datasets, while RoBERTa-large performs better on 4 datasets. However, after LoRA fine-tuning, we observe that RoBERTa-large outperforms in most cases. In fact, even when the base model is trained with a LoRA-rank three times larger, RoBERTa- large still performs better on 6 out 8 datasets. Given that the pretrained RoBERTa-large model was performing no differently from the base model, this observation supports our theoretical findings that deeper models are more expressive with LoRA training. Impact of Model Proximity on LoRA Rank While our theoretical results (Lemma 1, 2, and Theorem 5, 6, 7) imply that the frozen model that is closer to the target model achieves better results for a fixed LoRA-rank. To validate this, we compare the performance of pretrained RoBERTa-base with the randomly initialized RoBERTa-base fine-tuned using the same LoRA-ranks. 44Published as a conference paper at ICLR 2024 Model R MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B RoBERTabase 0 .330 .491 .316 0 .495 .682 .527 .024 RoBERTalarge 0 .318 .505 .684 0 .505 .369 .473 .032 RoBERTabase 2 .861 .950 .892 .632 .928 .891 .780 .907 RoBERTabase 6 .870 .948 .892 .629 .931 .900 .773 .909 RoBERTalarge 2 .904 .956 .917 .631 .946 .887 .884 .916 Table 2: Comparison of the fine-tuned performance of RoBERTa-base and RoBERTa-large using LoRA with different LoRA-ranks on the GLUE benchmark. Following Hu et al. (2022a), we report the overall (matched and mismatched) accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. Despite the absence of a clear pattern indicating which pretrained model is generally superior, after fine-tuning using LoRA, we observe that RoBERTa-large (340M) fine-tuned with LoRA-rankR = 2 outperforms RoBERTa-base (110M) with LoRA-rank R = 6 in 7 out of 8 tasks. This observation aligns with our theoretical conclusion that larger models require lower LoRA-ranks to achieve the desired performance. Model R MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Random 2 .523 .775 .691 .154 .627 .761 .542 .213 Pretrained .861 .950 .892 .632 .928 .891 .780 .907 Random 4 .535 .788 .696 .145 .625 .768 .542 .224 Pretrained .868 .950 .890 .634 .929 .898 .805 .910 Random 6 .544 .799 .696 .154 .632 .768 .542 .210 Pretrained .868 .948 .892 .629 .931 .900 .773 .909 Table 3: Comparison of the fine-tuned performance of randomly initialized and pretrained RoBERTa-base. Following Hu et al. (2022a), we report the overall (matched and mismatched) accuracy for MNLI, Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better for all metrics. We observe that the performance of the pretrained RoBERTa-base significantly surpasses that of the randomly initialized RoBERTa-base given the same LoRA-rank. This observation is consistent with our theoretical findings, which suggest that a frozen model closer to the target model yields better performance given the same LoRA-rank. The results in Table 3 demonstrate that the pretrained RoBERTa-base significantly surpasses the randomly initialized RoBERTa-base. This observation is consistent with our theoretical findings, suggesting that the pretrained model requires lower LoRA-ranks to achieve the desired performance. H E XTENSION TO CASES WITH DIFFERENT MODEL DIMENSIONS This discussion only applies to linear model approximation and FNN approximation. As highlighted in Sec. 2, our results can be easily extended to scenarios where the target model, f, and the frozen model, f, have different model dimensions. Specifically, for linear model or FNN approximation, we use D to represent the number of hidden neurons per layer in the target model and D for the frozen model. We particularly consider the cases where the frozen model is wider than the target model, i.e., D ≥ D. This is because the frozen model is typically overparameterized in practical applications. The key idea for extending our analysis to scenarios with different model dimensions is expanding the dimension of the target model. For the sake of simplicity, we focus on the simplest case, the linear model approximation, as an example. In this setting, the difference between the output of the 45Published as a conference paper at ICLR 2024 adapted model and the target model can be measured by f \u0012\u0014 x 0 \u0015\u0013 − \u0014 f(x) 0 \u0015 = LY l=1 (Wl + ∆Wl) \u0014 x 0 \u0015 − \u0014 Wx 0 \u0015 , (27) where x ∈ RD. Consequently, the last (D − D) columns and rows of QL l=1 (Wl + ∆Wl) does not affect the results at all. Denote the submatrix consisting of the first d rows and d columns of a matrix W by [W]d. Then, to approximate the target model, we aim to solve the following constrained optimization problem for a given LoRA-rank R ∈ [D]: min rank(∆Wl)≤R \r\r\r\r\r \" LY l=1 (Wl + ∆Wl) # D − W \r\r\r\r\r F . To solve this problem, we first define an expanded target matrix, denoted by fW ∈ RD×D. The expanded target matrix fW is constructed such that h fW i D = W, while the remaining entries matches the corresponding entries in Ql = 1LWl. Then, the error matrix E = fW − QL l=1 Wl, consists entirely of zeros except for the first D rows and D columns. Therefore, we obtain RE = rank(E) ≤ D. Given the expanded target matrix, we consider the updated constrained optimization problem as follows: min rank(∆Wl)≤R \r\r\r\r\r LY l=1 (Wl + ∆Wl) − fW \r\r\r\r\r F . (28) By Lemma 1, we obtain that when the LoRA-rank R ≥ ⌊D L ⌋, the optimal solution to (28) satisfies QL l=1 (Wl + ∆Wl) = fW, given thatD ≥ RE. This result implies that hQL l=1 (Wl + ∆Wl) i D = W and therefore the approximation error defined in (27) is 0 for all input x. A similar analysis can be conducted for FNN approximation. I E XTENDED FUTURE WORKS To the best of our knowledge, this paper is the first to offer a theoretical understanding of LoRA fine-tuning on both FNN and TFN. Our work delivers insightful results, elucidating the impact of rank, depth of the pre-trained model, and the distance between the pre-trained model and the target model on the expressive power of LoRA. Those theoretical results are further corroborated via our experiments. Despite these advancements, several intriguing questions still remain open. First, as observed in the numerical experiments, our construction of LoRA adapters for FNN and TFN may not be always optimal. Given that more complex models offer increased flexibility, an open ques- tion is whether we can devise a more parameter-efficient scheme to construct the LoRA adapters, thereby deriving a tighter bound on approximation error. Second, for TFN, we have only identi- fied the conditions under which the LoRA-adapted model exactly matches the target model, due to the analytical complexity of TFN. It would be interesting to quantify the approximation error when the rank is lower than required. Furthermore, for TFN, we constrain the target model and the frozen model to have identical embedding size and depth, and we omit the skip connections and layer norms for simplicity. Another intriguing direction would be to study the expressive power of LoRA under TFN cases with more general settings on TFN architectures. While our analysis does not involve any training process, an interesting direction for future research would be to consider gradient-based optimization algorithms and examine how efficiently LoRA can be optimized. Finally, theoretical questions about LoRA’s generalization to unseen data also remain unresolved. 46",
      "meta_data": {
        "arxiv_id": "2310.17513v3",
        "authors": [
          "Yuchen Zeng",
          "Kangwook Lee"
        ],
        "published_date": "2023-10-26T16:08:33Z",
        "pdf_url": "https://arxiv.org/pdf/2310.17513v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper presents the first theoretical analysis of the expressive power of Low-Rank Adaptation (LoRA) for Fully Connected Neural Networks (FNNs) and Transformer Networks (TFNs). It identifies the minimum LoRA-rank required for a frozen model to exactly match a target model and quantifies the approximation error when the LoRA-rank is below this threshold. For FNNs, it proves that any smaller target model can be accurately represented if the LoRA-rank is sufficient (width * depth of f / M, where M is the ratio of frozen to target depth). For TFNs, it shows adaptation to a target model of the same size with rank-(embedding size/2) LoRA adapters for attention weight matrices. The study offers theoretical insights into hyperparameter tuning and algorithm development, which are empirically validated.",
        "methodology": "The methodology starts with analyzing linear models, extending the Eckart-Young-Mirsky theorem to products of low-rank adapted matrices. For FNNs, it employs a two-step process: (i) 'Linearization' by setting large biases to activate all ReLUs in initial layers, and (ii) 'Weight Matrix Alignment' to match parameters using low-rank adapters and updated biases. This is further extended to multi-layer FNNs using 'model partition' strategies (uniform or general with varying LoRA-ranks). For TFNs, the approach addresses nonlinearities (softmax, ReLU) by segmenting transformer blocks and aligning outputs before activations, primarily adapting self-attention layers. All analyses rely on a non-singularity assumption for weight matrices, which is proven to hold for randomly generated matrices.",
        "experimental_setup": "Experiments are conducted on both synthetic and real datasets. For synthetic data, linear models and FNNs (D=16, varying depths) and TFNs (D=16, L=1, 2 attention heads) are used. Frozen models are either randomly initialized with Xavier uniform distribution or 'pretrained' to be closer to the target model. Validation uses Mean Squared Error (MSE) for regression and accuracy for classification tasks (binary and multi-class). LoRA adapters derived from theoretical construction are compared against those optimized by Adam with tuned learning rates ({10^-2, 10^-3, 10^-4}) and weight decay ({0, 10^-2, 10^-3, 10^-4}). Generalization performance is assessed with 400-800 training samples. Real-world validation employs the GLUE benchmark with RoBERTa-base (110M parameters) and RoBERTa-large (340M parameters), utilizing GPU resources and standard GLUE metrics.",
        "limitations": "The work exclusively focuses on the expressive power of LoRA, consciously excluding optimization and generalization aspects. Theoretical proofs rely on a mild non-singularity assumption for weight matrices and a bounded input space for FNNs. For TFNs, skip connections and layer norms are omitted for analytical feasibility, and the analysis is restricted to models with identical embedding size and depth. The paper's theoretical construction of LoRA adapters is noted to be suboptimal compared to gradient-based methods in some low-rank regions for multi-layer FNNs, possibly due to rigid intermediate output matching or uniform partitioning. For TFNs, the current work lacks quantification of approximation error when ranks are lower than required.",
        "future_research_directions": "Future research includes quantifying approximation errors for TFNs when LoRA-ranks are below the required threshold, refining LoRA adapter update algorithms for more parameter-efficient constructions and tighter error bounds, and extending TFN analysis to more general architectures (e.g., varying embedding sizes, depths, and incorporating skip connections and layer norms). Additionally, investigating gradient-based optimization algorithms for LoRA's efficiency and exploring the theoretical aspects of LoRA's generalization performance to unseen data are open research avenues."
      }
    },
    {
      "title": "Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning",
      "abstract": "We study matrix estimation problems arising in reinforcement learning (RL)\nwith low-rank structure. In low-rank bandits, the matrix to be recovered\nspecifies the expected arm rewards, and for low-rank Markov Decision Processes\n(MDPs), it may for example characterize the transition kernel of the MDP. In\nboth cases, each entry of the matrix carries important information, and we seek\nestimation methods with low entry-wise error. Importantly, these methods\nfurther need to accommodate for inherent correlations in the available data\n(e.g. for MDPs, the data consists of system trajectories). We investigate the\nperformance of simple spectral-based matrix estimation approaches: we show that\nthey efficiently recover the singular subspaces of the matrix and exhibit\nnearly-minimal entry-wise error. These new results on low-rank matrix\nestimation make it possible to devise reinforcement learning algorithms that\nfully exploit the underlying low-rank structure. We provide two examples of\nsuch algorithms: a regret minimization algorithm for low-rank bandit problems,\nand a best policy identification algorithm for reward-free RL in low-rank MDPs.\nBoth algorithms yield state-of-the-art performance guarantees.",
      "full_text": "arXiv:2310.06793v2  [cs.LG]  28 Oct 2023 Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning Stefan Stojanovic EECS KTH, Stockholm, Sweden stesto@kth.se Y assir Jedra EECS KTH, Stockholm, Sweden jedra@kth.se Alexandre Proutiere EECS KTH, Stockholm, Sweden alepro@kth.se Abstract W e study matrix estimation problems arising in reinforceme nt learning (RL) with low-rank structure. In low-rank bandits, the matrix to be re covered speciﬁes the expected arm rewards, and for low-rank Markov Decision Proc esses (MDPs), it may for example characterize the transition kernel of the MD P . In both cases, each entry of the matrix carries important information, and we seek estimation methods with low entry-wise error. Importantly, these meth ods further need to accommodate for inherent correlations in the available dat a (e.g. for MDPs, the data consists of system trajectories). W e investigate the p erformance of simple spectral-based matrix estimation approaches: we show that they efﬁciently recover the singular subspaces of the matrix and exhibit nearly-min imal entry-wise error. These new results on low-rank matrix estimation make it poss ible to devise reinforcement learning algorithms that fully exploit the u nderlying low-rank structure. W e provide two examples of such algorithms: a reg ret minimization algorithm for low-rank bandit problems, and a best policy id entiﬁcation algorithm for reward-free RL in low-rank MDPs. Both algorithms yield s tate-of-the-art performance guarantees. 1 Introduction Learning succinct representations of the reward function or of the system state dynamics in bandit and RL problems is empirically known to signiﬁcantly accelerate the search for efﬁcient policies [ 1, 2, 3]. It also comes with interesting theoretical challenges. T he design of algorithms learning and leveraging such representations and with prov able performance guarantees has attracted considerable attention recently, but remains largely open . In particular, signiﬁcant efforts have been made towards such design when the representation relie s on a low-rank structure. In bandits, assuming such a structure means that the arm-to-reward func tion can be characterized by a low- rank matrix [ 4, 5, 6, 7]. In MDPs, it implies that the reward function, the Q-function or the transition kernels are represented by low-rank matrices [ 8, 9, 10, 11, 12]. In turn, the performance of algorithms exploiting low-rank structures is mainly det ermined by the accuracy with which we are able to estimate these matrices. In this paper, we study matrix estimation problems arising in low-rank bandit and RL problems. T wo major challenges are associated with these problems. (i) Th e individual entries of the matrix carry important operational meanings (e.g. in bandits, an entry c ould correspond to the average reward Preprint. Under review .of an arm), and we seek estimation methods with low entry-wis e error. Such requirement calls for a ﬁne-grained analysis, typically much more involved th an that needed to only upper bound the spectral or Frobenius norm of the estimation error [ 13, 14, 15, 16, 17, 18, 19, 20]. (ii) Our estimation methods should further accommodate for inherent correlati ons in the available data (e.g., in MDPs, we have access to system trajectories, and the data is hence M arkovian). W e show that, essentially, spectral methods successfully deal with these challenges. Contributions.1) W e introduce three matrix estimation problems. The ﬁrst a rises in low-rank bandits. The second corresponds to scenarios in RL where the learner wishes to estimate the (low-rank) transition kernel of a Markov chain and to this ai m, has access to a generative model. The last problem is similar but assumes that the learner has a ccess to system trajectories only, a setting referred to as the forward model in the RL literature . For all problems, we establish strong performance guarantees for simple spectral-based estimat ion approaches: these efﬁciently recover the singular subspaces of the matrix and exhibit nearly-min imal entry-wise error. T o prove these results, we develop and combine involved leave-one-out arg uments and Poisson approximation techniques (to handle the correlations in the data). 2) W e apply the results obtained for our ﬁrst matrix estimation problem to devise an efﬁcient regret-minimization algorithm for low-rank bandits. W e pr ove that the algorithm enjoys ﬁnite-time performance guarantees, with a regret at most roughly scali ng as (m+ n) log3(T) ¯∆ /∆ 2 minwhere (m,n) are the reward matrix dimensions, T is the time horizon, ¯∆ is the average of the reward gaps between the best arm and all other arms, and ∆ min is the minimum of these gaps. 3) Finally, we present an algorithm for best policy identiﬁc ation in low-rank MDPs in the reward- free setting. The results obtained for the second and last ma trix estimation problems imply that our algorithm learns an ǫ-optimal policy for any reward function using only a number o f samples scaling as O(nA/ǫ2) up to logarithmic factors, where n and A denote the number of states and actions, respectively. This sample complexity is mini-max optimal [ 21], and illustrates the gain achieved by leveraging the low-rank structure (without thi s structure, the sample complexity would be Ω( n2A/ǫ2)). Notation. For any matrix A∈ Rm×n, Ai,: (resp. A:,j) denotes its i-th row (resp. its j-th column), Amin = min (i,j) Ai,j and Amax = max (i,j) Ai,j. W e consider the following norms for matrices: ∥A∥ denotes the spectral norm, ∥A∥1→∞ = max i∈[m] ∥Ai,:∥1, ∥A∥2→∞ = max i∈[m] ∥Ai,:∥2, and ﬁnally ∥A∥∞ = max (i,j)∈[m]×[n] |Ai,j|. If the SVD of Ais UΣ V⊤, we denote by sgn(A) = UV ⊤ the matrix sign function of A (see Deﬁnition 4.1 in [ 22]). Or×r denotes the set of (r × r) real orthogonal matrices. For any ﬁnite set S, let P(S) be the set of distributions over S. The notation a(n,m,T ) ≲ b(n,m,T ) (resp. a(n,m,T ) = Θ( b(n,m,T ))) means that there exists a universal constant C > 0 (resp. c,C > 0) such that a(n,m,T ) ≤ Cb(n,m,T ) (resp. cb(n,m,T ) ≤ a(n,m,T ) ≤ Cb(n,m,T )) for all n,m,T . Finally, we use a ∧ b = min( a,b) and a ∨ b = max(a,b). 2 Models and Objectives LetM ∈ Rm×n be an unknown rank rmatrix that we wish to estimate from T noisy observations of its entries. W e consider matrices arising in two types of lea rning problems with low-rank structure, namely low-rank bandits and RL. The SVD of M is UΣ V⊤ where the matrices U ∈ Rm×r and V ∈ Rn×r contain the left and right singular vectors of M, respectively, and Σ = diag(σ1,...,σ r). W e assume without loss of generality that the singular value s have been ordered, i.e., σ1 ≥ ... ≥ σr. The accuracy of our estimate ˆM of M will be assessed using the following criteria: (i) Singular subspace recovery. Let the SVD of ˆM be ˆUˆΣ ˆV⊤. T o understand how well the singular subspaces of M are recovered, we will upper bound minO∈Or×r ∥U − ˆUO∥2→∞ and minO∈Or×r ∥V − ˆVO∥2→∞ (the minO∈Or×r problem corresponds to the orthogonal Procrustes problem and its solution aligns ˆU and U as closely as possible, see Remark 4.1 in [ 22]). 2(ii) Matrix estimation. T o assess the accuracy of ˆM, we will upper bound the row-wise error ∥ ˆM−M∥1→∞ or ∥ ˆM−M∥2→∞, as well as the entry-wise error ∥ ˆM−M∥∞ (the spectral error ∥ ˆM− M∥ is easier to deal with and is presented in appendix only). W e introduce two classical quantities characterizing the h eterogeneity and incoherence of the matrix M [23, 24]. Let κ = σ1/σr, and let µ(U) = √ m/r∥U∥2→∞ (resp. µ(V) =√ n/r∥V∥2→∞) denote the row-incoherence (resp. column-incoherence) p arameter of M. Let µ = max {µ(U),µ(V)}. Next, we specify the matrices M of interest in low-rank bandits and RL, and the way the data used for their estimation is generated. Model I: Reward matrices in low-rank bandits.For bandit problems, M corresponds to the average rewards of various arms. T o estimate M, the learner has access to data sequentially generated as follows. In each round t= 1 ,...,T , an arm (it,jt) ∈ [m] × [n] is randomly selected (say uniformly at random for simplicity) and the learner obs erves Mit,jt + ξt, an unbiased sample of the corresponding entry of M. (ξt)t≥1 is a sequence of zero-mean and bounded random variables. Speciﬁcally, we assume that for all t≥ 1, |ξt| ≤ c1∥M∥∞ a.s., for some constant c1 >0. Model II: T ransition matrices in low-rank MDPs. In low-rank MDPs, we encounter Markov chains whose transition matrices have low rank r (refer to Section 5 for details). Let P ∈ Rn×n be such a transition matrix. W e assume that the correspondin g Markov chain is irreducible with stationary distribution ν. The objective is to estimate P from the data consisting of samples of transitions of the chain. More precisely, from the data, we w ill estimate the long-term frequency matrix M = diag( ν)P (Mij is the limiting proportion of transitions from state ito state j as the trajectory grows large). Observe that M is of rank r, and that Pi,: = Mi,:/∥Mi,:∥1. T o estimate M, the learner has access to the data (x1,...,x T) ∈ [n]T generated according to one of the following two models. (a) In the generative model, for any t ∈ [T], if tis odd, xt is selected at random according to some distribution ν0, and xt+1 is sampled from Pxt,:. (b) In the forward model, the learner has access to a trajectory (x1,...,x T) of length T of the Markov chain, where x1 ∼ ν0 and for any t≥ 1, xt+1 ∼ Pxt,:. 3 Matrix Estimation via Spectral Decomposition In the three models (Models I, II(a) and II(b)), we ﬁrst construct a matrix ˜M directly from the data, and from there, we build our estimate ˆM, typically obtained via spectral decomposition, i.e., by taking the best rank- rapproximation of ˜M. In the remaining of this section, we let ˆUˆΣ ˆV⊤ denote the SVD of ˆM. Next, we describe in more details how ˆM is constructed in the three models, and analyze the corresponding estimation error. 3.1 Reward matrices For Model I, fort= 1 ,...,T , we deﬁne ˜Mt = ( (Mit,jt + ξt)1{(i,j)=(it ,jt)} ) i,j∈[m]×[n] and ˜M = nm T ∑ T t=1 ˜Mt. Let ˆM denote the best rank- rapproximation of ˜M. Theorem 1. Let δ >0. W e introduce: B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) . Assume that T ≥ cµ4κ2r2(n+ m) log3 (e(m+ n)T/δ) for some universal constant c >0. Then there exists a universal constant C >0 such that the following inequalities hold with probability at 3least 1 − δ: (i) max ( ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ ) ≤ C (µ3κ2r3/2)√ mn(n∧ m) B, (ii) ∥ ˆM − M∥2→∞ ≤ C(µ3 κ2r3/2)√m∧ n ∥M∥∞B, (iii) ∥ ˆM − M∥∞ ≤ C ( µ11/2 κ2r1/2 + µ3κr3/2 m+ n√mn ) 1 (n∧ m)∥M∥∞B. Corollary 2. (Homogeneous reward matrix) When m = Θ( n), κ = Θ(1) , µ = Θ(1) , ∥M∥∞ = Θ(1) , r= Θ(1) , we say that the reward matrix Mis homogeneous. In this case, for any δ >0, when T ≥ c(n+ m) log3 ( e(m+ n)T/δ ) for some universal constant c> 0, we have with probability at least 1 − δ: max ( ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ ) ≲ 1√ T log3/2 ((n+ m)T δ ) , ∥ ˆM − M∥2→∞ ≲ (n+ m)√ T log3/2 ((n+ m)T δ ) , ∥ ˆM − M∥∞ ≲ √ (n+ m) T log3/2 ((n+ m)T δ ) . For a homogeneous reward matrix, ∥U∥2→∞ = Θ(1 /√m) and ∥M∥∞ = Θ(1) , and hence, from the above corollary, we obtain estimates whose relative err ors (e.g., ∥ ˆM − M∥∞/∥M∥∞) scale at most as √ m/T up to the logarithmic factor. W e may also compare the results of the above corollary to thos e of Theorem 4.4 presented in [ 22]. There, the data consists for each pair (i,j) of a noisy observation Mi,j + Ei,j. The Ei,j’s are independent across (i,j). This model is simpler than ours and does not include any corr elation in the data. But it roughly corresponds to the case where T = nmin our Model I. Despite having to deal with correlations, we obtain similar results as those of The orem 4.4: for example, ∥ ˆM − M∥∞ ≲√ 1/(n+ m) (up to logarithmic terms) with high probability. 3.2 T ransition matrices under the generative model For Model II(a), the matrix˜M records the empirical frequencies of the transitions: for a ny pair of states (i,j), ˜Mi,j = 1 ⌊T/2⌋ ∑ ⌊T/2⌋ k=1 1{(x2k−1,x2k)=(i,j)}. ˆM is the best rank- rapproximation of ˜M and the estimate ˆP of the transition matrix P is obtained normalizing the rows of ˆM: for all i∈ [n], ˆPi,: = { (ˆMi,:)+/∥(ˆMi,:)+∥1, if ∥(ˆMi,:)+∥1 >0, 1 n1n, if ∥(ˆMi,:)+∥1 = 0 . (1) where (·)+ is the function applying max(0,·) component-wise and 1n is the n-dimensional vector of ones. The next theorem is a simpliﬁed version and a consequ ence of a more general and tighter theorem presented in App. B.2. T o simplify the presentation of our results, we deﬁne g(M,T,δ ) = nlog( n √ T δ ) max { µ6κ6r3, log( n √ T δ )1{∃ℓ:T ∥ Mℓ,:∥ ∞≤1} log(1+ 1 T ∥ M∥ ∞ ) } . Theorem 3. Let δ >0. Introduce B = µκ √ (r∥M∥∞/T) log(n √ T/δ). Assume that we have (ν0)min = min i∈[n](ν0)i > 0. If (a) n ≥ clog2(nT3/2/δ) and (b) T ≥ cg(M,T,δ ) for some universal constant c >0, then there exists a universal constant C > 0 such that the following 4inequalities hold with probability at least 1 − δ: (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C κµ2r n∥M∥∞ B, (ii) ∥ ˆM− M∥2→∞ ≤ CκB, ∥ ˆP − P∥1→∞ ≤ C κ√n (ν0)min B, (iii) ∥ ˆM− M∥∞ ≤ Cκµ2r√n B, (iv) ∥ ˆP − P∥∞ ≤ C B (ν0)min [ √nκ∥M∥∞ (ν0)min + ( 1 + κB√n∥M∥∞ )κµ2r√n ] , where (iv) holds if in addition T ≥ cn∥M∥∞(ν0)−2 minrµ2κ4 log(n √ T/δ) Note that in theorem, the condition (a) on nhas been introduced just to simplify the expression of B (refer to App. B.2 for a full statement of the theorem without this condition). Corollary 4. (Homogeneous transition matrix) When κ = Θ(1) , µ = Θ(1) , r = Θ(1) , Mmax = Θ( Mmin), we say that the frequency matrix M is homogeneous. If T ≥ cnlog(nT) for some universal constant c> 0, then we have with probability at least 1 − min{n−2,T−1}: max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ √ log(nT) T , ∥ ˆM − M∥2→∞ ≲ 1 n √ log(nT) T , ∥ ˆM − M∥∞ ≲ 1 n √ log(nT) nT , ∥ ˆP − P∥1→∞ ≲ √ nlog(nT) T , ∥ ˆP − P∥∞ ≲ √ log(nT) nT . For a homogeneous frequency matrix, ∥U∥2→∞ = Θ(1 /√n), ∥M∥2→∞ = Θ(1 /n√n), ∥M∥∞ = Θ(1 /n2), ∥P∥1→∞ = 1 , ∥P∥∞ = Θ(1 /n). Thus for all these metrics, our estimates achieve a relative error scaling at most as √ n/T up to the logarithmic factor. 3.3 T ransition matrices under the forward model For Model II(b), we ﬁrst split the data intoτ subsets of transitions: for k = 1 ,...,τ , the k-th subset is ((xk,xk+1),(xk+τ,xk+1+τ),..., (xk+(Tτ −1)τ,xk+1+(Tτ −1)τ)) where Tτ = ⌊T/τ⌋. By separating two transitions in the same subset, we break the i nherent correlations in the data if τ is large enough. Now we let ˜M(k) be the matrix recording the empirical frequencies of the tra nsitions in the k-th subset: ˜M(k) i,j = 1 Tτ ∑ Tτ −1 l=0 1{(xk+lτ ,xk+1+lτ )=(i,j)} for any pair of states (i,j). Let ˆM(k) be the best r-rank approximation of ˜M(k). As in ( 1), we deﬁne the corresponding ˆP(k). Finally we may aggregate these estimates ˆM = 1 τ ∑ τ k=1 ˆM(k) and ˆP = 1 τ ∑ τ k=1 ˆP(k). W e present below the performance analysis for the estimates coming from a single subset; the analysis of the aggregate estimates easily follows. For anyε> 0, we deﬁne the ε-mixing time of the Markov chain with transition matrix P as τ(ε) = min{t ≥ 1 : max 1≤i≤n 1 2 ∥Pt i,: − ν⊤∥1 ≤ ε}, and its mixing time as τ⋆ = τ(1/4). The next theorem is a simpliﬁed version and a consequence of a more gen eral and tighter theorem presented in App. B.3. T o simplify the presentation, we deﬁne: h(M,T,δ ) = nτ⋆log( n √ T δ ) log(Tν−1 min) max { µ6κ6r3, log2( n√Tτ δ )1{∃ℓ:Tτ ∥ Mℓ,:∥ ∞≤1} log2(1+ 1 Tτ ∥ M∥ ∞ ) } . Theorem 5. Let δ >0. Assume that νmin = min i∈[n] νi >0 and that τ/(τ⋆log(Tν−1 min)) ∈ [c1,c2] for some universal constants c2 >c1 ≥ 2. Introduce: B = µκ √ rτ⋆∥M∥∞ T log (n√Tτ δ ) log ( T νmin ) . 5If (a) n ≥ cτ⋆log3/2(nT3/2/δ) log1/2(Tν−1 min) and (b) T ≥ ch(M,T,δ ) for some universal constant c >0, then there exists a universal constant C >0 such that the following inequalities hold with probability at least 1 − δ: (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C κµ2r n∥M∥∞ B, (ii) ∥ ˆM − M∥2→∞ ≤ CκB, ∥ ˆP − P∥1→∞ ≤ Cκ√n νmin B, (iii) ∥ ˆM − M∥∞ ≤ Cκµ2r√n B, (iv) ∥ ˆP − P∥∞ ≤ C B νmin [ √nκ∥M∥∞ νmin + ( 1 + κB√n∥M∥∞ )κµ2r√n ] , where (iv) holds if in addition T ≥ cn∥M∥∞ν−2 minτ⋆rµ2κ4 log(n √ T/δ) log(Tν−1 min). Note that our guarantees hold when τroughly scales as τ⋆log(Tν−1 min). Hence to select τ, one would need an idea of the latter quantity. It can be estimated typic ally using τ⋆ν−1 min samples [ 25] (which is small when compared to the constraint T ≥ ch(M,T,δ ) as soon as νmin = Ω(1 /n)). Further observe that in the theorem, the condition (a) can be removed (refer to App. B.3 for a full statement of the theorem without this condition). Corollary 6.(Homogeneous transition matrices) Assume that M is homogeneous (as deﬁned in Corollary 4). Let τ = log( Tn). If T ≥ cnlog2(nT) for some universal constant c >0, then we have with probability at least 1 − min{n−2,T−1}: max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ 1√ T log(nT), ∥ ˆM − M∥2→∞ ≲ 1 n √ T log(nT), ∥ ˆM − M∥∞ ≲ 1 n √ nT log(nT), ∥ ˆP − P∥1→∞ ≲ √ n T log(nT), ∥ ˆP − P∥∞ ≲ 1√ nT log(nT). As for the generative model, for a homogeneous frequency mat rix, our estimates achieve a relative error scaling at most as √ n/T up to the logarithmic factor for all metrics. Note that up to a logarithmic factor, the upper bound for ∥ ˆP − P∥1→∞ (and similarly for ˆM) matches the minimax lower bound derived in [ 26]. 3.4 Elements of the proofs The proofs of the three above theorems share similar argumen ts. W e only describe elements of the proof of Theorem 5, corresponding to the most challenging model. The most difﬁ cult result concerns the singular subspace recovery (the upper bounds (i) in our t heorems), and it can be decomposed into the following three steps. The ﬁrst two steps are meant t o deal with the Markovian nature of the data. The third step consists in applying a leave-one-out an alysis to recover the singular subspaces. Step 1: Multinomial approximation of Markovian data. W e treat the matrix ˜M(k) arising from one subset of data, and for simplicity, we remove the supersc ript (k), i.e., ˜M = ˜M(k). Note that Tτ˜M is a matrix recording the numbers of transitions observed in the data for any pair of states: denote by Ni,j this number for (i,j). W e approximate the joint distribution of N = ( Ni,j)(i,j) by a multinomial distribution with n2 components and parameter TτMi,j for component (i,j). Denote by Z = ( Zi,j)(i,j) the corresponding multinomial random variable. Using the m ixing property of the Markov chain and the choice of τ, we establish (see Lemma 21 in App. C) that for any subset Z of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}, we have P[N ∈ Z ] ≤ 3P[Z ∈ Z ]. Step 2: T owards P oisson random matrices with independent en tries. The random matrix Z does not have independent entries. Independence is however a req uirement if we wish to apply the leave- one-out argument. Consider the random matrix Y whose entries are independent Poisson random 6variables with mean TτMi,j for the (i,j)-th entry. W e establish the following connection between the distribution of Zand that of Y: for any Z ⊂ Nn2 , we have P[Z ∈ Z ] ≤ e√TτP[Y ∈ Z ]. Refer to Lemma 22 in App. C for details. Step 3: The leave-one-out argument for P oisson matrices. Combining the two ﬁrst steps provides a connection between the observation matrix ˜M and a Poisson matrix Y with independent entries. This allows us to apply a leave-one-out analysis to ˜M as if it had independent entries (replacing ˜M by Y). The analysis starts by applying the standard dilation tri ck (see Section 4.10 in [ 22]) so as to make ˜Msymmetric. Then, we can decompose the error ∥U− ˆU( ˆU⊤U)∥2→∞ (see Lemma 32 in App. E) into several terms. The most challenging of these terms is ∥(M − ˜M)(U − ˆU( ˆU⊤U))∥2→∞ = maxl∈[n] ∥(Ml,: − ˜Ml,:)(U − ˆU( ˆU⊤U))∥2 because of inherent dependence between M − ˜M and U − ˆU( ˆU⊤U). The leave-one-out analysis allows us to decouple this stat istical dependency. It consists in exploiting the row and column independence of ma trix ˜M to approximate ∥(Ml,: − ˜Ml,:)(U − ˆU( ˆU⊤U))∥2 by ∥(Ml,: − ˜Ml,:)(U − ˆU(l)(( ˆU(l))⊤U)∥2 where ˆU(l) is the matrix of eigenvectors of matrix ˜M(l) obtained by zeroing the l-th row and column of ˜M. By construction, (Ml,: − ˜Ml,:) and U − ˆU(l)(( ˆU(l))⊤U) are independent, which simpliﬁes the analysis. The proof is completed by a further appropriate decomposition of this term, combined with concentration inequalities for random Poisson matrices (see App. D). 4 Regret Minimization in Low-Rank Bandits Consider a low-rank bandit problem with a homogeneous rank-r reward matrix M. W e wish to devise an algorithm π with low regret. π selects in round t an entry (iπ t,jπ t ) based on previous observations, and receives as a feedback the noisy reward Miπ t,jπ t + ξt. The regret up to round T is deﬁned by Rπ(T) = TMi⋆,j⋆ − E[∑ T t=1 Miπ t,jπ t ], where (i⋆,j⋆) is an optimal entry. One could think of a simple Explore-Then-Commit (ETC) algorith m, where in the ﬁrst phase entries are sampled uniformly at random, and where in a second phase, the algorithm always selects the highest entry of ˆMbuilt using the samples gathered in the ﬁrst phase and obtain ed by spectral decomposition. When the length of the ﬁrst phase is T2/3(n+ m)1/3, the ETC algorithm would yield a regret upper bounded by O(T2/3(n+ m)1/3) for T = Ω(( n+ m) log3(n+ m)). T o get better regret guarantees, we present SME-AE (Success ive Matrix Estimation and Arm Elimination), an algorithm meant to identify the best entry as quickly as possible with a prescribed level of certainty. After the SME-AE has returned the estima ted best entry, we commit and play this entry for the remaining rounds. The pseudo-code of SME- AE is presented in Algorithm 1. The algorithm runs in epochs: in epoch ℓ, it samples Tℓ entries uniformly at random among all entries (in Tℓ, the constant C just depends on upper bounds of the parameters µ, κ, and ∥M∥∞, refer to App. G); from these samples, a matrix ˆM(ℓ) is estimated and Aℓ, the set of candidate arms, is pruned. The pruning procedure is based on the estimated gaps: ˆ∆ (ℓ) i,j = ˆM(ℓ) ⋆ − ˆM(ℓ) i,j where ˆM(ℓ) ⋆ = max i,j ˆM(ℓ) i,j . Algorithm 1: S uccesive Matrix Estimation and Arm Elimination ( SME-AE) Input: Arms [m] × [n], conﬁdence level δ ℓ= 1 ; A1 = [ m] × [n]; while |Aℓ| >1 do δℓ = δ/ℓ2; Tℓ = ⌈ C ( 2ℓ+2)2 (m+ n) log3 ( 22ℓ+4(m+ n)/δℓ )⌉ ; Sample uniformly at random Tℓ entries from A1: (Mit,jt + ξt)t=1,...,Tℓ ; Estimate ˆM(ℓ) via spectral decomposition as described in Section 3.1 ; Aℓ+1 = { (i,j) ∈ A ℓ : ˆ∆ (ℓ) i,j ≤ 2−(ℓ+2) } ; ℓ= ℓ+ 1; end Output:Recommend the remaining pair (ˆıτ,ˆτ) in Aℓ. 7The following theorem characterizes the performance of SME -AE and the resulting regret. T o simplify the notation, we introduce the gaps: for any entry (i,j), ∆ i,j = ( Mi⋆,j⋆ − Mi,j), ∆ min = min (i,j):∆ i,j >0 ∆ i,j, ∆ max = max (i,j) ∆ i,j, and ¯∆ = ∑ (i,j) ∆ i,j/(mn). W e deﬁne the function ψ(n,m,δ ) = c(m+n) log(e/∆ min) ∆ 2 min log3 (e(m+n) log(e/∆ min) ∆ minδ ) for some universal constant c> 0. Theorem 7. (Best entry identiﬁcation) F or any δ ∈ (0,1), SME-AE( δ) stops at time τ and recommends arm (ˆıτ,ˆτ) with the guarantee P ( (ˆıτ,ˆτ) = ( i⋆,j⋆),τ ≤ ψ(n,m,δ ) ) ≥ 1 − δ. Moreover , for any T ≥ 1 and α > 0, the sample complexity τ of SME-AE( 1/Tα) satisﬁes E[τ ∧ T] ≤ ψ(n,m,T −α) + T1−α. (Regret) Let T ≥ 1. Consider the algorithm πthat ﬁrst runs SME-AE( 1/T2) and then commits to its output (ˆıτ,ˆτ) after τ. W e have: Rπ(T) ≤ ¯∆ ( ψ(n,m,T −2) + 1 ) + ∆ max T . The proof of Theorem 7 is given in App. G. Note that the regret upper bounds hold for any time horizon T ≥ 1, and that it scales as O((m + n) log3(T) ¯∆ /∆ 2 min) (up to logarithmic factors in m,n and 1/∆ min). The cubic dependence in log3(T) is an artifact of our proof techniques. More precisely, it is due to the Poisson approximation used to obt ain entry-wise guarantees. Importantly, for any time horizon, the regret upper bound only depends on (m+ n) rather than mn(the number of arms / entries), and hence, the low-rank structure is efﬁc iently exploited. If we further restrict our attention to problems with gap ratio ∆ max/∆ min upper bounded by ζ, our regret upper bound becomes O(ζ(m+ n) log3(T)/∆ min), and can be transformed into the minimax gap-independent upper bound O(ζ((m+ n)T)1/2 log2(T)), see App. G. Finally note that Ω((( m+ n)T)1/2) is an obvious minimax regret lower bound for our low-rank bandit p roblem. A very similar low-rank bandit problem has been investigate d in [ 6]. There, under similar assumptions (see Assumption 1 and Deﬁnition 1), the authors devise an algorithm with both gap- dependent and gap-independent regret guarantees. The latt er are difﬁcult to compare with ours. Their guarantees exhibit a better dependence in T and ∆ min, but worse in the matrix dimensions n and m. Indeed in our model, b⋆in [ 6] corresponds to ∥M∥2→∞ and scales as √n. As a consequence, the upper bounds in [ 6] have a dependence in n and m scaling as √n(n+ m) in the worst case for gap-dependent guarantees and even nm (through the constant C2 in [ 6]) for gap-independent guarantees. 5 Representation Learning in Low-Rank MDPs The results derived for Models II(a) and II(b) are instrumen tal towards representation learning and hence towards model-based or reward-free RL in low-rank MDP s. In this section, we provide an example of application of these results, and mention other e xamples in Section 7. A low-rank MDP is deﬁned by (S,A,{Pa}a∈A,R,γ ) where S, A denote state and action spaces of cardinalities n and A, respectively, Pa denotes the rank- rtransition matrix when taking action a, Ris the reward function, and γ is the discount factor. W e assume that all rewards are in [0,1]. The value function of a policy π: S → A is deﬁned as Vπ R(x) = E[∑ ∞ t=1 γt−1R(xπ t,πt(xπ t))|xπ 1= x] where xπ tis the state visited under πin round t. W e denote by π⋆(R) an optimal policy (i.e., with the highest value function). Reward-free RL.In the reward-free RL setting (see e.g. [ 27, 28, 29]), the learner does not receive any reward signal during the exploration process. The latte r is only used to construct estimates { ˆPa}a∈A of {Pa}a∈A. The reward function Ris revealed at the end, and the learner may compute ˆπ(R) an optimal policy for the MDP (S,A,{ ˆPa}a∈A,R,γ ). The performance of this model-based approach is often assessed through Γ = sup R∥Vπ⋆(R) R − Vˆπ(R) R ∥∞. In tabular MDP , to identify an ǫ-optimal policy for all reward functions, i.e., to ensure th at Γ ≤ ǫ, we believe that the number of samples that have to be collected should be Ω( poly( 1 1−γ)n2A ǫ2 ) (the exact degree of the polynomial in 1/(1 − γ) has to be determined). This conjecture is based on the sample complexity lower bounds derived for reward-free RL in episodic tabular MDP [ 28, 30]. Now for low-rank MDPs, the equivalent lower bound would be Ω( poly( 1 1−γ)nA ǫ2 ) [21] (this minimax lower bound is valid for Block MDPs, a particular case of low-rank MDPs). 8Leveraging our low-rank matrix estimation guarantees, we p ropose an algorithm matching the aforementioned sample complexity lower bound (up to logari thmic factors) at least when the frequency matrices {Ma}a∈A are homogeneous. The algorithm consists of two phases: (1) i n the model estimation phase, it collects Atrajectories, each of length T/A, corresponding to the Markov chains with transition matrices {Pa}a∈A. From this data, it uses the spectral decomposition method described in § 3 to build estimates { ˆPa}a∈A. (2) In the planning phase, based on the reward function R, it computes the best policy ˆπ(R) for the MDP (S,A,{ ˆPa}a∈A,R,γ ). The following theorem summarizes the performance of this algorithm. T o simplify t he presentation, we only provide the performance guarantees of the algorithm for homogeneous tr ansition matrices (guarantees for more general matrices can be derived plugging in the results from Theorem 5). Theorem 8. Assume that for any a ∈ A , Ma is homogeneous (as deﬁned in Corollary 4). If T ≥ cnAlog2(nAT) for some universal constant c >0, then we have with probability at least 1 − min{n−2,T−1}: Γ = sup R∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≲ 1 (1−γ)2 √ nA T log(nAT). Theorem 8 is a direct consequence of Corollary 6 and of the fact that for any reward function R: ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2γ (1−γ)2 maxa∈A ∥Pa − ˆPa∥1→∞, see App. A. The theorem implies that if we wish to guarantee Γ ≤ ǫ, we just need to collect O( nA ǫ2(1−γ)4 ) samples up to a logarithmic factor. This sample complexity is minimax optimal in n, A, and ǫin view of the lower bound presented in [21]. 6 Related W ork Low-rank matrix estimation.Until recently, the main efforts on low-rank matrix recover y were focused on guarantees w .r.t. the spectral or Frobenius norm s, see e.g. [ 31] and references therein. The ﬁrst matrix estimation and subspace recovery guarantee s in ℓ2→∞ and ℓ∞ were established in [ 13], [ 14] via a more involved perturbation analysis than the classic al Davis-Kahan bound. An alternative approach based on a leave-one-out analysis was proposed in [ 16], and further reﬁned in [ 32, 15, 33], see [ 22] for a survey. Some work have also adapted the techniques bey ond the independent noise assumption [ 34, 35, 36], but for very speciﬁc structural dependence. W e deal with a stronger dependence, and in particular with Markovia n data (an important scenario in RL). The estimation of low-rank transition matrices of Markov ch ains has been studied in [ 26, 37] using spectral methods and in [ 38, 39] using maximum-likelihood approaches. [ 26] does not conduct any ﬁne-grained subspace recovery analysis (such as the leave- one-out), and hence the results pertaining to the ∥ · ∥ 1→∞-guarantees are questionable; refer to App. H for a detailed justiﬁcation. All these papers do not present entry-wise guarantees. It is worth mentioning that there exist other methods for matrix estimation that do not rely on spectral decompositions like ours, yet enjoy entry-wise matrix esti mation guarantees [ 40, 41, 42]. However, these methods require different assumptions than ours that may be too strong for our purposes, notably having access to the so-called anchor rows and colum ns. Moreover, we do not know if these methods also lead to guarantees for subspace recovery in the norm ∥ · ∥ 2→∞, nor how to extend those results to settings with dependent noise. Low-rank bandits.Low-rank structure in bandits has received a lot of attentio n recently [ 43, 4, 5, 44, 45, 6, 46, 7]. Different set-ups have been proposed (refer to App. H for a detailed exposition, in particular, we discuss how the settings proposed in [ 5, 6] are equivalent), and regret guarantees in an instance dependent and minimax sense have been both estab lished. T ypically minimax regret guarantees in bandits scale as √ T, but the scaling in dimension may defer when dealing with a low rank structure [ 5, 46, 6]. In [ 5], the authors also leverage spectral methods. They reduce the problem to a linear bandit of dimens ion nmbut where only roughly n+m dimensions are relevant. This entails that a regret lower bo und of order (n+ m) √ T is inevitable. Actually, in their reduction to linear bandits, they only us e a subspace recovery in Frobenius norm, which perhaps explains the scaling (n+ m)3/2 in their regret guarantees. It is worth noting that in [46], the authors manage to improve upon the work [ 5] and obtain a scaling order (m+ n) in the regret. Our algorithm leverages entry-wise guarantees whi ch rely on a stronger subspace recovery guarantee. This allows us to obtain a scaling √n+ min the regret. The work of [ 7] is yet another 9closely related work to ours. There, the authors propose an a lgorithm achieving a regret of order polylog(n+ m) √ T for a contextual bandit problem with low rank structure. How ever, their result only holds for rank 1 and their observation setup is differen t than ours because in their setting, the learner observes m entries per round while in ours the learner only observes one entry per round. In [ 6], the authors use matrix estimation with nuclear norm penal ization to estimate the matrix M. Their regret guarantees are already discussed in § 4. Some instance-dependent guarantees with logarithmic regr et for low rank bandits have been established in [ 43, 4, 44]. However, these results suffer what may be qualiﬁed as seri ous limitations. Indeed, [ 43, 44] provide instance dependent regret guarantees but only con sider low-rank bandits with rank 1, and the regret bounds of [ 43] are expressed in terms of the so-called column and row gaps (see their Theorem 1) which are distinct from the standa rd gap notions. [ 4] extend the results in [ 43] to rank r with the limitation that they require stronger assumptions than ours. Moreover, the computational complexity of their algorithm depends ex ponentially on the rank r; they require a search over spaces of size (m r ) and (n r ) . Our proposed algorithm does not suffer from such limitations. W e wish to highlight that our entry-wise guarantees for matr ix estimation are the key enabling tool that led us to the design and analysis of our proposed algorit hm. In fact, the need for such guarantees arises naturally in the analysis of gap-dependent regret bo unds (see Appendix G.1). Therefore, we believe that such guarantees can pave the way towards better , faster, and efﬁcient algorithms for bandits with low-rank structure. Low-rank Reinforcement Learning.RL with low rank structure has been recently extensively studied but always in the function approximation framework [ 47, 48, 49, 50, 51, 52, 8, 9, 10, 53, 11, 12]. There, the transition probabilities can be written as φ(x,a)⊤µ(x′) where the unknown feature functions φ(x,a),µ(x′) ∈ Rr belong to some speciﬁc class F of functions. The major issue with algorithms proposed in this literature is that they rely on s trong computational oracles (e.g., ERM, MLE), see [ 54, 55, 56] for detailed discussions. In contrast, we do not assume tha t the transition matrices are constructed based on a given restricted class o f functions, and our algorithms do not rely on any oracle and are computationally efﬁcient. In [ 40, 42], the authors also depart from the function approximation framework. There, they consider a l ow rank structure different than ours. Their matrix estimation method enjoys an entry-wise guaran tee, but requires to identify a subset of rows and columns spanning the range of the full matrix. Moreo ver, their results are only limited the generative models, which allows to actually rely on indepen dent data samples. 7 Conclusion and Perspectives In this paper, we have established that spectral methods efﬁciently recover low-rank matrices even in correlated noise. W e have investigated noise correlatio ns that naturally arise in RL, and have managed to prove that spectral methods yield nearly-minima l entry-wise error. Our results for low- rank matrix estimation have been applied to design efﬁcient algorithms in low-rank RL problems and to analyze their performance. W e believe that these resu lts may ﬁnd many more applications in low-rank RL. They can be applied (i) to reward-free RL in ep isodic MDPs (this setting is easier than that presented in § 5 since successive episodes are independent); (ii) to scenar ios corresponding to ofﬂine RL [ 57] where the data consists of a single trajectory generated un der a given behavior policy (from this data, we can extract the transitions (x,a,x ′) where a given action a is involved and apply the spectral method to learn ˆPa); (iii) to traditional RL where the reward function Rhas to be learnt (learning Ris a problem that lies in some sense between the inference pro blems in our Models I and II); (iv) to model-free RL where we would directl y learn the Qfunction as done in [58] under a generative model; (v) to low-rank RL problems with c ontinuous state spaces (this can be done if the transition probabilities are smooth in the sta tes, and by combining our methods to an appropriate discretization of the state space). Acknowledgment This research was supported by the W allenberg AI, Autonomou s Systems and Software Program (W ASP) funded by the Knut and Alice W allenberg Foundation. 10References [1] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CU RL: Contrastive Unsupervised Representations for Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 5639–5650. PMLR, 13–18 Jul 2020. (Cited on page 1.) [2] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laski n. Decoupling Representation Learning from Reinforcement Learning. In Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 9870– 9879. PMLR, 18–24 Jul 2021. (Cited on page 1.) [3] Y ash Chandak, Shantanu Thakoor, Zhaohan Daniel Guo, Y un hao T ang, Remi Munos, Will Dabney, and Diana L Borsa. Representations and exploration for deep reinforcement learning using singular value decomposition. In Proc. of ICML , 2023. (Cited on page 1.) [4] Branislav Kveton, Csaba Szepesvari, Anup Rao, Zheng W en , Y asin Abbasi-Y adkori, and S. Muthukrishnan. Stochastic low-rank bandits, 2017. (Cited on pages 1, 9, and 10.) [5] Kwang-Sung Jun, Rebecca Willett, Stephen J. Wright, and Robert D. Nowak. Bilinear bandits with low-rank structure. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machin e Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages 3163–3172. PMLR, 2019. (Cited on pages 1 and 9.) [6] Mohsen Bayati, Junyu Cao, and W anning Chen. Speed up the c old-start learning in two-sided bandits with many arms. arXiv preprint arXiv:2210.00340 , 2022. (Cited on pages 1, 8, 9, 10, and 50.) [7] Prateek Jain and Soumyabrata Pal. Online low rank matrix completion. In Proc. of ICLR , 2023. (Cited on pages 1, 9, and 50.) [8] W en Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal , and John Langford. Model- based RL in Contextual Decision Processes: P AC bounds and Ex ponential Improvements over Model-free Approaches. In Proceedings of the Thirty-Second Conference on Learning Th eory, volume 99 of Proceedings of Machine Learning Research , pages 2898–2933. PMLR, 25–28 Jun 2019. (Cited on pages 1 and 10.) [9] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and W en Sun. FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs. In Advances in Neural Information Processing Systems , volume 33, pages 20095–20107. Curran Associates, Inc., 2020. (Cited on pages 1 and 10.) [10] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan J iang, and Alekh Agarwal. Model- free representation learning and exploration in low-rank m dps. T o appear in Journal of Machine Learning Research (JMLR) , 2023. (Cited on pages 1 and 10.) [11] Masatoshi Uehara, Xuezhou Zhang, and W en Sun. Represen tation Learning for Online and Ofﬂine RL in Low-rank MDPs. In International Conference on Learning Representations , 2022. (Cited on pages 1 and 10.) [12] T ongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzal ez, Dale Schuurmans, and Bo Dai. Spectral decomposition representation for reinforcement learning. In Proc. of ICLR , 2023. (Cited on pages 1 and 10.) [13] Jianqing Fan, W eichen W ang, and Y iqiao Zhong. An ℓ∞ eigenvector perturbation bound and its application to robust covariance estimation. Journal of Machine Learning Research , 18(207):1–42, 2018. (Cited on pages 2 and 9.) [14] Justin Eldridge, Mikhail Belkin, and Y usu W ang. Unpert urbed: spectral analysis beyond davis- kahan. In Algorithmic Learning Theory , pages 321–358. PMLR, 2018. (Cited on pages 2 and 9.) [15] Joshua Cape, Minh T ang, and Carey E Priebe. The two-to-i nﬁnity norm and singular subspace geometry with applications to high-dimensional statistic s. The Annals of Statistics , 47(5):2405– 2439, 2019. (Cited on pages 2, 9, 18, 35, and 43.) [16] Emmanuel Abbe, Jianqing Fan, Kaizheng W ang, and Y iqiao Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. Annals of statistics , 48(3):1452, 2020. (Cited on pages 2, 9, and 34.) 11[17] Nathan Srebro and Adi Shraibman. Rank, trace-norm and m ax-norm. In International conference on computational learning theory , pages 545–560. Springer, 2005. (Cited on page 2.) [18] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, and Y ulin g Y an. Noisy matrix completion: Understanding statistical guarantees for convex relaxati on via nonconvex optimization. SIAM journal on optimization , 30(4):3098–3121, 2020. (Cited on page 2.) [19] Ohad Shamir and Shai Shalev-Shwartz. Matrix completio n with the trace norm: Learning, bounding, and transducing. The Journal of Machine Learning Research , 15(1):3401–3423, 2014. (Cited on page 2.) [20] Sahand Negahban and Martin J W ainwright. Restricted st rong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research , 13(1):1665–1697, 2012. (Cited on page 2.) [21] Y assir Jedra, Junghyun Lee, Alexandre Proutiere, and S e-Y oung Y un. Nearly optimal latent state decoding in block mdps. In International Conference on Artiﬁcial Intelligence and Statistics, pages 2805–2904. PMLR, 2023. (Cited on pages 2, 8, and 9.) [22] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, et al. Spe ctral methods for data science: A statistical perspective. F oundations and T rends® in Machine Learning , 14(5):566–806, 2021. (Cited on pages 2, 4, 7, 9, 34, 39, 40, 41, and 43.) [23] Emmanuel J Candès and T erence T ao. The power of convex re laxation: Near-optimal matrix completion. IEEE T ransactions on Information Theory , 56(5):2053–2080, 2010. (Cited on page 3.) [24] Benjamin Recht. A simpler approach to matrix completio n. Journal of Machine Learning Research, 12(12), 2011. (Cited on page 3.) [25] Geoffrey W olfer and Aryeh Kontorovich. Estimating the mixing time of ergodic markov chains. In Alina Beygelzimer and Daniel Hsu, editors, Proceedings of the Thirty-Second Conference on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pages 3120– 3159. PMLR, 25–28 Jun 2019. (Cited on page 6.) [26] Anru Zhang and Mengdi W ang. Spectral State Compression of Markov Processes. IEEE T ransactions on Information Theory , 66(5):3202–3231, 2020. (Cited on pages 6, 9, 18, 21, 44, 49, and 50.) [27] Emilie Kaufmann, Pierre Ménard, Omar Darwiche Domingu es, Anders Jonsson, Edouard Leurent, and Michal V alko. Adaptive Reward-Free Explorati on. In Proceedings of the 32nd International Conference on Algorithmic Learning Theory , volume 132 of Proceedings of Machine Learning Research , pages 865–891. PMLR, 16–19 Mar 2021. (Cited on page 8.) [28] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tia ncheng Y u. Reward-Free Exploration for Reinforcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 4870– 4879. PMLR, 13–18 Jul 2020. (Cited on page 8.) [29] Zihan Zhang, Simon Du, and Xiangyang Ji. Near Optimal Re ward-Free Reinforcement Learning. In Proceedings of the 38th International Conference on Machin e Learning , volume 139 of Proceedings of Machine Learning Research , pages 12402–12412. PMLR, 18–24 Jul 2021. (Cited on page 8.) [30] Pierre Menard, Omar Darwiche Domingues, Anders Jonsso n, Emilie Kaufmann, Edouard Leurent, and Michal V alko. Fast active learning for pure exp loration in reinforcement learning. In Proceedings of the 38th International Conference on Machin e Learning , volume 139 of Proceedings of Machine Learning Research , pages 7599–7608. PMLR, 18–24 Jul 2021. (Cited on page 8.) [31] Mark A. Davenport and Justin K. Romberg. An overview of l ow-rank matrix recovery from incomplete observations. IEEE J. Sel. T op. Signal Process. , 10(4):608–622, 2016. (Cited on page 9.) [32] T T ony Cai and Anru Zhang. Rate-optimal perturbation bo unds for singular subspaces with applications to high-dimensional statistics. The Annals of Statistics , 46(1):60–89, 2018. (Cited on page 9.) [33] Y uxin Chen, Jianqing Fan, Cong Ma, and Kaizheng W ang. Sp ectral method and regularized mle are both optimal for top-k ranking. Annals of statistics , 47(4):2204, 2019. (Cited on page 9.) 12[34] Lihua Lei. Uniﬁed ℓ2→∞ eigenspace perturbation theory for symmetric random matri ces. arXiv preprint arXiv:1909.04798 , 2019. (Cited on page 9.) [35] Emmanuel Abbe, Jianqing Fan, and Kaizheng W ang. An ℓp theory of pca and spectral clustering. The Annals of Statistics , 50(4):2359–2385, 2022. (Cited on page 9.) [36] Joshua Agterberg, Zachary Lubberts, and Carey E Priebe . Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and d ependence. IEEE T ransactions on Information Theory , 68(7):4618–4650, 2022. (Cited on page 9.) [37] Shujun Bi, Zhen Y in, and Y ihong W eng. A low-rank spectra l method for learning markov models. Optimization Letters , 17(1):143–162, 2023. (Cited on page 9.) [38] Xudong Li, Mengdi W ang, and Anru Zhang. Estimation of ma rkov chain via rank-constrained likelihood. In International Conference on Machine Learning , pages 3033–3042. PMLR, 2018. (Cited on page 9.) [39] Ziwei Zhu, Xudong Li, Mengdi W ang, and Anru Zhang. Learn ing markov models via low-rank optimization. Operations Research , 70(4):2384–2398, 2022. (Cited on page 9.) [40] Devavrat Shah, Dogyoon Song, Zhi Xu, and Y uzhe Y ang. Sam ple Efﬁcient Reinforcement Learning via Low-Rank Matrix Estimation. In Advances in Neural Information Processing Systems, volume 33, pages 12092–12103. Curran Associates, Inc., 20 20. (Cited on pages 9 and 10.) [41] Alekh Agarwal, Nan Jiang, Sham M Kakade, and W en Sun. Rei nforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, W A, USA, T ech. Rep , pages 10–4, 2019. (Cited on pages 9 and 18.) [42] T yler Sam, Y udong Chen, and Christina Lee Y u. Overcomin g the long horizon barrier for sample-efﬁcient reinforcement learning with latent low-r ank structure. ACM SIGMETRICS P erformance Evaluation Review , 50(4):41–43, 2023. (Cited on pages 9 and 10.) [43] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, Claire V ernade, and Zheng W en. Stochastic Rank-1 Bandits. In Aarti Singh and Jerry Zhu, edi tors, Proceedings of the 20th International Conference on Artiﬁcial Intelligence and St atistics, volume 54 of Proceedings of Machine Learning Research , pages 392–401. PMLR, 20–22 Apr 2017. (Cited on pages 9 and 10.) [44] Cindy Trinh, Emilie Kaufmann, Claire V ernade, and Rich ard Combes. Solving bernoulli rank- one bandits with unimodal thompson sampling. In Aryeh Konto rovich and Gergely Neu, editors, Proceedings of the 31st International Conference on Algori thmic Learning Theory , volume 117 of Proceedings of Machine Learning Research , pages 862–889. PMLR, 08 Feb– 11 Feb 2020. (Cited on pages 9 and 10.) [45] Y angyi Lu, Amirhossein Meisami, and Ambuj T ewari. Low- rank generalized linear bandit problems. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artiﬁcial Intelligence and Statistics, AIST ATS 2021, April 13-15, 2021, V irtual Event, volume 130 of Proceedings of Machine Learning Research , pages 460–468. PMLR, 2021. (Cited on page 9.) [46] Y ue Kang, Cho-Jui Hsieh, and Thomas Chun Man Lee. Efﬁcie nt frameworks for generalized low-rank matrix bandit problems. Advances in Neural Information Processing Systems , 35:19971–19983, 2022. (Cited on pages 9 and 50.) [47] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John L angford, and Robert E. Schapire. Contextual decision processes with low Bellman rank are P AC -learnable. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 1704–1713. PMLR, 06–11 Aug 2017. (Cited on page 10.) [48] Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alek h Agarwal, John Langford, and Robert E Schapire. On Oracle-Efﬁcient P AC RL with Rich Obser vations. In Advances in Neural Information Processing Systems , volume 31. Curran Associates, Inc., 2018. (Cited on page 10.) [49] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarw al, Miroslav Dudik, and John Langford. Provably efﬁcient RL with Rich Observations via L atent State Decoding. In Proceedings of the 36th International Conference on Machin e Learning , volume 97 of Proceedings of Machine Learning Research , pages 1665–1674. PMLR, 09–15 Jun 2019. (Cited on page 10.) 13[50] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic State Abstraction and Provably Efﬁcient Rich-Observation Reinf orcement Learning. In Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 6961–6971. PMLR, 13–18 Jul 2020. (Cited on page 10.) [51] Dylan Foster, Alexander Rakhlin, David Simchi-Levi, a nd Y unzong Xu. Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learni ng: A Disagreement-Based Perspective. In Proceedings of Thirty F ourth Conference on Learning Theory , volume 134 of Proceedings of Machine Learning Research , pages 2059–2059. PMLR, 15–19 Aug 2021. (Cited on page 10.) [52] Xuezhou Zhang, Y uda Song, Masatoshi Uehara, Mengdi W an g, Alekh Agarwal, and W en Sun. Efﬁcient Reinforcement Learning in Block MDPs: A Model-fre e Representation Learning Approach. In Proceedings of the 39th International Conference on Machin e Learning , volume 162 of Proceedings of Machine Learning Research , pages 26517–26547. PMLR, 17–23 Jul 2022. (Cited on page 10.) [53] Masatoshi Uehara, Xuezhou Zhang, and W en Sun. Represen tation learning for online and ofﬂine rl in low-rank mdps. arXiv preprint arXiv:2110.04652 , 2021. (Cited on page 10.) [54] Daniel Kane, Sihan Liu, Shachar Lovett, and Gaurav Maha jan. Computational-statistical gap in reinforcement learning. In Proceedings of Thirty Fifth Conference on Learning Theory , volume 178 of Proceedings of Machine Learning Research , pages 1282–1302. PMLR, 02–05 Jul 2022. (Cited on page 10.) [55] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. Learni ng in Observable POMDPs, without Computationally Intractable Oracles. In Advances in Neural Information Processing Systems , volume 35. Curran Associates, Inc., 2022. (Cited on page 10.) [56] Tianjun Zhang, T ongzheng Ren, Mengjiao Y ang, Joseph Go nzalez, Dale Schuurmans, and Bo Dai. Making Linear MDPs Practical via Contrastive Repres entation Learning. In Proceedings of the 39th International Conference on Machin e Learning , volume 162 of Proceedings of Machine Learning Research , pages 26447–26466. PMLR, 17–23 Jul 2022. (Cited on page 10.) [57] Ming Y in and Y u-Xiang W ang. Optimal Uniform OPE and Mode l-based Ofﬂine Reinforcement Learning in Time-Homogeneous, Reward-Free and T ask-Agnostic Settings. In Advances in Neural Information Processing Systems , volume 34, pages 12890–12903. Curran Associates, Inc., 2021. (Cited on page 10.) [58] Devavrat Shah, Dogyoon Song, Zhi Xu, and Y uzhe Y ang. Sam ple efﬁcient reinforcement learning via low-rank matrix estimation. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS’20. Curran Associates Inc., 2020. (Cited on page 10.) [59] Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomization and probabilistic techniques in algorithms and data analysis . Cambridge university press, 2017. (Cited on pages 22 and 23.) [60] Samuel B Hopkins, Tselil Schramm, Jonathan Shi, and Dav id Steurer. Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and plan ted sparse vectors. In Proceedings of the forty-eighth annual ACM symposium on Theory of Comput ing, pages 178–191, 2016. (Cited on page 26.) [61] George Bennett. Probability inequalities for the sum o f independent random variables. Journal of the American Statistical Association , 57(297):33–45, 1962. (Cited on page 26.) [62] Andrew D McRae and Mark A Davenport. Low-rank matrix com pletion and denoising under poisson noise. Information and Inference: A Journal of the IMA , 10(2):697–720, 2021. (Cited on page 31.) [63] Afonso S Bandeira and Ramon V an Handel. Sharp nonasympt otic bounds on the norm of random matrices with independent entries. The Annals of Probability , 44(4):2479–2506, 2016. (Cited on page 31.) [64] V ivek Farias, Andrew A Li, and Tianyi Peng. Near-optima l entrywise anomaly detection for low-rank matrices with sub-exponential noise. In International Conference on Machine Learning, pages 3154–3163. PMLR, 2021. (Cited on page 34.) 14[65] Joel A Tropp et al. An introduction to matrix concentrat ion inequalities. F oundations and T rends® in Machine Learning , 8(1-2):1–230, 2015. (Cited on page 34.) [66] Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and R obert Nowak. Bilinear bandits with low-rank structure. In International Conference on Machine Learning , pages 3163–3172. PMLR, 2019. (Cited on page 50.) [67] Y uxin Chen, Y uejie Chi, Jianqing Fan, Cong Ma, and Y ulin g Y an. Noisy matrix completion: Understanding statistical guarantees for convex relaxati on via nonconvex optimization. SIAM Journal on Optimization , 30(4):3098–3121, 2020. (Cited on page 50.) 15Contents 1 Introduction 1 2 Models and Objectives 2 3 Matrix Estimation via Spectral Decomposition 3 3.1 Reward matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3.2 Transition matrices under the generative model . . . . . . . . . . . . . . . . . . . 4 3.3 Transition matrices under the forward model . . . . . . . . . . . . . . . . . . . . . 5 3.4 Elements of the proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Regret Minimization in Low-Rank Bandits 7 5 Representation Learning in Low-Rank MDPs 8 6 Related W ork 9 7 Conclusion and Perspectives 10 A Preliminaries 18 A.1 Matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Mixing time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.3 V alue difference lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B Statement and proofs of the main results 19 B.1 Reward matrix estimation – Model I . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Transition matrix estimation under the generative mode l – Model II(a) . . . . . . . 20 B.3 Transition matrix estimation under the forward model – M odel II(b) . . . . . . . . 21 B.4 An additional lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C Comparison inequalities and the Poisson approximation ar gument 22 C.1 Preliminaries on Poisson approximation . . . . . . . . . . . . . . . . . . . . . . . 22 C.2 Poisson approximation for reward matrices – Model I . . . . . . . . . . . . . . . . 23 C.3 Approximations for transition matrices – Model II . . . . . . . . . . . . . . . . . . 24 C.3.1 Multinomial approximation . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.3.2 Poisson approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 D Concentration of matrices with Poisson and compound Poiss on entries 26 D.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.2 Random matrices with compound Poisson entries . . . . . . . . . . . . . . . . . . 27 D.3 Random matrices with Poisson entries . . . . . . . . . . . . . . . . . . . . . . . . 31 E Singular subspace recovery via the leave-one-out argumen t 34 16E.1 Subspace recovery for reward matrices . . . . . . . . . . . . . . . . . . . . . . . . 34 E.2 Subspace recovery for transition matrices . . . . . . . . . . . . . . . . . . . . . . 38 E.3 Error decomposition in the two-to-inﬁnity norm . . . . . . . . . . . . . . . . . . . 41 F Row-wise and entry-wise matrix estimation errors 43 F .1 Bounding ∥M− ˆM∥2→∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 F .2 Bounding ∥P − ˆP∥1→∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 F .3 Bounding ∥M− ˆM∥∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 F .4 Bounding ∥P − ˆP∥∞ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 G Low-rank bandits: proofs of results from Section 4 46 G.1 Gap-dependent guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 G.2 Gap-independent guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 H Related work 49 H.1 Low-rank transition matrix estimation . . . . . . . . . . . . . . . . . . . . . . . . 49 H.2 Low rank bandits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 17A Preliminaries In this section, we present a few results that are used throughout our analysis. A.1 Matrix norms Lemma 9.Let A∈ Rn×m,B ∈ Rm×r. Then: ∥AB∥2→∞ ≤ ∥ A∥1→∞∥B∥2→∞, (2) ∥AB∥2→∞ ≤ ∥ A∥2→∞∥B∥, (3) ∥AB∥∞ ≤ ∥ A∥2→∞∥B⊤∥2→∞. (4) Proof. The proof of the lemma directly follows from Hölder’s inequa lity (see for example Proposition 6.5 in [ 15]). A.2 Mixing time Lemma 10.(Lemma 5 in [ 26]) Let τ(ε) be the ε-mixing time of an irreducible Markov chain. Then if ε≤ δ <1/2, τ(ε) ≤ τ(δ) ( 1 + ⌈ log(δ/ε) log(1/(2δ)) ⌉) . A.3 V alue difference lemmas The following lemmas are used in Section 5 to prove Theorem 8. Recall the deﬁnition of value function of a policy π: Vπ R(x) = E[∑ ∞ t=1 γt−1R(xπ t,πt(xπ t))|xπ 1= x]. The (state, action) value function of πis also deﬁned as: for any state x∈ S and action a∈ A , Qπ R(x,a) = R(x,a) + γEx′∼P(·|x,a)[Vπ R(x′)]. W e denote by ˆQπ Rthe (state, action) value function of π in the MDP where P is replaced by its estimate ˆP, and let ˆπ(R) be the optimal policy for this MDP . Lemma 11. W e have that ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2 sup π ∥Qπ R− ˆQπ R∥∞ Proof. W e remove the subscript Rto simplify the notation. For any s, we have Vπ⋆ (s) − Vˆπ(s) = Qπ⋆ (s,π⋆(s)) − Qˆπ(s,ˆπ(s)) = [ Qπ⋆ (s,π⋆(s)) − ˆQπ⋆ (s,π⋆(s))] + [ ˆQˆπ(s,ˆπ(s)) − Qˆπ(s,ˆπ(s))] + [ ˆQπ⋆ (s,π⋆(s)) − ˆQˆπ(s,ˆπ(s))] ≤ 2 sup π ∥Qπ − ˆQπ∥∞, since ˆQπ⋆ (s,π⋆(s)) ≤ ˆQˆπ(s,ˆπ(s)) by deﬁnition of ˆπ. Lemma 12. (Proposition 2.1 in [ 41]) F or all policies π: ∥Qπ R− ˆQπ R∥∞ ≤ γ (1 − γ)2 max a∈A ∥Pa − ˆPa∥1→∞ Combining the two lemmas, we get: ∥Vπ⋆(R) R − Vˆπ(R) R ∥∞ ≤ 2γ (1 − γ)2 max a∈A ∥Pa − ˆPa∥1→∞. This inequality is used in the proof of Theorem 8. 18B Statement and proofs of the main results In this appendix, we present the proofs of the main theorems.In Subsection § B.1, we provide the proof of Theorem 1 and Corollary 2. In Subsection § B.2, we give a complete, non-simpliﬁed version of Theorem 3 from which one can deduce Theorem 3 and Corollary 4 given in the main text. Finally, in Subsection § B.3, we present a complete, non-simpliﬁed version of Theorem 5 and from the latter, deduce Theorem 5 and Corollary 6. B.1 Reward matrix estimation – Model I In this subsection, we present the proofs of Theorem 1. The proof of Corollary 2 is in fact immediate from Theorem 1. Proof of Theorem 1. Proof of (i) . Recall the results from Lemma 30: for all δ∈ (0,1), if B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) , (5) then for all T ≥ c1(µ4κ2r2 + 1)(m+ n) log3 ( e2(m+ n)T/δ ) , the event max(∥U − ˆU( ˆU⊤U)∥,∥V − ˆV(ˆV⊤V)∥) ≤ C1 ∥M∥∥M∥∞ σr(M)2 max(∥V∥2→∞∥U∥2→∞)B holds with probability at least 1 − δ for some universal constants c1,C1 > 0. T o obtain the form presented in Theorem 1, we simply recall the deﬁnitions κ = ∥M∥/σr(M), µ = max( √ m/r∥U∥2→∞, √ n/r∥V∥2→∞) and the bound ∥M∥∞/σr(M) ≤ (µ2κr)/√mn from Lemma 17. W e then substitute in the upper bound above. Note that µ,κ and r are larger than 1 by deﬁnition. Proof of (ii).T o establish the desired bound, we use the decomposition err or established in Lemma 34. Namely, under the event that ∥ ˜M − M∥ ≤ c1σr(M) for some universal constant c1 > 0 sufﬁciently small, there exists a universal constant c2 >0 such that ∥ ˆM− M∥2→∞ ≤ c2σ1(M) [ ∥U − ˆU( ˆU⊤U)∥2→∞ + µ √ r m ∥ ˜M − M∥ σr(M) ] . (6) Hence, we only need high probability bounds on ∥U − ˆU( ˆU⊤U)∥2→∞ which we established in (i), and on ∥ ˜M− M∥ which we also established in Proposition 26 under the compound Poisson entries model described ( 15). W e can extend the latter result under our observation mode l using the Poisson approximation Lemma 20, and ﬁnally write that for all δ∈ (0,1), using the same deﬁnition of B as above in ( 5), for all for all T ≥ c3 log3 ((n+ m)/δ), the following statement ∥ ˜M− M∥ σr(M) ≤ C3∥M∥∞ σr(M) B (7) holds with probability at least 1 − δ, for some universal constants c3,C3 > 0 large enough. Note that under the condition T ≥ c4µ4κ2r2 log3 (e(n+ m)/δ) for some universal constant c4 large enough, the high probability statement in ( 7) holds and in addition we also have ∥ ˜M − M∥ ≤ c1σr(M). There, we used the result of Lemma 17. The statement (ii) in Theorem 1 is obtained by ﬁrst substituting in ( 6), the upper bound we get in (i) and that we get in ( 7), and then, using σ1(M) ≤ √mn∥M∥∞ and the bound ∥M∥∞/σr(M) ≤ (µ2κr)/√mnfrom Lemma 17. Proof of (iii). T o establish the desired bound, we use the decomposition err or established in Lemma 36. Namely, under the event that ∥ ˜M − M∥ ≤ c1σr(M) for some universal constant c1 > 0 sufﬁciently small, there exists a universal constant c2 >0 such that ∥ ˆM− M∥∞ ≤ c2∥M∥2→∞ ( ∥M − ˜M∥ σr(M) ∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞ ) + c2∥M − ˆM∥2→∞(∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞). (8) 19T o upper bound the above error, we need to control: (a) ∥V − ˆVWˆV∥2→∞, which we have already done in (i); (b) ∥M − ˜M∥, which follows from Lemma 20 as established in the proof of (ii) (see the high probability statement ( 7)); and (c) ∥M − ˆM∥2→∞, which again we have already done in (ii). The statement (iii) in Theorem 1 follows from ﬁrst substituting in ( 8), the upper bounds we get from (a), (b) and (c), and then using ∥M∥∞/σr(M) ≤ (µ2κr)/√mn, µ= max( √ m/r∥U∥2→∞, √ n/r∥V∥2→∞), and the basic inequality ∥M∥2→∞ ≤ √m∥M∥∞ ≤√m+ n∥M∥∞. B.2 T ransition matrix estimation under the generative mode l – Model II(a) In this subsection, we present a complete, non-simpliﬁed ve rsion of Theorem 3, from which one can deduce Theorem 3 and Corollary 4 given in the main text. First, let us deﬁne the function gδ : Rn×n → R+ as gδ(M) = 1{∃ℓ:∥Mℓ,:∥∞≤1} log (ne δ ) log−1 ( 1 + 1 ∥M∥∞ ) + 1{∀ℓ:∥Mℓ,:∥∞>1} log (∥M∥∞ ne δ )√ ∥M∥∞. (9) W e also use the following notation:    A = 1 √ T √ ∥M∥1→∞ + ∥M⊤∥1→∞, B′ = µκ√ r n ( A + 1 Tgδ/ √ T(TM) log ( n √ T δ )) + √ r∥M∥∞ T log ( n √ T δ ) . W e ﬁrst recall a standard result quantifying how well ˜M approximates M. Lemma 13. ∀δ∈ (0,1), w .p. at least 1 − δ, ∥ ˜M − M∥ ≤ CA + C Tgδ/ √ T(TM) √ log( n √ T δ ). Proof. The lemma follows directly from Lemma 22 (replacing Tτ by T) and Lemma 28. Theorem 14. Assume that (ν0)min = min i∈[n](ν0)i >0. F or any δ >0, if ∥ ˜M − M∥ ≤ cσr(M), gδ/ √ T(TM) log(n √ T/δ) ≤ cTσr(M) and ∥M∥∞ log(n √ T/δ) ≤ cTσ2 r(M) for some universal constant c> 0, then there exists a universal constant C >0 such that with probability at least 1 − δ holds: (i) max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C B′ σr (M) , (ii) ∥ ˆM− M∥2→∞ ≤ CκB′, ∥ ˆP − P∥1→∞ ≤ C κ√n (ν0)min B′, (iii) ∥ ˆM− M∥∞ ≤ C ( ∥M∥2→∞+κB′ σr (M) + κµ√ r n ) B′, (iv) ∥ ˆP − P∥∞ ≤ C B′ (ν0)min [ √nκ∥M∥∞ (ν0)min + ∥M∥2→∞+κB′ σr (M) + κµ√ r n ] , where (iv) holds if in addition ∥ ˆM − M∥1→∞ ≤ 1 2 (ν0)min. Proof. The ﬁrst statement of the theorem follows from Lemma 22 (with T instead of Tτ), Lemmas 28 and 32. The remaining bounds are consequences of (i) and of the results presented in Appendix F. Proof of Theorem 3. Theorem 3 follows from Lemma 13 and Theorem 14 by simplifying the term B′ using B given in Theorem 3. As a result of this simpliﬁcation, as well as of the assumpti ons given in statement of Theorem 14, we obtain bounds on n,T required in Theorem 3. Furthermore, we use simple inequalities (check Lemma 17) to rewrite all terms depending on M as functions of ∥M∥∞ and (ν0)min. Remark 1. It is worth noting that Corollary 4 is a corollary of Theorem 14, and that the lower bound on n required in Theorem 3 is not required for this corollary. Moreover , results prese nted in this corollary are valid for almost all T ≥ cnlog(nT) - in the case when T ≍ [n2−ǫ,n2] for 20arbitrarily small ǫ >0, bounds in Corollary 4 contain additional log term, which is an artifact of our analysis (and splitting concentration into cases T ≲ n2 and T ≳ n2). This discontinuity in the range of T can be resolved, but at the price of a reduced readability. B.3 T ransition matrix estimation under the forward model – M odel II(b) In this subsection, we present a complete, non-simpliﬁed ve rsion of Theorem 5, from which one can deduce Theorem 5 and Corollary 6 given in the main text. Again, we use function the funcion gδ deﬁned in ( 9), and we introduce: B′ = µκ √ r n (√ ∥ν∥∞τ⋆ T log (ne δ ) log(Tν−1 min) + τ⋆ T gδ/√Tτ (TτM) log (n√Tτ δ ) log(Tν−1 min) ) + √ rτ⋆∥M∥∞ T log (n√Tτ δ ) log(Tν−1 min). Our analysis starts from the following lemma stating how wel l ˜M approximates M. Lemma 15. (Lemma 7 in [ 26]) F or τ ≥ 2τ⋆log(Tν−1 min) and for any δ ∈ (0,1), we have with probability at least 1 − δ: ∥ ˜M − M∥ ≤ C √ ∥ν∥∞τ T log (ne δ ) + Cτ T log (ne δ ) . Theorem 16. Assume that νmin = min i∈[n] νi > 0 and that τ/(τ⋆log(Tν−1 min)) ∈ [c1,c2] for some universal constants c2 > c1 ≥ 2. F or any δ > 0, if ∥ ˜M − M∥ ≤ cσr(M), gδ/ √ T(TτM) log(n√Tτ/δ) ≤ cTτσr(M) and ∥M∥∞ log(n√Tτ/δ) ≤ cTτσ2 r(M) for some universal constant c> 0, then there exists a universal constant C >0 such that with probability at least 1 − δ, (i) max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≤ C B′ σr (M) , (ii) ∥ ˆM− M∥2→∞ ≤ CκB′, ∥ ˆP − P∥1→∞ ≤ Cκ√n νmin B′, (iii) ∥ ˆM− M∥∞ ≤ C ( ∥M∥2→∞+κB′ σr(M) + κµ√ r n ) B′, (iv) ∥ ˆP − P∥∞ ≤ C B′ νmin [ √nκ∥M∥∞ νmin + ∥M∥2→∞+κB′ σr (M) + κµ√ r n ] , where (iv) holds if in addition ∥ ˆM − M∥1→∞ ≤ 1 2 νmin. Proof. The ﬁrst statement of the theorem follows from Lemmas 21, 22 and 32, whereas the next four bounds follow from (i) and the bounds presented in Appendix F. As for the generative model, Theorem 5 is a direct consequence of Theorem 16, and it is obtained by simplifying the term B′ to B. Corollary 6 is also easily derived from Theorem 16. B.4 An additional lemma Lemma 17.Let M be matrix and m× nmatrix with rank r, incoherence parameter µ >0, and condition number κ> 0. Then, we have ∥M∥∞ ≤ σ1(M) µ2r √nm ≤ σr(M) µ2κr√nm. Proof of Lemma 17. For all (i,j) ∈ [m] × [n], we have |Mi,j| = ⏐ ⏐ ⏐ ⏐ ⏐ r∑ ℓ=1 σℓ(M)ui,ℓvj,ℓ ⏐ ⏐ ⏐ ⏐ ⏐≤ σ1(M) r∑ ℓ=1 |ui,ℓvj,ℓ| ≤ σ1(M)∥U∥2→∞∥V∥2→∞ ≤ σ1(M) µr √nm ≤ σr(M) µκr√nm. The ﬁrst inequality follows from the triangular inequality and the fact that σ1(M) ≥ σ2(M) ≥ · · · ≥ σr(M). The second inequality follows from Cauchy-Schwarz inequa lity. The last inequalities follow by deﬁnition of the incoherence parameter and of the c ondition number. 21C Comparison inequalities and the Poisson approximation ar gument In this section, we state and prove the results related to the Poisson approximations used to handle the noise correlations in the data. W e start by presenting so me of the key tools behind the Poisson approximation argument. This argument comes in the form of c omparison inequalities. The latter are applied and speciﬁed ﬁrst to Model I (reward matrix estim ation), and then to Model II (transition matrix estimation). C.1 Preliminaries on Poisson approximation The Poisson approximation argument comes in the form of an in equality, which is presented in Lemma 19. However, the key idea behind the argument is, roughly speak ing, the equality in distribution between a multinomial distribution with t trials and n outcomes, and the joint distribution of n in dependent Poisson random variables with properly chosen parameters, conditioned on some particular event. This equality of dist ribution is powerful for our purposes precisely because of the independence between the Poisson r andom variables. Below , we present Lemma 18 that represents this idea. Lemma 18. (Heterogeneous analogue of Theorem 5.2 in [ 59]) Let Y(t) i ∼ Poisson(tpi), i = 1 ,...,n , be independent random variables with ∑ n i=1 pi = 1 . Moreover , let (Z(t) 1 ,Z(t) 2 ,...,Z (t) n ) ∼ Multinomial(t,(p1,...,p n)). Then distribution of (Y(t) 1 ,...,Y (t) n ) conditioned on ∑ n i=1 Y(t) i = sis the same as (Z(s) 1 ,...,Z (s) n ) irrespective of t. Proof. The proof follows similar steps as the proof of Theorem 5.2 in [ 59], but we provide it here for the sake of completeness. First, note that from the deﬁni tion of multinomial distributions: P((Z(s) 1 ,...,Z (s) n ) = ( a1,...,a n)) = s! a1! · · · an!pa1 1 · · · pan n (10) if ∑ n i=1 ai = s, and 0 otherwise. Since the sum of Poisson random variables is a Poi sson random variable with parameter equal to the sum of parameters of the initial random variables, we get that the random variable ∑ n i=1 Y(t) i ∼ Poisson(∑ n i=1 tpi) = Poisson( t). Hence we have: P ( (Y(t) 1 ,...,Y (t) n ) = ( a1,...,a n) ⏐ ⏐ ⏐ ⏐ ⏐ n∑ i=1 Y(t) i = s ) = P((Y(t) 1 ,...,Y (t) n ) = ( a1,...,a n)) P(∑ n i=1 Y(t) i = s) = s! exp(−t)ts n∏ i=1 (tpi)ai exp(−tpi) ai! = s! a1! · · · an!pa1 1 · · · pan n (11) where in the last step we used the independence of Y(t) i ’s and ∑ n i=1 ai = s. Note that equations ( 10) and ( 11) are exactly the same, which concludes the proof. Lemma 19. Consider the setting of Lemma 18 and let f : Rp → R+ be any non-negative function. Then: E [ f(Z(t) 1 ,...,Z (t) p ) ] ≤ e √ tE [ f(Y(t) 1 ,...,Y (t) p ) ] 22Proof. The proof is essentially the same as that of Theorem 5.7 in [ 59] with the exception that we use Lemma 18 instead of Theorem 5.6 in [ 59] and we repeat it here for the sake of completeness. E[f(Y(t) 1 ,...,Y (t) p )] = ∞∑ k=0 E [ f(Y(t) 1 ,...,Y (t) p ) ⏐ ⏐ ⏐ p∑ i=1 Y(t) i = k ] P ( p∑ i=1 Y(t) i = k ) ≥ E [ f(Y(t) 1 ,...,Y (t) p ) ⏐ ⏐ ⏐ p∑ i=1 Y(t) i = t ] P ( p∑ i=1 Y(t) i = t ) = E[f(Z(t) 1 ,...,Z (t) p )]P ( p∑ i=1 Y(t) i = t ) (12) where in the second line we used non-negativeness of f, and in the last line we used Lemma 18. Now , since ∑ p i=1 Y(t) i is a Poisson random variable with mean twe have P(∑ p i=1 Y(t) i = t) = tt exp(−t) t! and using simple inequality t! <e √ t( t e)t we can rewrite inequality ( 12) as follows: E[f(Y(t) 1 ,...,Y (t) p )] ≥ E[f(Z(t) 1 ,...,Z (t) p )] 1 e √ t (13) which gives statement of the lemma. C.2 Poisson approximation for reward matrices – Model I W e recall from Section 4 that the deﬁnition of the empirical reward matrix ˜M is given as follows ∀(i,j) ∈ [n] × [m], ˜Mi,j = nm T T∑ t=1 (Mit,jt + ξt)1{(it,jt)=(i,j)} (14) where (it,jt) are sampled uniformly at random from [n] × [m]. Due to independence between (i1,j1),..., (iT,jT) and ξ1,...,ξ T, we note that the observation model ( 14) is equivalent in distribution to the following one ∀(i,j) ∈ [n] × [m], ˜Mi,j = nm T Zi,j∑ t=1 (Mi,j + ξ′ i,j,t) where we for all (i,j) ∈ [n] × [j], (ξ′ i,j,t)t≥1 is a sequence of i.i.d. random variables copies, say of ξ1, and Zi,j = T∑ t=1 1{(it,jt)=(i,j)}. Observe that Z = ( Zi,j)(i,j) is a multinomial random variable whose parameters are deﬁne d by the fact that for all t ∈ [T], P((it,jt) = ( i,j)) = 1 /nm). W e denote P the joint probability of the entries of Zand sequences (ξi,j,t)t≥1, (i,j) ∈ [n] × [m]. Compound Poisson random matrix model. W e deﬁne a random matrix Y ∈ Rn×m generated by a Poisson model as follows: Yi,j ∼ Poisson (T/nm) , (i,j) ∈ [n] × [m] and denote P′ the joint probability of the entries of Y and the sequences (ξ′ i,j,t)t≥1, for (i,j) ∈ [n] × [m]. W e may then consider the matrix model Xi,j = Yi,j∑ t=1 (Mi,j + ξ′ i,j,t). (15) W e note that the entries of the matrix Xare distributed according to compound Poisson distributio ns. Below , we precise the Poisson approximation argument for th e reward matrix model. 23Lemma 20 (Poisson Approximation) . Let (Ω ,F,P) (resp. (Ω ,F,P′)) be the probability space under the matrix-plus-noise model (14) (resp. (15)). Then for any event E ∈ F , we have P (E) ≤ e √ TP′ (E) . Proof of Lemma 20. For convenience, we denote X = (( ξi,j,t)t≥1)i,j∈[n]×[m]. W e set f(Z,X) = 1{E} . Thanks to Lemma 19, given that Zis independent of X, we have E [f(Z,X)|X] ≤ e √ TE [f(Y,X)|X] . W e further take the expectation on X and write P(E) = E [f(Z,X)] ≤ e √ TE [f(Y,X)] = e √ T P′(E). C.3 Approximations for transition matrices – Model II W e restrict our attention to the forward model, Model II(b). The results for the generative model are simpler and can be easily deduced from those for the forwa rd model. Recall from Section 3.3 deﬁnition of matrix ˜M(k) and in the following discussion we ﬁx value of k ∈ [τ]. Deﬁne a matrix N = Tτ˜M(k) and note that it is equal to: Ni,j = Tτ −1∑ l=0 1{(xk+lτ ,xk+1+lτ )=(i,j)}, i,j = 1 ,2,...,n (16) Furthermore, let P1 be joint probability distribution of entries of N. C.3.1 Multinomial approximation Here we deﬁne a matrixZ ∈ Rn×n with entries: Zi,j = Tτ −1∑ t=0 1{(it,jt)=(i,j)}, i,j = 1 ,2,...,n, (17) where P((it,jt) = ( i,j)) = νiPi,j independently over i,j ∈ [n] and t ∈ [Tτ]. Denote by P2 joint probability distribution of entries of Z. Then we have: Lemma 21. Let N and Z be matrices obtained under the models ( 16) and (17), respectively. Then, for any subset Z of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}, we have P(N ∈ Z ) ≤ 3P(Z ∈ Z ). Proof. Note that by subsampling as explained in Section 3.3, for each kwe obtain a Markov chain with transition kernel Pτ((y,y′)|(x,x′)) = Pτ(y|x′)P(y′|y) and initial distribution ν(k) 0 (x,x′) = ν(k) 0 (x)P(x′|x) with ν(1) 0 (x) = ν0(x) and ν(k) 0 (x) = ∑ y∈[n] ν0(y)Pk−1(x|y) for k= 2 ,...,τ. Moreover, all chains share the same stationary distributio n given by Π ∈ Rn×n with Π x,x′ = ν(x)P(x′|x), x,x′ ∈ [n]. Now , recall deﬁnition of τ from Theorem 5 and note that according to Lemma 10 with δ= 1 4 and ε= νmin/(eT) we have τ(ε) ≤ τ and thus: max 1≤i≤n ∥Pτ i,: − ν⊤∥1 ≤ νmin eT . (18) Now , let z = ( zi,j)n i,j=1 ∈ { z∈ Nn2 : ∑ i,jzi,j = Tτ} be a tuple of ﬁxed integers. Deﬁne a set: S(z) := {(a2l+1,a2l+2)Tτ −1 l=0 ∈ ([n] × [n])Tτ : Tτ −1∑ l=0 1{(a2l+1,a2l+2)=(i,j)} = zi,j,∀i,j ∈ [n]} 24and note that |S(z)| = Tτ!(∏ n i,j=1 zi,j!)−1. By deﬁnition of Markovian and multinomial models, we have: P(N = z) = ∑ ν(k) 0 (xk−1,xk) Tτ −1∏ l=1 Pτ((xk−1+lτ,xk+lτ)|(xk−1+(l−1)τ,xk+(l−1)τ)) where the sum is over (xk−1+lτ,xk+lτ)Tτ −1 l=0 ∈ S (z), and P(Z = z) = Tτ! ∏ n i,j=1 zi,j! n∏ i,j=1 Π zi,j i,j . Now we ﬁx arbitrarily one of the summands in the expression fo r P(N = z) and note that: ⏐ ⏐ ⏐ν(k) 0 (xk−1,xk) Tτ −1∏ l=1 Pτ((xk−1+lτ,xk+lτ)|(xk−1+(l−1)τ,xk+(l−1)τ)) − n∏ i,j=1 Π zi,j i,j ⏐ ⏐ ⏐ = (Tτ −1∏ l=0 P(xk+lτ|xk−1+lτ) )⏐ ⏐ ⏐ν(k) 0 (xk−1) Tτ −1∏ l=1 Pτ(xk−1+lτ|xk+(l−1)τ) − Tτ −1∏ l=0 ν(xk−1+lτ) ⏐ ⏐ ⏐ ≤ (Tτ −1∏ l=0 P(xk+lτ|xk−1+lτ) )(Tτ −1∏ l=0 (ν(xk−1+lτ) + ǫ) − Tτ −1∏ l=0 ν(xk−1+lτ) ) ≤   n∏ i,j=1 Π zi,j i,j   Tτ∑ j=1 ( ǫ νmin )j(Tτ j ) ≤   n∏ i,j=1 Π zi,j i,j   Tτ∑ j=1 (eTτǫ jνmin )j ≤ 2   n∏ i,j=1 Π zi,j i,j   where in ﬁrst inequality we used Equation ( 18), where we then used the bound on binomial coefﬁcients (Tτ j ) ≤ (eTτ/j)j, and where in the last inequality, we used deﬁnition of ǫ. Since this upper bound holds irrespective of the summand, we deduc e that: |P(N = z) − P(Z = z)| ≤ 2 Tτ!∏ n i,j=1 zi,j!   n∏ i,j=1 Π zi,j i,j  = 2 P(Z = z). Now , let Z be any subset of {z∈ Nn2 : ∑ (i,j) zi,j = Tτ}. Then we have: P(N ∈ Z ) = ∑ z∈Z P(N = z) ≤ 3 ∑ z∈Z P(Z = z) = 3 P(Z ∈ Z ) as claimed in the lemma. C.3.2 Poisson approximation W e deﬁne a matrix Y ∈ Rn×n generated by the Poisson model as follows: Yi,j ∼ Poisson(TτMi,j), i,j = 1 ,2,...,n. (19) W e show that rare random events occur with approximately equ al probability for the Poisson and multinomial models: Lemma 22.Let Z and Y be matrices obtained under the models ( 17) and (19), respectively. Then for any Z ⊂ Nn2 , we have P(Z ∈ Z ) ≤ e√TτP(Y ∈ Z ). Proof. Proof of the lemma is a straightforward consequence of Lemma 19 with parameters Tτ, n2 and f = 1{Z} . 25D Concentration of matrices with Poisson and compound Poiss on entries As mentioned in Appendix C, our analysis relies on a Poisson approximation argument. A s a result, we will require tight concentration bounds for random matri ces with entries distributed according to compound Poisson distributions (when estimating the rew ard matrix) and Poisson distributions (when estimating the transition matrices). In § D.1, we present a few simple facts about Poisson and compound Poisson random variables, together with some othe r useful tools. In § D.2, we present two concentration results, required for the model with compoun d Poisson entries. Similarly, in § D.3, we present two concentration results, required for the model w ith Poisson entries. These concentration results will be extensively used in the forthcoming analysi s for the subspace recovery. It is worth noting that our results in § D.3 are sharper than those in § D.2 thanks to Bennett’s inequality. As a consequence, our results for estimating the reward matr ix exhibit a dependence in log3(n+ m) while in the estimation of the transitions, our results exhi bit a dependence in log2(n) and even log(n) in some regimes. D.1 Preliminaries W e ﬁrst present Theorem 23, which can be seen as a version of matrix Bernstein inequalit y. The theorem is borrowed from [ 60] and relies on a truncation trick. The proofs of our concentr ation results in § D.2 and § D.3 rely on this theorem. Theorem 23. (Proposition A.3 in [ 60]) Let {Zt}T t=1 be a sequence of m×nindependent zero-mean real random matrices. Suppose that for all 1 ≤ t≤ T, (i) P (∥Zt∥ ≥ β) ≤ p, and (ii)  E[Zt1{∥Zt∥>β}]  ≤ q, (20) hold for some quantities p∈ (0,1), and q≥ 0. Furthermore, assume there exists v≥ 0, such that (iii) max {     T∑ t=1 E [ ZtZ⊤ t ]     ,      T∑ t=1 E [ Z⊤ t Zt ]      } ≤ v. (21) Then, for all u> 0, P (     T∑ t=1 Zt     ≥ Tq + u ) ≤ Tp + (n+ m) exp ( − u2/2 v+ βu/3 ) . (22) T o apply Theorem 23, we need control of the tails of the entries of the random matr ix we study. In the case of Poisson entries, we will simply use the following standard fact about Poisson random variables. It is a simple consequence of Bennett’s inequali ty [ 61]. Lemma 24. Let Y be a P oisson random variable with mean λ. Then for , all θ ∈ R, we have E[eθY] ≤ exp(λ(eθ − 1)). Furthermore, we have for all u> 0 P(|Y − λ| >u) ≤ 2 exp (−λh(u/λ)) ≤ 2 exp ( − u2/2 λ+ u/3 ) , where h(u) = (1 + u) log(1 + u) − u. In the case of compound Poisson entries, we do not have any res ult similar to Bennett’s inequality. Instead, we derive a Bernstein-type concentration result o n these random variables. Lemma 25. Let (ξt)t≥1 be a sequence of zero-mean, σ2-subgaussian, i.i.d. random variables. Let Y be a P oisson random variables with mean λ. Let M be a positive constant. Then, the moment generating function of the compound P oisson random variabl e Z = ∑ Y i=1(M + ξi) satisﬁes the following: ∀u> 0, P(|Z− λM| >u) ≤ 2 exp ( − min ( u2 16eλL2 , u 4L )) , E [ |Z− λM|2] ≤ 18λL2, where L= max( M,σ). 26Proof of Lemma 25. First, we upper bound the moment generating function of ∑ Y i=10(M+ ξi). Let θ> 0, we have IZ(θ) ≜ E [ eθ(∑ Y i=1(M+ξi)) ] ≤ √ E [e2θMY ] E[e2θ∑ Y i=0 ξi ] ≤ exp (λ(e2θM − 1)2 )√ E[e2θ∑ Y i=0 ξi ] ≤ exp (λ2 ( (2θM)2e2θM + 2θM ))√ E[e2θ∑ Y i=0 ξi ], where in the ﬁrst inequality, we use Cauchy-Schwarz inequal ity, in the second inequality, we use the well known bound on the moment generating function of a Po isson random variable (if Y is a Poisson random variable with mean λ, then for all θ> 0, E[eθY] ≤ exp(λ(eθ − 1))), and in the last inequality, we use the elementary fact that ex − 1 ≤ x2ex + xfor all x∈ R. Next, we have E [ e2θ∑ Y i=1 ξi ] = E [ ∞∑ k=1 1{Y=k} exp ( 2θ k∑ i=1 ξi )] = ∞∑ k=1 P(Y = k)E [ exp ( 2θ k∑ i=1 ξi )] ≤ ∞∑ k=1 P(Y = k) exp(2kθ2σ2) ≤ exp ( λ(e2θ2σ2 − 1) ) ≤ exp ( λ ( 2θ2σ2e2θ2σ2 )) , where we use the fact that the ξi are σ2-subgaussian r.v., and the elementary inequality ex2 − 1 ≤ x2ex2 for all x∈ R. W e conclude that for all θ >0, IZ(θ) ≤ exp ( λ ( 2θ2M2e2θM + 2θ2σ2e2θ2σ2 ) + λθM ) . Next, we introduce L= max( M,σ). Then, for all α> 0, we deduce that IZ(θ) ≤ exp ( 2λθ2L2 ( eα + eα2 ) + λθM ) , ∀|θ| ≤ α 2L. By Markov inequality, and ﬁxing α= 1 , we have P(Z− λM >u) ≤ inf |θ|≤1/(2L) IZ(θ)e−λθM−θu ≤ exp ( − min ( u2 16eλL2 , u 4L )) . Similarly, we have P(λM − Z >u) ≤ exp ( − min ( u2 16eλL2 , u 4L )) . The ﬁnal tail bound follows from a union bound. Finally, stra ightforward computations yield an upper bound on E[|λM − Z|2]. Indeed, we have E[|λM − Z|2] ≤ 2E[|Y − λ|2]M2 + 2E   (Y∑ i=1 ξi )2 ≤ 2λM2 + 16λσ2 ≤ 18λL2. D.2 Random matrices with compound Poisson entries W e list below the two main concentration results that we need for the forthcoming analysis. In Proposition 26, we provide a high probability guarantee on the error betwee n the empirical mean reward matrix and the true matrix in operator norm. In Propos ition 27, we establish another concentration result that will be instrumental in the subsp ace recovery analysis. The proofs of the two results are similar with slight differences and they bot h rely on Theorem 23. The proofs are presented at the end of this subsection. 27Proposition 26. Under the random matrix model (15) with compound P oisson entries, for all δ ∈ (0,1), for all T ≥ 13(n+ m) log3 ((n+ m)/δ), the following statement ∥ ˜M − M∥ ≤ 36 √ 2L √ nm T (√ (n+ m) log (n+ m δ ) + log3/2 (n+ m δ )) holds with probability at least 1 − δ, where L= max( ∥M∥∞,σ). Proposition 27. Let A be a m× 2r nonrandom matrix, and B be a n× 2r nonrandom matrix. Then, under the random matrix model (15) with compound P oisson entries, and denoting L = max(∥M∥∞,σ), we have: (i) for all ℓ∈ [m], for all δ∈ (0,1), for all T ≥ mlog3(en/δ), the following event ∥(˜Mℓ,: − Mℓ,:)A∥ ≤ 73 √ 2L∥A∥2→∞ √ nm T (√ nlog (en δ ) + log3/2 (en δ )) (23) holds with probability at least 1 − δ; (ii) for all k∈ [n], for all δ∈ (0,1), for all T ≥ nlog3(em/δ), the following event ∥(˜M:,k − M:,k)⊤B∥ ≤ 73 √ 2L∥B∥2→∞ √ nm T (√ mlog (em δ ) + log3/2 (em δ )) (24) holds with probability at least 1 − δ. Proof of Proposition 26. T o simplify the notation, introduce the matrices Zi,j = ( ˜Mi,j−Mi,j)eie⊤ j, for all (i,j) ∈ [m] × [n], λ= T/mn, and L= max( ∥M∥∞,σ). W e remark that we can write ˜M − M = ∑ (i,j)∈[m]×[n] Zi,j. Starting from the above expression, we will apply Theorem 23 to obtain the desired result. First, we note that for all (i,j) ∈ [m] × [n], ∥Zi,j∥ = |˜Mi,j − Mi,j| and ˜Mi,j − Mi,j is a centered and normalized compound Poisson random variable. Thus, we have by Lemma 25, for all δ ∈ (0,1), P(∥Zi,j∥ >β ) ≤ δ/(2n2m2), where we deﬁne β = 4 Lmax (√ e λlog (4n2m2 δ ) , 1 λlog (4n2m2 δ )) , ≤ 4Lmax (√ 4e λ log (n+ m δ ) ,4 λlog (n+ m δ )) . Moreover, we have E [ ∥Zi,j∥1{∥Zi,j]∥ >β } ] ≤ √ E [∥Zi,j∥2] E [ 1{∥Zi,j]∥ >β } ] ≤ √ E[|˜Mi,j − Mi,j|2]P(∥Zi,j∥ >β) ≤ √ 9L2δ λn2m2 , where the ﬁrst inequality follows from Cauchy-Schwarz ineq uality, the second inequality follows from the expression of Zi,j, and the third inequality follows from Lemma 25. Next, we have      ∑ (i,j)∈[m]×[n] E [ Zi,jZ⊤ i,j ]       = ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ∑ i∈[m]  ∑ j∈[n] E [ ( ˜Mi,j − Mi,j )2]  eie⊤ i ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ = max i∈[m] ∑ j∈[n] E [ ( ˜Mi,j − Mi,j )2] ≤ 18nL2 λ . 28By symmetry, we obtain similarly       ∑ (i,j)∈[m]×[n] E [ Z⊤ i,jZi,j ]       ≤ 18mL2 λ . Let us set v= 18( n∧ m)L2/λ. W e conclude using Theorem 23 that, for all u> 0, P ( ∥ ˜M − M∥ > √ 9L2δ λ + u ) ≤ δ 2(nm) + (n+ m) exp ( − u2/2 v+ βu/3 ) ≤ δ 2(nm) + (n+ m) exp ( − 1 4 min (u2 v ,3u β )) . W e re-parametrize by choosing δ= 2( n+ m) exp(−(1/4) min(u2/v,3u/β)), and write P ( ∥ ˜M− M∥ > 3L √ δ√ λ + u ) ≤ δ (25) with u= max (√ 4vlog (2(n+ m) δ ) ,4β 3 log (2(n+ m δ )) ≤ max (√ 8vlog (n+ m δ ) ,8β 3 log (n+ m δ )) . By inspecting the deﬁnition of βand v, we note that under the condition λ= T nm ≥ 45 34 1 n∧ mlog3 (n+ m δ ) (26) then u≤ max (√ 8vlog (n+ m δ ) ,16L √ 2e 3 √ λ log3/2 (2(n+ m) δ )) ≤ L√ λ max (√ 3242(n∧ m) log (n+ m δ ) ,422√e 3 log (n+ m δ )) . After using the upper bound on u in ( 25), and after upper bounding δ by 1, we obtain, under the condition ( 26), ∥ ˜M − M∥ > L√ λ ( 3 + 12 √ 2(n+ m) log (n+ m δ ) + 43√e 3 log3/2 (n+ m δ )) > L√ λ ( 36 √ 2(n+ m) log (n+ m δ ) + 36 log 3/2 (n+ m δ )) > 36 √ 2L√ λ (√ (n+ m) log (n+ m δ ) + log3/2 (n+ m δ )) with probability at most δ. Noting that a stricter condition than ( 26) is T ≥ 13(n+ m) log3 (n+ m δ ) , we complete the proof. 29Proof of Proposition 27. T o simplify the notation, let us denote Zj = ( ˜Mℓ,j − Mℓ,j)Aj,:, λ = mn/T, and L= max( ∥M∥∞,σ). W e remark that we can write (˜Mℓ,: − Mℓ,:)A= ∑ j∈[n] (˜Mℓ,j − Mℓ,j)Aj,: = ∑ j∈[n] Zj. Starting from the above expression, we will apply 23 to obtain the desired result. First, we note that for all j ∈ [n], ∥Zj∥ = |˜Mℓ,j−Mℓ,j|∥Aj,:∥, and ˜Mℓ,j−Mℓ,j is a centered and normalized compound Poisson random variable. Thus, we have by Lemma 25, for all δ∈ (0,1), P (∥Zj∥ >∥A∥2→∞β) ≤ P (∥Zj∥ >∥Aj,:∥β) ≤ δ/(2n2), where we deﬁne β = 4 Lmax (√ e λlog (4n2 δ ) ,1 λlog (4n2 δ )) ≤ 4Lmax (√ 2e λ log (en δ ) ,2 λlog (en δ )) . Moreover, we have E [ ∥Zj∥1{∥Zj ∥>∥A∥2→∞β} ] ≤ √ E[∥Zj∥2]P (∥Zj∥ >∥A∥2→∞β) ≤ ∥ A∥2→∞ √ E[∥ ˜Mℓ,: − Mℓ,:∥2]δ 2n2 ≤ ∥ A∥2→∞ √ 9L2δ λn2 , where in the ﬁrst inequality, we use Cauchy-Schwarz inequal ity, and in the third inequality, the result of Lemma 25 to upper bound the variances. Next, we have      E  ∑ j∈[n] ZjZ⊤ j         ≤ ∑ j∈[n] E [ (˜Mℓ,j − Mℓ,j)2 ] ∥Aj,:∥2 ≤ 18L2∥A∥2 F λ ≤ 18L2n∥A∥2 2→∞ λ , where we simply used the expressions of Zj, j ∈ [n], the triangular inequality, and Lemma 25 to upper bound the variances. Similarly, we have       E  ∑ j∈[n] Z⊤ j Zj         ≤ 18L2n∥A∥2 2→∞ λ . W e set v= 18 L2n∥A∥2 2→∞/λ. Now we are ready to apply Theorem 23. W e get: P ( ∥(˜Mℓ,: − Mℓ,:)A∥ >∥A∥2→∞ √ 9L2δ λ + u ) ≤ δ 2n + nexp ( − 1 4 min (u2 v , 3u ∥A∥2→∞β )) . W e re-parametrize by choosing δ= 2 nexp(−(1/4) min(u2/v,3u/(∥A∥2→∞β))) and we write P ( ∥(˜Mℓ,: − Mℓ,:)A∥ >∥A∥2→∞ √ 9L2δ λ + u ) ≤ δ with u= max (√ 4vlog (2n δ ) ,4∥A∥2→∞β 3 log (2n δ )) ≤ max (√ 4vlog (en δ ) ,4∥A∥2→∞β 3 log (en δ )) . 30By inspecting the deﬁnition of βand v, we note that when the condition λ= T mn ≥ 43 34 1 nlog3 (en δ ) (27) holds, then u≤ max (√ 4vlog (en δ ) ,16 √ 2eL∥A∥2→∞ 3 √ λ log3/2 (en δ )) ≤ L∥A∥2→∞√ λ max (√ 2332nlog (en δ ) ,16 √ 2e 3 log3/2 (en δ )) ≤ 36 √ 2L∥A∥2→∞√ λ max (√ nlog (en δ ) ,log3/2 (en δ )) . (28) After using the upper bound in ( 28), and upper bounding δby 1, we obtain that, under the condition (27), ∥(˜Mℓ,: − Mℓ,:)A∥ > 73 √ 2L∥A∥2→∞√ λ (√ nlog (en δ ) + log3/2 (en δ )) holds with probability at most δ. W e can also reﬁne the condition ( 27) as follows T ≥ mlog3 (en δ ) . This concludes the proof of the statement ( 23) in the proposition. The statement ( 24) follows similarly. Therefore, we omit it. D.3 Random matrices with Poisson entries Recall from Section B.2, the deﬁnition of the function gδ from ( 9) and that A = 1√ T √ ∥M∥1→∞ + ∥M⊤∥1→∞. First we show the following lemma that provides an upper bou nd of the spectral norm. This lemma is used to derive Lemma 13. Lemma 28. Let Y ∈ Rn×n be a matrix with independent entries Yi,j ∼ T−1Poisson(TMij), i,j ∈ [n], and let 0 ≤ δ≤ 1. Then, w .p. at least 1 − δ, ∥Y − M∥ ≤ CA + C Tgδ(TM) √ log( ne δ ). Proof. The proof follows from that of Lemma 29 and that of Lemma 4 in [ 62], which is based on a spectral bound from [ 63]. W e use that the random variables |Yi,j − Mi,j| concentrate well around L = L11{∃ℓ:T∥Mℓ,:∥∞≤1} + L21{∀ℓ:T∥Mℓ,:∥∞>1} where L1 = 4 T−1 log−1(1 + ( T∥M∥∞)−1 ∧ nδ−1) log( ne δ ) and L2 = 4 √ T−1∥M∥∞ log ( T∥M∥∞ ne δ ) using exactly the same argument as in the ﬁrst step of Lemma 29. Moreover, we use upper bound on |E[(Yi,j − Mi,j)1{|Yi,j −Mi,j |<L}]| derived in the second step of Lemma 29. W e also derive upper bounds in the ℓ2→∞ norm. These bounds are used in the analysis of the singular subspace recovery in Lemma 32, and therefore in the proofs of Theorems 3 and 5. Lemma 29. Let Y ∈ Rn×n be a matrix with independent entries Yi,j ∼ T−1Poisson(TMij), i,j ∈ [n], for an arbitrary integer T >0. Let 0 ≤ δ ≤ 1. Then, for any 1 ≤ l ≤ nand any matrix A∈ Rn×p, with p≤ n, and independent of Yl,: we have, if T∥Ml,:∥∞ ≤ 1, ∥(Yl,: − Ml,:)A∥ ≲ ∥A∥F √ ∥Ml,:∥∞ log (ne δ ) √ T + ∥A∥2→∞ log2 (ne δ ) Tlog(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1)) else if T∥Ml,:∥∞ >1, ∥(Yl,: − Ml,:)A∥ ≲ ∥A∥F √ ∥Ml,:∥∞ log (ne δ ) √ T + ∥A∥2→∞ √ ∥Ml,:∥∞√ T log ( T∥Ml,:∥∞ ne δ ) log (ne δ ) with probability at least 1 − δ/n. 31Proof of Lemma 29. The lemma is an application of the truncated matrix Bernstei n theorem i.e. Theorem 23. In this theorem, T corresponds to nin Lemma 29, nin Theorem 23 corresponds to 1 in Lemma 29, and min Theorem 23 corresponds to nin Lemma 29. First note that for any l, we have (Yl,: −Ml,:)A= ∑ n i=1(Yl,i−Ml,i)Ai,:. Moreover, since each of these nsummands are independent, zero-mean random vectors, we can identify Zi’s from Theorem 23 with (Yl,i − Ml,i)Ai,: ∈ R1×n for i∈ [n]. T o apply Theorem 23, we need to verify its assumptions. This is done below . Step 1: Showing (i) in (20) First, recall Bennett’s concentration inequality from Le mma 24, which in our case implies that for any i,j ∈ [n]: P(|Yi,j − Mi,j| ≥ tMi,j) ≤ 2 exp (−h(t)TMi,j) . (29) Note that ∥Zi∥ in Theorem 23 in our case corresponds to: ∥(Yl,i − Ml,i)Ai,:∥ = |Yl,i − Ml,i|∥Ai,:∥ ≤ | Yl,i − Ml,i|∥A∥2→∞. W e consider two different cases: 1. T∥Ml,:∥∞ ≤ 1: W e let β1 = 4 T−1∥A∥2→∞ log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) log( ne δ ) and note that h(t) ≥ 1 2 tlog tfor t≥ 1. Thus, from Equation ( 29), we have: P ( |Yl,i − Ml,i| ≥ β1 ∥A∥2→∞ ) ≤ 2 exp ( − 2 log( ne δ ) log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) · log ( 4 log( ne δ ) T∥Ml,:∥∞ log(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) )) ≤ δ 2n2 . where, in the second inequality, we show using simple algebr a that log−1(1 + ( T∥Ml,:∥∞)−1 ∧ nδ−1) log ( 4 log( ne δ ) T∥Ml,:∥∞ log(1+(T∥Ml,:∥∞)−1∧nδ−1) ) ≥ 1 for δ≤ 1 and T∥Ml,:∥∞ ≤ 1. 2. T∥Ml,:∥∞ > 1: Here we deﬁne β2 := 4 ∥A∥2→∞ √ T−1∥Ml,:∥∞ log ( T∥Ml,:∥∞ ne δ ) . Then, according to Equation ( 29) and the approximation h(t) ≥ min{t2/4,t} for t≥ 0, we have: P ( |Yl,i − Ml,i| ≥ β2 ∥A∥2→∞ ) ≤ 2 exp ( − 4 min { log2(T∥Ml,:∥∞ ne δ ), √ T∥Ml,:∥∞ log(T∥Ml,:∥∞ ne δ ) }) ≤ 2 exp ( − 4 log(T∥Ml,:∥∞ ne δ ) ) ≤ 1 2T∥Ml,:∥∞ δ n2 . where, in the second inequality, we used that δ ≤ 1 and T∥Ml,:∥∞ > 1. Finally, we deﬁne β = β11{T∥Ml,:∥∞≤1} + β21{T∥Ml,:∥∞>1} and p = δ 2n1{T∥Ml,:∥∞≤1} + 1 2T∥Ml,: ∥∞ δ n1{T∥Ml,:∥∞>1} (since we took union bound over i∈ [n]). Step 2: Showing (ii) in (20) In our case the l.h.s. corresponds to ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥>β}]∥ = ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥≤β}]∥ which can be upper bounded by ∥A∥2→∞|E[(Yl,i − Ml,i)1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ }]|. For some integers κmin,κmax, let Yl,i ∈ 1 T[κmin,κmax] be interval of Yl,i for which indicator 1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ } is active and note that this is a superset of interval for which 1{|Yl,i−Ml,i|≤ β ∥ A∥ 2→∞ } is active. Then from the deﬁnition of Poisson random variables and the bounds derive d previously, we obtain: ⏐ ⏐ ⏐E[(Yl,i − Ml,i)1{|Yl,i−Ml,i|≤ β ∥ Ai,:∥ }] ⏐ ⏐ ⏐= 1 T ⏐ ⏐ ⏐ κmax∑ k=κmin (k− TMl,i)exp(−TMl,i)(TMl,i)k k! ⏐ ⏐ ⏐ = Ml,i ⏐ ⏐ ⏐ κmax−1∑ k=κmin−1 exp(−TMl,i)(TMl,i)k k! − κmax∑ k=κmin exp(−TMl,i)(TMl,i)k k! ⏐ ⏐ ⏐ ≤ Ml,i(P(TYl,i = κmin − 1) + P(TYl,i = κmax)) ≤ 2δ Tn2 min{T∥Ml,:∥∞,1}, 32where we assumed that κmin ≥ 1, otherwise we keep just the second probability term above. T hus, using previous two inequalities, we have: ∥E[(Yl,i − Ml,i)Ai,:1{∥(Yl,i−Ml,i)Ai,:∥>β}]∥ ≤ ∥ A∥2→∞ 2δ Tn2 min{T∥Ml,:∥∞,1} Step 3: Showing (iii) in (21) Using our deﬁnition Zi = ( Yl,i − Ml,i)Ai,: ∈ R1×n, we have that ZiZ⊤ i = ( Yl,i − Ml,i)2∥Ai,:∥2, Z⊤ i Zi = ( Yl,i − Ml,i)2A⊤ i,:Ai,:. Since Aand Yl,: are independent, we have: ∥ n∑ i=1 E[ZiZ⊤ i ]∥ = n∑ i=1 E[ZiZ⊤ i ] = n∑ i=1 ∥Ai,:∥2E(Yl,i − Ml,i)2 ≤ ∥ A∥2 Fmax i E(Yl,i − Ml,i)2 and ∥ n∑ i=1 E[Z⊤ i Zi]∥ = ∥ n∑ i=1 E(Yl,i − Ml,i)2A⊤ i,:Ai,:∥ ≤ n∑ i=1 E(Yl,i − Ml,i)2∥A⊤ i,:Ai,:∥ ≤ ∥ A∥2 Fmax i E(Yl,i − Ml,i)2. Now note that for Yl,i ∼ T−1Poisson(TMl,i), Var(Yl,i) = E(Yl,i − Ml,i)2 = T−1Ml,i. Thus, by setting v= T−1∥A∥2 F∥Ml,:∥∞, we get (iii). Plugging in all obtained quantities into Equation ( 22) ﬁnishes proof of the lemma. 33E Singular subspace recovery via the leave-one-out argumen t In this section, we present Lemma 30 and Lemma 32 providing sharp guarantees for the singular subspace recovery in two-to-inﬁnity norm. Obtaining such g uarantees is not trivial and requires the use of a rather technical analysis, namely the leave-one -out technique [ 16, 22]. However, such technique heavily relies on independence between entries o f the observed random matrix. W e use the Poisson approximation argument to address this, which i n turn requires to reproduce the leave- one-out analysis under a different random matrix observati on models (see ( 15) and ( 19)). W e wish to highlight that Farias et al. [ 64], like us, have also used the leave-one-out argument to obtain entry-wise guarantees for matrix estimation with su b-exponential noise. In our case, we use this argument as a sub-step of our analysis after performing the Poisson approximation. However, we believe that, our ﬁnal results are richer, more precise an d actually needed for our RL applications. Indeed, we are able to obtain guarantees in the norms ∥ · ∥2→∞ and ∥ · ∥1→∞ (these are not provided in [ 64]). Moreover, the entry-wise guarantees in [ 64] are only expressed in terms of the matrix dimensions m and n. Our guarantees on the other hand exhibit dependencies on th e dimensions m,n, the number of observation T and the conﬁdence level δ. Having guarantees with an explicit dependence for all T ≥ 1 and δ ∈ (0,1) is crucial in the design of our algorithm for low-rank bandits. E.1 Subspace recovery for reward matrices Lemma 30.Let δ∈ (0,1). Deﬁne: B = √ nm T (√ (n+ m) log (e(n+ m)T δ ) + log3/2 (e(n+ m)T δ )) . F or all T ≥ c(µ4κ2r2 + 1)(m+ n) log3 ( e2(m+ n)T/δ ) , the event max(∥U− ˆU( ˆU⊤U)∥,∥V − ˆV(ˆV⊤V)∥) ≤ C∥M∥∥M∥∞ σr(M)2 max(∥V∥2→∞∥U∥2→∞)B holds with probability at least 1 − δ, for some universal constants c,C >0. Proof of Lemma 30. The proof follows similar steps as that of Theorem 4.2 in [ 22], which is based on the leave-one-out analysis. Step 1: Dilation trick.In order to apply the leave-one-out analysis, we ﬁrst use a di lation trick [ 65] to reduce the problem to that of symmetric matrices. Deﬁne: S = [ 0 M M⊤ 0 ] and note that for matrix M with SVD M = UΣ V⊤, we have: S = 1√ 2 [ U U V −V ][ Σ 0 0 −Σ ] 1√ 2 [ U U V −V ] ⊤ := QDQ⊤. W e deﬁne, in a similar way, ˜S using ˜M, and let ˆQ ∈ R(n+m)×2r be the matrix of eigenvectors of the best 2r-rank approximation of ˜S. Note that: ∥Q− ˆQ( ˆQ⊤Q)∥2→∞ = max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } . (30) T o keep the notation simple, we will deﬁne WˆQ = ˆQ⊤Q. Further note that ∥ ˜S− S∥ = ∥ ˜M − M∥, σ 1(S) = σ1(M), and σ2r(S) = σr(M). (31) W e start the analysis under the model ( 15) and assume that ˜M has independent entries with compound Poisson distributions. W e will eventually invoke the Poisson approximation argument via Lemma 20 to deduce the ﬁnal result. 34Step 2: Error decomposition. W e apply the decomposition in Lemma 33 to obtain: ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (4∥ ˜SQ∥2→∞∥E∥ σ2r(S) + ∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) , where we set E = ˜S− S. W e observe that when ∥E∥ ≤ σ2r(S)/2, then ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (4∥SQ∥2→∞∥E∥ σ2r(S) + 3∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) . (32) Furthermore, we also have ∥ ˜S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥ sin(Q, ˆQ)∥2 ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥E∥2 σ2r(S)2 ≤ ∥ E(Q− ˆQWˆQ)∥2→∞ + ∥SQ∥2→∞∥E∥ 2σ2r(S) , where the ﬁrst inequality follows from the triangular inequ ality, the second inequality follows by the relation between the two-to-inﬁnity norm and the sin theore m (see e.g., [ 15]). The third inequality follows from Davis-Kahan’s theorem. The fourth inequality follows under the condition ∥E∥ ≤ σ2r(S)/2. W e ﬁnally obtain ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σ2r(S) (5∥SQ∥2→∞∥E∥ σ2r(S) + 3∥EQ∥2→∞ + 2∥E(Q− ˆQWˆQ)∥2→∞ ) . (33) Note that in the above inequality, we can control ∥E∥ using Proposition 26 and ∥EQ∥2→∞ using Proposition 27. However, the term ∥E(Q− ˆQWˆQ)∥2→∞ is not easy to control because E and (Q− ˆQWˆQ) are dependent on each other in a non-trivial way. T o control t his term, we use the leave-one-out analysis. Step 3: Leave-one-out analysis.W e deﬁne a matrix ˜S(ℓ) ∈ R(n+m)×(n+m) as follows: ˜S(ℓ) i,j = { ˜Si,j, if i̸= ℓor j ̸= ℓ Si,j, otherwise Then deﬁne ˆQ(ℓ) ∈ Rn×2r as a matrix of eigenvectors corresponding to the 2rgreatest (in absolute value) eigenvalues of matrix ˜S(ℓ). Deﬁne W˜U(ℓ) accordingly. W e have ∥E(Q− ˆQWˆQ)∥2→∞ ≤ max ℓ∈[n+m] ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 + ∥E∥2∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F. W e have by Proposition 26 that P (∥E∥ ≲ ∥M∥∞G) ≥ 1 − δ provided that (C1) T ≥ c1 mn m+ nlog3 (e(m+ n) δ ) and where we deﬁne G = √ mn T (√ (m+ n) log (e(m+ n) δ ) + log3/2 (e(m+ n) δ )) . Let us now introduce the event E1 as follows E1 = {∥E∥ ≤ ∥ M∥∞G} . 35Note that if the following condition holds (C2) T ≥ c2(µκr)2 ( (m+ n) log (e(m+ n) δ ) + log3 (e(m+ n) δ )) for c2 large enough then 16∥E∥ ≤ σr(M). Hence, under the event E1, using Lemma 31, we have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(M) , which further gives by triangular inequality ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 σr(M) + 16 ( ∥Eℓ,:Q∥2 + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞ ) σr(M) . Now , by Proposition 27, P ( ∥Eℓ,:(Q− Q(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞∥Q− Q(ℓ)WˆQ(ℓ) ∥2→∞G ) ≥ 1 − δ as long as the same condition (C1) holds with c1 large enough. So let us introduce the event E2 = { ∥Eℓ,:(Q− Q(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞∥Q− Q(ℓ)WˆQ(ℓ) ∥2→∞G } . W e further upper bound under the event E1 ∩ E2, ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞ ( ∥Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ) G. Note that, under the condition (C2) with c2 large enough, we can also obtain 16∥M∥∞ σr(M) G ≤ 1 2 which entails that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 32∥M∥∞∥Q− ˆQWˆQ∥2→∞ σr(M) G + 32(∥Eℓ,:Q∥ + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞) σr(M) . T o simplify the notation, let us deﬁne the three errors as x= ∥Q− ˆQWˆQ∥2→∞, y= ∥EQ∥2→∞ ≥ ∥ Eℓ,:Q∥2, z = ∥E∥∥Q∥2→∞. W e have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≲ (∥M∥∞G σr(M) + ∥E∥ σr(M) ) x+ 1 σr(M)(y+ z). By plugging the above in the previous inequality, we get ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥2 ≲ ∥M∥∞G (( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) x+ 1 σr(M)(y+ z) ) which entails ﬁnally ∥E(Q− ˆQWˆQ)∥2→∞ ≲ ( ∥E∥ σr(M) + G∥M∥∞ σ1(M) ) (y+ z) + (∥E∥ + G∥M∥∞) ( 1 + G∥M∥∞ σr(M) + ∥E∥ σr(M) ) x. (34) 36Step 4: Putting everything together . Combining the inequalities ( 33) and ( 34) gives x≤ C1 ( ∥E∥ σr(M) + ∥M∥∞G σr(M) )( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) x + C2 σr(M) ( 1 + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) y + C3 σr(M) ( ∥M∥ σr(M) + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) z. Under the events E1 and E2 and provided that the conditions (C1) and (C2) hold, for c1 and c2 are large enough, we have C1 ( ∥E∥ σr(M) + ∥M∥∞G σr(M) )( 1 + ∥M∥∞G σr(M) + ∥E∥ σr(M) ) ≤ 1 2, ( 1 + ∥E∥ σr(M) + G∥M∥ σr(M) ) ≤ 3, ( ∥M∥ σr(M) + ∥E∥ σr(M) + G∥M∥∞ σr(M) ) ≤ ( ∥M∥ σr(M) + 2 ) . Thus, we obtain x≤ 1 σr(M) ( y+ ∥M∥ σr(M)z ) . W e note that, under a similar conditions as before , we also ha ve by Proposition 26 and Proposition 27 that y≲ ∥M∥∞∥Q∥2→∞G z ≲ ∥M∥∞∥Q∥2→∞G with probability at least 1 − δ. Thus, we conclude after further simpliﬁcations that for so me C >0 large enough, we have P ( ∥Q− ˆQWˆQ∥2→∞ ≤ C∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G ) ≥ 1 − δ provided T ≥ c(µ4κ2r2 + 1)(m+ n) log3 (e(m+ n) δ ) , with G(n,m,T,δ ) = √ nm T (√ (n+ m) log (e(n+ m) δ ) + log3/2 (e(n+ m) δ )) . Step 5: Poisson approximation. T o conclude, we now invoke Lemma 20 which entails that under the true model ( 14), we have P ( ∥Q− ˆQWˆQ∥2→∞ >C ∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G(n,m,T,δ ) ) ≤ e √ Tδ provided T ≥ c(µ4κ2r2 + 1)(m+ n) log3 (e(m+ n)/δ). By re-parametrizing with δ′ = e √ Tδ, we obtain P ( ∥Q− ˆQWˆQ∥2→∞ >C ∥M∥∥M∥∞ σr(M)2 ∥Q∥2→∞G(n,m,T,δ ′/e √ T) ) ≤ δ′, again provided that T ≥ c(µ4κ2r2 + 1)( m + n) log3 ( e2(m+ n) √ T/δ′ ) . Recalling that ∥Q∥2→∞ = max( ∥V∥2→∞,∥U∥2→∞), we immediately obtain the ﬁnal result. 37Lemma 31. Under the notation used in the proof of Lemma 30, provided the condition ∥E∥ ≤ σ2r(S)/16, the following inequality holds: ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) Proof of Lemma 31. W e have ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ ∥ ˆQˆQ⊤ − ˆQ(ℓ)( ˆQ(ℓ))⊤∥F∥Q∥ ≤ 2∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F |σ2r( ˜S(ℓ)) − σ2r+1( ˜S(ℓ))| where the ﬁrst inequality follows the elementary fact that ∥AB∥F ≤ ∥ A∥F∥B∥, and the second inequality follows by Davis-Kahan. Now , by W eyl’s inequali ty, we have for all k ∈ [n + m], |σk( ˜S(ℓ)) − σk(S)| ≤ ∥ E(ℓ)∥ ≤ ∥ E∥, where the error matrix E(ℓ) = ˜Sℓ− S, and more precisely is deﬁned as follows: E(ℓ) i,j = {Ei,j if i̸= ℓor j ̸= ℓ, 0 otherwise. The crude inequality ∥E(ℓ)∥ ≤ ∥ E∥ follows from the fact that ∥E(ℓ)∥ is equal to the operator norm of a submatrix of E which will always be smaller than ∥E∥. Therefore, under the condition that ∥E∥ ≤ σ2r(S)/4, we have |σ2r( ˜S(ℓ)) − σ2r+1( ˜S(ℓ))| ≥ σ2r(S)/2. In summary, we obtain that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 4∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F σ2r(S) . Now , we further have by triangular inequality and by deﬁniti on of ˜S(ℓ): ∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F = ∥(eℓEℓ,: + (E:,ℓ − Eℓ,ℓeℓ)e⊤ ℓ) ˆQ(ℓ)∥F ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + ∥E:,ℓ − Eℓ,ℓeℓ∥2∥ ˆQ(ℓ)∥2→∞ ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + ∥E∥2∥ ˆQ(ℓ)∥2→∞ ≤ ∥ Eℓ,: ˆQ(ℓ)∥2 + 2∥E∥2∥ ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2→∞ where the last inequality follows under the condition that ∥E∥ ≤ 2σ2r(S). Indeed, we have under such condition that ∥ ˆQ(ℓ)∥2→∞ = ∥ ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2→∞ + ∥ ˆQ(ℓ)(sgn(( ˆQ(ℓ))⊤Q⊤) − ( ˆQ(ℓ))⊤Q)∥2→∞, and by Davis-Kahan’s inequality ∥sgn(( ˆQ(ℓ))⊤Q⊤) − ( ˆQ(ℓ))⊤Q∥ ≤ 2∥E(ℓ)∥2 (σ2r (S))2 ≤ 2∥E∥2 (σ2r (S))2 ≤ 1 2 . Similarly, we also have ∥Eℓ,: ˆQ(ℓ)∥2 ≤ 2∥Eℓ,: ˆQ(ℓ)( ˆQ(ℓ))⊤Q∥2. Hence, we obtain: ∥( ˜S− ˜S(ℓ)) ˆQ(ℓ)∥F ≤ 2∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 2∥E∥(∥ ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥2→∞) Which entails under the condition that ∥E∥ ≤ σ2r(S)/16 that ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 8∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 8∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ 2 After rearranging, we obtain ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F ≤ 16∥Eℓ,: ˆQ(ℓ)WˆQ(ℓ) ∥2 + 16∥E∥∥ ˆQWˆQ∥2→∞ σ2r(S) E.2 Subspace recovery for transition matrices Lemma 32.Let Y ∈ Rn×n be a matrix of independent P oisson entries with Yi,j ∼ 1 TPoisson(TMi,j), and let ˆU, ˆV be the matrices of left and right singular vectors of best r-rank 38approximation of Y. Let gδ be the function deﬁned in (9). Conditioned on the events where ∥Y − M∥ ≤ c1σr(M),gδ(TM) log(ne/δ) ≤ c2Tσr(M), √ ∥M∥∞ log(ne/δ) ≤ c3 √ Tσr(M) for some sufﬁciently small universal constants c1,c2,c3 > 0, we have, with probability at least 1 − δ, max { ∥U− ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } ≲ 1 σr(M) [ µ √ r n (σ1(M) σr(M) ∥Y − M∥ + 1 Tgδ(TM) log (ne δ )) + √ r∥M∥∞ T log (ne δ )] . Proof. The proof follows similar steps as the proof of Theorem 4.2 in [22]. In order to apply the leave-one-out technique, we ﬁrst repeat the symmetric dila tion trick as in Step 1 of proof of Lemma 30. W e deﬁne S = [ 0 M M⊤ 0 ] (35) and note that for matrix M with SVD M = UΣ V⊤, we have: S = 1√ 2 [ U U V −V ][ Σ 0 0 −Σ ] 1√ 2 [ U U V −V ] ⊤ := QDQ⊤. W e deﬁne ˜S as the symmetrized version of matrix Y, and let ˆQ ∈ Rn×2r be the matrix of eigenvectors of the best 2r-rank approximation of ˜S. Note that: ∥Q− ˆQ( ˆQ⊤Q)∥2→∞ = max { ∥U − ˆU( ˆU⊤U)∥2→∞,∥V − ˆV(ˆV⊤V)∥2→∞ } . W e will also repeatedly use the properties ( 31). T o keep the notation simple, deﬁne WˆQ = ˆQ⊤Q. Thus, proving Lemma 32 is equivalent to showing: ∥Q− ˆQWˆQ∥2→∞ ≲ 1 σr(M) [ ∥Q∥2→∞(σ1(M) σr(M)∥ ˜S− S∥ + 1 Tgδ(TM) log (ne δ ) ) + √ r∥M∥∞ T log (ne δ )] with high probability. Deﬁne E = ˜S− S. Now , as in Lemma 33, we have: ∥Q− ˆQWˆQ∥2→∞ ≤ 1 σr(M) (4∥ ˜SQ∥2→∞∥E∥ σr(M) + ∥EQ∥2→∞ + 2∥ ˜S(Q− ˆQWˆQ)∥2→∞ ) (36) under the assumption that ∥E∥ ≤ c1σr(M). Indeed, it is straightforward to show the same bounds as in Lemma 4.14 in [ 22] - note that the boundedness assumption is not used in these l emmas. W e bound the three terms in Equation ( 36) as follows: 1. T o bound the ﬁrst term, we use: ∥ ˜SQ∥2→∞ ≤ ∥ SQ∥2→∞ + ∥EQ∥2→∞ ≤ ∥ Q∥2→∞∥S∥ + ∥EQ∥2→∞ (37) where we ﬁrst used the triangle inequality and then ∥SQ∥2→∞ = ∥QD∥2→∞ ≤ ∥ Q∥2→∞∥S∥. 2. For the second term, according to Lemma 29, we obtain with probability at least 1 − δ: ∥EQ∥2→∞ ≲ 1 T [ ∥Q∥F √ T∥M∥∞ log(ne/δ) + gδ(TM) log(ne/δ)∥Q∥2→∞ ] . (38) Moreover, we will use ∥Q∥F ≤ √ 2rand ∥Q∥2→∞ ≤ µ√ r n, which follow from the low-rank and incoherence assumptions. 3. Finally, regarding the last term in Equation ( 36), we split it using the triangle inequality as follows: ∥ ˜S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ S(Q− ˆQWˆQ)∥2→∞ + ∥E(Q− ˆQWˆQ)∥2→∞, and from Step 3 of proof of Theorem 4.2 in [ 22] we have: ∥S(Q− ˆQWˆQ)∥2→∞ ≤ ∥ Q∥2→∞∥S∥∥Q⊤(Q− ˆQWˆQ)∥ ≲ ∥Q∥2→∞∥S∥ ∥E∥2 σ2r(M), (39) 39where we used ∥Q⊤(Q− ˆQWˆQ)∥ = ∥ sin Θ( Q, ˆQ)∥2. The remaining of the proof consists in bounding ∥E(Q− ˆQWˆQ)∥2→∞ = max ℓ=1,...,n∥Eℓ,:(Q − ˆQWˆQ)∥. First note that the matrix Q− ˆQWˆQ depends on E and thus we cannot apply Lemma 29 immediately. Instead, we will use the leave-one-out method, and deﬁne a matrix ˜S(ℓ) ∈ Rn×n as follows: ˜S(ℓ) i,j = { ˜Si,j, if i̸= ℓor j ̸= ℓ Si,j, otherwise Then deﬁne ˆQ(ℓ) ∈ Rn×2r as a matrix of eigenvectors corresponding to 2r greatest (in absolute value) eigenvalues of matrix ˜S(ℓ). Deﬁne WˆQ(ℓ) accordingly. Then we have: ∥E(Q− ˆQWˆQ)∥2→∞ ≤ 2 max 1≤ℓ≤n { ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥,∥Eℓ,:( ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ)∥ } . (40) (3a) Since Eℓ,: is statistically independent of Q− ˆQ(ℓ)WˆQ(ℓ) , the ﬁrst term from ( 40) can be bounded according to Lemma 29 for any 1 ≤ ℓ≤ nas follows: ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ ≲ 1 T [ ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥F √ T∥M∥∞ log (ne δ ) + gδ(TM) log (ne δ ) ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ ] with probability at least 1 − δ. After applying the triangle inequality to the second term a nd using ∥ · ∥ 2→∞ ≤ ∥ · ∥ F, we get: ∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ ≤ ∥ Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F. (41) Thus, combining the last two inequalities yields: ∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ ≲ ∥Q− ˆQWˆQ∥F √ T−1∥M∥∞ log (ne δ ) + T−1gδ(TM) log (ne δ ) ∥Q− ˆQWˆQ∥2→∞ + ∥ ˆQWˆQ − ˆQ(ℓ)WˆQ(ℓ) ∥F (√ T−1∥M∥∞ log (ne δ ) + T−1gδ(TM) log (ne δ )) (42) for all 1 ≤ ℓ≤ n. (3b) The second term from ( 40) can be bounded very roughly as follows: ∥Eℓ,:( ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ))∥ ≤ ∥ E∥∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F. (43) Similar to the Step 2.2 in the proof of Theorem 4.2 in [ 22], we have: ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F ≤ 16 σr(M)(∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ + ∥EQ∥2→∞ + ∥E∥∥Q− ˆQ(ℓ)WˆQ(ℓ) ∥2→∞ + ∥E∥∥Q∥2→∞). Applying again the inequality ( 41) and moving the term ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F to the left side of inequality, we get under assumption ∥E∥ ≤ c1σr(M) that: ∥ ˆQ(ℓ)WˆQ(ℓ) − ˆQWˆQ∥F ≲ 1 σr(M)(∥Eℓ,:(Q− ˆQ(ℓ)WˆQ(ℓ) )∥ + ∥EQ∥2→∞ + ∥E∥∥Q− ˆQWˆQ∥2→∞ + ∥E∥∥Q∥2→∞). (44) After substitution of the results from ( 37), ( 38), ( 39), ( 40), ( 42), ( 43) and ( 44) into Equation ( 36) and using assumptions stated in the lemma, we obtain the stat ement of the lemma. 40E.3 Error decomposition in the two-to-inﬁnity norm Below , we present a decomposition for the error of subspace recovery in the norm ∥ · ∥ 2→∞. W e borrow this result from [ 22] and provide its proof for completeness. Lemma 33 (Lemma 4.16 in [ 22]). Let S,˜S ∈ Rn×n be symmetric matrices and assume that Sand ˜Sare of rank r. Let Q, ˆQ∈ O n×r be the corresponding rsingular vectors of Sand ˜S, respectively. Denote E = ˜S− S. Under the condition ∥E∥ ≤ σr(S)/2, we have: ∥Q− ˆQˆQ⊤Q∥2→∞ ≤ 4∥ ˜SQ∥2→∞∥E∥ (σr(S))2 + ∥EQ∥2→∞ σr(S) + 2∥ ˜S(Q− ˆQˆQ⊤Q)∥2→∞ σr(S) Proof of Lemma 33. Since Sis a symmetric matrix of rank r, by SVD we write S = QΣ Q⊤, where the matrix Σ = diag(σ1(S),...,σ r(S)). For ease of notations, let us further denote W = ˆQQ⊤. W e have ∥Q− ˆQˆQ⊤Q∥2→∞ = ∥SQΣ −1 − ˆQW∥2→∞ ≤ ∥ ˜SQΣ −1 − ˆQW∥2→∞ + ∥EQΣ −1∥2→∞ ≤ ∥ ˜SQ− ˆQWΣ ∥2→∞ σr(S) + ∥EQ∥2→∞ σr(S) Now , we focus on the term ∥ ˜SQ− ˆQWΣ ∥2→∞. T o that end, we ﬁrst establish the identity ˆQWΣ = ˆQˆQ⊤QΣ = ˆQˆQ⊤SQ = ˆQˆQ⊤ ˆSQ+ ˆQˆQ⊤EQ = ˆQˆΣ ˆQ⊤Q+ ˆQˆQ⊤EQ = ˜SˆQˆQ⊤Q+ ˆQˆQ⊤EQ where we use the identities SQ = QΣ , ˆQ⊤ ˜S = ˆΣ ˆQ⊤, and ˆQˆΣ ˆQ⊤ = ˜SˆQˆQ⊤. Then, we observe that ∥ ˜SQ− ˆQWΣ ∥2→∞ = ∥ ˜SQ− ˜SˆQˆQ⊤Q+ ˆQˆQ⊤EQ∥2→∞ ≤ ∥ ˜S(Q− ˆQˆQ⊤Q)∥2→∞ + ∥ ˆQˆQ⊤EQ∥2→∞ Next, we note that when ∥E∥ ≤ σr(S)/2, we have ∥ ˆQˆQ⊤EQ∥2→∞ = ∥ ˜SˆQˆΣ −1 ˆQ⊤EQ∥2→∞ ≤ ∥ ˜SˆQ∥2→∞∥ˆΣ −1∥∥ ˆQ⊤∥∥E∥∥Q∥ ≤ ∥ ˜SˆQsgn( ˆQ⊤Q)∥2→∞∥E∥ σr( ˜S) . At this point, we try to bound ∥ ˜SˆQ∥2→∞ and σr( ˜S), under the condition ∥E∥ ≤ σr(S)/2. First, we can easily see by W eyl’s inequality we have |σr( ˜S) − σr(S)| ≤ ∥ E∥, which entails under the assumed condition that σr( ˜S) ≥ σr(S)/2. Next, we observe: ∥ ˜SˆQ∥2→∞ = ∥ ˜SˆQsgn( ˆQ⊤Q)∥2→∞ ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + ∥ ˜SˆQ∥2→∞∥sgn( ˆQ⊤Q) − ˆQ⊤Q∥ ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + 2∥ ˜SˆQ∥2→∞∥E∥2 σr(S)2 ≤ ∥ ˜SˆQˆQ⊤Q∥2→∞ + ∥ ˜SˆQ∥2→∞ 2 41where we used the Davis-Kahan’s inequality and properties o f the sgn(·), to upper bound ∥sgn( ˆQ⊤Q) − ˆQ⊤Q∥ ≤ ∥ sin( ˆQ,Q)∥2 ≤ 2∥E∥2/(σr(S))2. Thus, leading to: ∥ ˜SˆQ∥2→∞ ≤ 2∥ ˜SˆQˆQ⊤Q∥2→∞ Moving forward we obtain: ∥ ˆQˆQ⊤EQ∥2→∞ ≤ 4∥ ˜SˆQˆQ⊤Q∥2→∞∥E∥ σr(S) ≤ 4 ( ∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ + ∥ ˜SQ∥2→∞ ) ∥E∥ σr(S) ≤ 2∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ + 4 ( ∥ ˜SQ∥2→∞ ) ∥E∥ σr(S) Now , putting everything together we conclude that: ∥Q− ˆQˆQ⊤Q∥2→∞ ≤ 2∥ ˜S( ˆQˆQ⊤Q− Q)∥2→∞ σr(S) + 4∥ ˜SQ∥2→∞∥E∥ (σr(S))2 + ∥EQ∥2→∞ σr(S) 42F Row-wise and entry-wise matrix estimation errors In this appendix, we provide a series of results about quantifying the matrix estimation error using different norms. It is important to note that all these resul ts require a control of the error in the two- to-inﬁnity norm, which in turn requires a subspace recovery guarantee in the two-to-inﬁnity norm. Lemmas 35 and 37 are speciﬁc to our analysis for the estimation of the transit ion matrices. Lemmas 34 and 36 are common to the analysis of both the estimation of reward ma trices and transition matrices. The results presented in this appendix are used in the proofs of the main results, presented in Appendix B. F .1 Bounding ∥M− ˆM∥2→∞ Lemma 34. Let M, ˆM be as in § 2. Assume that there exists a sufﬁciently small universal con stant c1 >0 such that ∥M− ˜M∥ ≤ c1σr(M). Then, there exists a universal constant c2 >0 such that ∥ ˆM− M∥2→∞ ≤ c2σ1(M) [ ∥U − ˆU( ˆU⊤U)∥2→∞ + ∥U∥2→∞ ∥ ˜M − M∥ σr(M) ] . Proof. W e start by using deﬁnition of ˆM as a projection of matrix ˜M, and then use the triangle inequality and the inequality ( 3) to obtain: ∥ ˆM − M∥2→∞ = ∥Π ˆU ˜M − Π UM∥2→∞ = ∥(Π ˆU − Π U)(˜M − M) + Π U(˜M − M) + (Π ˆU − Π U)M∥2→∞ ≤ ∥ (Π ˆU − Π U)(˜M − M)∥2→∞ + ∥Π U(˜M − M)∥2→∞ + ∥(Π ˆU − Π U)M∥2→∞ ≤ ∥ Π ˆU − Π U∥2→∞(∥ ˜M − M∥ + ∥M∥) + ∥Π U∥2→∞∥ ˜M − M∥. (45) Moreover, we note that ∥Π U∥2→∞ = ∥U∥2→∞ (refer to Proposition 6.6 in [ 15]). In the remaining of the proof, we upper bound ∥Π ˆU − Π U∥2→∞ from ( 45). For any orthogonal matrix R ∈ O r×r, we have ∥Π ˆU − Π U∥2→∞ = ∥ ˆUˆU⊤ − UU⊤∥2→∞ = ∥ ˆURR⊤ ˆU⊤ − UR⊤ ˆU⊤ + UR⊤ ˆU⊤ − UU⊤∥2→∞ ≤ ∥ ˆURR⊤ ˆU⊤ − UR⊤ ˆU⊤∥2→∞ + ∥UR⊤ ˆU⊤ − UU⊤∥2→∞ ≤ ∥ ˆUR − U∥2→∞∥R⊤ ˆU⊤∥ + ∥U∥2→∞∥R⊤ ˆU⊤ − U⊤∥ ≤ ∥ U − ˆUR∥2→∞ + ∥U∥2→∞∥U − ˆUR∥. (46) Recall the deﬁnition of sgn function given in the notation presented in § 1 and choose the matrix R as R = sgn( ˆU⊤U). For this choice of Rwe have according to Davis-Kahan’s theorem (Corollary 2.8 in [ 22]): ∥U − ˆUR∥ ≤ √ 2∥ sin Θ( ˆU,U )∥ ≤ 2∥M− ˜M∥ σr(M) . (47) Deﬁne the matrix WˆU = ˆU⊤U. W e use the facts that ∥U − ˆUR∥2→∞ ≤ ∥ U − ˆUWˆU∥2→∞ + ∥ ˆU∥2→∞∥WˆU − R∥ (48) and that WˆU is very close to Raccording to the proof of Lemma 4.15 in [ 22] to show: ∥WˆU − R∥ = ∥ ˆU⊤U − sgn( ˆU⊤U)∥ = ∥ sin Θ( ˆU,U )∥2 ≤ 2∥M− ˜M∥2 σ2r(F) . (49) W e also have σi(R) = 1 for i ∈ [r] and according to W eyl’s inequality σmin(WˆU) ≥ σmin(R) − ∥WˆU− R∥ = 1 − ∥WˆU− R∥. Combining these results under assumption ∥M− ˜M∥ <σr(M)/ √ 2 we obtain: ∥W−1 ˆU ∥ = 1 σmin(WˆU) ≤ 1 1 − ∥WˆU − R∥ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) . 43Thus: ∥ ˆU∥2→∞ ≤ ∥ ˆUWˆU∥2→∞∥W−1 ˆU ∥ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) (∥U∥2→∞ + ∥U − ˆUWˆU∥2→∞). (50) Combining Equations ( 48), ( 49), ( 50) we get: ∥U − ˆUR∥2→∞ ≤ 1 1 − 2∥M−˜M∥2 σ2r (M) (∥U − ˆUWˆU∥2→∞ + 2∥M− ˜M∥2 σ2r(M) ∥U∥2→∞) and combining the last equality with ( 46) and ( 47), we have ∥Π ˆU − Π U∥2→∞ ≤ ∥U − ˆUWˆU∥2→∞ 1 − 2∥M−˜M∥2 σ2r (M) +   2∥M−˜M∥2 σ2r (M) 1 − 2∥M−˜M∥2 σ2r (M) + 2∥M− ˜M∥ σr(M)  ∥U∥2→∞. (51) Finally, substituting the obtained bound into Equation ( 45) and using assumption ∥M − ˜M∥ ≤ c1σr(M) for simpliﬁcation, we obtain the statement of the lemma. F .2 Bounding ∥P − ˆP∥1→∞ Lemma 35. Let P, ˆP be as in Model II in § 2. W e have: ∥ ˆP − P∥1→∞ ≤ 2 √n∥ ˆM − M∥2→∞ minj∈[n] ∥Mj,:∥1 . Proof. Starting with the deﬁnition of ˆP, we get: ∥ ˆP − P∥1→∞ = max i∈[n] ∥ ˆPi,: − Pi,:∥1 = max i∈[n]      (ˆMi,:)+ ∥(ˆMi,:)+∥1 − Mi,: ∥Mi,:∥1      1 ≤ 2 max i∈[n] ∥ ˆMi,: − Mi,:∥1 ∥Mi,:∥1 ≤ 2 √nmaxi∈[n] ∥ ˆMi,: − Mi,:∥ minj∈[n] ∥Mj,:∥1 , where the ﬁrst inequality follows from Lemma 2 in [ 26] and the second by equivalence of norms. Moreover, note that the above inequality holds even in the ca se when ∥(ˆMi,:)+∥1 = 0 (and thus ˆPi,: = 1 n1n), but the bound is vacuous in this case. F .3 Bounding ∥M− ˆM∥∞ Lemma 36. Let M, ˆM be as in § 2. Assume that there exists a sufﬁciently small universal con stant c1 >0 such that ∥M− ˜M∥ ≤ c1σr(M). Then, there exists a universal constant c2 >0 such that ∥ ˆM− M∥∞ ≤ c2∥M∥2→∞ ( ∥M − ˜M∥ σr(M) ∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞ ) + c2∥M − ˆM∥2→∞(∥V∥2→∞ + ∥V − ˆVWˆV∥2→∞). Proof. Similarly to the decomposition leading to Equation ( 45), we can upper bound the inﬁnity norm error easily from the following decomposition: ∥ ˆM − M∥∞ = ∥ ˆMΠ ˆV − MΠ V∥∞ ≤ ∥ ˆM∥2→∞∥Π ˆV − Π V∥2→∞ + ∥ ˆM− M∥2→∞∥Π V∥2→∞ ≤ (∥ ˆM − M∥2→∞ + ∥M∥2→∞)∥Π ˆV − Π V∥2→∞ + ∥ ˆM− M∥2→∞∥V∥2→∞, 44where we used the inequality ( 4) together with the triangle inequalities and the fact that p rojection matrices are symmetric. T o bound ∥Π ˆV − Π V∥2→∞, we use the same approach as that used in ( 51) (just replacing U by V), and we obtain: ∥Π ˆV − Π V∥2→∞ ≤ ∥V − ˆVWˆV∥2→∞ 1 − 2∥M−˜M∥2 σ2r (M) +   2∥M−˜M∥2 σ2r (M) 1 − 2∥M−˜M∥2 σ2r (M) + 2∥M− ˜M∥ σr(M)  ∥V∥2→∞. F .4 Bounding ∥P − ˆP∥∞ Lemma 37. Let P, ˆP be as in Model II in § 2. Assume that D = min i∈[n] ∥Mi,:∥1 > 0. If ∥ ˆM − M∥1→∞ ≤ 1 2 D, then ∥ ˆP − P∥∞ ≤ 2 ∥ ˆM− M∥∞ D + 2 √n∥M∥∞ D2 ∥ ˆM− M∥2→∞. Proof. First note that for any i∈ [n]: ⏐ ⏐ ⏐∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ⏐ ⏐ ⏐≤ ∥ (ˆMi,:)+ − Mi,:∥1 ≤ ∥ ˆMi,: − Mi,:∥1 ≤ ∥Mi,:∥1 2 , (52) where the ﬁrst inequality follows from the reverse triangle inequality, the second from | max(0,x)− y| ≤ | x− y| for all y >0 and x ∈ R, and the last inequality follows from the assumption in the lemma. This implies that ∥(ˆMi,:)+∥1 >0 for all i∈ [n], which further implies that ˆP is deﬁned by: for all i∈ [n], ˆPi,: = ( ˆMi,:)+/∥(ˆMi,:)+∥1. (53) Furthermore, we have for all i,j = 1 ,...,n , | ˆPi,j − Pi,j| = ⏐ ⏐ ⏐ ⏐ ⏐ (ˆMi,j)+ ∥(ˆMi,:)+∥1 − Mi,j ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐≤ ⏐ ⏐ ⏐ ⏐ ⏐ ˆMi,j ∥(ˆMi,:)+∥1 − Mi,j ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ⏐ ⏐ ⏐ ⏐ ⏐ 1 ∥(ˆMi,:)+∥1 − 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ = 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ 1 1 + ∥(ˆMi,:)+∥1−∥Mi,:∥1 ∥Mi,: ∥1 − 1 ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ = 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |ˆMi,j| ∥Mi,:∥1 ϕ ( ∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ∥Mi,:∥1 ) where we deﬁne ϕ(x) = |x/(1 + x)| for all x∈ R\\{−1}. Note that if |x| <1/2, then ϕ(x) ≤ 2|x|, which combined with ( 52) gives | ˆPi,j − Pi,j| ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ 2|ˆMi,j| ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ∥(ˆMi,:)+∥1 − ∥Mi,:∥1 ∥Mi,:∥1 ⏐ ⏐ ⏐ ⏐ ⏐ ≤ 1 ∥Mi,:∥1 ⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ 2 ∥Mi,:∥2 1 (⏐ ⏐ ⏐ˆMi,j − Mi,j ⏐ ⏐ ⏐+ |Mi,j| ) ∥ ˆMi,: − Mi,:∥1. Using the assumption ∥ ˆM − M∥1→∞ ≤ 1 2 mini∈[n] ∥Mi,:∥1 again, we can group ﬁrst two terms, and then use ∥ ˆM− M∥1→∞ ≤ √n∥ ˆM− M∥2→∞ to get the statement of the lemma. 45G Low-rank bandits: proofs of results from Section 4 G.1 Gap-dependent guarantees Proof of Theorem 7. First, we prove the result corresponding the best entry iden tiﬁcation problem. W e proceed in several steps. Step 1: entry-wise concentration. W e can easily verify that for all ℓ≥ 1, for all (i,j) ∈ [m] × [n], we have | ˆ∆ (ℓ) i,j − ∆ i,j| ≤ 2∥ ˆM(ℓ) − M⋆∥∞. Therefore, applying Theorem 1, we have, for δ >0, and Tℓ ≥ c1(m+n) log3((e2(m+n)(mn)/δℓ), P ( | ˆ∆ i,j − ∆ i,j| >2C1 √ e(m+ n) Tℓ log3 (e(m+ n)mnTℓ δℓ )) ≤ δℓ mn for some c1,C1 > 0 sufﬁciently large. In particular, we can choose C1 = C(µ11/2κ2r1/2 + µ3κr3/2(m + n)/√mn), and c1 = cµ4κ2r2, but under a homogeneous reward matrix these constants are Θ(1) . Thus, by a union bound and always under the same conditions, we have P ( max (i,j)∈[m]×[n] | ˆ∆ i,j − ∆ i,j| >2C1 √ e(m+ n) Tℓ log3 (e(m+ n)mnTℓ δℓ )) ≤ δℓ. Next, we wish to choose Tℓ so that we have P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ) ≥ 1 − δℓ. (54) Note that in order for the above guarantee to hold, it is sufﬁc ient to have: Tℓ ≥ c1(m+ n) log3 (e2(m+ n)(mn) δℓ ) , Tℓ ≥ 2√eC2 1 (m+ n)22(ℓ−2) log3 (e(m+ n)(mn) δℓ ) . This can be achieved if we choose Tℓ = ⌈ C322(ℓ−2)(m+ n) log3 (22(ℓ−2)(m+ n) δℓ )⌉ , (55) for some positive constant C3 >0 large enough which can be determined explicitly and only dep end on c1,C1. Indeed, this can be deduced from the basic fact that if T1/3 ℓ ≥ 2alog(2a) + 2 b, then T1/3 ℓ ≥ alog(T1/3 ℓ ) + b. W e spare the reader these tedious calculations and only arg ue that such C3 exists and can be computed explicitly. Step 2: Good events.W e deﬁne Sℓ = { (i,j) ∈ [n] × [m] : ∆ i,j ≤ 2−ℓ} and the good events under which we correctly ﬁnd the best entry as Eℓ = {Aℓ+1 ⊆ Sℓ+1} ∩ { (i⋆,j⋆) ∈ A ℓ+1}. W e show that the good event Eℓ happens with high probability conditionally on E1,..., Eℓ−1. Observe that by independence of the entries sampled at epoch ℓfrom those of the previous epochs, we have based on ( 54) P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ Now , conditionally on Eℓ−1,..., E1, under the event that maxi,j| ˆ∆ i,j− ∆ i,j| ≤ 2−(ℓ+2), if (i,j) ∈ Sc ℓ+1 ∩ Aℓ+1 then ˆ∆ (ℓ) i,j ≥ ∆ i,j − 2−(ℓ+2) >2−(ℓ+1) − 2−(ℓ+2) = 2 −(ℓ+2). 46Thus, we have P ( Aℓ+1 ⊆ Sℓ+1 ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ. Furthermore, note that under the event maxi,j| ˆ∆ i,j−∆ i,j| ≤ 2−(ℓ+2), we clearly have that ˆ∆ i⋆,j⋆ ≤ 2−(ℓ+2) and since (i⋆,j⋆) ∈ A ℓ conditionally on Eℓ−1 we conclude that P ( Eℓ ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ P ( max i,j | ˆ∆ i,j − ∆ i,j| ≤ 2−(ℓ+2) ⏐ ⏐ ⏐Eℓ−1,..., E1 ) ≥ 1 − δℓ Step 3: Sample complexity . First, we remark that when ℓ = ⌈log2(1/∆ min)⌉, we have Sℓ = {(i⋆,j⋆)}. Therefore, under the event E1 ∩ · · · ∩ E ⌈log2(1/∆ min)⌉ the algorithm will stop after τ rounds, and recommend the optimal (i⋆,j⋆), where τ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 Tℓ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 ⌈ C322(ℓ−2)(m+ n) log3 (22(ℓ−2)(m+ n) δℓ )⌉ ≤ ⌈log2(1/∆ min)⌉∑ ℓ=1 ⌈ C3 (m+ n) ∆ 2 min log3 ((m+ n)⌈log2 (1/∆ min)⌉2 ∆ 2 minδ )⌉ ≤ log2 ( 1 ∆ min )⌈ C3 (m+ n) ∆ 2 min log3 ((m+ n)⌈log2 (1/∆ min)⌉2 ∆ 2 minδ )⌉ ≤ ψ(n,m,δ ) := C4 (m+ n) log (e/∆ min) ∆ 2 min log3 (e(m+ n) log (e/∆ min) ∆ minδ ) where we recall the deﬁnition of Tℓ in ( 55), that δℓ = δ/ℓ2, and where C4 is a large enough universal constant. Hence, we have P ((iτ,jτ) = ( i⋆,j⋆),τ ≤ ψ(n,m,δ )) ≥ P   ⌈log2(1/∆ min)⌉⋂ ℓ=1 Eℓ  ≥ 1 − δ. (56) This conclude the proof of the guarantee for the best entry id entiﬁcation. Note that we can immediately conclude from the above guarantee ( 56) that the sample complexity of SME-AE (1/Tα) for all T ≥ 1, satisﬁes E[τ ∧ T] ≤ ψ(n,m,T −α) + T1−α. Indeed, we have E[τ ∧ T] = E[(τ ∧ T)1{τ≤ψ(n,m,T−α)}] + E[(τ ∧ T)1{τ>ψ(n,m,T−α)}] ≤ ψ(n,m,T −α) + TP(τ >ψ(n,m,T −α)) ≤ ψ(n,m,T −α) + T1−α, where the upper bound on the probability follows from ( 56) with δ= 1 /Tα. 47Next, we turn our attention to proving the regret upper bound . W e deﬁne Egood = {(ˆıτ,ˆτ) = (i⋆,j⋆),τ ≤ ψ(n,m, 1/T2)}. W e have Rπ(T) = TMi⋆,j⋆ − E [ T∑ t=1 Miπ t,jπ t ] = E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{Egood } ] + E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{Ec good } ] ≤ E [ T∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{τ≤ψ(n,m,T−2)} ] + ∆ maxTP(Ec good) ≤ E [ ∞∑ t=1 (Mi⋆,j⋆ − Miπ t,jπ t )1{τ∧ψ(n,m,T−2)>t} ] + ∆ max T ≤ E [ ∞∑ t=1 ¯∆ 1{τ∧ψ(n,m,T−2)>t} ] + ∆ max T ≤ ¯∆ ψ(n,m,T −2) + ∆ max T where in the second to last inequality, we used the tower rule together with the observation that E[(Mi⋆,j⋆ −Miπ t,jπ t )1{τ∧ψ(n,m,T−2)>t}|Ft−1] = ¯∆ 1{τ∧ψ(n,m,T−2)>t} where Ft−1 is the σ-algebra deﬁned by the observations up to time t− 1. This concludes the proof.G.2 Gap-independent guarantees An immediate consequence of the regret bound in Theorem 7 is that we can have a gap-independent bound under some additional assumption. Let us deﬁne ζ = ∆ max/∆ min, then the regret bound becomes Rπ(T) ≤ ζC4(m+ n) log (e/∆ min) ∆ min log3 (e(m+ n) log (e/∆ min) T2 ∆ min ) + ∆ max T . (57) At the same time, we also have the worst case bound Rπ(T) ≤ ζ∆ minT. (58) T aking the best of the two bounds ( 57) and ( 58) with the worst case choice for ∆ min, we get Rπ(T) = ˜O ( ζ √ (n+ m)Tlog2((n+ m)T) ) where the ˜Ohides additional log-log terms in m,n and T. 48H Related work In this section, we ﬁrst discuss the results for the estimation of a low-rank transition matrix presented in [ 26]. W e then give a more detailed account of the related work for low-rank bandits. H.1 Low-rank transition matrix estimation In [ 26], the authors try to estimate a low-rank transition matrix f rom the data consisting of a single trajectory of the corresponding Markov chain. In a sense, th is objective is similar to ours in Model II(b). The main results of [ 26] are presented in Theorem 1. First observe that our results a re more precise since we manage to get entry-wise guarantees. Then i t is also worth noting that, in the case of homogenous transition matrices, the upper bound on ∥ ˆP − P∥1→∞ stated in Theorem 1 in [ 26] are similar to the upper bounds we establish in Corollary 6. However, to obtain such bounds, we believe that it is necessary to ﬁrst derive guarantees for th e singular subspace recovery in the ℓ2→∞ norm, as we do. The authors of [ 26] do not present any step with such guarantees for the estimat ion of the singular subspaces. W e explain below why this step is m issing and where the analysis towards the upper bound ∥ ˆP − P∥1→∞ breaks in [ 26]. Proof of the guarantees for ∥ ˆP − P∥1→∞ in [ 26]. Note that in [ 26], the authors use F in lieu of M. W e keep our notation M below to be consistent with the rest of the manuscript. In the proof of Theorem 1 in [ 26], the authors use the following decomposition: ∥ ˆMi,: − Mi,:∥ ≤ ∥ (ˆMi,: − Mi,:)V∥ + (∥ ˆMi,: − Mi,:∥ + ∥Mi,:∥)C∥ ˜M− M∥ σr(M) . (59) They apply concentration results on ∥(˜M − M)V∥2→∞ (Lemma 8) and ∥M − ˜M∥ (Lemma 7) to bound the two terms from above. More precisely, their proof i ncludes (33) page 3217, a sequence of inequalities where these concentration results are used . In the ﬁfth line of (33), the authors apply (31), the concentration result on ∥(˜M − M)V∥2→∞, but to bound ∥(ˆM − M)V∥2→∞ instead. Replacing ˆM by ˜M is however not possible, and the analysis breaks here. Is there a simple solution? W e argue below that it is not easy to solve the aforementioned issue in the proof. W e ﬁrst claim that the two concentration bounds on ∥(˜M − M)V∥2→∞ and ∥M − ˜M∥ are not sufﬁcient for bounding the ﬁrst term from Equation ( 59). Speciﬁcally, for any row i: ∥(ˆMi,: − Mi,:)V∥ = ∥(˜Mi,: ˆV ˆV⊤ − Mi,:)V∥ = ∥(˜Mi,: − Mi,:)V + ˜Mi,:(ˆV ˆV⊤ − VV ⊤)V∥, and in order to analyze the second term inside the norm, we nee d to deal with dependence between ˜M and ˆV. Doing this naively using the triangle inequality and Cauch y-Schwarz inequality yields: ∥(ˆM − M)V∥2→∞ ≤ ∥ (˜M − M)V∥2→∞ + ∥ ˜M(ˆV ˆV⊤ − VV ⊤)V∥2→∞ ≤ ∥ (˜M − M)V∥2→∞ + ∥ ˜M∥1→∞∥V − ˆV(ˆV⊤V)∥2→∞. (60) It is not clear how bounds on ∥(˜M−M)V∥2→∞ and ∥M− ˜M∥ imply a bound on ∥(ˆM−M)V∥2→∞ since term ∥V − ˆV(ˆV⊤V)∥2→∞ does not seem to be directly bounded by these two terms. W e can think of bounding ∥V − ˆV(ˆV⊤V)∥2 using Davis-Kahan’s inequality: ∥V − ˆV(ˆV⊤V)∥2→∞ ≤ ∥ V − ˆV(ˆV⊤V)∥2 ≲ ∥M − ˜M∥ σr(M) , where we neglect the higher order term (see Equations ( 47),(48),(49)). Then, with the upper bound on ∥M− ˜M∥, we may obtain an upper bound on ∥ ˆP−P∥1→∞ but that does not have a fast decaying rate as that claimed in Theorem 1 in [ 26] or in our main theorems. It is also worth noting that assuming proof of Theorem 1 in [ 26] holds or that more speciﬁcally, the series of inequalities leading to Equation (33) holds, o ne could greatly simplify the singular subspace recovery problem. In particular, since ∥ ˜M(V − ˆV ˆV⊤V)∥2→∞ = ∥(˜M− ˆM)V∥2→∞ ≤ ∥ (ˆM − M)V∥2→∞ + ∥EV∥2→∞ 49we can rewrite Equation ( 36) (wlog for symmetric matrix M with eigenvector matrix V) as: ∥V − ˆV(ˆV⊤V)∥2→∞ ≤ 1 σr(M) ( (2 + 4∥E∥ σr(M) )∥EV∥2→∞ + 2∥(ˆM− M)V∥2→∞ + 4∥MV∥2→∞∥E∥ σr(M) ) . (61) Now if (33) in [ 26] was true, we could use the correspoding bound of the critica l term ∥(ˆM − M)V∥2→∞. This would not only greatly simplify proofs given in litera ture based on leave-one- out-technique, but also extend their work to Markov depende nt random variables (which has not been done before). Lastly, note that we cannot skip estimati on of singular subspaces by combining Equation ( 60) and ( 61) since inequality 2∥ ˜M∥1→∞ <σr(M) does not hold in general. H.2 Low rank bandits Here we survey models for low-rank bandits that have emergedrecently in the literature but that are not directly related to our model. Nonetheless our guarante es can be exported there. [ 66] considers a bi-linear bandit model which seems more genera l than that of considered [ 6]. Indeed, they assume that the observed reward in round t after selecting a pair (x,z) ∈ X × Z , is x⊤Θ z+ ξt where X ⊂ Rm and Z ⊂ Rn are ﬁnite. They assume that Θ ∈ Rm×n is low rank. If we assume that X = {e1,...,e m} and Z = {e1,...,e n}, we then recover our model and that of [ 6] with M = Θ . However, we can also argue that if we restrict our attention to mvectors from X , say X ′ = {x1,...,x m}, that span Rm, and n vectors from Z, say Z′ = {z1,...,z n}, that span Rn, then in our model and that of [ 6], Mi,j = x⊤ iΘ zj, for all (i,j) ∈ [m] × [n]. Note that in this case, the rank of M is equal to that of Θ . In fact, in the ﬁrst phase of the algorithm proposed by [ 66], the authors also restrict their attention to sets X ′ and Z′ such that λmin(∑ m i=1 xix⊤ i) and λmin(∑ m i=1 ziz⊤ i ) are maximized. T o simplify our exposition, we do not use the m odel presented by [ 66], instead we use that of [ 6]. [46] considers a generalized bandit framework with low rank str ucture which is rather different than the bandit framework we consider. There, the algorithm is ba sed on the two stage idea introduced by [66], which consists in ﬁrst estimating the subspace, then redu cing the problem to a low-dimensional linear bandit with ambient dimension nmbut with roughly n+ mrelevant dimensions. They are able obtain a minimax regret scaling as (n+ m) √ T. It is worth noting that both these works do not have gap-dependent bounds. [ 7] is another relevant work. There, the authors consider a low -rank bandit problem similar to ours but slightly more restrictive. At time t, they recommend an arm ρ(j)t for each user j, and they observe the corresponds rewards. In other words they observ e mentries per round, while in our case we only observe one entry per round. They show that with an exp lore-then-commit algorithm, they attain a regret of order polylog (n+ m)T2/3. Their regret guarantees require an entry-wise matrix estimation guarantee with scaling comparable to ours. They use the result of Chen et al. [ 67] which again is only valid for independent entries and does not acco unt for repetitive sampling. T o remedy that they rely on ad-hoc pre-processing steps (see remarks 2 , 3 and 4 in [ 7]). In our case, we believe that our matrix estimation guarantees can be immediately us ed in their setting and this would lead to a regret scaling of order (n+ m)1/3T2/3 with the more reasonable constraint that we only observe one entry at each round. The authors also obtain an polylog (n+ m) √ T guarantee but for rank-1 reward matrices only. 50",
      "meta_data": {
        "arxiv_id": "2310.06793v2",
        "authors": [
          "Stefan Stojanovic",
          "Yassir Jedra",
          "Alexandre Proutiere"
        ],
        "published_date": "2023-10-10T17:06:41Z",
        "pdf_url": "https://arxiv.org/pdf/2310.06793v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses matrix estimation problems in low-rank reinforcement learning (RL) and bandits, focusing on recovering expected arm rewards or transition kernels with low entry-wise error, even with correlated data. It introduces simple spectral-based matrix estimation approaches that efficiently recover singular subspaces and achieve nearly-minimal entry-wise error. Key contributions include: 1) Proposing three matrix estimation problems (low-rank bandits, low-rank MDPs with generative model, low-rank MDPs with forward model) and providing strong performance guarantees for spectral methods, utilizing leave-one-out arguments and Poisson approximation techniques for correlated data. 2) Developing SME-AE, an efficient regret-minimization algorithm for low-rank bandits with state-of-the-art regret guarantees scaling as O((m+n)log^3(T) * \bar{\\Delta} / \\Delta_min^2). 3) Presenting a minimax-optimal sample complexity algorithm for best policy identification in reward-free low-rank MDPs, achieving O(nA/\\epsilon^2) up to logarithmic factors.",
        "methodology": "The core methodology involves simple spectral decomposition. For matrix estimation, an empirical matrix \\tilde{M} is constructed from data, and the estimate \\hat{M} is obtained by taking its best rank-r approximation. \\hat{P} (for transition matrices) is derived by normalizing rows of \\hat{M}. To handle correlated data (especially in MDPs), the proofs develop and combine leave-one-out arguments and Poisson approximation techniques. The proof strategy involves: 1) Multinomial approximation of Markovian data, 2) Approximation towards Poisson random matrices with independent entries, and 3) Applying a leave-one-out analysis (including a dilation trick for symmetry) to decouple statistical dependencies. For low-rank bandits, the SME-AE algorithm employs an iterative process of sampling, matrix estimation via spectral decomposition, and arm elimination based on estimated reward gaps. For reward-free RL in low-rank MDPs, the approach has two phases: model estimation using spectral decomposition from trajectories to build estimates of transition matrices, followed by a planning phase to compute the optimal policy.",
        "experimental_setup": "The research paper is primarily theoretical, focusing on mathematical proofs and performance guarantees rather than empirical experiments. The 'experimental setup' refers to the data generation models for which the theoretical guarantees are derived: 1) Low-Rank Bandits (Model I): Arm (i,j) is sampled (e.g., uniformly), and a noisy reward M_i,j + \\xi_t is observed, where \\xi_t are zero-mean and bounded. 2) Low-Rank MDPs (Model IIa, Generative Model): For estimating transition matrix P (or frequency matrix M = diag(\\nu)P), data consists of T pairs (x_t, x_{t+1}) where x_t is drawn from a distribution and x_{t+1} from P_{x_t,:}. 3) Low-Rank MDPs (Model IIb, Forward Model): Data consists of a single trajectory (x_1, ..., x_T) of a Markov chain, where x_1 is from \\nu_0 and x_{t+1} from P_{x_t,:}. The forward model's data is split into subsets to manage correlations. Validation is through rigorous finite-time performance guarantees, regret bounds, and sample complexity analyses, comparing against known minimax lower bounds.",
        "limitations": "The design of algorithms for low-rank structures with provable performance guarantees in RL remains largely open. A specific limitation of the proposed regret bound for low-rank bandits (SME-AE) is the 'cubic dependence in log^3(T)', which is identified as an artifact of the proof techniques, specifically due to the Poisson approximation used for entry-wise guarantees. While the proposed algorithm improves upon some prior works in terms of scaling with matrix dimensions (m+n instead of n*m or sqrt(n)*(n+m)), other related works might exhibit better dependence on T and minimum reward gaps. The comparison with other matrix estimation methods notes that some require strong assumptions like access to 'anchor rows and columns' or are limited to 'generative models' with independent data samples, which the authors' method avoids.",
        "future_research_directions": "The authors believe their low-rank matrix estimation results can find many more applications in low-rank RL. Specific future research directions include: (i) Applying the results to reward-free RL in episodic MDPs, which is a simpler setting than studied; (ii) Extending to offline RL scenarios where data is a single trajectory generated under a behavior policy; (iii) Applying to traditional RL where the reward function R needs to be learned, positioning this problem between the studied Models I and II; (iv) Utilizing the methods for model-free RL to directly learn the Q-function; (v) Adapting to low-rank RL problems with continuous state spaces by combining the methods with appropriate discretization, assuming smooth transition probabilities."
      }
    },
    {
      "title": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing",
      "abstract": "The matching principles behind optimal transport (OT) play an increasingly\nimportant role in machine learning, a trend which can be observed when OT is\nused to disambiguate datasets in applications (e.g. single-cell genomics) or\nused to improve more complex methods (e.g. balanced attention in transformers\nor self-supervised learning). To scale to more challenging problems, there is a\ngrowing consensus that OT requires solvers that can operate on millions, not\nthousands, of points. The low-rank optimal transport (LOT) approach advocated\nin \\cite{scetbon2021lowrank} holds several promises in that regard, and was\nshown to complement more established entropic regularization approaches, being\nable to insert itself in more complex pipelines, such as quadratic OT. LOT\nrestricts the search for low-cost couplings to those that have a\nlow-nonnegative rank, yielding linear time algorithms in cases of interest.\nHowever, these promises can only be fulfilled if the LOT approach is seen as a\nlegitimate contender to entropic regularization when compared on properties of\ninterest, where the scorecard typically includes theoretical properties\n(statistical complexity and relation to other methods) or practical aspects\n(debiasing, hyperparameter tuning, initialization). We target each of these\nareas in this paper in order to cement the impact of low-rank approaches in\ncomputational OT.",
      "full_text": "Low-rank Optimal Transport: Approximation, Statistics and Debiasing Meyer Scetbon CREST, ENSAE meyer.scetbon@ensae.fr Marco Cuturi Apple and CREST, ENSAE cuturi@apple.com Abstract The matching principles behind optimal transport (OT) play an increasingly impor- tant role in machine learning, a trend which can be observed when OT is used to disambiguate datasets in applications (e.g. single-cell genomics) or used to improve more complex methods (e.g. balanced attention in transformers or self-supervised learning). To scale to more challenging problems, there is a growing consensus that OT requires solvers that can operate on millions, not thousands, of points. The low- rank optimal transport (LOT) approach advocated in Scetbon et al. [2021] holds several promises in that regard, and was shown to complement more established entropic regularization approaches, being able to insert itself in more complex pipelines, such as quadratic OT. LOT restricts the search for low-cost couplings to those that have a low-nonnegative rank, yielding linear time algorithms in cases of interest. However, these promises can only be fulﬁlled if the LOT approach is seen as a legitimate contender to entropic regularization when compared on properties of interest, where the scorecard typically includes theoretical properties (statistical complexity and relation to other methods) or practical aspects (debiasing, hyperparameter tuning, initialization). We target each of these areas in this paper in order to cement the impact of low-rank approaches in computational OT. 1 Introduction Optimal transport (OT) is used across data-science to put in correspondence different sets of observa- tions. These observations may come directly from datasets, or, in more advanced applications, depict intermediate layered representations of data. OT theory provides a single grammar to describe and solve increasingly complex matching problems (linear, quadratic, regularized, unbalanced, etc...), making it gain a stake in various areas of science such as as single-cell biology Schiebinger et al. [2019], Yang et al. [2020], Demetci et al. [2020], imaging Schmitz et al. [2018], Heitz et al. [2020], Zheng et al. [2020] or neuroscience Janati et al. [2020], Koundal et al. [2020]. Regularized approaches to OT. Solving OT problems at scale poses, however, formidable chal- lenges. The most obvious among them is computational: the Kantorovich [1942] problem on discrete measures of size nis a linear program that requires O(n3 log n) operations to be solved. A second and equally important challenge lies in the estimation of OT in high-dimensional settings, since it suffers from the curse-of-dimensionality Fournier and Guillin [2015]. The advent of regularized approaches, such as entropic regularization [Cuturi, 2013], has pushed these boundaries thanks for faster algorithms [Chizat et al., 2020, Clason et al., 2021] and improved statistical aspects [Genevay et al., 2018a]. Despite these clear strengths, regularized OT solvers remain, however, costly as they typically scale quadratically in the number of observations. Scaling up OT using low-rank couplings. While it is always intuitively possible to reduce the size of measures (e.g. using k-means) prior to solving an OT between them, a promising line of work proposes to combine both [Forrow et al., 2019, Scetbon et al., 2021, 2022]. Conceptually, these Preprint. Under review. arXiv:2205.12365v2  [stat.ML]  15 Sep 2022low-rank approaches solve simultaneously both an optimal clustering/aggregation strategy with the computation of an effective transport. This intuition rests on an explicit factorization of couplings into two sub-couplings. This has several computational beneﬁts, since its computational cost becomes linear in nif the ground cost matrix seeded to the OT problem has itself a low-rank. While these computational improvements, mostly demonstrated empirically, hold several promises, the theoretical properties of these methods are not yet well established. This stands in stark contrast to the Sinkhorn approach, which is comparatively much better understood. Our Contributions. The goal of this paper is to advance our knowledge, understanding and practical ability to leverage low-rank factorizations in OT. This paper provides ﬁve contributions, targeting theoretical and practical properties of LOT:(i) We derive the rate of convergence of the low-rank OT to the true OT with respect to the non-nnegative rank parameter.(ii) We make a ﬁrst step towards a better understanding of the statistical complexity of LOT by providing an upper-bound of the statistical error, made when estimating LOT using the plug-in estimator; that upper-bound has a parametric rate O( √ 1/n) that is independent of the dimension. (iii) We introduce a debiased version of LOT: as the Sinkhorn divergence [Feydy et al., 2018], we show that debiased LOT is nonnegative, metrizes the weak convergence, and that it interpolates between the maximum mean discrepancy [Gretton et al., 2012] and OT. (iv) We exhibit links between the bias induced by the low-rank factorization and clustering methods. (v) We propose practical strategies to tune the step-length and the initialization of the algorithm in [Scetbon et al., 2021]. Notations. We consider (X,dX) and (Y,dY) two nonempty compact Polish spaces and we denote M+ 1 (X) (resp. M+ 1 (Y)) the space of positive Radon probability measures on X(resp. Y). For all n ≥1, we denote ∆n the probability simplex of size nand ∆∗ n the subset of ∆n of positive histograms. We write 1n ≜ (1,..., 1)T ∈Rn and we denote similarly ∥·∥2 the Euclidean norm and the Euclidean distance induced by this norm depending on the context. 2 Background on Low-rank Optimal Transport Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and c: X×Y→ R+ a nonnegative and continuous function. The Kantorovitch formulation of optimal transport between µand νis deﬁned by OTc(µ,ν) ≜ min π∈Π(µ,ν) ∫ X×Y c(x,y)dπ(x,y) , (1) where the feasible set is the set of distributions over the product spaceX×Y with marginals µand ν: Π(µ,ν) ≜ { π∈M+ 1 (X×Y ) s.t. P1#π = µ, P2#π = ν } , with P1#π (resp. P2#π), the pushforward probability measure of π using the projection maps P1(x,y) = x(resp. P2(x,y) = y). When there exists an optimal coupling solution of (1) supported on a graph of a function, we call such function a Monge map. In the discrete setting, one can reformulate the optimal transport problem as a linear program over the space of nonnegative matrices satisfying the marginal constraints. More precisely, let aand bbe respectively elements of ∆∗ n and ∆∗ m and let also X ≜ {x1,...,x n}and Y ≜ {y1,...,y m}be respectively two subsets of Xand Y. By denoting µa,X ≜ ∑n i=1 aiδxi and νb,Y ≜ ∑m j=1 bjδyj the two discrete distributions associated and writing C ≜ [c(xi,yj)]i,j, the discrete optimal transport problem can be formulated as OTc(µa,X,νb,Y) = min P∈Πa,b ⟨C,P⟩ where Πa,b ≜ {P ∈Rn×m + s.t. P1m = a,PT1n = b}. (2) Scetbon et al. [2021] propose to constrain the discrete optimal transport problem to couplings that have a low-nonnegative rank: Deﬁnition 1. Given M ∈ Rn×m + , the nonnegative rank of M is deﬁned by: rk+(M) ≜ min{q|M = ∑q i=1 Ri,∀i,rk(Ri) = 1,Ri ≥0}. Note that for any M ∈ Rn×m + , we always have that rk+(M) ≤ min(n,m). For r ≥ 1, we consider the set of couplings satisfying marginal constaints with nonnegative-rank of at most ras Πa,b(r) ≜ {P ∈Πa,b, rk +(P) ≤r}. The discrete Low-rank Optimal Transport (LOT) problem is deﬁned by: LOTr,c(µa,X,νb,Y) ≜ min P∈Πa,b(r) ⟨C,P⟩. (3) 2To solve this problem, Scetbon et al. [2021] show that Problem (3) is equivalent to min (Q,R,g)∈C1(a,b,r)∩C2(r) ⟨C,Q diag(1/g)RT⟩, (4) where C1(a,b,r ) ≜ { (Q,R,g ) ∈Rn×r + ×Rm×r + ×(R∗ +)r s.t. Q1r = a,R1r = b } and C2(r) ≜{ (Q,R,g ) ∈Rn×r + ×Rm×r + ×Rr + s.t. QT1n = RT1m = g } .They propose to solve it using a mirror descent scheme and prove the non-asymptotic stationary convergence of their algorithm. While Scetbon et al. [2021] only focus on the discrete setting, we consider here its extension for arbitrary probability measures. Following [Forrow et al., 2019], we deﬁne the set of rank-rcouplings satisfying marginal constraints by: Πr(µ,ν) ≜ {π∈Π(µ,ν) : ∃(µi)r i=1 ∈M+ 1 (X)r, (νi)r i=1 ∈M+ 1 (Y)r, λ∈∆∗ r s.t. π= r∑ i=1 λiµi⊗νi}. This more general deﬁnition of LOT between µ∈M+ 1 (X) and ν ∈M+ 1 (Y) reads: LOTr,c(µ,ν) ≜ inf π∈Πr(µ,ν) ∫ X×Y c(x,y)dπ(x,y) . (5) Note that this deﬁnition of LOTr,c is consistent as it coincides with the one deﬁned in (3) on discrete probability measures. Observe also that Πr(µ,ν) is compact for the weak topology and therefore the inﬁmum in (5) is attained. See Appendix A for more details. 3 Approximation Error of LOT to original OT as a function of rank Our goal in this section is to obtain a control of the error induced by the low-rank constraint when trying to approximate the true OT cost. We provide ﬁrst a control of the approximation error in the discrete setting. The proof is given in Appendix B.1. Proposition 1. Let n,m ≥2, X ≜ {x1,...,x n}⊂X , Y ≜ {y1,...,y m}⊂Y and a∈∆∗ n and b∈∆∗ m. Then for 2 ≤r≤min(n,m), we have that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≤∥ C∥∞ln(min(n,m)/(r−1)) Remark 1. Note that this result improves the control obtained in [Liu et al., 2021], where they obtain that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≲ ∥C∥∞ √nm(min(n,m) −r) as we have for any z,z′≥1, |ln(z) −ln(z′)|≤| z−z′|. It is in fact possible to obtain another control of the approximation error by partitioning the space where the measures are supported. For that purpose let us introduce the notion of entropy numbers. Deﬁnition 2. Let (Z,d) a metric space, W⊂Z and k≥1 an integer. Then by denotingBZ(z,ε) ≜ {y∈Z : d(z,y) ≤ε}, we deﬁne the k-th (dyadic) entropy number of Was Nk(W,d) ≜ inf{εs.t. ∃z1,...,z 2k ∈Z : W⊂∪ 2k i=1BZ(zi,ε)}. For example, any compact set Wof Rd admits ﬁnite entropy numbers, and by denoting R ≜ supw∈W∥w∥2, we have Nk(W,∥·∥2) ≤4R/2k/d. We obtain next a control of the approximation error of LOTr,c to the true OT cost using entropy numbers (see proof in Appendix B.2). Proposition 2. Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and assume that cis L-Lipschitz w.r.t.xand y. Then for any r≥1, we have |LOTr,c(µ,ν) −OTc(µ,ν)|≤ 2Lmax(N⌊log2(⌊√r⌋)⌋(X,dX),N⌊log2(⌊√r⌋)⌋(Y,dY)) This results in the following bound for the p-Wasserstein distance for any p≥1 on Rd. Corollary 1. Let d ≥1, p ≥1, Xa compact subspace of Rd and µ,ν ∈M+ 1 (X). By denoting R≜ supx∈X∥x∥2, we obtain that for any r≥1, |LOTr,∥·∥p 2 (µ,ν) −OT∥·∥p 2 (µ,ν)|≤ 4dp(8R2)p rp/2d . 3As per the Proof of Proposition 2 we can provide a tighter control, assuming a Monge map exists. Corollary 2. Under the same assumptions of Proposition 2 and by assuming in addition that there exists a Monge map solving OTc(µ,ν), we obtain that for any r≥1, |LOTr,c(µ,ν) −OTc(µ,ν)|≤ LN⌊log2(r)⌋(Y,dY) . When X= Yare a subspaces of Rd, a sufﬁcient condition for a Monge map to exists is that either µ or νis absolutely continuous with respect to the Lebesgue measure and that cis of the form h(x−y) where h: X→ R+ is a strictly convex function [Santambrogio, 2015, Theorem 1.17]. Therefore if µis absolutely continuous with respect to the Lebesgue measure, we obtain for any r≥1 and p> 1 |LOTr,∥·∥p 2 (µ,ν) −OT∥·∥p 2 (µ,ν)|≤ 2dp(8R2)p rp/d . 4 Sample Complexity of LOT We now focus on the statistical performance of the plug-in estimator for LOT. In the following we assume that X= Yfor simplicity. Given µ,ν ∈M+ 1 (X), we denote the empirical measures associated ˆµn ≜ 1 n ∑n i=1 δXi and ˆνn ≜ 1 n ∑n i=1 δYi, where (Xi,Yi)n i=1 are sampled independently from µ⊗ν. We consider the plug-in estimator deﬁned as LOTr,c(ˆµn,ˆνn), and we aim at quantifying the rate at which it converges towards the true low-rank optimal transport cost LOTr,c(µ,ν). Before doing so, in the next Proposition we show that this estimator is consistent on compact spaces. The proof is given in Appendix B.3. Proposition 3. Let r≥1 and µ,ν ∈M+ 1 (X), then LOTr,c(ˆµn,ˆνn) −−−−−→ n→+∞ LOTr,c(µ,ν) a.s. Next we aim at obtaining the convergence rates of our plug-in estimator. In the following Proposition, we obtain a non-asymptotic upper-bound of the statistical error. See Appendix B.4 for the proof. Proposition 4. Let r ≥1 and µ,ν ∈M+ 1 (X). Then, there exists a constant Kr such that for any δ >0 and n≥1, we have, with a probability of at least 1 −2δ, that LOTr,c(ˆµn,ˆνn) ≤LOTr,c(µ,ν) + 11∥c∥∞ √r n + Kr∥c∥∞ [√ log(40/δ) n + √rlog(40/δ) n ] . This result is, to the best of our knowledge, the ﬁrst attempt at providing a statistical control of low-rank optimal transport. We provide an upper-bound of the plug-in estimator which converges towards LOTr,c at a parametric rate and which is independent of the dimension on general compact metric spaces. While we fall short of providing a lower bound that could match that upper bound, and therefore provide a complete statistical complexity result, we believe this result might provide a ﬁrst explanation on why, in practice, LOTr,c displays better statistical properties than unregularized OT and its curse of dimensionality [Dudley, 1969]. In addition, that upper bound compares favorably to known results on entropic optimal transport. The rate of entropy regularized OT does not depend on the ambient dimension with respect to n, but carries an exponential dependence in dimension with respect to the regularization parameter ε[Mena and Niles-Weed, 2019]. By contrast, the term associated with the nonnegative rank rin our bound has no direct dependence on dimension. Our next aim is to obtain an explicit rate with respect to rand n. In Proposition 4, we cannot control explicitly Kr in the general setting. Indeed, in our proof, we obtain that Kr ≜ 14/miniλ∗ i where (λ∗ i)r i=1 ∈∆∗ r are the weights involved in the decomposition of one optimal solution of the true LOTr,c(µ,ν). Therefore the control of Kr requires additional assumptions on the optimal solutions of LOTr,c(µ,ν). In the following Proposition, we obtain an explicit upper-bound of the plug-in estimator with respect to rand nin the asymptotic regime. Proposition 5. Let r≥1, δ >0 and µ,ν ∈M+ 1 (X). Then there exists a constant Nr,δ such that if n≥Nr,δ then with a probability of at least 1 −2δ, we have LOTr,c(ˆµn,ˆνn) ≤LOTr,c(µ,ν) + 11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n . 4Note that one cannot recover the result obtained in Proposition 5 from the one obtained in Proposition 4 as we have that Kr ≥14r− −−−− → r→+∞ +∞. In order to prove the above result, we use an extension of the McDiarmid’s inequality when differences are bounded with high probability [Kutin, 2002]. See proof in Appendix B.5 for more details. 5 Debiased Formulation of LOT We introduce here the debiased formulation of LOTr,c and show that it is able to distinguish two distributions, metrize the convergence in law and can be used as a new objective in order to learn distributions. We focus next on the debiasing terms involving measures with themselvesLOTr,c(µ,µ) in this new divergence, and show that they can be interpreted as deﬁning a new clustering method generalizing k-means for any geometry. 5.1 On the Proprieties of the Debiased Low-rank Optimal Transport When it comes to learn (or generate) a distribution in ML applications given samples, it is crucial to consider a divergence that is able to distinguish between two distributions and metrize the convergence in law. In general, LOTr,c(µ,µ) ̸= 0 and the minimum of LOTr,c(ν,µ) with respect to νwill not necessarily recover µ. In order to alleviate this issue we propose a debiased version of LOTr,c deﬁned for any µ,ν ∈M+ 1 (X) as DLOTr,c(µ,ν) ≜ LOTr,c(µ,ν) −1 2[LOTr,c(µ,µ) + LOTr,c(ν,ν)] . Note that DLOTr,c(ν,ν) = 0 . In the next Proposition, we show that, as the Sinkhorn diver- gence [Genevay et al., 2018b, Feydy et al., 2018], DLOTr,c interpolates between the Maximum Mean Discrepancy (MMD) and OT. See proof in Appendix B.6. Proposition 6. Let µ,ν ∈M+ 1 (X). Let us assume that cis symmetric, then we have DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) . If in addition we assume the cis Lipschitz w.r.t toxand y, then we have DLOTr,c(µ,ν) − −−−− → r→+∞ OTc(µ,ν) . Next, we aim at showing some useful properties of the debiased low-rank OT for machine learning applications. For that purpose, let us ﬁrst recall some deﬁnitions. Deﬁnition 3. We say that the cost c : X×X → R+ is a semimetric on Xif for all x,x′ ∈X , c(x,x′) = c(x′,x) and c(x,x′) = 0 if and only ifx= x′. In addition we say thatchas a negative type if ∀n≥2, x1,...,x n ∈X and α1,...,α n ∈R such that ∑n i=1 αi = 0, ∑n i,j=1 αiαjc(xi,xj) ≤0. We say also thatchas a strong negative type if for allµ,ν ∈M+ 1 (X), µ̸= ν =⇒ ∫ X2 c(x,y)d[µ− ν] ⊗[µ−ν] <0. Note that if chas a strong negative type, then chas a negative type too. For example, all Euclidean spaces and even separable Hilbert spaces endowed with the metric induced by their inner products have strong negative type. Also, onRd, the squared Euclidean distance has a negative type [Sejdinovic et al., 2013]. We can now provide stronger geometric guarantees for DLOTr,c. In the next Proposition, we show that for a large class of cost functions, DLOTr,c is nonnegative, able to distinguish two distributions, and metrizes the convergence in law. The proof is given in Appendix B.8. Proposition 7. Let r ≥1, and let us assume that cis a semimetric of negative type. Then for all µ,ν ∈M+ 1 (X), we have that DLOTr(µ,ν) ≥0 . In addition, if chas strong negative type then we have also that DLOTr,c(µ,ν) = 0 ⇐⇒µ= ν and µn →µ ⇐⇒DLOTr,c(µn,µ) →0 . where the convergence of the sequence of probability measures considered is the convergence in law. 5Observe that when chas strong negative type, ν →DLOTr,c(ν,µ) ≥0 and it admits a unique global minimizer at ν = µ. Therefore, DLOTr,c has desirable properties to be used as a loss. It is also worth noting that, in order to obtain the metrization of the convergence in law, we show the following Proposition. See proof in Appendix B.7. Proposition 8. Let r ≥1 and (µn)n≥0 and (νn)n≥0 two sequences of probability measures such that µn →µand νn →νwith respect to the convergence in law. Then we have that LOTr,c(µn,νn) →LOTr,c(µ,ν) . 5.2 Low-Rank Transport Bias and Clustering We turn next to the debiasing terms appearing inDLOT and exhibit links between LOT and clustering methods. Indeed, in the discrete setting, the low-rank bias of a probability measure µdeﬁned as LOTk,c(µ,µ) can be seen as a generalized version of the k-means method for any geometry. In the next Proposition we obtain a new formulation of LOTk,c(µ,µ) viewed as a general clustering method on arbitrary metric space. See proof in Appendix B.9. Proposition 9. Let n≥k≥1, X ≜ {x1,...,x n}⊂X and a∈∆∗ n. If cis a semimetric of negative type, then by denoting C = (c(xi,xj))i,j, we have that LOTk,c(µa,X,µa,X) = min Q ⟨C,Qdiag(1/QT1n)QT⟩ s.t. Q∈Rn×k + , Q1k = a. (6) Let us now explain in more details the link between (6) and k-means. When Xis a subspace of Rd, c is the squared Euclidean distance and a= 1n, we recover exactly the k-means algorithm. Corollary 3. Let n≥k≥1 and X ≜ {x1,...,x n}⊂ Rd. We have that LOTk,∥·∥2 2 (µ1n,X,µa,X) = 2 min Q,z1,...,zk n∑ i=1 k∑ q=1 Qi,q∥xi −zq∥2 2 s.t. Q∈{0,1}n×k, Q1k = 1n . In the general setting, solving LOTk,c(µa,X,µa,X) for a given geometry c, and a prescribed histro- gram aoffers a new clustering method where the assignment of the points to the clusters is determined by the matrix Q∗solution of (6). 6 Computing LOT: Adaptive Stepsizes and Better Initializations We target in this section practical issues that arises when using [Scetbon et al., 2021, Algo.3] to solve (4). Scetbon et al. [2021] propose to apply a mirror descent scheme with respect to the Kullback- Leibler divergence which boils down to solve at each iteration k≥0 the following convex problem using the Dykstra’s Algorithm [Dykstra, 1983]: (Qk+1,Rk+1,gk+1) ≜ argmin ζ∈C1(a,b,r)∩C2(r) KL(ζ,ξk) . (7) where (Q0,R0,g0) ∈ C1(a,b,r ) ∩ C2(r), ξk ≜ (ξ(1) k ,ξ(2) k ,ξ(3) k ), ξ(1) k ≜ Qk ⊙ exp(−γkCRkdiag(1/gk)), ξ(2) k ≜ Rk ⊙exp(−γkCTQkdiag(1/gk)), ξ(3) k ≜ gk ⊙exp(γkωk/g2 k) with [ωk]i ≜ [QT kCRk]i,i for all i ∈{1,...,r }, KL(w,r) ≜ ∑ iwilog(wi/ri) and (γk)k≥0 is a sequence of positive step sizes. In the general setting, each iteration of their algorithm requires O(nmr) operations and when the ground cost matrix Cadmits a low-rank factorization of the form C = ABT where A ∈Rn×q and B ∈Rm×q with q ≪min(n,m), then the total complexity per iteration becomes linear O((n+ m)rq). Note that for the squared Euclidean cost on Rd, we have that q= d+ 2. In the following we investigate two practical aspects of the algorithm: the choice of the step sizes and the initialization. Adaptive choice of γk. Scetbon et al. [2021] show experimentally that the choice of (γk)k≥0 does not impact the solution obtained upon convergence, but rather the speed at which it is attained. Indeed the larger γk is, the faster the algorithm will converge. As a result, their algorithm simply relies on a ﬁxed γschedule. However, the range of admissible γdepends on the problem considered and it may 6vary from one problem to another. Indeed, the algorithm might fail to converge as one needs to ensure at each iteration kof the mirror descent scheme that the kernels ξk do not admit 0 entries in order to solve (7) using the Dykstra’s Algorithm. Such a situation can occur when the terms involved in the exponentials become too large which may depend on the problem considered. Therefore, it may be of particular interest for practitioners to have a generic range of admissible values forγindependently of the considered problem, in order to alleviate parameter tuning issues. We propose to consider instead an adaptive choice of (γk)k≥0 along iterations. D’Orazio et al. [2021], Bayandina et al. [2018] have proposed adaptive mirror descent schemes where, at each iteration, the step-size is normalized by the squared dual-norm of the gradient. Applying such a strategy in our case amounts to consider at each iteration γk = γ ∥(CRdiag(1/g),CTQdiag(1/g),−D(QTRC)/g2) ∥2∞ , (8) where the initial γ > 0 is ﬁxed. By doing so, we are able to guarantee a lower-bound of the exponential terms involved in the expression of the kernels ξk at each iteration and prevent them from having 0 entries. We recommend to set such as global γ ∈[1,10], and observe that this range works whatever the problem considered. On the choice of the initialization. As LOTr,c(4) is a non-convex optimization problem, the ques- tion of choosing an efﬁcient initialization arises in practice. Scetbon et al. [2021] show experimentally that the convergence of the algorithm does not depend on the initalization chosen if no stopping criterion is used. Indeed, their experimental ﬁndings support that only well behaved local minimas are attractive. However, in practice one needs to use a stopping criterion in order to terminate the algorithm. We do observe in many instances that using trivial initializers may result in spurious local minima, which trigger the stopping criterion early on and prevent the algorithm to reach a good solution. Based on various experimentations, we propose to consider a novel initialization of the algorithm. Our initialization aims at being close to a well-behaved local minimum by clustering the input measures. When the measures are supported on Euclidean space, we propose to ﬁnd r centroids (zi)r i=1 of one of the two input discrete probability measures using k-means and to solve the following convex barycenter problem: min Q,R ⟨CX,Z,Q⟩+ ⟨CY,Z,R⟩−εH(Q) −εH(R) s.t. Q1n = a, R1n = b, QT1r = RT1r, (9) where CX,Z = (c(xi,zj))i,j, CY,Z = (c(yi,zj))i,j, and H(P) = −∑ i,jPi,j(log(Pi,j −1). In practice we ﬁx ε = 1 /10 and we then initialize LOTr,c using (Q,R) solution of (9) and g ≜ QT1r(= RT1r). Note that (Q,R,g ) is an admissible initialization and ﬁnding the centroids as well as solving (9) requires O((n+ m)r) algebraic operations. Therefore such initialization does not change the total complexity of the algorithm. In the general (non-Euclidean) case, we propose to initialize the algorithm by applying our generalized k-means approach deﬁned in (6) on each input measure where we ﬁx the common marginal to be g = 1r/r. More precisely, by denoting CX,X = ( c(xi,xj))i,j and CY,Y = ( c(yi,yj))i,j, we initialize the algorithm by solving: Q∈argmin Q ⟨CX,X,Qdiag(1/QT1n)QT⟩ s.t. Q∈Rn×k + , Q1k = a, QT1n = 1r/r. R∈argmin R ⟨CY,Y ,Rdiag(1/RT1m)RT⟩ s.t. R∈Rm×k + , R1k = b, RT1n = 1r/r. (10) Note that again the(Q,R,g ) obtained is an admissible initialization and the complexity of solving(10) is of the same order as solving (4), thus the total complexity of the algorithm remains the same. 7 Experiments In this section, we illustrate experimentally our theoretical ﬁndings and show how our initialization provide practical improvements. For that purpose we consider 3 synthetic problems and one real world dataset to: (i) provide illustrations on the statistical rates of LOTr,c, (ii) exhibit the gradient ﬂow of the debiased formulation DLOTr,c, (iii) use the clustering method induced by LOTr,c, and (iv) show the effect of the initialization. All experiments were run on a MacBook Pro 2019 laptop. 7101 102 103 sample size 10 2 10 1 DLOT dimension = 2 101 102 103 sample size 10 2 10 1 100 dimension = 5 101 102 103 sample size 10 1 100 dimension = 7 101 102 103 sample size 10 1 100 dimension = 10 Upper Bound (r = 1) DLOT: r = 10 DLOT: r = 50 DLOT: r = 100 Figure 1: In this experiment, we consider a mixture of 10 anisotropic Gaussians supported on Rd and we plot the value of DLOTr,c between two independent empirical measures associated to this mixture when varying the number of samples nand the dimension dfor multiple ranks r. The ground cost considered is the squared Euclidean distance. Note that LOTr(µ,µ) ̸= 0 and therefore we use DLOTr,c(µ,µ) instead to evaluate the rates. Each point has been obtained by repeating 10 times the experiment. We compare the empirical rates obtained with the theoretical one derived in Proposition 4 for r= 1. We observe that our theoretical results match the empirical ones and, as expected, the rates do not depend on d. DLOT, r=100 LOT, r=100 Figure 2: We compare the gradient ﬂows (µt)t≥0 (in red) starting from a Gaussian distribution, µ0, to a moon shape distribution (in blue), ν, in 2D when minimizing either L(µ) ≜ DLOTr,c(µ,ν) or L(µ) ≜ LOTr,c(µ,ν). The ground cost is the squared Euclidean distance and we ﬁx r = 100. We consider 1000 samples from each distribution and and we plot the evolution of the probability measure obtained along the iterations of a gradient descent scheme. We also display in green the vector ﬁeld in the descent direction. We show that the debiased version allows to recover the target distribution while LOTr,c is learning a biased version with a low-rank structure. Statistical rates. We aim at showing the statistical rates of the plug-in estimator of LOTr,c. As LOTr,c(µ,µ) ̸= 0 and as we do not have access to this value given samples from µ, we consider instead the debiased version of the low-rank optimal transport, DLOTr,c. In ﬁgure 1, we show that the empiricial rates match the theoretical bound obtained in Proposition 4. In particular, we show that that these rates does not depend on the dimension of the ground space. Note also that we recover our theoretical dependence with respect to the rank r: the higher the rank, the slower the convergence. Gradient Flows using DLOT. We illustrate here a practical use of DLOT for ML application. In ﬁgure 6, we consider Y1,...,Y n independent samples from a moon shape distribution in 2D, and by denoting ˆνn the empirical measure associated, we show the iterations obtained by a gradient descent scheme on the following optimization problem: min X∈Rn×2 DLOTr,c(µ1n/n,X,ˆνn) . We initialize the algorithm using n= 1000 samples drawn from a Gaussian distribution. We show that the gradient ﬂow of our debiased version is able to recover the target distribution. We also compare it with the gradient ﬂow of the biased version (LOT) and show that it fails to reproduce the target distribution as it is learning a biased one with a low-rank structure. Application to Clustering. In this experiment we show some applications of the clustering method induced by LOTr,c. In ﬁgure 3, we consider 6 datasets with different structure and we aim at recovering the clusters using (6) for some well chosen costs. We compare the clusters obtained when considering either the squared Euclidean cost (which amounts at applying the k-means) and the 8Figure 3: In this experiment, we draw 1000 samples from multiple distributions from the python package scikit-learn [Pedregosa et al., 2011] and we apply the method proposed in (6) for two different costs: in the top row we consider the squared Euclidean distance while in the bottom row, we consider the shortest path distance on the graph associated with the ground cost c(x,y) = 1 −k(x,y) where kis a Gaussian kernel. In the two ﬁrst problem (starting from the left), we ﬁx r= 2, in the next three problem we ﬁx r= 3 and in the last one we ﬁx r= 4. We observe that the ﬂexibility of our method allows to recover the clustering for a well chosen ground cost. 105 107 109 Operations 0.2 0.3 0.4 0.5LOT cost Init: kmeans, r=10 Init: kmeans, r=100 Init: gkmeans, r=10 Init: gkmeans, r=100 Init: rank_2, r=10 Init: rank_2, r=100 Init: random, r=10 Init: random, r=100 106 107 108 109 Operations 10 9 10 7 10 5 10 3 LOT criterion: k Figure 4: In this experiment, we consider the Newsgroup20 dataset [Pedregosa et al., 2011] constituted of texts and we embed them into distributions in 50D using the same pre-processing steps as in [Cuturi et al., 2022]. We compare different initialization when applying the algorithm of [Scetbon et al., 2021] to compare random texts viewed as distributions for multiple choices of rank r. The ground cost considered in the squared Euclidean distance. We repeat the experiments 50 times by sampling randomly multiple problems of similar size (≃250 samples). We normalize the cost matrix by its maximum value in order to have comparable LOT cost. We consider 4 different initialization: the one using k-means algorithm (9), the one using the generalized k-means (10), the rank-2 initialization [Scetbon et al., 2021] and a random initialization where Q,R and gare drawn from Gaussians. We compare both the cost value and the criterion value (∆k) along the iterations of the MD scheme. Note that the curves obtained do not start at the same point in time as we start plotting the curves after obtaining the initial point which in some case requires more algebraic operations (e.g. kmeans methods). First we observe that whatever the initialization considered, the algorithm converges toward the same value. In addition, we observe that both k-means and general k-means are able to initialize well the algorithm by avoiding bad local minima at initialization while the two other initialization are close to spurious local minima at initialization. shortest-path distance on the data viewed as a graph. We show that our method is able to recover the clusters on these settings for well chosen costs and therefore the proposed algorithm in Scetbon et al. [2021] can be seen as a new alternative in order to clusterize data. Effect of the Initialization. Our goal here is to show the effect of the initialization. In ﬁgure 4, we display the evolution of the cost as well as the value of the stopping criterion along the iterations of the MD scheme solving (4) when considering different initialization. The x-axis corresponds to the total number of algebraic operations. This number is computed at each iteration of the outer loop of the algorithm proposed in Scetbon et al. [2021] and is obtained by computing the complexity of all the operations involved in their algorithm to reach it. We consider this notion of time instead of 9CPU/GPU time as we do not want to be architecture/machine dependent. Recall also that the stopping criterion introduced in [Scetbon et al., 2021] is deﬁned for all k≥1 by ∆k ≜ 1 γ2 k (KL((Qk,Rk,gk),(Qk−1,Rk−1,gk−1)) + KL((Qk−1,Rk−1,gk−1),(Qk,Rk,gk))), where ((Qk,Rk,gk))k≥0 is the sequence solution of (7). First, we show that whatever the initializa- tion chosen, the algorithm manages to converge to an efﬁcient solution if no stopping criterion is used. However, the choice of the initialization may impact the termination of the algorithm as some initialization might be too close to some spurious local minima. Indeed, the initial points obtained using a “rank 2” or random initialization can be close to spurious and non-attractive local minima, which may trigger the stopping criterion too early and prevent the algorithm from continuing to run in order to converge towards an attractive and well behaved local minimum. We show also that the initialization we propose in (9) and (10) are sufﬁciently far away from bad local minima and allow the algorithm to converge directly toward the desired solution. The right ﬁgure of Fig.4 shows two main observations: (i) that the initial point obtained using a “rank 2” or random initialization can be close to spurious and non-attractive local minima, which may trigger the stopping criterion too early and prevent the algorithm from continuing to run in order to converge towards an attractive and well behaved local minimum. (ii) When initialiazing the algorithm using kmeans methods, we show that our stopping criterion is a decreasing function of time meaning that the algorithm converges directly towards the desired solution. Conclusion. We assembled in this work theoretical and practical arguments to support low-rank factorizations for OT. We have presented two controls: one concerning the approximation error to the true optimal transport and another concerning the statistical rates of the plug-in estimator. The latter is showed to be independent of the dimension, which is of particular interest when studying OT in ML settings. We have motivated further the use of LOT as a loss by introducing its debiased version and showed that it possesses desirable properties: positivity and metrization of the convergence in law. We have also presented the links between the bias induced by such regularization and clustering methods, and studied empirically the effects of hyperparameters involved in the practical estimation of LOT. The strong theoretical foundations provided in this paper motivate further studies of the empirical behaviour of LOT estimator, notably on ﬁnding suitable local minima and on improvements on the convergence of the MD scheme using other adaptive choices for step sizes. 10Acknowledgements. This work was supported by a \"Chaire d’excellence de l’IDEX Paris Saclay\". The authors would also like to thank Gabriel Peyré and Jaouad Mourtada for enlightening conversa- tions on the topics discussed in this work. References Anastasia Bayandina, Pavel Dvurechensky, Alexander Gasnikov, Fedor Stonyakin, and Alexander Titov. Mirror descent and convex optimization problems with non-smooth inequality constraints. In Large-scale and distributed optimization, pages 181–213. Springer, 2018. Lenaic Chizat, Pierre Roussillon, Flavien Léger, François-Xavier Vialard, and Gabriel Peyré. Faster wasserstein distance estimation with the sinkhorn divergence. Advances in Neural Information Processing Systems, 33, 2020. Christian Clason, Dirk A. Lorenz, Hinrich Mahler, and Benedikt Wirth. Entropic regularization of continuous optimal transport problems. Journal of Mathematical Analysis and Applications, 494 (1):124432, 2021. ISSN 0022-247X. doi: https://doi.org/10.1016/j.jmaa.2020.124432. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, pages 2292–2300, 2013. Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. arXiv preprint arXiv:2201.12324, 2022. Pinar Demetci, Rebecca Santorella, Björn Sandstede, William Stafford Noble, and Ritambhara Singh. Gromov-wasserstein optimal transport to align single-cell multi-omics data. BioRxiv, 2020. Ryan D’Orazio, Nicolas Loizou, Issam Laradji, and Ioannis Mitliagkas. Stochastic mirror descent: Convergence analysis and adaptive variants via the mirror stochastic polyak stepsize.arXiv preprint arXiv:2110.15412, 2021. Richard Mansﬁeld Dudley. The speed of mean glivenko-cantelli convergence. The Annals of Mathematical Statistics, 40(1):40–50, 1969. Richard L Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical Association, 78(384):837–842, 1983. Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun-Ichi Amari, Alain Trouvé, and Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences. arXiv preprint arXiv:1810.08278, 2018. Aden Forrow, Jan-Christian Hütter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan Weed. Statistical optimal transport via factored couplings. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 2454–2465. PMLR, 2019. Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of the empirical measure. Probability Theory and Related Fields, 162(3-4):707–738, 2015. Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity of sinkhorn divergences. arXiv preprint arXiv:1810.02733, 2018a. Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver- gences. In Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics, Proceedings of Machine Learning Research. PMLR, 09–11 Apr 2018b. Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773, 2012. Matthieu Heitz, Nicolas Bonneel, David Coeurjolly, Marco Cuturi, and Gabriel Peyré. Ground metric learning on graphs. Journal of Mathematical Imaging and Vision, pages 1–19, 2020. 11Hicham Janati, Thomas Bazeille, Bertrand Thirion, Marco Cuturi, and Alexandre Gramfort. Multi- subject meg/eeg source imaging with sparse multi-task regression. NeuroImage, page 116847, 2020. Leonid Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):227–229, 1942. Sunil Koundal, Rena Elkin, Saad Nadeem, Yuechuan Xue, Stefan Constantinou, Simon Sanggaard, Xiaodan Liu, Brittany Monte, Feng Xu, William Van Nostrand, et al. Optimal mass transport with lagrangian workﬂow reveals advective and diffusion driven solute transport in the glymphatic system. Scientiﬁc reports, 10(1):1–18, 2020. Samuel Kutin. Extensions to mcdiarmid’s inequality when differences are bounded with high probability. Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep. TR-2002-04, 2002. Weijie Liu, Chao Zhang, Nenggan Zheng, and Hui Qian. Approximating optimal transport via low-rank and sparse factorization. arXiv preprint arXiv:2111.06546, 2021. Gonzalo Mena and Jonathan Niles-Weed. Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem. Advances in Neural Information Processing Systems, 32, 2019. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011. Filippo Santambrogio. Optimal transport for applied mathematicians. Birkäuser, NY, 55(58-63):94, 2015. Meyer Scetbon, Marco Cuturi, and Gabriel Peyré. Low-rank sinkhorn factorization, 2021. Meyer Scetbon, Gabriel Peyré, and Marco Cuturi. Linear-time gromov wasserstein distances using low rank couplings and costs. ICML, 2022. Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-transport analysis of single-cell gene expression identiﬁes developmental trajectories in reprogramming. Cell, 176(4):928–943, 2019. Morgan A Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi, Gabriel Peyré, and Jean-Luc Starck. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. SIAM Journal on Imaging Sciences, 11(1):643–678, 2018. Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The Annals of Statistics , pages 2263–2291, 2013. Karren Dai Yang, Karthik Damodaran, Saradha Venkatachalapathy, Ali C Soylemezoglu, GV Shiv- ashankar, and Caroline Uhler. Predicting cell lineages using autoencoders and optimal transport. PLoS computational biology, 16(4):e1007828, 2020. Xinye Zheng, Jianbo Ye, James Z Wang, and Jia Li. Scott: Shape-location combined tracking with optimal transport. SIAM Journal on Mathematics of Data Science, 2(2):284–308, 2020. 12Supplementary materials A On the Deﬁnition of LOT r,c Let (X,dX) and (Y,dY) two nonempty compact Polish spaces, µ ∈M+ 1 (X), ν ∈M+ 1 (Y) two probability measures on these spaces and c: X×Y→ R+ a nonnegative and continuous function. We deﬁne the generalized low-rank optimal transport between µand νas LOTr,c(µ,ν) ≜ inf π∈Πr(µ,ν) ∫ X×Y c(x,y)dπ(x,y) . where Πr(µ,ν) ≜ {π∈Π(µ,ν) : ∃(µi)r i=1 ∈M+ 1 (X)r, (νi)r i=1 ∈M+ 1 (Y)r, λ∈∆∗ r s.t. π= r∑ i=1 λiµi⊗νi}. As Xand Yare compact, Πr(µ,ν) is tight, then Prokhorov’s theorem applies and the closure of Πr(µ,ν) is sequentially compact. Let us now show that Πr(µ,ν) is closed. Indeed, Let (πn)n≥0 a sequence of Πr(µ,ν) converging towards π∗. Then by deﬁnition there exists for all k ∈[|1,r|], (µ(k) n )n≥0, (ν(k) n )n≥0 and (λ(k) n )n≥0 such that for all n≥0 πn = r∑ i=1 λ(k) n µ(k) n ⊗ν(k) n . However, (µ(k) n )n≥0 and (ν(k) n )n≥0 are also tight, and Prokhorov’s theorem applies, therefore we can extract a common subsequence such that for all k, µ(k) n →µ(k) ∗ and ν(k) n →ν(k) ∗ In addition as (λn)n≥0 live in the simplex ∆r, we can also extract a sub-sequence, such that λn →λ∗∈∆r. Finally by unicity of the limit we obtain that π∗= r∑ k=1 λ(k) ∗ µ(k) ∗ ⊗ν(k) ∗ . Finally, by denoting I ≜ {k: λ(k) ∗ >0}, and by considering i∗∈I, we obtain that π∗= r∑ i∈I\\{i∗} λ(i) ∗ µ(i) ∗ ⊗ν(i) ∗ + r−|I|+1∑ j=1 λ(i∗) ∗ r−|I|+ 1µ(i∗) ∗ ⊗ν(i∗) ∗ . from which follows that π∗∈Πr(µ,ν). B Proofs B.1 Proof of Proposition 1 Proposition. Let n,m ≥2, X ≜ {x1,...,x n}⊂X , Y ≜ {y1,...,y m}⊂Y and a ∈∆∗ n and b∈∆∗ m. Then for 2 ≤r≤min(n,m), we have that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|≤∥ C∥∞ln(min(n,m)/(r−1)) Proof. Let P ∈argminP∈Πa,b⟨C,P⟩. As P is a nonnegative matrix, its nonnegative rank cannot exceed min(n,m). Assume for simplicity, that n = m, then there exists (Ri)n i=1 nonnegative matrices of rank 1 such that P = n∑ i=1 Ri . 13As for all i∈[|1,n|], Ri is a rank 1 matrix, there exist ˜qi,˜ri ∈Rn + such that Ri = ˜qi˜rT i . Then by denoting qi = ˜qi/|˜qi|, ri = ˜ri/|˜ri|and λi = |˜qi||˜ri|where for any h∈Rn |h|≜ ∑n i=1 hi, we obtain that P = n∑ i=1 λiqirT i . Without loss of generality, we can consider the case where λ1 ≥···≥ λn. Let us now denote λ := ( λ1,...,λ n), and by using the fact the P is a coupling we obtain that λ ∈∆n. Also, by deﬁnition of λ, we have that for all k∈[|1,n|], λk ≤1/k. Let us now deﬁne ˜P ≜ r−1∑ i=1 λiqirT i + ( n∑ i=r λi ) αrβT r where αr ≜ ∑n i=rλiqi∑n i=rλi βr ≜ ∑n i=rλiri∑n i=rλi Remark that ˜P ∈Πa,b(r), therefore we obtain that |LOTr,c(µa,X,νb,Y) −OTc(µa,X,νb,Y)|= LOTr,c(µa,X,νb,Y) −OTx(µa,X,νb,Y) ≤⟨C, ˜P⟩−⟨C,P⟩ ≤⟨C, ( n∑ i=r λi ) αrβT r ⟩−⟨C, n∑ i=r λiqirT i ⟩ ≤⟨C, ( n∑ i=r λi ) αrβT r ⟩ ≤∥C∥∞ n∑ i=r λi ≤∥C∥∞ n∑ i=r 1 i ≤∥C∥∞ln(n/(r−1)) B.2 Proof of Proposition 2 Proposition 10. Let µ∈M+ 1 (X), ν ∈M+ 1 (Y) and let us assume that cis L-Lipschitz w.r.t.xand y. Then for any r≥1, we have |LOTr,c(µ,ν) −OTc(µ,ν)|≤ 2Lmax(N⌊log2(⌊√r⌋)⌋(X,dX),N⌊log2(⌊√r⌋)⌋(Y,dY)) Proof. As Xand Yare compact, N⌊log2(⌊√r⌋)⌋(X,d),N⌊log2(⌊√r⌋)⌋(Y,d) <+∞and then by de- noting εX ≜ N⌊log2(⌊√r⌋)⌋(X,dX), there exists x1,...,x ⌊√r⌋∈X, such that X⊂ ⋃r i=1 BX(xi,ε) from which we can extract a partition (Si,X)⌊√r⌋ i=1 of X such that for all i ∈ [|1,⌊√r⌋|], and x,y ∈Si,X, dX(x,y) ≤εX. Similarly we can build a partition (Si,Y)⌊√r⌋ i=1 of Y. Let us now deﬁne for all k∈[|1,⌊√r⌋|], µk ≜ µ|Sk,X µ(Sk,X) and νk ≜ ν|Sk,Y ν(Sk,Y) with the convention that 0 0 = 0, we can deﬁne πr ≜ ⌊√r⌋∑ i,j=1 π∗(Si,X×Sj,Y)νj ⊗µi . 14First remarks that πr ∈Πr(µ,ν). Indeed we have for any measurable set B πr(X× B) = ⌊√r⌋2 ∑ j=1 νj(B) r∑ i=1 π∗(Si,X×Sj,Y) = ⌊√r⌋∑ j=1 νj(B)ν(Sj,Y) = ⌊√r⌋∑ j=1 ν|Sj,X(B) = ν(B) , similarly πr(A×Y) = µ(A) and we have that ⌊√r⌋2 ≤r. Therefore we obtain that |LOTr,c(µ,ν) −OTc(µ,ν)|= LOTr,c(µ,ν) −OTc(µ,ν) ≤ ∫ X×Y c(x,y)dπr(x,y) − ∫ X×Y c(x,y)dπ∗(x,y) ≤ ⌊√r⌋∑ i,j=1 ∫ Si,X×Sj,Y c(x,y)d[πr(x,y) −π∗(x,y)] ≤ ⌊√r⌋∑ i,j=1 π∗(Si,X×Sj,Y) ×[ sup (x,y)∈Si,X×Sj,Y c(x,y) − inf (x,y)∈Si,X×Sj,Y c(x,y)] ≤L[εX+ εY] from which the result follows. Corollary. Under the same assumptions of Proposition 2 and by assuming in addition that there exists a Monge map solving OTc(µ,ν), we obtain that for any r≥1, |LOTr,c(µ,ν) −OTc(µ,ν)|≤ LN⌊log2(r)⌋(Y,dY) Proof. Let us denote T a Monge map solution of OTc(µ,ν) and as in the proof above, let us consider a partition of (Si,Y)r i=1 of Ysuch that for all i ∈[|1,r|], and x,y ∈Si,Y, dY(x,y) ≤εY with εY≜ N⌊log2(r)⌋(Y,dY). Let us now deﬁne for all k∈[|1,⌊√r⌋|], µk ≜ µ|T−1(Sk,Y) µ(T−1(Sk,Y)) and νk ≜ ν|Sk,Y ν(Sk,Y) with the convention that 0 0 = 0, we can deﬁne πr ≜ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y)νk ⊗µk . 15Again we have that πr ∈Πr(µ,ν), and we obtain that |LOTr,c(µ,ν) −OTc(µ,ν)|= LOTr,c(µ,ν) −OTc(µ,ν) ≤ ∫ X×Y c(x,y)dπr(x,y) − ∫ X×Y c(x,y)dπ∗(x,y) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,y)dµk(y) ⊗νk(y) − r∑ k=1 ∫ T−1(Sk,Y) c(x,T(x))dµ(x) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,y)dµk(y) ⊗νk(y) − r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y c(x,T(x))dµk(x) ⊗νk(y) ≤ r∑ k=1 π∗(T−1(Sk,Y) ×Sk,Y) ∫ T−1(Sk,Y)×Sk,Y [c(x,y) −c(x,T(x))]dµk(x) ⊗νk(x) ≤LεY from which the result follows. Note that to obtain the above inequalities, we use the fact that π∗is supported on the graph of T, and therefore we have have for all k∈[|1,r|], π∗(T−1(Sk,Y) ×Sk,Y) = µ(T−1(Sk,Y)) = ν(Sk,Y). B.3 Proof of Proposition 3 Proposition. Let r≥1 and µ,ν ∈M+ 1 (X), then LOTr,c(ˆµn,ˆνn) −−−−−→ n→+∞ LOTr,c(µ,ν) a.s. Proof. Let π∗solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (X)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. Note that by deﬁnition, we have that µ= r∑ i=1 λ∗ iµ∗ i and ν = r∑ i=1 λ∗ iν∗ i . Let us now deﬁne πµ and πµ both elements of M+ 1 (X× [|1,r|]) as follows: πµ(A×{k}) ≜ λkµk(A) and πν(A×{k}) ≜ λkνk(A) for any measurable set Aand k∈[|1,r|] . Observe that the right marginals of πµ and πν is the same and we will denote it ρ. We can now deﬁne for all x,y ∈X the family of kernels (kµ(·,x))x∈X ∈M+ 1 ([|1,r|])X and (kν(·,y))y∈X ∈ M+ 1 ([|1,r|])X corresponding to the disintegration with respect to the projection of respectively µ and ν. Let us now consider nindependent samples (Zµ i )n i=1 and (Zν i )n i=1 such that for all i∈[|1,n|], Zµ i ∼kµ(·,Xi) and Zν i ∼kν(·,Yi) and let us deﬁne for all k∈[|1,r|] ˜µk ≜ 1 n n∑ i=1 1Zµ i =kδXi and ˜νk ≜ 1 n n∑ i=1 1Zν i=kδYi . Let us now deﬁne ˜π≜ r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ] ⊗ [ ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ] 16with the convention that 0 0 = 0. Now it is easy to check that ˜π∈Πr(ˆµ,ˆν), indeed we have that ˜π(A×X) = r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk(A) + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ(A) − r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk(A) ][ 1 − r−1∑ k=1 min(|˜µk|,|˜νk|) ] = ˆµ(A) in addition by construction we have that⏐⏐⏐⏐⏐ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ⏐⏐⏐⏐⏐= ⏐⏐⏐⏐⏐ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ⏐⏐⏐⏐⏐= 1 − r−1∑ k=1 min(|˜µk|,|˜νk|) and both ˆµ−∑r−1 k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk and ˆν−∑r−1 k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk are positive measures. Therefore we obtain that LOTr,c(ˆµ,ˆν) ≤ ∫ X2 c(x,y)d˜π(x,y) Now we aim at showing at ∫ X2 c(x,y)d˜π(x,y) →LOTr,c(µ,ν) a.s.. Indeed ﬁrst observe that from the law of large numbers we have that for all k ∈[|1,r|], |˜µk|→ λ∗ k and similarly |˜νk|→ λ∗ k. In addition, for all k,q we have that almost surely, ˜µk ⊗˜νq converges weakly towards λ∗ kλ∗ qµk ⊗νq. Indeed one can consider the following algebra F≜ { (x,y) ∈X2 →f(x)g(y) f,g ∈C(X) } , and then by Stone-Weierstrass, one obtains by density the desired result. Now remark that ∫ X2 c(x,y)d˜π(x,y) = r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ∫ X2 c(x,y)d˜µk ⊗˜νk + 1 ˜λr ∫ Z2 c(x,y)d˜µr ⊗˜νr + 1 ˜λr r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| )∫ X2 c(x,y)d˜µr ⊗˜νk + 1 ˜λr r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )∫ X2 c(x,y)d˜µk ⊗˜νr + 1 ˜λr r−1∑ k,q=1 ∫ X2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) from which follows directly that ∫ X2 c(x,y)d˜π(x,y) →LOTr,c(µ,ν) a.s.Let us now denote for all n≥1, πn a solution of LOTr,c(ˆµ,ˆν). Let ω∈Ω an element of the probability space where live the random variables (Xi)i≥0 and (Yi)i≥0 such that ∫ X2 c(x,y)d˜π(ω)(x,y) →LOTr,c(µ,ν). As Xis compact Thanks to Prokhorov’s Theorem, we can extract a sequence such that(π(ω) n )n≥0 converge weakly towards π(ω) ∈Πr(µ,ν). In addition we have that for all n≥1∫ X2 c(x,y)dπ(ω) n (x,y) ≤ ∫ X2 c(x,y)d˜π(ω)(x,y) And by considering the limit we obtain that∫ c(x,y)dπ(ω)(x,y) ≤LOTr,c(µ,ν) However π(ω) ∈Πr(µ,ν) and by optimality we obtain that∫ c(x,y)dπ(ω)(x,y) = LOTr,c(µ,ν) This holds for an arbitrary subsequence of (π(ω) n )n≥0, from which follows that∫ c(x,y)dπ(ω) n (x,y) →LOTr,c(µ,ν). Finally this holds almost surely and the result follows. 17B.4 Proof of Proposition 4 Proposition. Let r ≥1 and µ,ν ∈M+ 1 (X). Then, there exists a constant Kr such that for any δ >0 and n≥1, we have, with a probability of at least 1 −2δ, that LOTr,c(ˆµn,ˆνn) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + Kr∥c∥∞ [√ log(40/δ) n + √rlog(40/δ) n ] Proof. We reintroduce the same notation as in the proof of Proposition 3. Let π∗ solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (Z)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. As before let us also consider πµ and πµ deﬁned as πµ(A×{k}) ≜ λkµk(A) and πν(A×{k}) ≜ λkνk(A) for any measurable set Aand k∈[|1,r|] and denote ρtheir common right marginal. We also consider n independent samples (Zµ i )n i=1 and (Zν i )n i=1 such that for all i ∈[|1,n|], Zµ i ∼ kµ(·,Xi) and Zν i ∼kν(·,Yi) and we denote for all k∈[|1,r|] ˜µk ≜ 1 n n∑ i=1 1Zµ i =kδXi and ˜νk ≜ 1 n n∑ i=1 1Zν i=kδYi Let us now deﬁne ˆπ≜ r∑ i=1 1 λ∗ k ˜µk ⊗˜νk . Our goal is to control the following quantity: ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐, First observe that E [∫ Z2 c(x,y)dˆπ(x,y) ] = r∑ i=1 1 λ∗ k E [∫ Z2 c(x,y)d˜µk(x)d˜νk(y) ] = r∑ i=1 1 λ∗ kn2 × ∑ i,j E [ c(Xi,Yj)1Zµ i =k1Zν j=k ] Moreover, we have that E [ c(Xi,Yj)1Zµ i =k1Zν j=k ] = ∫ (Z×[|1,r|])2 c(x,y)1z=k1z′=kdπµ(x,z)dπν(y,z′) = ∫ (Z×[|1,r|])2 c(x,y)1z=k1z′=kdµz(x)dνz′(y)dρ(z)dρ(z′) = λ2 k ∫ Z2 c(x,y)dµk(x)dνk(y) from which follows that E [∫ Z2 c(x,y)dˆπ(x,y) ] = r∑ i=1 λ∗ k ∫ Z2 c(x,y)dµk(x)dνk(y) = LOTr,c(µ,ν) Now let us deﬁne for all (xi,zi)n i=1,(yi,z′ i) ∈(Z× [|1,r|])n, g((x1,z1),..., (xn,zn),(y1,z′ 1),..., (yn,z′ n)) ≜ r∑ q=1 1 λ∗qn2 ∑ i,j c(xi,yj)1zi=q1z′ j=q , 18since Zis compact and cis continuous, we have that |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|= ⏐⏐⏐⏐⏐⏐ r∑ q=1 1 λ∗qn2 ∑ j [c(xk,yj)1zk=q −c(˜xk,yj)1˜zk=q]1z′ j=q ⏐⏐⏐⏐⏐⏐ = ⏐⏐⏐⏐⏐⏐ 1 λ∗zkn2 n∑ j=1 c(xk,yj)1z′ j=zk − 1 λ∗ ˜zkn2 n∑ j=1 c(˜xk,yj)1z′ j=˜zk ⏐⏐⏐⏐⏐⏐ ≤∥c∥∞ n2 [∑n j=1 1z′ j=zk λ∗zk + ∑n j=1 1z′ j=˜zk λ∗ ˜zk ] ≤ 2∥c∥∞ min 1≤q≤r λ∗q 1 n Then by applying the McDiarmid’s inequality we obtain that forδ >0, with a probability at least of 1 −δ, we have ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐≤ 2∥c∥∞ min 1≤q≤r λ∗q √ log(2/δ) n Now we aim at building a coupling ˜π ∈Πr(ˆµ,ˆν) from ˆπ. Let us consider the same as the one introduce in the proof of Proposition B.3, that is ˜π≜ r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) [ ˆµ− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk| ˜µk ] ⊗ [ ˆν− r−1∑ k=1 min(|˜µk|,|˜νk|) |˜νk| ˜νk ] with the convention that 0 0 = 0. Let us now expand the above expression, and by denoting ˜λr = 1 −∑r−1 k=1 min(|˜µk|,|˜νk|) we obtain that ˜π= r−1∑ k=1 min(|˜µk|,|˜νk|) |˜µk||˜νk| ˜µk ⊗˜νk + 1 ˜λr ˜µr ⊗˜νr + 1 ˜λr ˜µr ⊗ [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) ˜νk ] + 1 ˜λr [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) ˜µk ] ⊗˜νr + 1 ˜λr [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) ˜µk ] ⊗ [r−1∑ k=1 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) ˜νk ] 19Now we aim at controlling the following quantity ⏐⏐∫ Z2 c(x,y)dˆπ(x,y) − ∫ Z2 c(x,y)d˜π(x,y) ⏐⏐and we observe that ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] = r−1∑ k=1 ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) (11) + ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) (12) + 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) (13) + 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| ) c(x,y)d˜µk(x)d˜νr(y) (14) + 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) (15) Let us now control each term of the RHS of the above equality. Let us ﬁrst consider the term in Eq. 11, remark that we have ⏐⏐⏐⏐ ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) ⏐⏐⏐⏐ ≤ ⏐⏐⏐⏐ [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ]⏐⏐⏐⏐∥c∥∞|˜µk||˜νk| ≤ ⏐⏐⏐⏐ [|˜µk||˜νk| λ∗ k −min(|˜µk|,|˜νk|) ]⏐⏐⏐⏐∥c∥∞ ≤min(|˜µk|,|˜νk|) ⏐⏐⏐⏐ max(|˜µk|,|˜νk|) λ∗ k −1 ⏐⏐⏐⏐∥c∥∞ ≤min(|˜µk|,|˜νk|) λ∗ k |max(|˜µk|,|˜νk|) −λ∗ k|∥c∥∞ ≤min(|˜µk|,|˜νk|) λ∗ k max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞)∥c∥∞ ≤∥c∥∞max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞) where we have denoted ˜λµ ≜ (|˜µk|)r k=1 and ˜λν ≜ (|˜νk|)r k=1. Now observe that P ( max(∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞) ≥t ) ≤2P ( ∥˜λµ −λ∗∥∞≥t ) ≤P ( dK(λ∗,˜λµ) ≥t 2 ) ≤4 exp(−nt2/2) where dK is the Kolmogorov distance. In addition we have max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ≤1 + 1 min 1≤i≤r λ∗ i max ( ∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞ ) Combining the two above controls, we obtain that for all δ >0, with a probability of at least 1 −δ, ⏐⏐⏐⏐ ∫ Z2 c(x,y) [ 1 λ∗ k −min(|˜µk|,|˜νk|) |˜µk||˜νk| ] d˜µk(x)˜νk(y) ⏐⏐⏐⏐≤∥c∥∞ √ 2 ln 8/δ n + ∥c∥∞ n 2 ln 8/δ min 1≤i≤r λ∗ i 20Let us now consider the term in Eq. 12, we have that ⏐⏐⏐⏐ ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) ⏐⏐⏐⏐≤|˜µr||˜νr| λ∗r˜λr ⏐⏐⏐⏐⏐1 − r∑ i=1 min(|˜µk|,|˜νk|) −λr ⏐⏐⏐⏐⏐∥c∥∞ ≤max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ )r−1∑ k=1 |λ∗ k −min(|˜µk|,|˜νk|)|∥c∥∞ ≤max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ∥c∥∞(∥λ∗−˜λµ∥1 + ∥λ∗−˜λν∥1) ≤2∥c∥∞max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) However we have that P ( max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) ≥t ) ≤2P ( ∥λ∗−˜λµ∥1 ≥t ) In addition we have that E(∥λ∗−˜λµ∥1) ≤√r n and by applying the McDiarmid’s Inequality, we obtain that for all δ >0, with a probability of 1 −δ ∥λ∗−˜λµ∥1 ≤ √r n + √ 2 ln(2/δ) n Therefore we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐ ∫ Z2 c(x,y) [1 λ∗r − 1 ˜λr ] d˜µr(x)˜νr(y) ⏐⏐⏐⏐≤2∥c∥∞   √r n + √ 2 ln(8/δ) n + 2 ln(8/δ) + √ 2rln(8/δ) n× min 1≤i≤r λ∗ i   For the term in Eq. 13 and 14, we obtain that ⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) ⏐⏐⏐⏐⏐ ≤|˜µr| ˜λr r−1∑ k=1 (|˜νk|−min(|˜µk|,|˜νk|)) ∥c∥∞ ≤|˜µr| ˜λr [˜λr −|˜νr|]∥c∥∞ ≤[|˜λr −λ∗ r|+ |λ∗ r −˜νr|]∥c∥∞ ≤3∥c∥∞max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) Therefore we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜νk| ) c(x,y)d˜µr(x)d˜νk(y) ⏐⏐⏐⏐⏐≤3∥c∥∞ [√r n + √ 2 ln(2/δ) n ] Finally the last term in Eq. 15 can be controlled as the following:⏐⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) ⏐⏐⏐⏐⏐⏐ ≤∥c∥∞ ˜λr r−1∑ k,q=1 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) |˜µk||˜νq| ≤∥c∥∞ ˜λr r−1∑ k=1 (|˜µk|−min(|˜µk|,|˜νk|)) r−1∑ k=1 (|˜νk|−min(|˜µk|,|˜νk|)) ≤3∥c∥∞max(∥λ∗−˜λµ∥1,∥λ∗−˜λν∥1) 21and we obtain that with a probability of at least 1 −δ, ⏐⏐⏐⏐⏐⏐ 1 ˜λr r−1∑ k,q=1 ∫ Z2 ( 1 −min(|˜µk|,|˜νk|) |˜µk| )( 1 −min(|˜µq|,|˜νq|) |˜νq| ) c(x,y)d˜µk(x)d˜νq(y) ⏐⏐⏐⏐⏐⏐ ≤3∥c∥∞ [√r n + √ 2 ln(2/δ) n ] Then by applying a union bound we obtain that with a probability of at least 1 −δ ⏐⏐⏐⏐ ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] ⏐⏐⏐⏐≤∥c∥∞  11 √r n + 12 √ 2 ln 40/δ n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n× min 1≤i≤r λ∗ i   Now observe that LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤ ∫ Z2 c(x,y)d˜π(x,y) − ∫ Z2 c(x,y)dπ∗(x,y) ≤ ∫ Z2 c(x,y)d[˜π−ˆπ](x,y) + ∫ Z2 c(x,y)d[ˆπ−π∗](x,y) and by combining the two control we obtain that with a probability of at least 1 −2δ, LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤∥c∥∞ [ 11 √r n + 12 √ 2 ln 40/δ n + 1 α ( 2 √ log(2/δ) n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n )] ≤11∥c∥∞ √r n + 14∥c∥∞ α √ log(40/δ) n + 2∥c∥∞max(6, √ 2r) log(40/δ) nα where α≜ min 1≤i≤r λ∗ i and the result follows. B.5 Proof Proposition 5 Proposition. Let r ≥1, δ >0 and µ,ν ∈M+ 1 (X). Then there exists a constant Nr,δ such that if n≥Nr,δ then with a probability of at least 1 −2δ, we have LOTr,c(ˆµn,ˆνn) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n . Proof. We consider the same notations as in the proof of Proposition 4. In particular let us deﬁne for all (xi,zi)n i=1,(yi,z′ i) ∈(Z× [|1,r|])n, g((x1,z1),..., (xn,zn),(y1,z′ 1),..., (yn,z′ n)) ≜ r∑ q=1 1 λ∗qn2 ∑ i,j c(xi,yj)1zi=q1z′ j=q , Recall that we have |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|≤ ∥c∥∞ n2 [∑n j=1 1z′ j=zk λ∗zk + ∑n j=1 1z′ j=˜zk λ∗ ˜zk ] ≤2∥c∥∞ n max ( ˜λµ λ∗  ∞ ,  ˜λν λ∗  ∞ ) ≤2∥c∥∞ n + 2∥c∥∞ n× min 1≤i≤r λ∗ i max ( ∥˜λµ −λ∗∥∞,∥˜λν −λ∗∥∞ ) In fact if we have a control in probability of the bounded difference we can use an extension of the McDiarmid’s Inequality. For that purpose let us ﬁrst introduce the following deﬁnition. 22Deﬁnition 4. Let (Xi)m i=1, mindependent random variables and ga measurable function. We say that g is weakly difference-bounded with respect to (Xi)m i=1 by (b,β,δ ) if P(|g(X1,...,X m) −g(X′ 1,...,X ′ m)|≤ β) ≥1 −δ with X′ i = Xi except for one coordinate kwhere X′ k is an independent copy of Xk. Furthermore for any (xi)m i=1 and (x′ i)m i=1 where for all coordinate except on xj = x′ j |g(x1,...,x m) −g(x′ 1,...,x ′ m)|≤ b. Let us now introduce an extension of McDiarmid’s Inequality [Kutin, 2002]. Theorem 1. Let (Xi)m i=1, mindependent random variables and g a measurable function which is weakly difference-bounded with respect to (Xi)m i=1 by (b,β/m, exp(−Km)), then if 0 < τ≤ T(b,β,K ) and m≥M(b,β,K,τ ), then P(|g(X1,...,X m) −E(g(X1,...,X m))|≥ τ) ≤4 exp (−τ2m 8β2 ) where T(b,β,K ) ≜ min (14c 2 ,4β √ K, β2K b ) M(b,β,K,τ ) ≜ max (b β,β √ 40,3 (24 K + 3 ) log (24 K + 3 ) ,1 τ ) Given the above Theroem we can obtain an asymptotic control of the deviation ofgfrom its mean. Let δ′>0 and let us denote m≜ 2n b≜ 2∥c∥∞ n× min 1≤i≤r λ∗ i K ≜ log(1/δ′) 2n β ≜ 4∥c∥∞  1 + 1 min 1≤i≤r λ∗ i √ 2 log(4/δ′) n   Observe now that with a probability of at least 1 −exp(−Km) |g(..., (xk,zk),... ) −g(..., (˜xk,˜zk),... )|≤ 2∥c∥∞ n  1 + 1 min 1≤i≤r λ∗ i √ 2 log(4/δ′) n   Let us now ﬁx δ >0 and let us choose δ′such that δ′≜ 4/nand τ ≜ β √ 4 log(4/δ) n , then we obtain that for nsufﬁciently large (such that n≥M(b,β,K,τ )/2 and τ ≤T(b,β,K )), we have that with a probability of at least 1 −δ ⏐⏐⏐⏐LOTr,c(µ,ν) − ∫ Z2 c(x,y)dˆπ(x,y) ⏐⏐⏐⏐≤4∥c∥∞  1 + 1 min 1≤i≤r λ∗ i √ 2 log(n) n   √ 4 log(4/δ) n ≤4∥c∥∞ √ 4 log(4/δ) n + 16 √ 5∥c∥∞ √ log(n) log(4/δ) n× min 1≤i≤r λ∗ i Recall also from the proof of Proposition 4, that we have with a probability of at least 1 −δ ⏐⏐⏐⏐ ∫ Z2 c(x,y)d[ˆπ(x,y) −˜π(x,y)] ⏐⏐⏐⏐≤∥c∥∞  11 √r n + 12 √ 2 ln 40/δ n + 6 ln(40/δ) + 2 √ 2rln(40/δ) n× min 1≤i≤r λ∗ i   23Finally by imposing in addition that √ n log(n) ≥ 1 min 1≤i≤r λ∗ i , √n≥ √ log(40/δ) min 1≤i≤r λ∗ i and √n≥ √r min 1≤i≤r we obtain that for nis large enough (such that (such that n≥M(b,β,K,τ )/2 and τ ≤T(b,β,K )) and satysﬁng the above inequalities, we have with a probability of at least 1 −2δthat LOTr,c(ˆµ,ˆν) −LOTr,c(µ,ν) ≤11∥c∥∞ √r n + 77∥c∥∞ √ log(40/δ) n B.6 Proof Proposition 6 Proposition. Let µ,ν ∈M+ 1 (X). Let us assume that cis symmetric, then we have DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) . If in addition we assume the cis Lipschitz w.r.t toxand y, then we have DLOTr,c(µ,ν) − −−−− → r→+∞ OTc(µ,ν) . Proof. When r= 1, it is clear that for any µ,ν ∈M+ 1 (X), Πr(µ,ν) = {µ⊗ν}and thanks to the symmetry of c, we have directly that DLOT1,c(µ,ν) = 1 2 ∫ X2 −c(x,y)d[µ−ν] ⊗d[µ−ν](x,y) = 1 2MMD−c(µ,ν) . The limit is a direct consequence of Proposition 2. B.7 Proof of Proposition 8 Proposition. Let r≥1 and (µn)n≥0 and (νn)n≥0 two sequences of probability measures such that µn →µand νn →νwith respect to the convergence in law. Then we have that LOTr,c(µn,νn) →LOTr,c(µ,ν) . Proof. Let us denote πan optimal solution of LOTr,c(µ,ν) and let us denote (µ(i))r i=1, (ν(i))r i=1 and (λ(i))r i=1 the decomposition associated. In the following Lemma, we aim at building speciﬁc decompositions of the sequences (µn)n≥0 and (νn)n≥0. Lemma 1. Let r ≥1, µ ∈ M+ 1 (X) and (µ(i))r i=1 ∈ M+ 1 (X) and (λ(i))r i=1 ∈∆∗ r such that µ= ∑r i=1 λiµ(i). Then for any sequence of probability measures (µn)≥0 such that µn →µ, there exist for all i∈[|1,r|] a sequence of nonnegative measures (µ(i) n )n≥0 such that µ(i) n →λiµ(i) for all i∈[|1,r|] and r∑ i=1 µ(i) n = µn for all n≥0 Proof. For r = 1 the result is clear. Let us now show the result for r = 2. Let us denote (˜µ(1) n ) a sequence converging weakly towards λ1µ(1). Then by denoting µ(1) n ≜ µn −(µn −˜µ(1) n )+ where (·)+ correspond to the non-negative part of the measure, we have that µ(1) n ≥0, µ(1) n →λ1µ(1), µ(2) n ≜ µn −µ(1) n ≥0, µ(2) n →λ2µ(2) and µn = µ(1) n + µ(2) n for all n≥0 24which is the result. Let r≥2 and let us assume that the result holds for all 1 ≤k≤r. Let us now consider a decomposition of µsuch that µ = ∑r+1 i=1 λiµ(i). By denoting ˜µ(1) ≜ ∑r i=1 λiµ(i) ∑r i=1 λi , we obtain that µ= ( r∑ i=1 λi ) ˜µ(1) + λr+1µ(r+1) . Then by recursion we have that there exists sequences of nonnegative measures (˜µ(1) n ) and (µ(r+1) n ) such that ˜µ(1) n → ( r∑ i=1 λi ) ˜µ(1), µ(r+1) n →λr+1µ(r+1) and µn = ˜µ(1) n + µ(r+1) n for all n≥0 Now observe that ˜µ(1) n |˜µ(1) n | →˜µ(1) = ∑r i=1 λi∑r i=1 λi µ(i). Therefore applying the recursion on this problem allows us to obtain a decomposition of ˜µ(1) n of the form ˜µ(1) n |˜µ(1) n | = r∑ i=1 µ(i) n where µ(i) n ≥0 and µ(i) n → λi∑r i=1 λi µ(i) . Therefore we obtain that µn = r∑ i=1 |˜µ(1) n |µ(i) n + µ(r+1) n where µ(i) n ≥0, |˜µ(1) n |µ(i) n →λiµ(i) for all i∈[|1,r|] and µ(r+1) n ≥0, µ(r+1) n →λr+1µ(r+1) from which follows the result. Let us now consider such decompositions of (µn)n≥0 and (νn)n≥0 such that each factor converges toward the target decomposition of µ. Now let us build the following coupling: ˜πn ≜ r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |µ(k) n ||ν(k) n | µ(k) n ⊗µ(k) n + 1 1 −∑r−1 k=1 min(|µ(k) n |,|ν(k n |) [ |µn|− r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |µ(k) n | µ(k) n ] ⊗ [ νn − r−1∑ k=1 min(|µ(k) n |,|ν(k) n |) |ν(k) n | ν(k) n ] with the convention that 0 0 = 0. Now it is easy to check that ˜πn ∈Πr(µn,νn), and we have that LOTr,c(µn,νn) ≤ ∫ X2 d(x,y)d˜πn(x,y) →LOTr,c(µ,ν) and by Prokhorov’s theorem and the optimality of the limit of (˜πn)n≥0 (up to an extraction) we obtain that LOTr,c(µn,νn) →LOTr,c(µ,ν). B.8 Proof Proposition 7 Proposition. Let r ≥1, and let us assume that cis a semimetric of negative type. Then for all µ,ν ∈M+ 1 (X), we have that DLOTr(µ,ν) ≥0 . In addition, if chas strong negative type then we have also that DLOTr,c(µ,ν) = 0 ⇐⇒µ= ν and µn →µ ⇐⇒DLOTr,c(µn,µ) →0 . where the convergence of the sequence of probability measures considered is the convergence in law. 25Proof. Let π∗solution of LOTr,c(µ,ν). Then there exists λ∗∈∆∗ r, (µ∗ i)r i=1,(ν∗ i)r i=1 ∈M+ 1 (X)r such that π∗= r∑ i=1 λ∗ iµ∗ i ⊗ν∗ i. Note that by deﬁnition, we have that µ= r∑ i=1 λ∗ iµ∗ i and ν = r∑ i=1 λ∗ iν∗ i, By deﬁnition we have also that LOTr,c(µ,µ) ≤ r∑ k=1 λ∗ k ∫ X2 c(x,y)dµ∗ k ⊗µ∗ k similarly for LOTr,c(ν,ν) we have LOTr,c(ν,ν) ≤ r∑ k=1 λ∗ k ∫ X2 c(x,y)dν∗ k ⊗ν∗ k Therefore we have DLOTr,c(µ,ν) ≥ r∑ k=1 λ∗ k (∫ X2 c(x,y)dµ∗ k ⊗ν∗ k −1 2 [∫ X2 c(x,y)dµ∗ k ⊗µ∗ k + ∫ X2 c(x,y)dν∗ k ⊗ν∗ k ]) ≥ r∑ k=1 λ∗ k ∫ X2 −c(x,y)d[µ∗ k −ν∗ k] ⊗[µ∗ k −ν∗ k] ≥ r∑ k=1 λ∗ k 2 Dc(µ∗ k,ν∗ k) where for any any probability measures α,β on Xwe deﬁne Dc(α,β) ≜ 2 ∫ X2 c(x,y)dα⊗β− ∫ X2 c(x,y)dα⊗α− ∫ X2 c(x,y)dβ⊗β However, as cis assumed to have a negative type, we have that Dc(µ∗ k,ν∗ k) ≥0 ∀k In addition if we assume that chas a strong negative type, then we obtain directly that DLOTr,c(µ,ν) = 0 =⇒ µ∗ k = ν∗ k ∀k. Let us now show that DLOTr,c metrize the convergence in law. The direct implication is a direct consequence of the Proposition 8. Conversely, if DLOTr,c(µn,µ) →0, then by compacity of X and thanks to the Prokhorov’s theorem we can extract a subsequence of µn →µ∗, and thanks to Proposition 8, we also obtain that DLOTr,c(µn,µ) →DLOTr,c(µ∗,µ). Finally we deduce that DLOTr,c(µ∗,µ) = 0 and µ∗= µ. B.9 Proof Proposition 9 Proposition. Let n≥k≥1, X ≜ {x1,...,x n}⊂X and a∈∆∗ n. If cis a semimetric of negative type, then by denoting C = (c(xi,xj))i,j, we have that LOTk,c(µa,X,µa,X) = min Q ⟨C,Qdiag(1/QT1n)QT⟩s.t. Q∈Rn×k + , Q1k = a. (16) Proof. First remarks that one can reformulate the LOTk,c problem as LOTk,c(µ,µ) ≜ min g∈∆∗ k min (x,y)∈K2a,g k∑ i=1 xT i Cyi gi 26where Ka,g ≜ {x ∈Rnk s.t. Ax = [a,g]T, x ≥0} A≜ ( 1T n ⊗Ik IT n ⊗1k ) and xi ≜ [x(i−1)×n+1,...,x i×n]T, yi ≜ [y(i−1)×n+1,...,y i×n]T for all i∈[|1,k|] Indeed the above optimization problem is just a reformulation of LOTk,c(µ,µ) where we have vectorized the couplings in a column-wise order. Let us now show the following lemma from which the result will follow. Lemma 2. Under the same assumption of Proposition 9 we have that for all g∈∆∗ k min (x,y)∈K2a,g k∑ i=1 xT i Cyi gi = min x∈Ka,g k∑ i=1 xT i Cxi gi Proof. Let (x∗,y∗) solution of the LHS optimization problem. Then we have that k∑ i=1 (x∗ i)TCx∗ i gi ≥ k∑ i=1 (x∗ i)TCy∗ i gi k∑ i=1 (y∗ i)TCy∗ i gi ≥ k∑ i=1 (x∗ i)TCy∗ i gi Therefore we obtain that 0 ≤ k∑ i=1 (x∗ i)TCx∗ i gi − k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (x∗ i)TC(x∗ i −y∗ i) gi 0 ≤ k∑ i=1 (y∗ i)TCy∗ i gi − k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (y∗ i −x∗ i)TCy∗ i gi Then by symmetry of C, we obtain by adding the two terms that k∑ i=1 (x∗ i −y∗ i)TC(x∗ i −y∗ i) gi ≥0 However, thanks to the linear constraints, we have that for alli∈[|1,k|], n−1∑ q=0 x∗ (i−1)×n+1+q = n−1∑ q=0 y∗ (i−1)×n+1+q = gi Therefore (x∗ i −y∗ i)T1n = 0 and thanks to the negativity of the cost function cwe obtain that (x∗ i −y∗ i)TC(x∗ i −y∗ i) ≤0 Therefore we have that (xi −yi)TC(xi −yi) = 0 from which follows that k∑ i=1 (x∗ i)TCx∗ i gi = k∑ i=1 (x∗ i)TCy∗ i gi = k∑ i=1 (y∗ i)TCy∗ i gi and the result follows. As the above result holds for any g∈∆∗ k, we obtain that LOTk,c(µ,µ) = min g∈∆∗ k min x∈Ka,g k∑ i=1 (x∗ i)TCx∗ i gi Then by formulating back this problem in term of matrices, we obtain that LOTk,c(µ,µ) = min g∈∆∗ k min Q∈Πa,g ⟨C,Qdiag(1/g)QT⟩ from which the result follows. 27C Additional Experiments C.1 Comparison of the γschedules 106 107 108 109 Operations 0.45 0.50 0.55 0.60 0.65LOT cost Uniform Distributions Adaptive :  = 1 Adaptive :  = 10 Adaptive :  = 100 Constant:  = 1 Constant:  = 10 Constant:  = 100 105 106 107 108 109 1010 Operations 2 4 6 Mixture of Gaussians Constant:  = 0.1 Constant:  = 1 Constant:  = 10 Figure 5: In this experiment, we compare two strategies for the choice of the step-size in the MD scheme proposed by Scetbon et al. [2021] on two different problems. More precisely, we compare the constant γschedule with the proposed adaptive one and compare them when the distributions are sampled from either uniform distributions (left) or mixtures of anisotropic Gaussians (right). We show that the range of admissible γwhen considering a constant schedule varies from one problem to another. Indeed, in the right plot, we observe that the algorithm converges only when γ ≤1, while in the left plot, the algorithm manages to converge for γ ≤100. We also observe that our adaptive strategy allows to have a consistent choice of admissible values for γwhatever the problem considered. It is worth noticing that whatever the γchosen, the algorithm converges towards the same value, however the larger γis chosen in its admissible range, the faster the algorithm converges. C.2 Gradient Flows between two Moons DLOT, r=100 LOT, r=100 Figure 6: We compare the gradient ﬂows (µt)t≥0 (in red) starting from a moon shape distribution, µ0, to another moon shape distribution (in blue),ν, in 2D when minimizing eitherL(µ) ≜ DLOTr,c(µ,ν) or L(µ) ≜ LOTr,c(µ,ν). The ground cost is the squared Euclidean distance and we ﬁx r = 100. We consider 1000 samples from each distribution and and we plot the evolution of the probability measure obtained along the iterations of a gradient descent scheme. We also display in green the vector ﬁeld in the descent direction. We show that the debiased version allows to recover the target distribution while LOTr,c is learning a biased version with a low-rank structure. 28",
      "meta_data": {
        "arxiv_id": "2205.12365v2",
        "authors": [
          "Meyer Scetbon",
          "Marco Cuturi"
        ],
        "published_date": "2022-05-24T20:51:37Z",
        "pdf_url": "https://arxiv.org/pdf/2205.12365v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper addresses the need for scalable Optimal Transport (OT) solvers for large datasets by advancing the understanding and practical application of Low-rank Optimal Transport (LOT). The main contributions include deriving the rate of convergence of LOT to true OT, providing the first statistical complexity upper-bound for LOT's plug-in estimator (O(sqrt(1/n)) independent of dimension), introducing a debiased LOT (DLOTr,c) that is non-negative, metrizes weak convergence, and interpolates between MMD and OT. It also establishes links between low-rank factorization bias and clustering methods (generalizing k-means) and proposes practical strategies for adaptive step-length tuning and improved initialization for LOT algorithms.",
        "methodology": "The core methodology involves constraining the discrete OT problem to couplings with low-nonnegative rank. The problem (Eq. 3) is reformulated (Eq. 4) and solved using a mirror descent scheme. The paper extends LOT for arbitrary probability measures by factorizing couplings into a sum of Kronecker products. Approximation error is analyzed using entropy numbers. For statistical analysis, a plug-in estimator for LOT is considered. A debiased version, DLOTr,c, is introduced to ensure desirable properties for machine learning applications, showing its relation to MMD and OT. Furthermore, LOT's self-matching cost (LOTk,c(µ,µ)) is interpreted as a generalized k-means clustering method. Practical improvements to the mirror descent algorithm include an adaptive step-size strategy (Eq. 8) that normalizes by the squared dual-norm of the gradient and novel initialization methods based on clustering (k-means for Euclidean space (Eq. 9) and generalized k-means for non-Euclidean cases (Eq. 10)).",
        "experimental_setup": "Experiments were conducted on 3 synthetic problems and 1 real-world dataset (Newsgroup20 dataset, text embedded into 50D distributions). For statistical rates, two independent empirical measures from a mixture of 10 anisotropic Gaussians on Rd were used, with varying sample sizes (n) and dimensions (d), considering multiple ranks (r). Gradient flows of DLOTr,c and LOTr,c were compared by minimizing losses from a Gaussian distribution to a moon shape distribution in 2D, with r=100 and 1000 samples. Clustering applications utilized 6 scikit-learn datasets, comparing squared Euclidean distance (k-means) with shortest-path distance on a graph, for fixed ranks (r=2, 3, or 4). Initialization effects were studied on the Newsgroup20 dataset with approximately 250 samples, comparing k-means, generalized k-means, rank-2, and random initializations, tracking LOT cost and stopping criterion over algebraic operations. All experiments were run on a MacBook Pro 2019 laptop.",
        "limitations": "Existing entropic regularized OT solvers scale quadratically, posing computational challenges. While low-rank approaches show empirical promise, their theoretical properties were not well-established prior to this work. The LOT optimization problem is non-convex, meaning convergence can be sensitive to initialization and potentially lead to spurious local minima. The provided statistical upper bound for the plug-in estimator does not have a matching lower bound, thus a complete statistical complexity result is not achieved. The constant Kr in Proposition 4 is not explicitly controlled in the general setting, requiring additional assumptions. The fixed gamma schedule for mirror descent in previous work requires problem-specific tuning, as the admissible range varies.",
        "future_research_directions": "Future research should focus on further investigating the empirical behavior of the LOT estimator. Specific areas include developing methods to find suitable local minima for the non-convex LOT problem and exploring alternative adaptive strategies for step-size selection to improve the convergence of the mirror descent scheme. Additionally, a key theoretical direction is to derive a lower bound for the statistical complexity that matches the upper bound presented in this paper, to provide a more complete understanding of LOT's statistical properties."
      }
    },
    {
      "title": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
      "abstract": "Neural networks have achieved tremendous success in a large variety of\napplications. However, their memory footprint and computational demand can\nrender them impractical in application settings with limited hardware or energy\nresources. In this work, we propose a novel algorithm to find efficient\nlow-rank subnetworks. Remarkably, these subnetworks are determined and adapted\nalready during the training phase and the overall time and memory resources\nrequired by both training and evaluating them are significantly reduced. The\nmain idea is to restrict the weight matrices to a low-rank manifold and to\nupdate the low-rank factors rather than the full matrix during training. To\nderive training updates that are restricted to the prescribed manifold, we\nemploy techniques from dynamic model order reduction for matrix differential\nequations. This allows us to provide approximation, stability, and descent\nguarantees. Moreover, our method automatically and dynamically adapts the ranks\nduring training to achieve the desired approximation accuracy. The efficiency\nof the proposed method is demonstrated through a variety of numerical\nexperiments on fully-connected and convolutional networks.",
      "full_text": "LOW-RANK LOTTERY TICKETS : FINDING EFFICIENT LOW -RANK NEURAL NETWORKS VIA MATRIX DIFFERENTIAL EQUATIONS Steffen Schotthöfer∗ Karlsruhe Institute of Technology 76131 Karlsruhe (Germany) steffen.schotthoefer@kit.edu Emanuele Zangrando∗ Gran Sasso Science Institute 67100 L’Aquila (Italy) emanuele.zangrando@gssi.it Jonas Kusch University of Innsbruck 6020 Innsbruck (Austria) jonas.kusch1@gmail.com Gianluca Ceruti EPF Lausanne 1015 Lausanne (Switzerland) gianluca.ceruti@epfl.ch Francesco Tudisco Gran Sasso Science Institute 67100 L’Aquila (Italy) francesco.tudisco@gssi.it ABSTRACT Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to ﬁnd efﬁcient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are signiﬁcantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efﬁciency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks. 1 Introduction While showing great performance in terms of classiﬁcation records, most state-of-the-art neural networks require an enormous amount of computation and memory storage both for the training and the evaluation phases [27]. These requirements not only increase infrastructure costs and energy consumption, but also make the deployment of artiﬁcial neural networks to infrastructures with limited resources such as mobile phones or smart devices prohibitive. On the other hand, it is well-known that networks’ weights contain structures and redundancies that can be exploited for reducing the parameter space dimension without signiﬁcantly affecting the overall accuracy [9, 3, 17]. Network pruning is a popular line of research that addresses this problem by removing redundant pa- rameters from pre-trained models. Typically, the initial network is large and accurate, and the goal is to produce a smaller network with similar accuracy. Methods within this area include weight sparsiﬁcation [20, 24, 49] and quantization [60, 10], with different pruning techniques, including search-based heuris- tics [24], reinforcement learning [2, 23] and genetic algorithms [43]. More recent work has considered ∗These authors contributed equally to this work. arXiv:2205.13571v2  [cs.LG]  18 Oct 2022pruning during training, by formulating pruning as a data-driven optimization problem [20, 26, 27]. The resulting “dynamical pruning” boils down to a parameter-constrained training phase which, however, has been mostly focused on requiring sparse or binary weights so far. Rather than enforcing sparsity or binary variables, in this work we constrain the parameter space to the manifold of low-rank matrices. Neural networks’ parameter matrices and large data matrices in general are seldom full rank [53, 56, 48, 15]. Constraining these parameters to lie on a manifold deﬁned by low-rank matrices is thus a quite natural approach. By interpreting the training problem as a continuous-time gradient ﬂow, we propose a training algorithm based on the extension of recent Dynamical Low-Rank Approximation (DLRA) algorithms [5, 6, 4]. This approach allows us to use low-rank numerical integrators for matrix Ordinary Differential Equations (ODEs) to obtain modiﬁed forward and backward training phases that only use the small-rank factors in the low-rank representation of the parameter matrices and that are stable with respect to small singular values. This is a striking difference with respect to recent alternative “vanilla” low-rank training schemes [57, 31] which simply factorize the weight matrices as the product of two low-rank factors UV ⊤and apply a descent algorithm alternatively on the two variables U and V. We perform several experimental evaluations on fully-connected and convolutional networks showing that the resulting dynamical low-rank training paradigm yields low-parametric neural network architectures which compared to their full-rank counterparts are both remarkably less-demanding in terms of memory storage and require much less computational cost to be trained. Moreover, the trained low-rank neural networks achieve comparable accuracy to the original full architecture. This observation is reminiscent of the so-called lottery tickets hypothesis — dense neural networks contain sparse subnetworks that achieve high accuracy [17] — and suggests the presence of low-rank winning tickets: highly-performing low-rank subnetworks of dense networks. Remarkably, our dynamical low-rank training strategy seems to be able to ﬁnd the low-rank winning tickets directly during the training phase independent of initialization. 2 Related work on low-rank methods Low-rank factorization using the SVD and other matrix decomposition techniques have been extensively studied in the scientiﬁc computing and machine learning communities. The challenge of compressing and speeding up large-scale neural networks using low-rank methods has sparked wide-spread research interest in recent years and signiﬁcant effort has been put towards developing low-rank factorization strategies for deep neural networks. Previous works can roughly be categorized in approaches with ﬁxed low rank and variable low rank during training time. Fixed rank approaches decompose weight matrices using SVD or tensor decompositions of pre-trained networks and ﬁne-tune the factorized network [50, 12, 55, 38], constrain weight matrices to have a ﬁxed low rank during training [30, 57, 31], or create layers as a linear combination of layers of different rank [29]. Hence, these methods introduce the rank of the matrix decomposition as another hyperparameter to be ﬁne-tuned. Rank-adaptive methods mitigate this issue by automatic determination and adaption of the low-rank structure after training. In particular, [33, 34] apply heuristics to determine the rank of the matrix decomposition ahead of time, whereas [ 59] encourages low-rank weights via a penalized loss that depends on approximated matrix ranks. Few methods have been proposed recently that adapt the ranks of the weight matrix alongside the main network training phase. In [ 40], the authors set up the neural network training as a constrained optimization problem with an upper bound on the ranks of the weights, which is solved in an alternating approach resulting in an NP-hard mixed integer program. The authors of [ 28] formulate a similar constrained optimization problem resulting in a mixed discrete-continuous optimization scheme which jointly addresses the ranks and the elements of the matrices. However, both these approaches require knowledge of the full weight matrix (and of its singular value decomposition) during training and overall are more computational demanding than standard training. In this work we overcome the above issues and propose a training algorithm with reduced memory and computational requirements. To this end, we reinterpret the optimization problem of a neural network as a gradient ﬂow of the network weight matrices and thus as a matrix ODE. This continuous formulation allows us to use recent advances in DLRA methods for matrix ODEs which aim at evolving the solution 2of the differential equation on a low-rank manifold. The main idea of DLRA [ 35], which originates from the Dirac-Frenkel variational principle [14, 18], is to approximate the solution through a low-rank factorization and derive evolution equations for the individual factors. Thereby, the full solution does not need to be stored and the computational costs can be signiﬁcantly reduced. To ensure robustness of the method, stable integrators have been proposed in [44] and [6]. Instead of evolving individual low-rank factors in time, these methods evolve products of low-rank factors, which yields remarkable stability and exactness properties [32], both in the matrix and the tensor settings [36, 46, 45, 8]. In this work, we employ the “unconventional” basis update & Galerkin step integrator [6] as well as its rank-adaptive extension [4], see also [37, 7]. The rank-adaptive unconventional integrator chooses the approximation ranks according to the continuous-time training dynamics and allows us to ﬁnd highly-performing low-rank subnetworks directly during the training phase, while requiring reduced training cost and memory storage. 3 Low-rank training via gradient ﬂow Consider a feed-forward fully-connected neural network N(x) = zM, with z0 = x ∈ Rn0 , zk = σk(Wkzk−1 + bk) ∈ Rnk, k = 1 ,...,M (the convolutional setting is discussed in the sup- plementary material §6.6). We consider the training of Nbased on the optimization of a loss function L(W1,...,W M; N(x),y) by means of a gradient-based descent algorithm. For example, when using gradient descent, the weights of Nat iteration t∈N are updated via Wt+1 k = Wt k −λ∇WkL(W1,...,W M; N(x),y) ∀k= 1,...,M (1) with a learning rate λ. When the weight matrices Wk are dense, both the forward and gradient evaluations of the network require a large number of full matrix multiplications, with high computational expense and large memory footprint. This renders the training and the use of large-scale neural networks a difﬁcult challenge on limited-resource devices. At the same time, a wealth of evidence shows that dense networks are typically overparameterized and that most of the weights learned this way are unnecessary [48, 15]. In order to reduce the memory and computation costs of training, we propose a method that performs the minimization over the manifold of low-rank matrices. To this end, we assume that the ideal Wk can be well-approximated by a matrix of rank rk ≪nk,nk+1 of the form UkSkV⊤ k ∈Rnk×nk−1 , where Uk ∈Rnk×rk, Vk ∈Rnk−1×rk are thin and tall matrices having orthonormal columns spanning optimal subspaces which capture essential properties of parameters, and Sk ∈Rrk×rk is a tiny full-rank matrix that allows us to extrapolate the useful information from the learned subspaces Uk and Vk. Traditional descent algorithms such as (1) do not guarantee the preservation of the low-rank structure UkSkV⊤ k when updating the weights during training and require knowledge of the whole Wk rather than the factors Uk,Sk,Vk. Here we reinterpret the loss minimization as a continuous-time gradient ﬂow and derive a new training method that overcomes all aforementioned limitations. Minimizing the loss function with respect to Wk is equivalent to evaluating the long time behaviour of the following matrix ODE that allows us to interpret the training phase as a continuous process: ˙Wk(t) = −∇WkL(W1,...,W M; N(x),y), (2) where the “dot” denotes the time-derivative. LetMrk denote the manifold of matrices with rank rk and assume at a certain time t0 the weights are in the manifold, i.e., Wk(t0) ∈Mrk. Using this continuous- time interpretation allows us to derive a strategy to evolve the weights according to the dynamics in (2) so that Wk(t) ∈Mrk for all t ≥t0. To this end, in §3.1 we exploit the fact that Wk(t) admits a time-dependent factorization [13] Wk(t) = Uk(t)Sk(t)Vk(t)⊤to rewrite (2) as a system of matrix ODEs for each of the individual factors Uk,Sk and Vk. Then, in §4 we propose an algorithm to efﬁciently integrate the system of ODEs. We show in §4.1 that such algorithm reduces the loss monotonically and is accurate, in the sense that UkSkV⊤ k ≈Wk, i.e. the learned low-dimensional subspaces Uk and Vk well-match the behaviour of the full-rank network Wk solution of (2), through the action of the learned Sk. Remarkably, using the dynamics of the individual factors will also allow us to adaptively adjust the rank rk throughout the continuous-time training process. 33.1 Coupled dynamics of the low-rank factors via DLRA We consider the dynamical system of a single weight matrix Wk, while the remaining weight matrices are ﬁxed in time and are treated as parameters for the gradient. In the following, we omit writing these parameters down for efﬁciency of exposition. Assuming Wk(t) ∈Mrk, we can formulate (2) as min { ∥˙Wk(t) + ∇WkL(Wk(t))∥F : ˙Wk(t) ∈TWk(t)Mrk } (3) where TWk(t)Mrk is the tangent space of Mrk at position Wk(t). In order to solve (3), we further observe that (3) can be equivalently formulated as the following Galerkin condition [35]: ⟨˙Wk(t) + ∇WkL(Wk(t)),δWk⟩= 0 ∀δWk ∈TWk(t)Mrk . (4) From Wk = UkSkV⊤ k , a generic element δWk of the tangent space TWk(t)Mrk can be written as δWk = δUkSkV⊤ k + UkδSkV⊤ k + UkSkδV⊤ k , where δUk and δVk are generic elements of the tangent space of the Stiefel manifold with rk orthonormal columns at the points Uk and Vk, respectively, and δSk is a generic rk ×rk matrix, see e.g. [35, §2] for details. Additionally, the Gauge conditions U⊤ k δUk = 0 and V⊤ k δVk = 0 must be imposed to ensure orthogonality of the basis matrices, and the uniqueness of the representation of the tangent space elements. Similarly, by the chain rule applied several times we have ˙Wk = d dt { UkSkV⊤ k } = ˙UkSkV⊤ k + Uk ˙SkV⊤ k + UkSk ˙V⊤ k . Now, the Galerkin condition (4) becomes ⟨˙UkSkV⊤ k + Uk ˙SkV⊤ k + UkSk ˙V⊤ k + ∇WkL(Wk(t)),δWk⟩= 0, ∀δWk ∈TWk(t)Mrk (5) with U⊤ k ˙Uk = 0 and V⊤ k ˙Vk = 0. If we choose δWk = UkδSkV⊤ k in (5), we obtain ⟨U⊤ k ˙UkSkV⊤ k Vk + U⊤ k Uk ˙SkV⊤ k Vk + U⊤ k UkSk ˙V⊤ k Vk + U⊤ k ∇WkL(Wk(t))Vk,δSk⟩= 0 . Thus, using the Gauge conditions, we obtain ⟨˙Sk + U⊤ k ∇WkL(Wk(t))Vk,δSk⟩= 0, which has to hold for a generic rk ×rk matrix δSk. We obtain this way an evolution equation for the Sk(t) factor. Similarly, specifying (5) for the two choices δWk = δUkSkV⊤ k and δWk = UkSkδV⊤ k , allows us to obtain the following system of differential equations for the individual factors of Wk:    ˙Sk = −U⊤ k ∇WkL(Wk(t))Vk , ˙Uk = −(I−UkU⊤ k )∇WkL(Wk(t))VkS−1 k , ˙Vk = −(I−VkV⊤ k )∇WkL(Wk(t))⊤UkS−⊤ k . (6) 4 KLS-based training algorithm In order to perform an efﬁcient and robust rank-constrained training step, we numerically integrate the system of ODEs (6). Our approach is based on the “unconventional KLS integrator” [ 6] and its rank-adaptive version [4]. The pseudocode of the proposed training strategy is presented in Algorithm 1. The main idea of the KLS algorithm is to alternately represent the product Wk = UkSkV⊤ k as Wk = KkV⊤ k and Wk = UkL⊤ k, consider the corresponding coupled ODEs from (6), and then perform three main steps: 1,2. K&L-steps (in parallel). Update the current Kk and Lk by integrating the differential equations {˙Kk(t) = −∇WkL(Kk(t)V⊤ k )Vk, K k(0) = UkSk, ˙Lk(t) = −∇WkL(UkLk(t)⊤)⊤Uk, L k(0) = VkS⊤ k , (7) from t= 0 to t= η; then form new orthonormal basis matrices ˜Uk and ˜Vk spanning the range of the computed Kk(η) and Lk(η). 43. S-step. Update the current Sk by integrating the differential equation ˙Sk(t) = −˜U⊤ k ∇WkL( ˜UkSk(t)˜V⊤ k )˜Vk (8) from t= 0 to t= η, with initial value condition Sk(0) = ˜U⊤ k UkSkV⊤ k ˜Vk . An important feature of this algorithm is that it can be extended to rank adaptivity in a relatively straightforward manner [4], letting us dynamically evolve the rank of Sk (and thus the rank of Wk) during the computation. This is particularly useful, as we may expect the weight matrices to have low ranks but we may not know what the “best” ranks for each layer are. Typically, dynamically adapting the ranks of a low-rank optimization scheme is a challenging problem as moving from the manifold Mrk to Mrk±1 introduces singular points [19, 1]. Instead, treating the training problem as a system of matrix differential equations allows us to overcome this issue with a simple trick: at each step of the KLS integrator we double the dimension of the basis matrices ˜Uk and ˜Vk computed in the K- and L-steps by computing orthonormal bases spanning [Kk(η) |Uk] and [Lk(η) |Vk], respectively, i.e. by augmenting the current basis with the basis computed in the previous time step. Then, after the new Sk matrix is computed via the S-step, a truncation step is performed, removing from the newly computed Sk matrix all the singular values that are under a certain threshold ϑ. Of course, adding the rank-adaptivity to the integrator comes at a cost. In that case, each step requires to perform an SVD decomposition of twice the size of the current rank of Sk in order to be able to threshold the singular values. Moreover, the dimension of the bases Uk and Vk may grow, which also may require additional computational effort. However, if the ranks remain small throughout the dynamics, this computational overhead is negligible, as we will further discuss in §4.3 and §5. 4.1 Error analysis and convergence In this section we present our main theoretical results, showing that (a) the low-rank matrices UkSkV⊤ k formed by the weights’ factors computed with Alg. 1 are close to the true solution of (2), and (b) that the loss function decreases during DLRT, provided the singular value threshold ϑis not too large, i.e., is bounded by a constant times the square of the time-step size η (see Theorem 1). In the version we present here, part of the statements are presented in an informal way for the sake of brevity. We refer to the supplementary material §6.1 for details and for the proofs. Assume the gradient ﬂow Fk(Z) = −∇WkL(W1,...,Z,...,W M,N(x),y) in (2) is locally bounded and locally Lipschitz continuous, with constants C1 and C2, respectively. Then, Theorem 1. Fixed xand y, let Wk(t) be the (full-rank) continuous-time solution of (2) and let Uk,Sk,Vk be the factors computed with Algorithm 1 after t steps. Assume that the K,L,S steps (7) and (8) are integrated exactly from 0 to η. Assume moreover that, for any Z ∈Mrk sufﬁciently close to Wk(tη), the whole gradient ﬂow Fk(Z) is “ε-close” to Mrk. Then, ∥UkSkV⊤ k −Wk(tη)∥F ≤c1ε+ c2η+ c3ϑ/η k = 1,...,M where the constants c1, c2 and c3 depend only on C1 and C2. In particular, the approximation bound does not depend on the singular values of the exact nor the approximate solution. Observe that, while the loss function Ldecreases monotonically along any continuous-time solution Wk(t) of (2), it is not obvious that the loss decreases when the integration is done onto the low-rank manifold via Algorithm 1. The next result shows that this is indeed the case, up to terms of the order of the truncation tolerance ϑ. More precisely, we have Theorem 2. Let Wt k = Ut kSt k(Vt k)⊤be the low rank weight matrix computed at step tof Algorithm 1 and let L(t) = L(Wt 1,...,W t M,N(x),y). Then, for a small enough time-step ηwe have L(t+ 1) ≤L(t) −αη+ βϑ where αand βare positive constants that do not depend on t, ηand ϑ. 4.2 Efﬁcient implementation of the gradients All the three K,L,S-steps require the evaluation of the gradient ﬂow of the loss function with respect to the whole matrix Wk. Different approaches to efﬁciently compute this gradient may be used. The strategy we 5Algorithm 1: Dynamic Low Rank Training Scheme (DLRT) Input :Initial low-rank factors S0 k ∼r0 k ×r0 k; U0 k ∼nk ×r0 k;V0 k ∼nk−1 ×r0 k for k= 1,...,M ; iter: maximal number of descent iterations per epoch; adaptive: Boolean ﬂag that decides whether or not to dynamically update the ranks; ϑ: singular value threshold for adaptive procedure. 1 for each epoch do 2 for t= 0 to t= iter do 3 for each layer kdo 4 Kt k ←Ut kSt k /* K-step */ 5 Kt+1 k ←one-step-integrate {˙K(t) = −∇KL(K(t)(Vt k)⊤zk−1 + bt k), K(0) = Kt k } 6 Lt k ←Vt k(St k)⊤ /* L-step */ 7 Lt+1 k ←one-step-integrate {˙L(t) = −∇LL(Ut kL(t)⊤zk−1 + bt k), L(0) = Lt k } 8 if adaptive then /* Basis augmentation step */ 9 Kt+1 k ←[Kt+1 k |Ut k] 10 Lt+1 k ←[Lt+1 k |Vt k] 11 Ut+1 k ←orthonormal basis for the range of Kt+1 k /* S-step */ 12 Mk ←(Ut+1 k )⊤Ut k 13 Vt+1 k ←orthonormal basis for the range of Lt+1 k 14 Nk ←(Vt+1 k )⊤Vt k 15 ˜St k ←MkSt kN⊤ k 16 St+1 k ←one-step-integrate {˙S(t)= −∇SL ( Ut+1 k S(t) (Vt+1 k )⊤zk−1+bt k ) ,S(0)= ˜St k } 17 if adaptive then /* Rank compression step */ 18 P,Σ,Q ←SVD(St+1 k ) 19 St+1 k ←truncate Σ using the singular value threshold ϑ 20 Ut+1 k ←Ut+1 k ˜P where ˜P = [ﬁrst rt+1 k columns of P] 21 Vt+1 k ←Vt+1 k ˜Qwhere ˜Q= [ﬁrst rt+1 k columns of Q] /* Bias update step */ 22 bt+1 k ←one-step-integrate {˙b(t)= −∇bL(Ut+1 k St+1 k (Vt+1 k )⊤zk−1+b(t)),b(0)= bt k } discuss below aims at reducing memory and computational costs by avoiding the computation of the full gradient, working instead with the gradient with respect to the low-rank factors. To this end, we note that for the K-step it holds ∇WkL(Kk(t)V⊤ k )Vk = ∇KkL(Kk(t)V⊤ k ). Hence, the whole gradient can be computed through a forward run of the network with respect to Kk zk = σk ( Kk(t)V⊤ k zk−1 + bk ) , k = 1,...,M (9) and taping the gradient with respect to Kk. In this way, the full gradient does not need to be computed and the overall computational costs are comprised of running a forward evaluation while taping gradients with respect to Kk, analogously to the traditional back-propagation algorithm. The L- and S-steps can be evaluated efﬁciently in the same manner, by evaluating the network while taping the gradients with respect to Lk and Sk, respectively. Hence, instead of a single gradient tape (or chain rule evaluation) of the full weight matrix network, we have three gradient tapes, one for each low rank step, whose combined computational footprint is less than the full matrix tape. We provide detailed formulas for all the three gradient tapes in the supplementary material §6.5. 4.3 Implementation details, computational costs and limitations Each step of Alg. 1 requires the computation of two orthonormal bases for the ranges of Kt+1 k and Lt+1 k . There are of course different techniques to compute such orthonormal matrices. In our implementation 6we use the QR algorithm, which is known to be one of the most efﬁcient and stable approaches for this purpose. In the adaptive strategy the singular values of St+1 k are truncated according to a parameter ϑ. To this end, in our implementation, we use the Frobenius norm of Σ. Precisely, we truncate Σ = diag(σi) at step 19 of Alg. 1 by selecting the smallest principal r×rsubmatrix such that (∑ i≥r+1 σ2 i)1/2 ≤ϑ. Finally, one-step-integrate denotes a numerical procedure that integrates the corresponding ODE from time t= 0 to t= η. In practice one can employ different numerical integrators, without affecting the ability of the algorithm to reduce the loss function (see [4, Thm. 5]) while maintaining the low-rank structure. In our implementation we used two methods: 1. Explicit Euler. This method applied to the gradient ﬂow coincides with one step of Stochastic Gradient Descent (SGD), applied to the three factors Kk,Lk,Sk independently. 2. Adam. Here we formally compute the new factors by modifying the explicit Euler step as in the Adam optimization method. Note that Nesterov accelerated SGD is known to coincide with a particular linear multistep ODE integrator [51]. While Adam does not directly correspond to a numerical integrator to our knowledge, in our tests it resulted in a faster decrease of the loss than both Euler (SGD) and Nesterov accelerated SGD. For both choices, the target time step ηcorresponds to the value of the learning rate, which we set to 0.2 for Euler. For Adam, we use the default dynamical update, setting 0.001 as starting value. Computational cost. To obtain minimal computational costs and memory requirements for the K-step, the ordering of evaluating KkV⊤ k zk−1 in (9) is important. First, we compute ˜z := V⊤ k zk−1 ∈Rrk which requires O(rknk−1) operations. Second, we compute Kk˜z which requires O(rknk) operations. Adding the bias term and evaluating the activation function requires O(nk) operations. Hence, combined over all layers we have asymptotic cost of O(∑ krk(nk + nk+1)). Taping the forward evaluation to compute the gradient with respect to Kk as discussed in §4.2 does not affect the asymptotic costs, i.e. the costs of computing the K-step at layer kassuming a single data point xrequires CK ≲ ∑ krk(nk + nk+1) operations. In a similar manner, we obtain the computational costs of the L- and S-steps, which are again CL,S ≲ ∑ krk(nk + nk+1). Moreover, the QR decompositions used in the K- and L-step require O (∑ kr2 k(nk + nk−1) ) operations and computing the SVD in the truncation step has worst- case cost of O (∑ kr3 k ) . Hence, assuming rk ≪nk,nk+1, the cost per step of our low-rank method is CDLRA ≲ ∑ kr2 k(nk + nk−1), opposed to the dense network training, which requires Cdense ≲∑ knknk+1 operations. In terms of memory cost, note that we only need to store rk(1 + nk + nk+1) parameters per layer during the algorithm, corresponding to the matrices St k,Ut k,V t k. Moreover, at the end of the training we can further compress memory by storing the product of the trained weight factors UkSk, rather than the individual matrices. Limitations. A requirement for DLRT’s efﬁciency is thatrk ≪nk,nk+1. When the truncation threshold ϑis too small, Alg. 1 does not provide advantages with respect to standard training. This is also shown by Fig. 1. Moreover, in the terminology of [54], DLRT is designed to reduce training costs corresponding to model parameters and to the optimizer. To additionally decrease activation costs, DLRT can be combined with micro-batching or checkpointing approaches. Finally, the choice of ϑintroduces one additional hyperparameter which at the moment requires external knowledge for tuning. However, our experiments in §5 show that relatively large values of ϑyield competing performance as compared to a number of baselines, including standard training. 5 Numerical Results We illustrate the performance of DLRT Algorithm 1 on several test cases. The code is implemented in both Tensorﬂow (https://github.com/CSMMLab/DLRANet) and PyTorch (https://github.com/COMPiLELab/DLRT). The networks are trained on an AMD Ryzen 93950X CPU and a Nvidia RTX 3090 GPU. Timings are measured on pure CPU execution. 5.1 Performance analysis on MNIST dataset We partition MNIST dataset [11] in randomly sampled train-validation-test sets of size 50K-10K-10K. Images are pixelwise normalized; no further data augmentation or regularization has been used. 7(a) Training timings  (b) Prediction timings Figure 1: Comparison of batch execution and training times of 5-layer, 5120-neurons low-rank networks of different ranks and a non-factorized reference network with the same architecture on the MNIST dataset. Training times shown correspond to one epoch for a batch of 256 datapoints. Prediction times refer instead to the whole dataset. All the times are the average over 1000 runs. Fixed-rank fully-connected feed-forward network timings. First, we compare the training time of the adaptive DLRT Alg. 1 on a5-layer fully-connected [5120,5120,5120,5120,10] network ﬁxing the ranks of layers 1-4, i.e. choosing a speciﬁc starting rank r0 k for the input weight factors and truncating Σ at line 19 of Alg. 1 to the principal r0 k ×r0 k submatrix, rather than via a threshold. Next, we measure the average prediction time on the whole MNIST dataset over 1000 runs. Fig. 1(a) and 1(b) show that both timings scale linearly with the rank of the factorizations, and that for sufﬁciently small ranks DLRT is faster than the full-rank baseline both in terms of training and of prediction. Rank evolution and performance of DLRT for different singular value thresholds. Next, we demonstrate the capabilities of DLRT to determine the rank of the network’s weight matrices auto- matically during the network training using Algorithm 1. The Adam optimizer with default learning rate is used for the gradient update. We train fully connected5-layer networks, of which the ﬁrst 4 are replaced by low-rank layers in the subsequent tests. The activation function is chosen to be ReLU for the hidden layers, and softmax for the output layer. The training loss is sparse categorical cross entropy and we additionally measure the model’s accuracy. We use batch size256 and train for 250 epochs. We choose ϑ= τ∥Σ∥, thus we truncate the singular values of the current St k by a fraction τ of the total Frobenius norm. The smaller τ, the more singular values are kept. Figures 2 (a) and (b) show the evolution of the rank adaptive layers of a 5-layer 500-neuron network in a long time case study for τ = 0.05 and τ = 0.15. We can see that within the ﬁrst epoch the initial full matrix ranks are reduced signiﬁcantly, to 27 for τ = 0.15, and to ∼85 for τ = 0.05 respectively. Within the ﬁrst 50 epochs, the layer ranks are already close to their ﬁnal ranks. This indicates that the rank adaptive algorithm is only needed for the ﬁrst few training epochs, and can then be replaced by the computationally cheaper ﬁxed-low-rank training (by setting the Boolean variable adaptive to False in Algorithm 1). Figure 3 compares the mean test accuracy of 5-layer networks with 500 and 784 neurons with different levels of low-rank compression, over ﬁve independent runs with randomly sampled train-test-val sets. The networks can be compressed via dynamical low-rank training by more than 95%, while only losing little more than1% test accuracy compared to the dense reference network marked in red. Remark that restricting the space of possible networks to a given rank regularizes the problem, since such a restriction can be understood as adding a PCR regularization term to the loss function. This can be seen from the tendency of not overﬁtting and reaching improved test accuracies compared to the corresponding dense network for moderate compression ratios. Also note that adaptive-low rank training eliminates the need for hyperparameter grid search in terms of layer-weights, due to automatic rank adaptation. The rank dynamics for all conﬁgurations can be seen in the supplementary material §6.3. Finally, in the supplementary material §6.4 we compare the use of DLRT with the vanilla approach which simply thresholds the singular values of the full-rank network. Our results show that advantageous low-rank winning tickets exist, but are not easy to ﬁnd. In fact, the vanilla low-rank subnetworks perform very poorly. From this point of view, our approach can be seen as an efﬁcient dynamical pruning technique, able to determine high-performing low-rank subnetworks in a given dense network. Remarkably, our 8(a) Rank evolution for τ = 0.15  (b) Rank evolution of τ = 0.05 Figure 2: Rank evolution (layers 1-4) of 5-layer [500,500,500,500,10] fully-connected net on MNIST. Figure 3: Mean test accuracy over parameters’ number and compression rate for 5 runs with randomly sampled train-test-val sets on 5-layer fully-connected nets. Red dots denote the full-rank baseline. numerical experiments suggest that low-rank winning tickets can be trained from the start and do not to heavily depend on the initial weight guess. Convolutional layers: LeNet5. Here we compare the proposed dynamical low-rank training scheme on LeNet5 [39] on MNIST, against the full-rank reference and several baselines. SVD prune [ 61] and LRNN [28] are the closest approaches to our DLRT: they dynamically train low-rank layers by adding a rank-penalty to the loss function, and by complementing the standard training step via an SVD projection step in the latter and a pruning step in the former. While computing low-rank factors for each layer, thus reducing memory storage of the network, this training approach is more expensive than training the full network. GAL [42], SSL [62], and NISP [58] are pruning methods which aim at learning optimal sparse weights (rather than low-rank) by adding sparsity-promoting regularization terms to the training loss. As for LRNN, these methods do not reduce the computational cost of the training phase (as indicated with the <0% in Table 1). Analogously to [ 28], our adaptive low-rank training technique is applied to the convolutional layers by ﬂattening the tensor representing the convolutional kernel into a matrix. Details are provided in the supplementary material §6.6. All the models are trained for 120 epochs using SGD with a ﬁxed learning rate of 0.2. Results in Table 1 show that the DLRT algorithm is able to ﬁnd low-rank subnetworks with up to 96.4% less parameters than the full reference, while keeping the test accuracy above 95%. Compared to the baseline methods, we achieve better compression rates but observe lower accuracy. However, unlike the baseline references, DLRT automatically prunes the singular values during training, without requirement to solve any additional optimization problem, thus signiﬁcantly improving time and memory efﬁciency of both forward and backward phases, with respect to the full reference. Robustness with respect to small singular values and comparison with vanilla low-rank parametrization. A direct way to perform training enforcing a ﬁxed rank for the weight matrices is to parameterize each weight as Wk = UkV⊤ k and alternating training with respect to Uk and to Vk. This is the strategy employed for example in [57, 31]. This vanilla low-rank parametrization approach has a number of disadvantages with respect to DLRT, on top of the obvious non-adaptive choice of the rank. First, DLRT guarantees approximation and descent via Theorems 1 and 2. Second, we observe that the vanilla factorization gives rise to an ill-conditioned optimization method when small singular values occur. 90 2 4 6 8 epoch 20 40 60 80 100test accuracy [%] no decay DLRT UVT factorization 0 2 4 6 8 epoch decay DLRT UVT factorization 0 2 4 6 8 epoch 0.0 0.5 1.0 1.5 2.0 2.5train loss no decay DLRT UVT factorization 0 2 4 6 8 epoch decay DLRT UVT factorization         T est Accuracy                                                                                            Train Loss Figure 4: Mean learning curves with standard deviation of Lenet5 on MNIST over 10 runs of DLRT compared to a vanilla layer factorizationWk = UkV⊤ k . Both methods are implemented with ﬁxed learning rate of 0.01, and batch size of 128. The weight matrices are either completely randomly initialized (“no decay”) or are initialized with a random choice forced to have an exponential decay on the singular values (“decay”). Table 1: Results of the training of LeNet5 on MNIST dataset. Effective parameters represent the number of parameters we have to save for evaluating the network and those we need in order to train via the DLRT Alg.1. The compression ratio (c.r.) is the percentage of parameter reduction with respect to the full model (<0% indicates that the ratio is negative). “ft” indicates that the model has been ﬁne-tuned. “LeNet5” denotes the standard LeNet5 architecture trained with SGD. NN metrics Evaluation Train method test acc. ranks params c.r. params c.r. LeNet5 99.2% [20 ,50,500,10] 430500 0% 430500 0% DLRT τ = 0.11 98 .0% [15 ,46,13,10] 47975 88 .86% 50585 88 .25% τ = 0.15 97 .8% [13 ,31,9,10] 34435 92 .0% 35746 91 .7% τ = 0.2 97 .2% [10 ,20,7,10] 25650 94 .04% 26299 93 .89% τ = 0.3 95 .3% [6 ,9,4,10] 15520 96.4% 15753 96.34% SSL [62] (ft) 99.18% 110000 74 .4% <0% NISP [58] (ft) 99.0% 100000 76 .5% <0% GAL [42] 98.97% 30000 93 .0% <0% LRNN [28] 98.67% [3 ,3,9,9] 18075 95 .8% <0% SVD prune [61] 94.0% [2 ,5,89,10] 123646 71 .2% <0% This problem is peculiar to the low-rank manifold itself [35, 16], whose local curvature is proportional to the inverse of the smallest singular value of the weight matrices. In contrast, the numerical integration strategy at the basis of DLRT is designed to take advantage of the structure of the manifold and is robust with respect to small singular values [ 32]. This can be seen from the bound of Theorem 1, where the constants are independent of the singular values of the weight matrices, and is illustrated by Figure 4, where DLRT shows a much faster convergence rate with respect to vanilla SGD performed on each factor of the parametrization UkV⊤ k , when applied to train LeNet5 on MNIST. Both methods are implemented with the same ﬁxed learning rate. 5.2 Results on ImageNet1K and Cifar10 with ResNet-50, AlexNet, and VGG16 Finally, we assess the capability of compressing different architectures on large scale training sets. We train a full-rank baseline model and compare it to DLRT using the same starting weights on an Nvidia A-100 GPU. The used optimizer is SGD with momentum factor 0.1 and no data-augmentation techniques are used. We compare the results on ResNet-50, VGG16, and AlexNet models, on the Cifar10 and ImageNet1k datasets, and with respect to a number of low-parametric alternative baselines methods. For DLRT, the last layers of the networks have been adapted to match the corresponding classiﬁcation tasks. Detailed results are reported in Table 2, where we show the test accuracy (reported as the difference with respect to the full baseline) as well as compression ratios. With Cifar10, we archive a train compression of 77.5% with an accuracy loss of just 1.89% for VGG16 and 84.2% train compression at 1.79% accuracy loss for AlexNet. In the ImageNet1k benchmark, we achieve a train compression rate of 14.2%, with an test accuracy loss of 0.5% in top-5 accuracy on ResNet-50 and 78.4% train compression with 2.19 top-5 accuracy loss on VGG16. 10Table 2: Results on ImageNet1k (left) and Cifar10 (right). The compression ratio is the percentage of parameter reduction with respect to the full model. DLRT is used withτ = 0.1. The number of parameters of the full models are: 33.6M (VGG16); 23.6M (AlexNet); 29.6M (ResNet-50). We report difference in test accuracy (top-5 test accuracy for ImageNet1k) with respect to the full baselines. ImageNet1k test acc.[%] compression rate method (to baseline) eval[%] train[%]ResNet-50 DLRT −0.56 54 .1 14 .2PP-2[52] −0.8 52 .2 <0 PP-1[52] −0.2 44 .2 <0 CP[25] −1.4 50 .0 <0 SFP[22] −0.2 41 .8 <0 ThiNet[47] −1.5 36 .9 <0 VGG16 DLRT −2.19 86 78 .4PP-1[52] −0.19 80 .2 <0 CP[25] −1.80 80 .0 <0 ThiNet[47] −0.47 69 .04 <0 RNP(3X)[41] −2.43 66 .67 <0 Cifar10 test acc.[%] compression rate method (to baseline) eval[%] train[%]VGG16 DLRT −1.89 56 77 .5GAL[42] −1.87 77 <0 LRNN[28] −1.9 60 <0 AlexNet DLRT −1.79 86 .3 84 .2NISP[58] −1.06 − <0 6 Appendix 6.1 Proofs of the main results We provide here a proof of Theorems 1 and 2. The proof is based on a number of classical results as well as recent advances in DLRA theory, including [35, 44, 32, 4, 6]. Recall that, for a ﬁxed layer k, we reinterpret the training phase as a continuous-time evolution of the weights on the manifold of low-rank matrices, as illustrated in Fig. 5(a-b). This boils down to solving the manifold-constrained matrix differential equation min { ∥˙Wk(t) −Fk(Wk(t))∥: ˙Wk(t) ∈TWk(t)Mrk } , (10) where ∥·∥ is the Frobenius norm and Fk denotes the gradient ﬂow of the loss with respect to the k-th matrix variable, namely Fk(Z) = −∇WkL(W1,...,Z,...,W M; N(x),y) . For the sake of simplicity and for a cleaner notation, as all the results we will present hold for a generic k, we drop the subscript kfrom now on. In particular, we assume W is the weight matrix of a generic hidden layer with ninput and moutput neurons. In order for our derivation to hold, we require the following two properties: (a) Discrete time weight update  (b) Continuous time weight update (c) Galerkin condition Figure 5: Panels (a)-(b): Graphical re-interpretation of the weight update step as a time-continuous process. Panel (c): Orthogonal projection onto the tangent space of the low-rank manifold Mr. The dashed line depicts the projection resulting in ˙Wk(t), which is the tangent element minimizing the distance between ∇WkL(Wk(t)) and the tangent space TWk(t)Mr at the approximation Wk(t). 11P1. The gradient ﬂow Fis locally bounded and locally Lipschitz continuous, with constants C1 and C2, respectively. Namely, we assume there existC1,C2 >0 (independent of k) such that ∥F(Z)∥≤ C1 ∥F(Z) −F( ˜Z)∥≤ C2∥Z−˜Z∥ for all Z, ˜Z ∈Rm×n. P2. The whole gradient ﬂow is “not too far” from the rank-rmanifold Mr. Precisely, we assume that for any Z ∈Mr arbitrary close to W(t), the whole gradient ﬂow F(Z) near tis such that ∥(I−P(Z))F(Z)∥≤ ε, where P(Z) denotes the orthogonal projection onto TZMr. Note that both assumptions are valid for low-rank neural network training. In particular, Lipschitz continuity and boundedness of the gradient are standard assumptions in optimization and are satisﬁed by the gradient of commonly used neural networks’ losses. Moreover, assuming the gradient ﬂow to be close to the low-rank manifold is an often encountered empirical observation in neural networks [53, 48, 15]. In order to derive the proof of Theorems 1 and 2 we ﬁrst present a number of relevant background lemmas. The ﬁrst lemma shows that the subspace generated by the K-step in Algorithm 1 after the QR-decomposition is O(η(η+ ε)) close to the range of the exact solution, where ηis the time-step of the integrator and εis the eigenvalue truncation tolerance. Lemma 1 ([6, Lemma 2]) . Let W1 be the solution at time t = η of the full problem (2) with initial condition W0. Let U1 be the matrix obtained with the K-step of the ﬁxed-rank Algorithm 1, after one step. Under the assumptions P1 and P2 above, we have ∥U1U1,⊤W1 −W1∥≤ θ where θ= C1C2(4eC2η + 9)η2 + (3eC2η + 4)εη. Proof. The local error analysis of [32] shows that there exists L1 such that ∥U1L1,⊤−W1∥≤ θ. It follows that, ∥U1L1,⊤−W1∥2 = ∥U1L1,⊤−U1U1,⊤W1 + U1U1,⊤W1 −W1∥2 = ∥U1U1,⊤(U1L1,⊤−W1) + (I−U1U1,⊤)(−W1)∥2 = ∥U1U1,⊤(U1L1,⊤−W1)∥2 + ∥(I−U1U1,⊤)W1∥2. Therefore, ∥U1U1,⊤(U1L1,⊤−W1)∥2 + ∥(I−U1U1,⊤)W1∥2 ≤θ2. Hence, since both terms must be bounded by θ2 individually, we obtain the stated result. In the next lemma we show that also the space generated by the Lstep is close by the exact solution. Namely, combined with the previous result, we have Lemma 2 ([6, Lemma 3]). Let W1, U1 be deﬁned as above. Let V1 be the matrix obtained from the L-step of the ﬁxed-rank Algorithm 1, after one step. The following estimate holds: ∥U1U1,⊤W1V1V1,⊤−W1∥≤ 2θ. Proof. The L-step is obtained as the K-step applied to the transposed function G(Y) = F(Y⊤)⊤. Due to the invariance of the Frobenius norm under transposition, property P1 holds. Similarly, property P2 continues to be satisﬁed because ∥(I−P(Y))G(Y)∥= ∥(I−P(Y⊤))F(Y⊤)∥≤ ε, 12where the equality P(Y)Z⊤= [ P(Y⊤)Z ]⊤has been used [35, §4]. It follows from Lemma 1 that ∥U1U1,⊤W1 −W1∥≤ θ, ∥V1V1,⊤W1,⊤−W1,⊤∥≤ θ. (11) This implies that ∥U1U1,⊤W1V1V1,⊤−W1∥≤∥ U1U1,⊤W1V1V1,⊤−W1V1V1,⊤+ W1V1V1,⊤−W1∥ ≤∥U1U1,⊤W1V1V1,⊤−W1V1V1,⊤∥+ ∥W1V1V1,⊤−W1∥ ≤∥ ( U1U1,⊤W1 −W1) V1V1,⊤∥+ ∥V1V1,⊤W1,⊤−W1,⊤∥ ≤∥U1U1,⊤W1 −W1∥·∥V1V1,⊤∥2 + ∥V1V1,⊤W1,⊤−W1,⊤∥. Because ∥V1V1,⊤∥2 = 1, the stated result follows from (11). With the previous lemmas, we are in the position to derive the local error bound for the ﬁxed-rank KLS integrator of Section 4. Lemma 3 (Local Error, [6, Lemma 4]). Let W1,U1,V 1 be deﬁned as above and let S1 be the matrix obtained with the S-step of Algorithm 1 after one step. The following local error bound holds: ∥U1S1V1,⊤−W1∥≤ η(ˆc1ε+ ˆc2η), where the constants ˆci are independent of the singular values of W1 and S1. Proof. From Lemma 2 and the equality Y1 = U1S1V1,⊤, we have that ∥Y1 −W1∥≤∥ Y1 −U1U1,⊤W1V1V1,⊤∥+ ∥U1U1,⊤W1V1V1,⊤−W1∥ ≤∥U1(S1 −U1,⊤W1V1)V1,⊤∥+ 2θ ≤∥S1 −U1,⊤W1V1∥+ 2θ. The local error’s analysis is reduced to bound the term ∥S1 −U1,⊤W1V1∥. For 0 ≤t ≤η, we thus introduce the following auxiliary quantity: ˜S(t) := U1,⊤W(t)V1. We observe that the term W(t) can be re-written as W(t) = U1U1,⊤W(t)V1V1,⊤+ ( W(t) −U1U1,⊤W(t)V1V1,⊤ ) = U1 ˜S(t)V1,⊤+ R(t), where R(t) denotes the term in big brackets. For 0 ≤t≤η, it follows from Lemma 2 and the bound C1 of the function Fthat ∥W(t) −W(η)∥≤ ∫ η 0 ∥˙W(s)∥ds= ∫ η 0 ∥F(W(s))∥ds≤C1η. Therefore, the term R(t) is bounded by ∥R(t)∥≤∥R (t) −R(η)∥+ ∥R(η)∥≤ 2C1η+ 2θ. We re-cast the function F ( W(t) ) as F ( W(t) ) = F ( U1 ˜S(t)V1,⊤+ R(t) ) = F ( U1 ˜S(t)V1,⊤) + D(t) where the defect D(t) is given by D(t) := F ( U1 ˜S(t)V1,⊤+ R(t) ) −F ( U1 ˜S(t)V1,⊤) . Via the Lipschitz constant C2 of the function F, the defect is bounded by ∥D(t)∥≤ C2∥R(t)∥≤ 2C2(C1η+ θ). 13Now, we compare the two differential equations ˙˜S(t) = U1,⊤F ( U1 ˜S(t)V1,⊤) V1 + U1,⊤D(t)V1, ˜S(0) = U1,⊤W0V1, ˙S(t) = U1,⊤F ( U1S(t)V1,⊤) V1, S (0) = U1,⊤W0V1. The solution S1 obtained in the second differential equation is the same as given by the S-step of the KLS integrator of Section 4. By construction, the solution obtained in the ﬁrst differential equation at time t= ηis ˜S(η) = U1,⊤W1V1. With the Gronwall inequality we obtain ∥S1 −U1,⊤W1V1∥≤ ∫ η 0 eC2(η−s) ∥D(s)∥ds≤eLη2C2(C1η+ θ)η. The result yields the statement of the theorem using the deﬁnition of θ. We are now in the position to conclude the proof of Theorem 1. Proof of Theorem 1. In Lemma 3, the local error for the ﬁxed-rank integrator of §4 has been provided. The local error in time of the rank-adaptive version is directly obtained via a triangle inequality: ∥U1S1V1,⊤−W(η)∥≤ ˆc1εη+ ˆc2η2 + ϑ, where ϑis the tolerance parameter chosen for the truncation procedure. Here, we abuse the notation and we maintain the same nomenclature U1,S1,and V1 also for the novel low-rank approximation obtained via the truncation procedure. Thus, we conclude the proof using the Lipschitz continuity of the function F. We move from the local error in time to the global error in time by a standard argument of Lady Windermere’s fan [21, Section II.3]. Therefore, the error after tsteps of the rank-adaptive Algorithm 1 is given by ∥UtStVt,⊤−W(tη)∥≤ c1ε+ c2η+ c3ϑ/η. To conclude with, we prove that after one step the proposed rank-adaptive DLRT algorithm decreases along the low-rank approximations. We remind that only property P1 needs to be assumed here. Proof of Theorem 2. Let ˆY(t) = U1S(t)V1,⊤. Here, S(t) denotes the solution for t∈[0,η] of the S-step of the rank-adaptive integrator . It follows that d dtL(ˆY(t)) = ⟨∇L(ˆY(t)), ˙ˆY(t)⟩ = ⟨∇L(ˆY(t)),U1 ˙S(t)V1,⊤⟩ = ⟨U1,⊤∇L(ˆY(t))V1, ˙S(t)⟩ = ⟨U1,⊤∇L(ˆY(t))V1, −U1,⊤∇L(ˆY(t))V1⟩= −∥U1,⊤∇L(ˆY(t))V1∥2 . The last identities follow by deﬁnition of the S-step. For t∈[0,η] we have d dtL(ˆY(t)) ≤−α2 (12) where α= min0≤τ≤1 ∥U1,⊤∇L (ˆY(τη) ) V1∥. Integrating (12) from t= 0 until t= η, we obtain L(ˆY1) ≤L(ˆY0) −α2η. Because the subspace U1 and V1 contain by construction the range and co-range of the initial value, we have that ˆY0 = U0S0V0,⊤[4, Lemma 1]. The truncation is such that ∥Y1 −ˆY1∥≤ ϑ. Therefore, L(Y1) ≤L(ˆY1) + βϑ where β = max0≤τ≤1 ∥∇L ( τY 1 + (1 −τ)ˆY1) ∥. Hence, the stated result is obtained. 146.2 Detailed timing measurements Table 3 displays the average batch training times of a 5-layer, 5120-neuron dense network on the MNIST dataset, with a batch size of 500 samples. We average the timings over 200 batches and additionally display the standard deviation of the timings corresponding to the layer ranks. The batch timing measures the full K,L and S steps, including back-propagation and gradient updates, as well as the loss and metric evaluations. Table 3: Average batch training times for ﬁxed low-rank training of a 5-layer fully-connected network with layer widths [5120,5120,5120,5120,10]. Different low-rank factorizations are compared ranks mean time [s] std. deviation [s] full-rank 0.320 ±0.005227 [320,320,320,320,320] 0 .855 ±0.006547 [160,160,160,160,10] 0 .387 ±0.005657 [80,80,80,80,10] 0 .198 ±0.004816 [40,40,40,40,10] 0 .133 ±0.005984 [20,20,20,20,10] 0 .098 ±0.005650 [10,10,10,10,10] 0 .087 ±0.005734 [5,5,5,5,10] 0 .071 ±0.005369 Table 4 shows the average test time of a 5-layer, 5120-neuron dense network, for different low-rank factorizations and the full rank reference network. The timings are averaged over 1000 evaluations of the 60K sample MNIST training data set. We measure the K step forward evaluation of the low-rank networks as well as the loss and prediction accuracy evaluations. Table 4: Average dataset prediction times for ﬁxed low-rank training of a 5-layer fully-connected network with layer widths [5120,5120,5120,5120,10]. Different low-rank factorizations are compared. ranks mean time [s] std. deviation [s] full-rank 1.2476 ±0.0471 [2560,2560,2560,2560,10] 1 .4297 ±0.0400 [1280,1280,1280,1280,10] 0 .7966 ±0.0438 [640,640,640,640,10] 0 .4802 ±0.0436 [320,320,320,320,10] 0 .3286 ±0.0442 [160,160,160,160,10] 0 .2659 ±0.0380 [80,80,80,80,10] 0 .2522 ±0.0346 [40,40,40,40,10] 0 .2480 ±0.0354 [20,20,20,20,10] 0 .2501 ±0.0274 [10,10,10,10,10] 0 .2487 ±0.0276 [5,5,5,5,10] 0 .2472 ±0.0322 6.3 Detailed training performance of adaptive low-rank networks Tables 5 and 6 display a detailed overview of the adaptive low-rank results of §5.1. The displayed ranks are the ranks of the converged algorithm. The rank evolution of the 5-Layer, 500-Neuron test case can be seen in Fig. 6. The Evaluation parameter count corresponds to the parameters of the K step of the dynamical low-rank algorithm, since all other matrices are no longer needed in the evaluation phase. The training parameter count is evaluated as the number of parameters of the Sstep of the adaptive dynamical low rank training, with maximal basis expansion by 2r, where ris the current rank of the network. We use the converged ranks of the adaptive low-rank training to compute the training parameters. Note that during the very ﬁrst training epochs, the parameter count is typically higher until the rank reduction has reached a sufﬁciently low level. 15a) Rank evolution for τ = 0.17  b) Rank evolution for τ = 0.15 c) Rank evolution for τ = 0.13  d) Rank evolution for τ = 0.11 e) Rank evolution for τ = 0.09  f) Rank evolution for τ = 0.07 g) Rank evolution for τ = 0.05  h) Rank evolution for τ = 0.03 Figure 6: Rank evolution of the dynamic adaptive low-rank training algorithm for the 5-layer, 500-neuron dense architecture. 16Table 5: Dynamical low rank training for 5-layer 500-neurons network. c.r. denotes the compression rate relative to the full rank dense network. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. full-rank 98.54 ± 0.03% [500 ,500,500,500,10] 1147000 0% 1147000 0% 0.03 98 .49 ± 0.02% [176 ,170,171,174,10] 745984 34 .97% 1964540 -71.27% 0.05 98 .56 ± 0.02% [81 ,104,111,117,10] 441004 61 .56% 1050556 8 .40% 0.07 98 .52 ± 0.08% [52 ,67,73,72,10] 283768 75 .26% 633360 44 .78% 0.09 98 .34 ± 0.14% [35 ,53,51,46,10] 199940 82 .57% 429884 62 .52% 0.11 98 .11 ± 0.46% [27 ,40,37,38,10] 154668 86 .52% 324904 71 .67% 0.13 97 .50 ± 0.23% [20 ,31,32,30,10] 123680 89 .22% 255500 77 .72% 0.15 97 .22 ± 0.29% [17 ,25,26,24,10] 101828 91 .13% 207320 81 .92% 0.17 96 .90 ± 0.45% [13 ,21,24,20,10] 86692 92 .45% 174728 84 .76% Table 6: Dynamical low rank training for 5-layer 784-neurons network. c.r. denotes the compression rate relative to the full rank dense network. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. full-rank 98.53 ± 0.04% [784 ,784,784,784,10] 2466464 0% 2466464 0% 0.03 98 .61 ± 0.07% [190 ,190,190,190,10] 1199520 51 .37% 2968800 -20.36% 0.05 98 .59 ± 0.06% [124 ,120,125,126,10] 784000 68 .22% 1805268 26 .80% 0.07 98 .58 ± 0.03% [76 ,86,85,83,10] 525280 78 .71% 1151864 53 .29% 0.09 98 .49 ± 0.05% [56 ,67,63,59,10] 392000 84 .41% 836460 66 .08% 0.11 98 .12 ± 0.21% [35 ,49,47,43,10] 280672 88 .63% 584240 76 .31% 0.13 97 .95 ± 0.23% [29 ,35,38,34,10] 221088 91 .04% 453000 81 .63% 0.15 97 .81 ± 0.17% [22 ,29,27,27,10] 172480 93 .01% 348252 85 .88% 0.17 97 .40 ± 0.25% [17 ,23,22,23,10] 141120 94 .28% 281724 88 .57% 6.3.1 Lenet5 experiment In Table 7 we report the results of ﬁve independent runs of the dynamic low-rank training scheme on Lenet5; we refer to §5.1 for further details. For each column of the table, we report the mean value together with its relative standard deviations. No seed has been applied for splitting the dataset and generating the initial weights conﬁguration. Table 7: Mean results and standard relative deviations of the dynamic low-rank training algorithm over ﬁve independent runs on Lenet5. Adaptive learning rate of 0.05 with 0.96−exponentially decaying tax. NN metrics Evaluation Train τ test acc. ranks params c.r. params c.r. 0.11 95 .420 ±1.865% [15 ,46,13,10] 47975 88 .9% 50585 88 .2% 0.15 95 .527 ±1.297% [13 ,31,9,10] 34435 92 .0% 35746 91 .69% 0.2 95 .009 ±1.465% [10 ,20,7,10] 25650 94 .04% 26299 93 .89% 0.3 92 .434 ±1.757% [6 ,9,4,10] 15520 96 .39% 15753 96 .34% 6.4 Detailed training performance of low-rank pruning The proposed low-rank training algorithm does not need to be applied to train a network from random initial weight guesses. When an already trained network is available, the proposed method can be employed as a memory-efﬁcient pruning strategy. A straightforward approach to reduce a trained fully- connected network to a rank rnetwork is to compute an SVD for all weight matrices and to truncate those decompositions at rank r. However, while this choice is optimal to present weight matrices, it might signiﬁcantly reduce the accuracy of the network. Hence, retraining the determined low-rank subnetwork is commonly necessary to obtain desirable accuracy properties. Three key aspects are important to obtain an efﬁcient pruning method for low-rank methods: 1. Retraining preserves the low-rank structure of the subnetwork. 172. Retraining does not exhibit the memory footprint of the fully connected network. 3. Retraining ﬁnds the optimal network among possible low rank networks. Let us note that the attractor of the proposed dynamical low-rank evolution equations fulﬁlls these three requirements. Recall that for the evolution equations we have (3): min { ∥˙Wk(t) + ∇WkL(Wk(t))∥F : ˙Wk(t) ∈TWk(t)Mrk } . (13) The condition ˙Wk(t) ∈TWk(t)Mrk ensures that the weight matrices remain of low-rank. Moreover, as previously discussed, the training method only requires memory capacities to store low-rank factors. At the attractor, i.e., when ˙Wk = 0, the last condition ensures that the attractor minimizes∥∇WkL(Wk(t))∥F. That is, the attractor is the optimal low-rank subnetwork in the sense that it picks the network with minimal gradient. To underline the effectiveness of our low-rank method as a pruning technique, we take the fully connected network from Table 6. To demonstrate the poor validation accuracy when simply doing an SVD on the full 784 by 784 weight matrices and truncating at a given smaller rank, we perform this experiment for ranks r ∈{10,20,30,40,50,60,70,80,90,100}. It turns out that though reducing memory requirements, this strategy leads to unsatisfactory accuracy of about 10%, see the ﬁrst column of Table 8. Then, we use the proposed low-rank training methods with ﬁxed rank rto retrain the network. As starting points, we use the low-rank networks which have been determined by the truncated SVD. Retraining then reaches desired accuracies that are comparable to the previously determined low-rank networks in Table 6. Table 8: Pruning methods with 784 Neurons per layer test accuracy Evaluation SVD low-rank training ranks params c.r. 98.63% 98 .63% [784 ,784,784,784,10] 2466464 0% 9.91% 98 .16% [100 ,100,100,100,10] 635040 74 .25% 9.67% 98 .44% [90 ,90,90,90,10] 572320 76 .80% 9.15% 98 .47% [80 ,80,80,80,10] 509600 79 .34% 9.83% 98 .58% [70 ,70,70,70,10] 446880 81 .88% 9.67% 98 .41% [60 ,60,60,60,10] 384160 84 .42% 9.83% 98 .39% [50 ,50,50,50,10] 321440 86 .97% 10.64% 98 .24% [40 ,40,40,40,10] 258720 89 .51% 10.3% 98 .24% [30 ,30,30,30,10] 196000 92 .05% 9.15% 97 .47% [20 ,20,20,20,10] 133280 94 .60% 10.9% 95 .36% [10 ,10,10,10,10] 70560 97 .14% 6.5 Detailed derivation of the gradient In this section, we derive the computation of the gradients in the K, L and S steps in detail. For this, let us start with the full gradient, i.e., the gradient of the loss with respect to the weight matrix Wk. We have ∂Wℓ jk L= nM∑ iM=1 ∂zM iM L∂Wℓ jk zM iM = nM∑ iM=1 ∂zM iM L∂Wℓ jk σM  ∑ iM−1 WiMiM−1 zM−1 iM−1 + bM iM   = nM∑ iM=1 ∂zM iM Lσ′ M  ∑ iM−1 WiMiM−1 zM−1 iM−1 + bM iM  ∂Wℓ jk  ∑ iM−1 WiMiM−1 zM−1 iM−1  . (14) For a general α, let us deﬁne σ′ α,iα := σ′ α  ∑ iα−1 Wα iαiα−1 zα−1 iα−1 + bα iα   (15) 18and note that for α̸= ℓ ∂Wℓ jk  ∑ iα−1 Wα iαiα−1 zα−1 iα−1  = ∑ iα−1 Wα iαiα−1 ∂Wℓ jk zα−1 iα−1 , (16) whereas for α= ℓwe have ∂Wℓ jk   nα−1∑ iα−1=1 Wα iαiα−1 zα−1 iα−1  = ∑ iα−1 δjiαδkiα−1 zα−1 iα−1 . (17) Therefore, recursively plugging (15), (16) and (17) into (14) yields ∂Wℓ jk L= nM∑ iM=1 ∂zM iM Lσ′ M,iM ∑ iM−1 Wα iMiM−1 ∂Wℓ jk zM−1 iM−1 = nM∑ iM=1 ∂zM iM Lσ′ M,iM ∑ iM−1 Wα iMiM−1 σ′ M−1,iM−1 ∑ iM−2 Wα iM−1iM−2 ∂Wℓ jk zM−2 iM−2 = ··· = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Wℓ jk   nℓ−1∑ iℓ−1=1 Wℓ iℓiℓ−1 zℓ−1 iℓ−1   (18) = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ ∑ iℓ−1 δjiℓδkiℓ−1 zℓ−1 iℓ−1 = ∑ iℓ+1,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,jδjiℓzℓ−1 k Written in matrix notation and making use of the Hadamard product deﬁned as y◦A◦x= (yiAijxj)ij, for A∈Rm×n, x∈Rn and y∈Rm, we have: ∂WℓL=∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤ z⊤ ℓ−1 Now, let us move to deriving the K, L and S-steps for the dynamical low-rank training. For the K-step, we represent the weight matrix Wℓ as Wℓ iℓiℓ−1 = ∑ mKℓ iℓmVℓ iℓ−1m. Hence, reusing the intermediate result (18) yields ∂Kℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Kℓ jk   nℓ−1∑ iℓ−1=1 ∑ m Kℓ iℓmVℓ iℓ−1mzℓ−1 iℓ−1   = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m δjiℓδkmVℓ iℓ−1mzℓ−1 iℓ−1 = ∑ iℓ+1,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 δjiℓVℓ iℓ−1kzℓ−1 iℓ−1 In matrix notation we obtain ∂KℓL=∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤( V⊤ ℓ zℓ−1 )⊤ = ∂WℓLVℓ, 19which is exactly the right-hand side of the K-step. Hence, the K-step can be computed by a forward evaluation of Land recording the gradient tape with respect to Kℓ. Similarly, for the L-step, we represent Wℓ as Wℓ iℓiℓ−1 = ∑ mUℓ iℓmLℓ iℓ−1m. Hence, ∂Lℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Lℓ jk   nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmLℓ iℓ−1m   = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmδjiℓ−1 δkmzℓ−1 iℓ−1 = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓUℓ iℓmzℓ−1 j . In matrix notation, we obtain ∂LℓL=  U⊤ ℓ ∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤ z⊤ ℓ−1   ⊤ = (∂WℓL)⊤Uℓ. Lastly, for the S-step we write Wℓ iℓiℓ−1 = ∑ n,mUℓ iℓmSmnVℓ iℓ−1n. Then, ∂Sℓ jk L= ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ∂Sℓ jk (∑ n,m Uℓ iℓmSmnVℓ iℓ−1n ) = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓ nℓ−1∑ iℓ−1=1 ∑ m Uℓ iℓmδjmδknVℓ iℓ−1nzℓ−1 iℓ−1 = ∑ iℓ,···,iM ∂zM iM L M∏ α=ℓ+1 σ′ α,iαWα iαiα−1 σ′ ℓ,iℓUℓ iℓjVℓ iℓ−1kzℓ−1 iℓ−1 . In matrix notation, we have ∂SℓL= U⊤ ℓ ∂zML⊤ ( σ′ ℓ ◦ M∏ α=ℓ+1 W⊤ α ◦σ′ α )⊤( V⊤ ℓ zℓ−1 )⊤ = U⊤ ℓ ∂WℓLVℓ. 6.6 Low-rank matrix representation and implementation of convolutional layers A generalized convolutional ﬁlter is a four-mode tensor W ∈RF×C×J×K consisting of F ﬁlters of shape C×J×K, which is applied to a batch of N input C−channels image signals Zof spatial dimensions U ×V as the linear mapping, (Z∗W)(n,f,u,v ) = J∑ j=1 K∑ k=1 C∑ c=1 W(f,c,j,k )Z(n,c,u −j,v −k) . (19) In order to train the convolutional ﬁlter on the low-rank matrix manifold, we reshape the tensor W into a rectangular matrix Wresh ∈RF×CJK. This reshaping is also considered in e.g. [28]. An option is, to see the convolution as the contraction between an three-mode tensor Zunfolded of patches and the reshaped kernel matrix Wresh using Pytorch’s fold-unfold function. We can construct the unfold by stacking the vectorized version of sliding patterns of the kernel on the original input, obtaining in this way a tensor Zunfolded ∈RN×CJK×L, where L denotes the dimension of ﬂatten version of the output of the 2-D 20convolution. Thus, equation 19 can be rewritten as a tensor mode product: (Z∗W)(n,f,u,v ) = J∑ j=1 K∑ k=1 C∑ c=1 Wresh(f,(c,j,k ))Zunfolded(n,(c,j,k ),(u,v)) = r∑ p= U(f,p) r∑ q=1 S(p,q) J∑ j=1 K∑ k=1 C∑ c=1 V((c,j,k ),q)Zunfolded(n,(c,j,k ),(u,v)) (20) As it is shown in (20), we can a decompose the starting weight Wresh = USV ⊤and then do all the training procedure as a function of the factors (U,S,V ), without ever reconstructing the kernel. Then we can apply the considerations of fully connected layers. Acknowledgements. The work of S. Schotthöfer was funded by the Priority Programme SPP2298 “Theoretical Foundations of Deep Learning” by the Deutsche Forschungsgemeinschaft (DFG). The work of J. Kusch was funded by the Deutsche Forschungsgemeinschaft (DFG) – 491976834. The work of G. Ceruti was supported by the SNSF research project “Fast algorithms from low-rank updates”, grant number 200020-178806. The work of F. Tudisco and E. Zangrando was funded by the MUR-PNRR project “Low-parametric machine learning”. Special thanks to Prof. Martin Frank for the PhD mentorship of Steffen Schottöfer. References [1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009. [2] A. Ashok, N. Rhinehart, F. Beainy, and K. M. Kitani. N2n learning: Network to network compression via policy gradient reinforcement learning. In International Conference on Learning Representations, 2018. [3] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129–146, 2020. [4] G. Ceruti, J. Kusch, and C. Lubich. A rank-adaptive robust integrator for dynamical low-rank approximation. BIT Numerical Mathematics, 2022. [5] G. Ceruti and C. Lubich. Time integration of symmetric and anti-symmetric low-rank matrices and Tucker tensors. BIT Numerical Mathematics, 60(3):591–614, 2020. [6] G. Ceruti and C. Lubich. An unconventional robust integrator for dynamical low-rank approximation. BIT. Numerical Mathematics, 62(1):23–44, 2022. [7] G. Ceruti, C. Lubich, and D. Sulz. Rank-adaptive time integration of tree tensor networks. arXiv:2201.10291, 2022. [8] G. Ceruti, C. Lubich, and H. Walach. Time integration of tree tensor networks. SIAM Journal on Numerical Analysis, 59(1):289–313, 2021. [9] Y . Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE international conference on computer vision, pages 2857–2865, 2015. [10] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y . Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. Advances in neural information processing systems, 2016. [11] L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141–142, 2012. [12] E. L. Denton, W. Zaremba, J. Bruna, Y . LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efﬁcient evaluation. Advances in neural information processing systems, 27, 2014. [13] L. Dieci and T. Eirola. On smooth decompositions of matrices. SIAM Journal on Matrix Analysis and Applications, 20(3):800–819, 1999. 21[14] P. A. M. Dirac et al. The principles of quantum mechanics. Number 27. Oxford university press, 1981. [15] R. Feng, K. Zheng, Y . Huang, D. Zhao, M. Jordan, and Z.-J. Zha. Rank diminishing in deep neural networks. arXiv:2206.06072, 2022. [16] F. Feppon and P. F. Lermusiaux. A geometric approach to dynamical model order reduction. SIAM Journal on Matrix Analysis and Applications, 39(1):510–538, 2018. [17] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2018. [18] J. Frenkel. Wave mechanics, advanced general theory, volume 1. Oxford, 1934. [19] B. Gao and P.-A. Absil. A riemannian rank-adaptive method for low-rank matrix completion. Computational Optimization and Applications, 81(1):67–90, 2022. [20] Y . Guo, A. Yao, and Y . Chen. Dynamic network surgery for efﬁcient dnns. Advances in neural information processing systems, 29, 2016. [21] E. Hairer, S. P. Nørsett, and G. Wanner.Solving ordinary differential equations. I. Nonstiff problems, volume 8 of Springer Series in Computational Mathematics. Springer-Verlag, Berlin, second edition, 1993. [22] Y . He, G. Kang, X. Dong, Y . Fu, and Y . Yang. Soft ﬁlter pruning for accelerating deep convolutional neural networks, 2018. [23] Y . He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. AMC: AutoML for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision, pages 784–800, 2018. [24] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks. InIEEE International Conference on Computer Vision, pages 1389–1397, 2017. [25] Y . He, X. Zhang, and J. Sun. Channel pruning for accelerating very deep neural networks, 2017. [26] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017. [27] Z. Huang and N. Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304–320, 2018. [28] Y . Idelbayev and M. A. Carreira-Perpiñán. Low-rank compression of neural nets: Learning the rank of each layer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8046–8056, 2020. [29] Y . Ioannou, D. Robertson, J. Shotton, R. Cipolla, and A. Criminisi. Training CNNs with low-rank ﬁlters for efﬁcient image classiﬁcation. In International Conference on Learning Representations, 2016. [30] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference. BMVA Press, 2014. [31] M. Khodak, N. Tenenholtz, L. Mackey, and N. Fusi. Initialization and regularization of factorized neural layers. In International Conference on Learning Representations, 2021. [32] E. Kieri, C. Lubich, and H. Walach. Discretized dynamical low-rank approximation in the presence of small singular values. SIAM Journal on Numerical Analysis, 54(2):1020–1038, 2016. [33] H. Kim, M. U. K. Khan, and C.-M. Kyung. Efﬁcient neural network compression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12569–12577, 2019. [34] Y .-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications, 2015. [35] O. Koch and C. Lubich. Dynamical low-rank approximation. SIAM Journal on Matrix Analysis and Applications, 29(2):434–454, 2007. [36] O. Koch and C. Lubich. Dynamical tensor approximation. SIAM Journal on Matrix Analysis and Applications, 31(5):2360–2375, 2010. 22[37] J. Kusch and P. Stammer. A robust collision source method for rank adaptive dynamical low-rank approximation in radiation therapy. arXiv:2111.07160, 2021. [38] V . Lebedev, Y . Ganin, M. Rakhuba, I. Oseledets, and V . Lempitsky. Speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. In International Conference on Learning Representations, 2015. [39] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [40] C. Li and C. J. R. Shi. Constrained optimization based low-rank approximation of deep neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. [41] J. Lin, Y . Rao, J. Lu, and J. Zhou. Runtime neural pruning. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [42] S. Lin, R. Ji, C. Yan, B. Zhang, L. Cao, Q. Ye, F. Huang, and D. Doermann. Towards optimal structured CNN pruning via generative adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2790–2799, 2019. [43] H. Liu, K. Simonyan, O. Vinyals, C. Fernando, and K. Kavukcuoglu. Hierarchical representations for efﬁcient architecture search. In International Conference on Learning Representations, 2018. [44] C. Lubich and I. V . Oseledets. A projector-splitting integrator for dynamical low-rank approximation. BIT Numerical Mathematics, 54(1):171–188, 2014. [45] C. Lubich, T. Rohwedder, R. Schneider, and B. Vandereycken. Dynamical approximation by hierarchical Tucker and tensor-train tensors. SIAM Journal on Matrix Analysis and Applications, 34(2):470–494, 2013. [46] C. Lubich, B. Vandereycken, and H. Walach. Time integration of rank-constrained Tucker tensors. SIAM Journal on Numerical Analysis, 56(3):1273–1290, 2018. [47] J.-H. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level pruning method for deep neural network compression, 2017. [48] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):1–73, 2021. [49] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efﬁcient inference. In International Conference on Learning Representations, 2017. [50] T. N. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655–6659, 2013. [51] D. Scieur, V . Roulet, F. Bach, and A. d’Aspremont. Integration methods and accelerated optimization algorithms. In Advances In Neural Information Processing Systems, 2017. [52] P. Singh, V . Kumar Verma, P. Rai, and V . P. Namboodiri. Play and prune: Adaptive ﬁlter pruning for deep model compression. In Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-19, pages 3460–3466. International Joint Conferences on Artiﬁcial Intelligence Organization, 7 2019. [53] S. P. Singh, G. Bachmann, and T. Hofmann. Analytic insights into structure and rank of neural network Hessian maps. In Advances in Neural Information Processing Systems, volume 34, 2021. [54] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. Ré. Low-memory neural network training: A technical report. arXiv:1904.10631, 2019. [55] A. Tjandra, S. Sakti, and S. Nakamura. Compressing recurrent neural network with tensor train. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 4451–4458. IEEE, 2017. [56] M. Udell and A. Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144–160, 2019. 23[57] H. Wang, S. Agarwal, and D. Papailiopoulos. Pufferﬁsh: communication-efﬁcient models at no extra cost. Proceedings of Machine Learning and Systems, 3:365–386, 2021. [58] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016. [59] W. Wen, C. Xu, C. Wu, Y . Wang, Y . Chen, and H. Li. Coordinating ﬁlters for faster deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 658–666, 2017. [60] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng. Quantized convolutional neural networks for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4820–4828, 2016. [61] H. Yang, M. Tang, W. Wen, F. Yan, D. Hu, A. Li, H. Li, and Y . Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsiﬁcation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 678–679, 2020. [62] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. In International Conference on Learning Representations, 2018. 24",
      "meta_data": {
        "arxiv_id": "2205.13571v2",
        "authors": [
          "Steffen Schotthöfer",
          "Emanuele Zangrando",
          "Jonas Kusch",
          "Gianluca Ceruti",
          "Francesco Tudisco"
        ],
        "published_date": "2022-05-26T18:18:12Z",
        "venue": "Proceedings NeurIPS 2022",
        "pdf_url": "https://arxiv.org/pdf/2205.13571v2.pdf",
        "github_url": "https://github.com/COMPiLELab/DLRT"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Dynamic Low-Rank Training (DLRT), a novel algorithm that finds efficient low-rank subnetworks in neural networks directly during the training phase. This addresses the significant memory footprint and computational demands of large neural networks. DLRT restricts weight matrices to a low-rank manifold and dynamically adapts their ranks using techniques derived from dynamic model order reduction for matrix differential equations. It provides approximation, stability, and descent guarantees, leading to significantly reduced training and evaluation time and memory resources while maintaining comparable accuracy to full-rank networks. This approach is likened to finding \"low-rank winning tickets\" without heavy reliance on initial weight guesses.",
        "methodology": "DLRT reinterprets neural network training as a continuous-time gradient flow, formulating it as a system of matrix Ordinary Differential Equations (ODEs) for the low-rank factors (Uk, Sk, Vk) of the weight matrices Wk = UkSkV⊤k. The algorithm employs an \"unconventional KLS integrator\" and its rank-adaptive extension to numerically solve these ODEs. This involves alternating K-steps and L-steps (updating Kk=UkSk and Lk=VkS⊤k, respectively, and forming new orthonormal bases) and an S-step (updating Sk). Rank adaptivity is achieved by augmenting basis dimensions and subsequently truncating singular values of Sk based on a user-defined threshold (ϑ). Gradients are computed efficiently by taping them with respect to the low-rank factors (Kk, Lk, Sk) rather than the full weight matrix, reducing computational overhead. Numerical integration uses either Explicit Euler (SGD) or Adam optimizers. QR decomposition is used for orthonormal basis computation, and SVD for singular value truncation. For convolutional layers, filters are reshaped into matrices to enable low-rank decomposition.",
        "experimental_setup": "The DLRT algorithm was evaluated across various neural network architectures, including fully-connected and convolutional networks (LeNet5, ResNet-50, AlexNet, VGG16), on datasets such as MNIST, Cifar10, and ImageNet1K. For MNIST, networks with 5 layers and up to 5120 neurons were trained, alongside LeNet5, using Adam or SGD optimizers, ReLU/softmax activations, sparse categorical cross-entropy loss, and batch sizes of 128-256 for up to 250 epochs. For ImageNet1K and Cifar10, ResNet-50, AlexNet, and VGG16 were trained with SGD and momentum. Comparisons were made against full-rank baselines and numerous existing compression/pruning methods (e.g., SVD prune, LRNN, GAL, SSL, NISP, PP, CP, SFP, ThiNet, RNP), focusing on training/prediction times, rank evolution, test accuracy, parameter count, and compression ratios. The implementation used TensorFlow and PyTorch, running on AMD Ryzen 9 3950X CPU and Nvidia RTX 3090/A-100 GPUs.",
        "limitations": "The efficiency of DLRT is contingent on the ranks (rk) being significantly smaller than the dimensions of the weight matrices (nk, nk+1). If the singular value truncation threshold (ϑ) is too small, the method offers no advantages over standard training. DLRT primarily reduces training costs related to model parameters and the optimizer, but does not inherently reduce activation costs, requiring combination with other techniques like micro-batching or checkpointing for further reduction. The truncation threshold ϑ introduces an additional hyperparameter that currently requires external tuning. Although achieving superior compression, DLRT may yield slightly lower accuracy compared to some baseline pruning methods. Initial training epochs may also exhibit higher parameter counts until rank reduction stabilizes.",
        "future_research_directions": "While not explicitly detailed, potential future research directions include investigating methods to automatically tune the singular value truncation threshold (ϑ), reducing the need for manual hyperparameter selection. Further exploration could involve combining DLRT with complementary techniques such as micro-batching or checkpointing to achieve additional reductions in activation costs. Applying and evaluating DLRT on more complex or emerging neural network architectures (e.g., Transformers) and extending its applicability to diverse domains beyond image classification could also be fruitful avenues for future work.",
        "experimental_code": "import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch import float16   \n\nclass dlr_opt:\n\n    def __init__(self,NN,tau = 0.01,theta = 0.1,absolute = False,\n                KLS_optim = None,**kwargs):\n\n        \"\"\"\n        initializer for the dlr_opt (dynamical low rank optimizer) class.\n        INPUTS:\n        NN: neural network with custom layers, methods and attributes needed (look at Lenet5 for an example) \n        tau : learning rate (integration step)\n        theta : tolerance for singular values\n        absolute : flag variable, True if theta has to be interpreted as an absolute tolerance  \n        KLS_optim : Pytorch integrator to perform the integration step\n        \"\"\"\n\n        self.NN = NN\n        self.tau = tau\n        self.theta = theta\n        self.absolute = absolute\n        self.kw = dict(kwargs)\n        self.KLS_optim = KLS_optim\n\n        if self.KLS_optim is not None:\n\n            self.integrator = self.KLS_optim(self.NN.parameters(),lr = self.tau,**kwargs)\n\n        else:\n\n            self.integrator = torch.optim.SGD(self.NN.parameters(),lr = self.tau,**kwargs)\n\n\n    @torch.no_grad()\n    def K_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n                    \n                    U_hat = torch.hstack((l.K[:,:l.dynamic_rank],l.U[:,:l.dynamic_rank]))\n\n                    try:\n                        U_hat,_ = torch.linalg.qr(U_hat)\n                    except:\n                        U_hat,_ = np.linalg.qr(U_hat)\n                        U_hat = torch.tensor(U_hat)\n                    l.U_hat[:,:2*l.dynamic_rank] = U_hat\n                    l.M_hat[:2*l.dynamic_rank,:l.dynamic_rank] = l.U_hat[:,:2*l.dynamic_rank].T@l.U[:,:l.dynamic_rank]\n                \n                else:\n\n                    try:\n                        U_hat,_ = torch.linalg.qr(l.K)\n\n                    except:\n                        U_hat,_ = np.linalg.qr(U_hat)\n                        U_hat = torch.tensor(U_hat)\n                    l.M_hat.data = U_hat.T@l.U.data\n                    l.U.data = U_hat\n\n    @torch.no_grad()\n    def postprocess_step(self):\n        \n        self.K_postprocess_step()\n        self.L_postprocess_step()\n\n    @torch.no_grad()\n    def K_integration_step(self):\n        \n        self.zero_bias_grad()\n        self.integrator.step()\n\n    @torch.no_grad()\n    def zero_bias_grad(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'bias') and l.bias is not None:\n\n                l.bias.grad = None\n\n            if hasattr(l,'weight') and l.weight is not None:\n\n                l.weight.grad = None\n\n    @torch.no_grad()\n    def L_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    V_hat = torch.hstack((l.L[:,:l.dynamic_rank],l.V[:,:l.dynamic_rank]))\n                    try :\n                        V_hat,_ = torch.linalg.qr(V_hat)\n                    except:\n                        V_hat,_ = np.linalg.qr(V_hat.detach().numpy())\n                        V_hat= torch.tensor(V_hat)\n                    l.V_hat[:,:2*l.dynamic_rank] = V_hat\n                    l.N_hat[:2*l.dynamic_rank,:l.dynamic_rank] = l.V_hat[:,:2*l.dynamic_rank].T@l.V[:,:l.dynamic_rank]\n\n                else:\n\n                    try :\n                        V_hat,_ = torch.linalg.qr(l.L)\n                    except:\n                        V_hat,_ = np.linalg.qr(V_hat.detach().numpy())\n                        V_hat= torch.tensor(V_hat)\n                    l.N_hat.data = V_hat.T@l.V.data\n                    l.V.data = V_hat\n\n\n    \n    @torch.no_grad()\n    def L_integration_step(self):\n\n\n        self.integrator.step()\n        self.integrator.zero_grad()\n\n    @torch.no_grad()\n    def K_and_L_integration_step(self):\n        \n        self.zero_bias_grad()\n        self.integrator.step()\n\n    @torch.no_grad()\n    def S_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    s = l.M_hat[:2 * l.dynamic_rank, :l.dynamic_rank]@l.S_hat[: l.dynamic_rank, :l.dynamic_rank]@l.N_hat[:2 * l.dynamic_rank, :l.dynamic_rank].T\n                    l.S_hat[:2*l.dynamic_rank,:2*l.dynamic_rank] = s\n\n                else:\n\n                    s = l.M_hat@l.S_hat@l.N_hat.T\n                    l.S_hat.data = s\n\n\n\n    @torch.no_grad()\n    def K_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n                \n                    K = l.U[:,:l.dynamic_rank]@l.S_hat[:l.dynamic_rank,:l.dynamic_rank]\n                    l.K[:,:l.dynamic_rank] = K\n\n                else:\n\n                    K = l.U.data@l.S_hat\n                    l.K.data = K\n\n\n\n    @torch.no_grad()\n    def L_preprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    L = l.V[:,:l.dynamic_rank]@l.S_hat[:l.dynamic_rank,:l.dynamic_rank].T\n                    l.L[:,:l.dynamic_rank] = L\n\n                else:\n\n                    L = l.V.data@l.S_hat.T\n                    l.L.data = L\n\n\n    @torch.no_grad()\n    def S_postprocess_step(self):\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                if not l.fixed:\n\n                    # rank adaption\n\n                    s_small = torch.clone(l.S_hat[:2 * l.dynamic_rank, :2 * l.dynamic_rank])\n                    try:\n                        u2, d, v2 = torch.linalg.svd(s_small)\n                    except Exception as e:\n                        print(e)\n                        print(s_small)\n                        u2, d, v2 = np.linalg.svd(s_small)\n\n                    tmp = 0.0\n                    tol = self.theta * torch.linalg.norm(d) if not self.absolute else self.theta \n                    rmax = int(np.floor(d.shape[0] / 2))\n                    for j in range(0, 2 * rmax - 1):\n                        tmp = torch.linalg.norm(d[j:2 * rmax - 1])\n                        if tmp < tol:\n                            rmax = j\n                            break\n\n                    rmax = min([rmax, l.rmax])\n                    rmax = max([rmax, 2])\n\n                    l.S_hat[:rmax,:rmax] = torch.diag(d[:rmax])\n                    l.U[:, :rmax] = l.U_hat[:, :2 * l.dynamic_rank]@u2[:, :rmax]\n                    l.V[:,:rmax] =  l.V_hat[:,:2 * l.dynamic_rank]@(v2[:, :rmax])\n                    l.dynamic_rank = int(rmax)\n\n    \n    @torch.no_grad()\n    def S_integration_step(self):\n\n        self.integrator.step()\n        self.integrator.zero_grad()\n    \n\n    @torch.no_grad()\n    def preprocess_step(self):\n\n        self.K_preprocess_step()\n        self.L_preprocess_step()\n\n    @torch.no_grad()\n    def step(self,closure = None):\n\n        \"\"\"\n        optimizer step for the dlrt.\n        INPUTS:\n        closure : function to compute the loss and backpropagate a second time (Pytorch standard)\n        \"\"\"\n\n        # self.K_integration_step()\n        # self.L_integration_step()\n        self.K_and_L_integration_step()\n        self.K_postprocess_step()\n        self.L_postprocess_step()\n        self.S_preprocess_step()\n        self.zero_grad()\n        if closure is not None:\n            with torch.set_grad_enabled(True):\n                loss = closure()\n                loss.backward()\n        self.S_integration_step()\n        self.S_postprocess_step()\n    \n    @torch.no_grad()\n    def zero_grad(self):\n        for p in self.NN.parameters():\n            if p.requires_grad:\n                p.grad = None\n\n\n    @torch.no_grad()\n    def activate_S_fine_tuning(self):\n\n        params = []\n\n        for l in self.NN.layer:\n\n            if hasattr(l,'lr') and l.lr:\n\n                l.K.requires_grad = False\n                l.L.requires_grad = False\n                l.S_hat = torch.nn.Parameter(l.S_hat[:l.dynamic_rank,:l.dynamic_rank])\n                l.fixed = True\n                l.U = torch.nn.Parameter(l.U[:,:l.dynamic_rank],requires_grad = False)\n                l.V = torch.nn.Parameter(l.V[:,:l.dynamic_rank],requires_grad = False)\n                l.step = 'S'\n                params.append(l.S_hat)\n        params = torch.nn.ParameterList(params)\n        self.integrator = self.KLS_optim(params,lr = self.tau,**self.kw)\n\n\n    @torch.no_grad()\n    def S_finetune_step(self):\n\n        self.integrator.step()",
        "experimental_info": "Network Architecture:\n- LeNet5 model using custom low-rank convolutional (`Conv2d_lr`) and linear (`Linear`) layers.\n- Layer configuration (from `models_folder/Lenet5.py`):\n    - `Conv2d_lr(in_channels=1, out_channels=20, kernel_size=5, stride=1, rank=20)`\n    - `torch.nn.ReLU()`\n    - `torch.nn.MaxPool2d(kernel_size=2, stride=2)`\n    - `Conv2d_lr(in_channels=20, out_channels=50, kernel_size=5, stride=1, rank=50)`\n    - `torch.nn.ReLU()`\n    - `torch.nn.MaxPool2d(kernel_size=2, stride=2)`\n    - `torch.nn.Flatten()`\n    - `Linear(800, out_features=500, rank=500)`\n    - `torch.nn.ReLU()`\n    - `Linear(500, out_features=10)` (This last layer is a standard linear layer as `rank` is `None`)\n- All specified low-rank layers use adaptive rank (`fixed=False` is default in custom layers when `rank` is provided).\n\nDataset:\n- MNIST dataset.\n- Data split: 60,000 samples for training (further split into 50,000 for training and 10,000 for validation) and 10,000 for testing.\n- Image preprocessing: Reshaped to (1, 28, 28) and normalized by dividing by 255.\n\nOptimizer:\n- `dlr_opt` (Dynamical Low Rank Optimizer) is used.\n- Internal KLS optimizer: `torch.optim.SGD`.\n- Learning rate (`tau`): 0.05 (default from `run_lenet_mnist.py`).\n- Rank adaptation threshold (`theta`): Explored values include 0.4 and 0.45 (from `run_lenet_mnist.py`). A default of 0.08 is also mentioned in `dlrt_factorization_run.py`.\n- Rank initialization:\n    - For adaptive rank layers, `U` and `V` factors are initialized as random orthonormal matrices.\n    - `S_hat` is initialized as a diagonal matrix from absolute random normal values.\n    - For fixed rank layers (if used), `S_hat` values are modified by a decay factor of `1/(decay)**k`.\n- Basis orthogonalization: `torch.linalg.qr` (with `numpy.linalg.qr` as a fallback) is used for orthonormal basis computation during the `K_postprocess_step` and `L_postprocess_step`.\n- Singular value truncation: `torch.linalg.svd` (with `numpy.linalg.svd` as a fallback) is used in `S_postprocess_step` for rank adaptation. The dynamic rank is adjusted based on the `theta` threshold, ensuring a minimum rank of 2.\n\nTraining:\n- Epochs: 20.\n- Batch Size: 128.\n- Cross-validation runs: 5.\n- Loss function: `torch.nn.CrossEntropyLoss`.\n- Gradient computation: Gradients are computed with respect to the low-rank factors (`K`, `L`, `S_hat`) via multiple forward and backward passes (`populate_gradients` method with 'K', 'L', and 'S' steps) orchestrated by the `dlr_opt.step` method.\n\nComputational Details:\n- Device: CUDA is used if available, otherwise CPU.\n- Convolutional layer reshaping: Filters are implicitly reshaped into matrices using `F.unfold` within the `Conv2d_lr` forward pass to enable low-rank decomposition."
      }
    },
    {
      "title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation",
      "abstract": "While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data. Our code is available at\nhttps://github.com/cjyaras/deep-lora-transformers.",
      "full_text": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation Can Yaras, Peng Wang, Laura Balzano, and Qing Qu Department of Electrical Engineering & Computer Science, University of Michigan June 11, 2024 Abstract While overparameterization in machine learning models offers great benefits in terms of optimization and generalization, it also leads to increased computational requirements as model sizes grow. In this work, we show that by leveraging the inherent low-dimensional structures of data and compressible dynamics within the model parameters, we can reap the benefits of overparameterization without the computational burdens. In practice, we demonstrate the effec- tiveness of this approach for deep low-rank matrix completion as well as fine-tuning language models. Our approach is grounded in theoretical findings for deep overparameterized low-rank matrix recovery, where we show that the learning dynamics of each weight matrix are con- fined to an invariant low-dimensional subspace. Consequently, we can construct and train com- pact, highly compressed factorizations possessing the same benefits as their overparameterized counterparts. In the context of deep matrix completion, our technique substantially improves training efficiency while retaining the advantages of overparameterization. For language model fine-tuning, we propose a method called “Deep LoRA”, which improves the existing low-rank adaptation (LoRA) technique, leading to reduced overfitting and a simplified hyperparameter setup, while maintaining comparable efficiency. We validate the effectiveness of Deep LoRA on natural language tasks, particularly when fine-tuning with limited data. Our code is available at https://github.com/cjyaras/deep-lora-transformers. 1 Introduction In recent years, there has been a growing interest within the realm of deep learning inoverparam- eterization, which refers to employing a greater number of model parameters than necessary to interpolate the training data. While this may appear counterintuitive initially due to the risk of over- fitting, it has been demonstrated to be an effective modeling approach (Zhang et al., 2021; Wu et al., 2017; Allen-Zhu et al., 2019a; Buhai et al., 2020; Xu et al., 2018), primarily attributed to improved optimization landscape and implicit algorithmic regularization. In the context of large language models (LLMs) (Radford et al., 2019; Brown et al., 2020), empirical scaling laws (Kaplan et al., 2020) suggest that larger models are more sample efficient, often requiring fewer samples to reach the same test loss. Taking the problem of low-rank matrix recovery as an illustrative example, the seminal work of Arora et al. (2019) showed that deeper factorizations better promote low-rank solutions as a function of depth, consequently mitigating overfitting in the overparameterized regime compared to a classical two-layer factorization approach; see Figure 1 (left). On the other hand, increasing 1 arXiv:2406.04112v2  [cs.LG]  10 Jun 2024Figure 1: Benefits of depth & width in overparameterized matrix completion with d = 100 , r∗ = 5, ϵl = 10−3 and 30% of entries observed.Left: Recovery error vs. width for shallow and deep factorizations. Right: Number of GD iterations to converge to10−10 error vs. width. We observe that depth prevents overfitting, while width improves convergence. the width of each layer substantially reduces the number of iterations to reach the same training error; see Figure 1 (right). While overparameterization offers remarkable benefits, it also comes with its computational challenges. The significantly increased number of parameters inevitably results in dramatically higher computational costs. This naturally raises a fundamental question: can we attain the benefits of overparameterization with a substantial reduction in computational costs? In this work, we show that we can achieve this by exploiting low-dimensional structures of data and compressible learning dynamics in the model weights. In the context of low-rank matrix recovery via deep overparameterized factorizations,we discover an interesting phenomenon that for each weight matrix,the learning dynamics only happen within an approximately invariant low-dimensional subspace throughout all iterations. We rigorously prove this for deep matrix factorization, which also allows us to compress the number of training parameters significantly when dealing with deep matrix completion. Consequently, we can construct and train a nearly equivalent, yet much smaller, compressed factorization without sacrificing the advantages of its overparameterized counterpart. Interestingly, we empirically find that the above phenomenon can also be observed when em- ploying deep overparameterized weight updates for fine-tuning language models; see Figure 2 for an illustration. Therefore, we can adapt our idea of compressing deep matrix factorization to improve language model fine-tuning. For fine-tuning large-scale pretrained language models, re- cently low-rank adaptation (LoRA) stands out as the most commonly-used technique due to its effectiveness and efficiency (Hu et al., 2021). The basic idea of LoRA is to freeze the pretrained weights and adapt each one to new tasks by adding and optimizing an update in the form of a two-layer low-rank decomposition. Nonetheless, in practical scenarios, selecting the optimal rank of the decomposition can pose a significant challenge. If the rank is not chosen properly, it may lead to overfitting, particularly when we overestimate the rank or when there is limited downstream data available. We deal with this drawback of LoRA by employing a deep (three-layer) overparameterized factorization for the trainable update, which is constructed and optimized via the compression technique used for deep matrix completion. As such, our new method, which we term as Deep LoRA, enjoys notable advantages over the original LoRA method, namely (i)less overfitting by exploiting depth, and (ii)fewer hyperparameterswithout rankr and scale α having to be carefully tuned across all layers, all while having a comparable parameter efficiency due to compression. 2Figure 2: Invariant low-dimensional subspaces in deep overparameterized adaptation of lan- guage models.Fine-tuning BERT (Devlin et al., 2019) with deep overparameterized adaptation on the STS-B dataset (Cer et al., 2017).Left: Singular valuespectra across all adapted layers at the end of fine-tuning. Middle: Alignment of subspaces formed by top 8 right singular vectors between current adapted weights and final adapted weights throughout training.Right: Training losscon- tinues to decrease in iterations after subspace alignment with final adapted weights. See Section 4 for more details. Contributions. We summarize our contributions below. • Practical contributions. We develop efficient compression methods by exploring compressible learning dynamics in overparameterized factorizations. Our method enjoys the benefits of over- parameterization while significantly improving its efficiency. We demonstrate the effectiveness not only on deep matrix completion, but also for improving LoRA for language model fine-tuning. • Theoretical contributions. Our methods are inspired by our theoretical results for deep matrix factorization. Mathematically, we rigorously prove the existence of invariant low-dimensional subspaces throughout gradient descent for each weight matrix, and show how they can con- structed in practice. Related Works There is a great deal of literature on implicit regularization in the setting of matrix factorization/linear networks (Neyshabur et al., 2015; Gunasekar et al., 2017; Arora et al., 2019; Moroshko et al., 2020; Timor et al., 2023; Ji and Telgarsky, 2019; Gidel et al., 2019; You et al., 2020; Liu et al., 2022), as well as low-rank learning in deep networks (Jaderberg et al., 2014; Sainath et al., 2013; Denil et al., 2013; Khodak et al., 2020; Oymak et al., 2019; Min Kwon et al., 2024; Tarzanagh et al., 2023). Similarly, there is an abundance of work discussing the benefits of overparameterization (Du and Hu, 2019; Arora et al., 2018b; Allen-Zhu et al., 2019b; Arpit and Bengio, 2019). 2 Warm-up Study: Deep Matrix Factorization Towards gaining theoretical insights into the phenomena in Figure 2, we first build some intuition based on the problem of deep matrix factorization. Under simplified settings, we rigorously unveil the emergence of low-dimensionality and compressibility in gradient descent learning dynamics. 2.1 Basic Setup Given a low-rank matrixΦ ∈ Rdx×dy with rank(Φ) = r∗, we approximate the matrixΦ by anL-layer deep overparameterized factorization f(Θ) := WLWL−1 ··· W2W1 = WL:1, (1) 3Figure 3: Evolution of SVD of weight matrices.We visualize the SVD dynamics of the first layer weight matrix of anL = 3 layer deep matrix factorization for a random matrix withd = 30, r∗ = 3, ϵl = 1 throughout GD without weight decay. Left: Magnitude of the i-th singular value σi(t) at iteration t. Middle: Angle ∠(vi(t), vi(0)) between the i-th right singular vector at iteration t and initialization. Right: Angle ∠(ui(t), ui(0)) between the i-th left singular vector at iteration t and initialization. where Θ = (Wl)L l=1 are the parameters with weightsWl ∈ Rdl×dl−1 for l ∈ [L]. We consider the case where the weights are all squared0 = d1 = ··· = dL = d, and learn the parametersΘ by solving min Θ ℓ(Θ) = 1 2∥f(Θ) − Φ∥2 F (2) via gradient descent (GD) from scaledorthogonal initialization, i.e., we initialize parametersΘ(0) such that Wl(0)Wl(0)⊤ = Wl(0)⊤Wl(0) = ϵ2 l Id, l∈ [L] (3) where ϵl > 0. We assume this for ease of analysis, and believe that our results could hold for arbitrary small initialization. For each weight matrix, the GD iterations can be written as Wl(t + 1) = (1 − ηλ)Wl(t) − η∇Wlℓ(Θ(t)), l∈ [L] (4) for t = 0, 1, 2, . . ., where η >0 is the learning rate andλ ≥ 0 is an optional weight decay parameter. 2.2 Main Theorem We show that learning only occurs within an invariant low-dimensional subspace of the weight matrices, whose dimensionality depends on rank(Φ). Theorem 2.1. Let Wl(t) satisfy the initialization scheme (3) and updates (4), and suppose Φ ∈ Rd×d is at most rank r and let m := d − 2r > 0. Then there exist orthogonal matrices (Ul)L l=1 ⊂ Od×d and (Vl)L l=1 ⊂ Od×d (depending only on Θ(0) and Φ) satisfying Vl+1 = Ul for l ∈ [L − 1], such that Wl(t) admits the decomposition Wl(t) = Ul \u0014fWl(t) 0 0 ρl(t)Im \u0015 V ⊤ l (5) for all l ∈ [L] and t ≥ 0, where fWl(t) ∈ R2r×2r with fWl(0) = ϵlI2r, and ρl(t) = ρl(t − 1) · (1 − ηλ − η · Y k̸=l ρ2 k(t − 1)) (6) for all l ∈ [L] and t ≥ 1 with ρl(0) = ϵl. In the following, we discuss several implications of our result and its relationship to previous work. 4• SVD dynamics of weight matrices. The decomposition (5) is closely related to the singular value decomposition (SVD) of Wl(t). Specifically, let Ul = [ Ul,1 Ul,2], Vl = [ Vl,1 Vl,2], where Ul,1, Vl,1 ∈ Od×2r, Ul,2, Vl,2 ∈ Od×(d−2r). Let fWl(t) = eUl(t)eΣl(t) eV ⊤ l (t) be an SVD of fWl(t), where eUl(t), eVl(t) ∈ O2r and eΣl(t) ∈ R2r×2r is a diagonal matrix. Then, by (5) we can write Wl(t) as Wl(t) = h Ul,1 eUl(t) Ul,2 i\u0014eΣl(t) 0 0 ρ(t)Im \u0015h Vl,1 eVl(t) Vl,2 i⊤ which is essentially an SVD of Wl(t) (besides the ordering of singular values). According to this, we can verify thatρ(t) is a (repeated) singular value undergoing minimal changes across iterations illustrated in Figure 3 (left). Additionally, these repeated singular values correspond to invariant subspaces Ul,2, Vl,2 that are stationary throughout GD, as seen in Figure 3 (middle and right). • Low-rank bias.From (6), we can show under mild assumptions that the GD trajectory for each weight matrix either remains or tends towards a solution with rank at most2r. This is true whether we employ implicit or explicit regularization. Indeed, if we use small initializationϵl ≈ 0 with no weight decay λ = 0, then the fact that ρl is a decreasing sequence (w.r.t. iteration) implies that the approximate rank ofWl(t) can be no more than2r throughout the entire trajectory. On the other hand, if we use weight decay withλ >0, then we have ρl(t) → 0 as t → ∞. This forces Wl(t) towards a solution of rank at most2r when the training converges. See Appendix E.2 for a formal statement and proof. This result is consistent with previous findings on low-rank and simplicity bias in deep networks (Huh et al., 2022; Galanti and Poggio, 2022; Li et al., 2020; Chou et al., 2024). • Comparison to prior arts.In contrast to existing work studying implicit bias of GD towards low- rank solutions (Gunasekar et al., 2017; Arora et al., 2019), our result explicitly shows how GD finds these solutions. Moreover, unlike previous work on implicit bias (Min et al., 2021; Gissin et al., 2019; Arora et al., 2019; Vardi and Shamir, 2021), we also examine the effect of weight decay, which is commonly employed during the training of deep networks. Our analysis is distinct from that of (Saxe et al., 2014, 2019), which studied continuous time dynamics under the special (separable) setting WL:1(0) = UV ⊤ with Φ = UΣV ⊤. In comparison, our result applies to discrete time dynamics and holds for initialization that is agnostic to the target matrix. It should also be noted that our result does not depend onbalanced initialization like those in (Arora et al., 2018a), as the initialization scaleϵl for each layer can be arbitrarily different from one another. A sketch of analysis. We now provide a rough sketch for the beginning of the proof of Theorem 2.1 in the special case of small initialization ϵl = ϵ ≈ 0 for all l ∈ [L] and λ = 0 , highlighting the construction of the invariant subspace at initialization. The full proof can be found in Appendix E.1. Proof sketch. Since ϵL ≈ 0, from the gradient ofℓ(Θ) (see Appendix E), we have G1 := ∇W1 ℓ(Θ(0)) ≈ −W⊤ L:2(0)Φ (7) implying that the rank ofG1 is (approximately) at mostr. Now consider the subspaceS = N(G1)∩ N(G⊤ 1 W1(0)), where we have dim S ≥d − 2r. Then, there exist orthonormal sets {vi}d−2r i=1 and {ui}d−2r i=1 which satisfy G1vi = 0, ui ∝ W1(0)vi and therefore G⊤ 1 ui ∝ G⊤ 1 W1(0)vi = 0 so along with the orthogonality ofW1(0), the pairs(ui, vi) form singular vector pairs of bothW1(0) and W1(1) simultaneously as they remain unchanged by the gradient updateG1, giving the last 5Figure 4: Network compression for deep matrix factorization.Comparison of trajectories for opti- mizing the original problem (2) vs. the compressed problem(9) with L = 3, d = 1000, r = r∗ = 5, and ϵl = 10−3. Left: Principal components of end-to-end GD trajectories. Right: Training loss vs. wall-time comparison. d − 2r columns of V1 and U1 respectively. To see that we can takeV2 = U1, for instance, we note that ∇W2 ℓ(Θ(0)) · ui ≈ −W⊤ L:3(0)ΦW⊤ 1 (0)ui ∝ W⊤ L:3(0)Φvi = 0 by (7), showing thatui are invariant under gradient updates in the second layer. 2.3 Compression of Overparameterized Factorization We now show that, as a consequence of Theorem 2.1 and the proof sketch, we can run GD on dramatically fewer parameters to achieve a nearidentical end-to-end trajectory as the original (full- width) factorization; see Figure 4. Constructing the “equivalent” compressed factorization. More specifically, given thatΦ is at most rank r and d − 2r >0, from Theorem 2.1 we observe that WL:1(t) = UL,1 fWL:1(t)V ⊤ 1,1 +  LY l=1 ρl(t) ! · UL,2V ⊤ 1,2 ≈ UL,1 fWL:1(t)V ⊤ 1,1| {z } =:fC( eΘ,UL,1,V1,1) , ∀ t = 1, 2, . . . , (8) when we use initialization of small scale (i.e., (ϵl)L l=1 are small). Here, fWL:1 = fWL fWL−1 ··· fW1 with compressed weightsfWl ∈ R2r×2r. Correspondingly,fC( eΘ, UL,1, V1,1) denotes the compressed function with compressed parameters eΘ = (fWl)L l=1. As such, we can expect that solving min eΘ ℓC( eΘ) = 1 2∥fC( eΘ, UL,1, V1,1) − Φ∥2 F (9) will approximately give the same solution as (2). Constructing the factors(Ul, Vl)L l=1. As Theorem 2.1 only showed the existence of(Ul, Vl)L l=1, to solve (9) via GD, we need a practical recipe for constructing(Ul, Vl)L l=1 efficiently at initialization of small scale ϵl. This can be achieved based upon our proof sketch in Section 2.2: we computeG1 = ∇W1 ℓ(Θ(0)) ∈ Rd×d, find an orthonormal set {vi}d−2r i=1 contained in S = N(G1) ∩ N(G⊤ 1 W1(0)), and complete to an orthonormal basis to yield V1. The remaining Ul, Vl can then be iteratively 6constructed via Ul = Wl(0)Vl/ϵl, Vl+1 = Ul, l= 1, ··· , L− 1, and UL = WL(0)VL/ϵL. Finally, we take the first2r columns of UL and V1 to yield UL,1 and V1,1, respectively. It should be noted that these compressed factors are related to,yet distinct from,spectral initialization, which is well-studied in the literature (Chi et al., 2019; Khodak et al., 2020; St¨oger and Soltanolkotabi, 2021). Since Ul,1, Vl,1 are constructed via orthogonal complements to nullspaces involving the gradient, these directions do indeed correlate with the top singular subspaces ofΦ in the deep matrix factorization case (although we do not use the singular value information). On the other hand, our approach is more general through the lens of compression, as it can be applied to a given deep overparameterized factorization trained on anarbitrary loss. Optimization, complexity, and approximation error. In summary, we can approximately solve the original problem by solving(9) via GD for the compressed parameters eΘ = (fWl)L l=1, starting from small initialization (ϵl ≈ 0). The factors UL,1, V1,1 can be efficiently constructed based upon an iterative scheme that we discussed above from the initial weights. Comparing the parameter counts of the compressedfC( eΘ, UL,1, V1,1) vs. the original f(Θ), we only need to optimize4L ·r2 parameters compared to the originalL ·d2. Since r ≪ d, our approach leads to significant improvement in efficiency during GD; see Figure 4 (right). On the other hand, compression requires some additional computation to construct the factorsUL,1 and V1,1 prior to training, which involves taking a gradient of the first weight in the original factorization followed by an SVD or QR decomposition to compute an orthonormal basis for S. While this requires an additional O(d3) compute, this has the same complexity as a single iteration of GD for the original factorization and is therefore a negligible overhead when comparing the two. Finally, the following result demonstrates that our compression method can achieve an almost identical end-to-end trajectory when we use small initializations; see Figure 4 (left). Proposition 2.2. For r such thatm := d −2r >0, if we run GD on the compressed weightseΘ as described above for the loss(9), we have \r\r\rf(Θ(t)) − fC( eΘ(t), UL,1, V1,1) \r\r\r 2 F ≤ m · LY l=1 ϵ2 l for any iteratet = 0, 1, 2, ··· . Here, ϵl is the initialization scale for the weightWl(0). The key idea of Theorem 2.2 is that GD is invariant under orthogonal transformations, and each factor fWl in the end-to-end factorization in(9) is the result of an orthogonal transformation ofWl. Then, the approximation errorm · QL l=1 ϵ2 l is only due to the approximation we showed in(8). We defer the full proof to Appendix E.3. 3 Application I: Deep Matrix Completion In this section, we show that we can generalize our method in Section 2 from vanilla matrix factor- ization to solving low-rank matrix completion problems (Candes and Recht, 2012; Cand`es and Tao, 2010; Davenport and Romberg, 2016) via compressed deep factorizations. Given a ground-truth Φ ∈ Rd×d with rank r∗ ≪ d, the goal of low-rank matrix completion is to recoverΦ from only a few number of observations encoded by a mask Ω ∈ {0, 1}d×d. Adopting a matrix factorization approach, we minimize the objective ℓmc(Θ) = 1 2∥Ω ⊙ (f(Θ) − Φ)∥2 F , (10) 7Figure 5: Network compression for deep matrix completion.Comparison of trajectories for op- timizing the original problem (10) vs. the compressed problem (11) with γ discrepant updates (γ = 0.01) and ablating γ (γ = 0) with L = 3, d = 1000, r = r∗ = 5, ϵl = 10−3 and 20% of entries observed. Left: Principal components of end-to-end trajectories of each factorization.Middle: Recov- ery error vs. iteration comparison.Right: Recovery error vs wall-time comparison. where f(Θ) is the deep overparameterized factorization introduced in(1). The problem simplifies to deep matrix factorization (2) that we studied earlier when Ω = 1d1⊤ d in the full observation case. Additionally,(10) reduces to vanilla (shallow) matrix factorization whenL = 2, whose global optimality and convergence have been widely studied under various settings (Jain et al., 2013; Zheng and Lafferty, 2016; Sun and Luo, 2016; Ge et al., 2016; Bhojanapalli et al., 2016; Ge et al., 2017; Gunasekar et al., 2017; Li et al., 2019; Chi et al., 2019; Li et al., 2018b; Soltanolkotabi et al., 2023; Sun et al., 2018; Zhang et al., 2020; Ding et al., 2021). A double-edged sword of overparameterization. In practice, the true rank r∗ is not known – instead, we assume to have an upper boundr of the same order asr∗, i.e., r∗ ≤ r ≪ d. Surprisingly, overparameterization has advantages in terms of both depthL and width r: • Benefits of depth: mitigating overfitting.When r > r∗, it has been demonstrated (Arora et al., 2019) that optimizing deeper factorizations (i.e., L ≥ 3) generalize better in the low sample regime, while their shallow counterparts overfit; see Figure 1 (left). • Benefits of width: improving convergence.On the other hand, increasing the width r of the deep factorization beyondr∗ results in accelerated convergence in terms of iterations, see Figure 1 (right). However, the advantages of overparameterization come with the challenges of much higher compu- tational costs. For anL-layer factorization of (full)-widthd, we requireO(L·d3) multiplications per iteration to evaluate gradients and need to storeO(L · d2) parameters, where d is often very large. Using ideas from Section 2, however, we can obtain the benefits of overparameterization without the extra computational costs. Compression for deep matrix completion. Given the similarity between deep matrix factoriza- tion and completion (i.e., Ω = 1d1⊤ d vs arbitrary Ω), it seems straightforward to generalize our compression methods in Section 2.3 to deep matrix completion. However, as shown by the orange trace in Figure 5, direct application does not work well, as the compressed factorization’s trajectory diverges from that of the original. This is because the compressed subspaces UL,1, V1,1 ∈ Rd×2br computed at the initializationΘ(0) via the gradient ∇W1 ℓmc(Θ(0)) ≈ −W⊤ L:2(0)[Ω ⊙ Φ] can be misaligned with the true subspace due to the perturbation by the maskΩ. 8Nonetheless, this issue can be mitigated by slowly updatingUL,1, V1,1 during training. Specifi- cally, compared to (9), we minimize min eΘ,UL,1,V1,1 1 2∥Ω ⊙ (fC( eΘ, UL,1, V1,1) − Φ)∥2 F (11) via GD by updating eΘ, UL,1, V1,1 simultaneously every iteration, with a learning rateη on eΘ along with a discrepant learning rate γη on UL,1, V1,1. Because we updateUL,1, V1,1 slower than updating eΘ, we generally chooseγ >0 to be small and tuned accordingly for the given problem. As a result, we reduce computational costs toO((L + d) · r2) multiplications per iteration for computing gradients, andO(d·r +L·r2) parameters. Yet still the trajectory of the deep compressed factorization ultimately aligns with that of the original, while converging roughly5× faster w.r.t. wall-time, as demonstrated in Figure 5. Moreover, the accelerated convergence induced by the full- width trajectory results in the compressed factorization being3× faster than randomly initialized factorizations of similar width – see Appendix D.1 for more details. 4 Application II: Model Fine-tuning In this section, we show that our compression idea can be further extended to parameter-efficient fine-tuning of pretrained language models, specifically via low-rank adaptation (LoRA) (Hu et al., 2021). In particular, inspired by our approach for deep matrix completion, we propose Deep Low- Rank Adaptation (Deep LoRA), which consistently outperformsvanilla LoRA in the limited sample regime. 4.1 Deep Low-Rank Adaptation (Deep LoRA) Background on LoRA. With the ever-growing size of pretrained models and countless down- stream tasks, full model fine-tuning is often computationally infeasible. Given a pretrained model whose parameters consist of a collection of dense weight matrices {W0k}m k=1 ⊂ Rd×d (e.g., the query/key/value projections of a transformer (Vaswani et al., 2017)), LoRA seeks to adapt each layer to a given task byfreezing the pretrained weight{W0k}m k=1 and optimizing an extra trainable low-rank factorization on top. In other words, the fine-tuned weightWk is given by Wk = W0k + α r W(2) k W(1) k where α > 0 is a tunable scale parameter and W(2) k ∈ Rd×r, W(1) k ∈ Rr×d with r ≪ d, thereby substantially reducing the number of trainable parameters during fine-tuning. Proposed method: Deep LoRA. For vanilla LoRA, if we view adapting each weight matrix of model as an individual low-rank factorization problem, we have demonstrated in previous sections that overparameterization and subsequently compressing factorizations improves generalization with minimal extra computational costs. With this in mind, we can employ a deep overparameter- ized adaptation of each pretrained weight as Wk = W0k + W(L) k ··· W(2) k W(1) k| {z } =:∆Wk (12) where each W(i) k is full-width, i.e., W(i) k ∈ Rd×d. Here, L >0 is the depth, and typically we choose L = 3, which is precisely the setting of Figure 2. From the figure, we can see that (i) all the converged weights {∆Wk}m k=1 are very low-rank (left panel), (ii) the learning dynamics each for each weight 9Figure 6: Deep LoRA shows better performance on few shot fine-tuning over vanilla LoRA, with varying numbers of training samples. For each case, we draw n samples at random from STS-B over 20 trials with different seeds, and measure performance on the validation split of each method using the same train set. Figure 7: Deep LoRA finds lower rank solutionscompared to vanilla LoRA. We plot a his- togram of numerical ranks for Deep LoRA and vanilla LoRA with r = 8 after adapting to STS-B with 256 samples. The numerical rank is computed as the number of singular values σi greater than 10−8 and dσ1ϵ where ϵ is machine epsilon. Figure 8: Deep LoRA is more robust to the choice of rank compared to vanilla LoRA. For each choice of rank r, we draw 16 samples at random from STS-B over 5 trials with differ- ent seeds, and measure perfor- mance on the validation split of each method using the same train set. approximately stay within the same invariant subspace throughout the iterations (middle panel), and (iii) this happens independent of the training loss decreasing (right panel). These observations imply that deep overparameterized factorizations in Deep LoRA arehighly compressible, so we can apply the compression method from deep matrix completion in Section 3 to compress the learning dynamics for each individual weight for model fine-tuning. Here, the major differences of our compression approach for deep LoRA from that of deep matrix completion is that (i) we have a separate compressed factorization for each layer to be adapted, and (ii) the fine-tuned loss function can be tailored for specific tasks (e.g., the cross-entropy) besides theℓ2 loss. Advantages of Deep LoRA. Compared to vanilla LoRA, Deep LoRA has clear advantages that we highlight below. More details are provided in Section 4.2. • Less overfitting in limited data regime. Fine-tuning overparameterized models using LoRA can still result in overfitting in few shot or limited data regime (Sebastian Raschka, 2023). In comparison, the extra depth in (12) of Deep LoRA can help prevent overfitting (see Figure 6), which is similar to deep matrix completion in Figure 1. • Robustness to the hyperparameter r. As shown in Figure 8, by exploiting the intrinsic low- dimensional dynamics in GD via overparameterization in width, our approach is robust to the choice of the rankr in fine-tuning. Deep LoRA only requires3r2 additional trainable parameters for each adapted layer compared to vanilla LoRA, wherer is relatively small (e.g.,r = 8). 4.2 More Experimental Details To evaluate our approach, we use a pretrained BERT (Devlin et al., 2019) base model and apply adaptation on all attention and feedforward weights in the transformer, resulting in 72 adapted layers in total. Unless stated otherwise, we user = 8 for both vanilla and Deep LoRA throughout all experiments, in which case Deep LoRA has roughly 0.01% more parameters (with respect to BERT) 10Table 1:Improvement of Deep LoRA over vanilla LoRA for limited data GLUE fine-tuning.For each task, we draw 1024 samples at random over 10 trials with different seeds, and report the performance gap (with variance) on the validation split between Deep LoRA and vanilla LoRA using the same train set. CoLA MNLI MRPC QNLI QQP ∆ +0 .090±0.002 +0.011±0.0005 +0.0042±0.001 +0.048±0.0009 +0.005±0.0002 RTE SST-2 STS-B Overall ∆ +0 .029±0.002 +0.019±0.0006 +0.018±0.00006 +0.028±0.002 than vanilla LoRA. We utilize Adam (Kingma and Ba, 2014) as an optimizer for both methods. See Appendix B for more details on the experimental setup. Advantage I: Better generalization with limited data. We first evaluate our approach on tasks in the GLUE benchmark (Wang et al., 2018), which is a standard benchmark for natural language understanding. To test the performance in a limited data setting, for one given trial of a single task, we randomly sample 1024 examples from the task data for fine-tuning, a nd compare the difference in performance on the same train set between Deep LoRA and vanilla LoRA on the entire validation split. From the results shown in Table 1, we can see that Deep LoRA delivers significant improvements across most tasks compared to vanilla LoRA, and on average improves performance by nearly 3 points, a notable margin. This improvement in performance becomes more pronounced in scenarios with severely lim- ited data, such as few-shot settings. Applying the same sampling procedure as in the prior study to the STS-B dataset, we assess both approaches using only n ∈ {16, 64, 256} training instances. Experiments in Figure 6 illustrate that Deep LoRA consistently surpasses vanilla LoRA across all sample sizes, with the most significant difference observed whenn = 16. Deep LoRA finds lower rank solutions. We find that at the end of fine-tuning, Deep LoRA finds lower rank solutions for∆Wk than vanilla LoRA, as shown in Figure 7. In the limited data setting (256 samples), we see that all adapted layers in the vanilla LoRA saturate the constrained numerical rankr = 8,while most layers in Deep LoRA are perturbed by matrices with numerical ranks between 0 and 4.1 This suggests that Deep LoRA can adaptively select the appropriate rank for each layer depending on the task. This low-rank bias induces implicit regularization during the fine-tuning process and ultimately prevents overfitting to the task, particularly when only few training samples are available. As a practical consideration, Deep LoRA also requires a fraction of the memory cost to store compared to vanilla LoRA due to the parsimony in adapted weights. Advantage II: Robustness to choice of rank r. Due to the scarcity of the target training data, choosing the rank r in LoRA is a delicate process – it needs to be large enough to capture the com- plexity in modeling the downstream task, while small enough to prevent overfitting. The proposed Deep LoRA, on the other hand, avoids catastrophic overfitting as we increaser, as demonstrated in Figure 8. This observation mirrors the behavior seen in deep matrix completion, as illustrated in Figure 1. For shallow factorizations, an overestimation of rankr leads to an increase in the general- ization error. In contrast, deep factorizations remain resilient to overfitting. 1A majority of them are in fact zero, i.e., no change from pretrained weights. 11Finally, we show that Deep LoRA outperforms vanilla LoRA for few-shot natural language generation fine-tuning in Appendix C. We also provide an ablation study in Appendix D.2 on the compression mechanism for Deep LoRA and show that it is crucial for accelerating training. 5 Conclusion & Future Directions In this work, we have provided an in-depth exploration of low-dimensionality and compressibil- ity in the dynamics of deep overparameterized learning, providing theoretical understandings in the setting of deep matrix factorization and applications to efficient deep matrix completion and language model adaptation. Finally, we outline a couple potential future directions following this work. Compressibility in non-linearsettings. Although the results on network compression in Section 2 exploit the specific gradient structure of deep matrix factorizations, we believe that our analysis can provide meaningful direction for analyzing the fully non-linear case. To sketch an idea, consider the setting of Section 2.1 except with anon-linear factorization, i.e., (1) becomes f(Θ) := WLσ(WL−1 ··· σ(W2σ(W1))) (13) where σ is (for example) the entry-wise ReLU activation. For concreteness, consider theL = 3 case. The gradient of the loss with respect to, e.g.,W2 in (2) is given by ∇W2 ℓ(Θ) = [h(W2σ(W1)) ⊙ (W⊤ 3 E)]σ(W1)⊤ where h is the entry-wise unit step function and E = f(Θ) − Φ. Comparing this to the gradient in the linear setting (14), there is a great deal of shared structure, with the two main differences being the non-linearity applied to W1 in the post factor and a projection on the inner term via h(W2σ(W1)). However, we still have the low-rank structure ofW⊤ 3 E, and the zeroing out of certain entries preserves approximate spectral properties of the matrix (Chatterjee, 2015). Moreover, this projection is akin to the masking via Ω as in deep matrix completion from Section 3, for which we do find compressible dynamics. In Figure 9, we plot the singular value spectrum of the above gradient at small initialization, finding that the topr∗ singular values separate from the rest of the spectrum. This suggests that we may be able to identify a low-dimensional subspace along which we can achieve similar dynamics to the full parameter space. Extensions to Deep LoRA. We have demonstrated the efficacy of Deep LoRA for natural language understanding and generation in Section 4 and Appendix C respectively. However, it would be meaningful to evaluate Deep LoRA in other modalities, e.g., diffusion models, where fine-tuning on limited data is commonplace. Moreover, the high degree of alignment at initialization to the final adapted subspaces shown in Figure 2 suggests that SGD (rather than Adam) can be used for the outer factors of Deep LoRA, further reducing memory costs. Finally, exploring the use of second-order methods to accelerate fine-tuning along the rank- r subspace could be a potential improvement. Implications for representation learning. The low-rank bias in the end-to-end features of deep networks may have important connections to emergent phenomena in representation learning, such as deep neural collapse (Zhai et al., 2024; Zhou et al., 2022b; Yaras et al., 2022; Wang et al., 2022; Zhou et al., 2022a; Zhu et al., 2021; Beaglehole et al., 2024; Li et al., 2024), whereby the last- layer representations exhibit surprisingly simple structures. Moreover, by uncovering the low-rank 12Figure 9: Low-rank gradient of non-linear factorizations at initialization.Singular values spec- trum of ∇W2 ℓ(Θ) at initialization for non-linear factorization with L = 3, d = 1000, r∗ = 5, and ϵl = 10−3. The top 5 singular values separate from the tail of the spectrum. evolution ofindividual weights, we could shed light on more intricate phenomena such asprogressive neural collapse (He and Su, 2023; Wang et al., 2023). Acknowledgement The work of L.B., P.W., and C.Y. were supported in part by DoE award DE-SC0022186, ARO YIP award W911NF1910027, and NSF CAREER award CCF-1845076. Q.Q., P.W., and C.Y. acknowledge support from ONR N00014-22-1-2529 and NSF CAREER CCF-214390. Q.Q. also acknowledges sup- port from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF-2212326, and NSF IIS 2312842, an AWS AI Award, a gift grant from KLA, and MICDE Catalyst Grant. References Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319–7328, 2021. Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers.Advances in neural information processing systems, 32, 2019a. Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over- parameterization. In International conference on machine learning, pages 242–252. PMLR, 2019b. Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. InInternational Conference on Learning Representations, 2018a. Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning, pages 244–253. PMLR, 2018b. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32, 2019. 13Devansh Arpit and Yoshua Bengio. The benefits of over-parameterization at initialization in deep relu networks. arXiv preprint arXiv:1901.03611, 2019. Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correla- tion with human judgments. InProceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/W05-0909. Daniel Beaglehole, Peter S´uken´ık, Marco Mondelli, and Mikhail Belkin. Average gradient outer product as a mechanism for deep neural collapse. arXiv preprint arXiv:2402.13728, 2024. Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Rares-Darius Buhai, Yoni Halpern, Yoon Kim, Andrej Risteski, and David Sontag. Empirical study of the benefits of overparameterization in learning latent variable models. InInternational Conference on Machine Learning, pages 1211–1219. PMLR, 2020. Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization.Communications of the ACM, 55(6):111–119, 2012. Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. InProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics, 2017. Sourav Chatterjee. Matrix estimation by universal singular value thresholding.The Annals of Statistics, 43(1): 177–214, 2015. Arnav Chavan, Zhuang Liu, Deepak Gupta, Eric Xing, and Zhiqiang Shen. One-for-all: Generalized lora for parameter-efficient fine-tuning. arXiv preprint arXiv:2306.07967, 2023. Yuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization: An overview. IEEE Transactions on Signal Processing, 67(20):5239–5269, 2019. Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, and Holger Rauhut. Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank.Applied and Computational Harmonic Analysis, 68:101595, 2024. Jeff Dalton and Atul Deshmane. Artificial neural networks.IEEE Potentials, 10(2):33–36, 1991. Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observa- tions. IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016. Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando De Freitas. Predicting param- eters in deep learning. Advances in neural information processing systems, 26, 2013. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019. 14Lijun Ding, Liwei Jiang, Yudong Chen, Qing Qu, and Zhihui Zhu. Rank overspecified robust matrix recovery: Subgradient method and exact recovery.Advances in Neural Information Processing Systems, 34:26767–26778, 2021. Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. InInterna- tional Conference on Machine Learning, pages 1655–1664. PMLR, 2019. Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. InProceedings of the First Workshop on Neural Machine Translation, pages 56–60, 2017. Tomer Galanti and Tomaso Poggio. Sgd noise and implicit low-rank bias in deep neural networks. Technical report, Center for Brains, Minds and Machines (CBMM), 2022. Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in neural information processing systems, 29, 2016. Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pages 1233–1242. PMLR, 2017. Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in linear neural networks. Advances in Neural Information Processing Systems, 32, 2019. Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely. The implicit bias of depth: How incremental learning drives generalization. In International Conference on Learning Representations, 2019. Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in neural information processing systems, 30, 2017. Hangfeng He and Weijie J Su. A law of data separation in deep learning.Proceedings of the National Academy of Sciences, 120(36):e2221704120, 2023. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. InInternational Conference on Learning Representations, 2021. Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks.Transactions on Machine Learning Research, 2022. Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference 2014. British Machine Vision Association, 2014. Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating min- imization. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 665–674, 2013. Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In7th International Conference on Learning Representations, ICLR 2019, 2019. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Mikhail Khodak, Neil A Tenenholtz, Lester Mackey, and Nicolo Fusi. Initialization and regularization of factorized neural layers. InInternational Conference on Learning Representations, 2020. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 15Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix adaptation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=NjNfLdxr3A. Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018a. Pengyu Li, Xiao Li, Yutong Wang, and Qing Qu. Neural collapse in multi-label learning with pick-all-label loss. In Forty-first International Conference on Machine Learning, 2024. Qiuwei Li, Zhihui Zhu, and Gongguo Tang. The non-convex geometry of low-rank matrix optimization. Information and Inference: A Journal of the IMA, 8(1):51–96, 2019. Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2–47. PMLR, 2018b. Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning. InInternational Conference on Learning Representations, 2020. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. InText Summarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //www.aclweb.org/anthology/W04-1013. Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In International Conference on Machine Learning, pages 14153–14172. PMLR, 2022. Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada. On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks. InInternational Conference on Machine Learning, pages 7760–7768. PMLR, 2021. Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, and Qing Qu. Efficient low-dimensional com- pression of overparameterized models. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 ofProceedings of Machine Learning Research, pages 1009–1017. PMLR, 02–04 May 2024. Edward Moroshko, Blake E Woodworth, Suriya Gunasekar, Jason D Lee, Nati Srebro, and Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. Advances in neural information processing systems, 33:22182–22193, 2020. Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. In Workshop at International Conference of Learning Represeatations, 2015. Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201–206, 2017. Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian,. In ICML Workshop on Generalization in Deep Networks, 2019. Long version at https://arxiv.org/abs/1906.05392. Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. Bleu: a method for automatic evaluation of machine translation. pages 311–318, 2002. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 16Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020. Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In2013 IEEE international conference on acoustics, speech and signal processing, pages 6655–6659. IEEE, 2013. A Saxe, J McClelland, and S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. InProceedings of the International Conference on Learning Represenatations 2014. International Conference on Learning Represenatations 2014, 2014. Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. Proceedings of the National Academy of Sciences, 116(23):11537–11546, 2019. PhD Sebastian Raschka. Practical tips for finetuning llms using lora (low-rank adaptation), Nov 2023. URL https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms . Mahdi Soltanolkotabi, Dominik St¨oger, and Changzhi Xie. Implicit balancing and regularization: General- ization and convergence guarantees for overparameterized asymmetric matrix sensing. InThe Thirty Sixth Annual Conference on Learning Theory, pages 5140–5142. PMLR, 2023. Dominik St¨oger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learning: Opti- mization and generalization guarantees for overparameterized low-rank matrix reconstruction.Advances in Neural Information Processing Systems, 34:23831–23843, 2021. Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Foundations of Computational Mathematics, 18:1131–1198, 2018. Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization.IEEE Transactions on Information Theory, 62(11):6535–6579, 2016. Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak. Transformers as support vector machines. In NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning, 2023. Nadav Timor, Gal Vardi, and Ohad Shamir. Implicit regularization towards rank minimization in relu net- works. In International Conference on Algorithmic Learning Theory, pages 1429–1459. PMLR, 2023. Gal Vardi. On the implicit bias in deep-learning algorithms.Communications of the ACM, 66(6):86–93, 2023. Gal Vardi and Ohad Shamir. Implicit regularization in relu networks with the square loss. InConference on Learning Theory, pages 4224–4258. PMLR, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing systems, 30, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi- task benchmark and analysis platform for natural language understanding. InInternational Conference on Learning Representations, 2018. Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu. Linear convergence analysis of neural collapse with unconstrained features. In OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop), 2022. Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu. Understanding deep rep- resentation learning via layerwise feature compression and discrimination.arXiv preprint arXiv:2311.02960, 2023. 17Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,Tim Rault,R ´emi Louf,Morgan Funtowicz,et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017. Ji Xu, Daniel J Hsu, and Arian Maleki. Benefits of over-parameterization with em. Advances in Neural Information Processing Systems, 31, 2018. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment.arXiv preprint arXiv:2312.12148, 2023. Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu. Neural collapse with normalized features: A geometric analysis over the riemannian manifold.Advances in neural information processing systems, 35: 11547–11560, 2022. Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant learning rates for double over-parameterization. InAdvances in Neural Information Processing Systems, 2020. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language model fine-tuning. InConference on Parsimony and Learning, pages 202–227. PMLR, 2024. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learn- ing (still) requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. InThe Eleventh International Conference on Learning Representations, 2022. Yuqian Zhang, Qing Qu, and John Wright. From symmetry to geometry: Tractable nonconvex problems. arXiv preprint arXiv:2007.06753, 2020. Zhong Zhang, Bang Liu, and Junming Shao. Fine-tuning happens in tiny subspaces: Exploring intrinsic task-specific subspaces of pre-trained language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1701–1713, 2023. Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using burer- monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016. Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu. On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features. InInternational Conference on Machine Learning, pages 27179–27202. PMLR, 2022a. Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui Zhu. Are all losses created equal: A neural collapse perspective. Advances in Neural Information Processing Systems, 35:31697–31710, 2022b. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features.Advances in Neural Information Processing Systems, 34:29820–29834, 2021. 18Appendix Appendix A Related Works Appendix B Experimental Details Appendix C Evaluation on Natural Language Generation Appendix D Ablating Compression Mechanism Appendix E Proofs In Appendix A, we provide an in-depth discussion of related works. In Appendix B, we provide further details for experiments in Section 4. In Appendix C, we carry out additional experiments for evaluating Deep LoRA for few-shot natural language generation fine-tuning. In Appendix D, we carry out an ablation study for the compression mechanism presented in the main paper, for both deep matrix completion and Deep LoRA. In Appendix E, we provide proofs of all claims from Section 2. A Related Works & Future Directions Implicit regularization. The first work to theoretically justify implicit regularization in matrix fac- torization (Gunasekar et al., 2017) was inspired by empirical work that looked into the implicit bias in deep learning (Neyshabur et al., 2015) and made the connection with factorization approaches as deep nets using linear activations2. Since then a long line of literature has investigated deep factorizations and their low-rank bias, including (Arora et al., 2019; Moroshko et al., 2020; Timor et al., 2023); in fact there is so much work in this direction that there is already a survey in the Communications of the ACM (Vardi, 2023). Several older works explicitly imposed low-rank factorization in deep networks (Jaderberg et al., 2014; Sainath et al., 2013) or studied a low-rank factorization of the weights after the learning process (Denil et al., 2013). Newer works along these lines discuss initialization and relationships to regularization (Khodak et al., 2020). The work in Oymak et al. (2019) also discusses low-rank learning in deep nets, by studying the singular vectors of the Jacobian and arguing that the “information space” or top singular vectors of the Jacobian are learned quickly. Very recent work has shown that the typical factorization of the weights in an attention layer of a transformer into key and query layers has an implicit bias towards low-rank weights (Tarzanagh et al., 2023). Overparameterization. There is a sizeable body of work discussing the various benefits of overpa- rameterization in deep learning settings, of which we discuss a few. Du and Hu (2019) demonstrate that width is provably necessary to guarantee convergence of deep linear networks. Arora et al. (2018b) show that overparameterization can result in an implicit acceleration in optimization dy- namics for training deep linear networks. Allen-Zhu et al. (2019b) argue that overparameterization 2Of course linear activation has been considered throughout the history of artificial neural nets, but the fact that a multilayer network with linear activation has an equivalent one-layer network meant this architecture was summarily ignored. This is evidenced in (Dalton and Deshmane, 1991): “In summary, it makes no sense to use a multilayered neural network when linear activation functions are used.” 19plays a fundamental role in rigorously showing that deep networks find global solutions in polyno- mial time. Arpit and Bengio (2019) shows that depth in ReLU networks improves a certain lower bound on the preservation of variance in information flowing through the network in the form of activations and gradients. LoRA and its variants. There is a substantial body of existing work in the realm of parameter efficient fine-tuning – see Xu et al. (2023) for a comprehensive survey. However, the method that has arguably gained the most traction in recent years is LoRA (Hu et al., 2021), in which trainable rank decomposition matrices are added on top of frozen pretrained weights to adapt transformers to downstream tasks. Since then, there has been a plethora of variants. Generalized LoRA (Chavan et al., 2023) proposes a general formulation of LoRA that encapsulates a handful of other parameter efficient adapters. AdaLoRA (Zhang et al., 2022) parameterizes the updates in an SVD-like form and iteratively prunes the inner factor to dynamically control the factorization rank. VeRA (Kopiczko et al., 2024) parameterizes the updates via diagonal adapters which are transformed via random projections that are shared across layers. The idea of LoRA was initially inspired by the notion of lowintrinsic dimension of fine-tuning pretrained models. Intrinsic dimension for an objective was first proposed in Li et al. (2018a), where it was defined to be the minimum dimensionality needed for a random subspace to contain a solution to the objective. Using this idea, Aghajanyan et al. (2021) demonstrated that the objective of fine-tuning a pretrained model has a low intrinsic dimension. Building on this, Zhang et al. (2023) learns the intrinsic subspace of a given fine-tune task from the original parameter trajectory to investigate transferability between these task-specific subspaces. B Experimental Details The pretrained BERT and T5 models are retrieved from the HuggingFace transformers library (Wolf et al., 2019) as google-bert/bert-base-cased and google-t5/t5-base respectively. We choose the best learning rate for each method from η ∈ {10−5, 10−4, 10−3, 10−2} on STS-B with 1024 samples, and find thatη = 10−4 and α = 8 works best for vanilla LoRA, whileη = 10−2 with γ = 10−2 works best for Deep LoRA (althoughγ can be chosen relatively freely). We tried using a linear decay learning rate but found worse results in the limited data setting for both vanilla and Deep LoRA. We use a maximum sequence length of 128 tokens for all tasks. Vanilla LoRA is initialized in the same fashion as the original paper (i.e., W(2) k is initialized to all zeros, W(1) k is initialized to be Gaussian with standard deviation 1), whereas Deep LoRA is compressed from a full-width 3-layer factorization with orthogonal initialization of scaleϵl = 10−3. We use a train batch size of 16, and train all models until convergence in train loss, and use the final model checkpoint for evaluation. For generative tasks, we use beam search Freitag and Al-Onaizan (2017) with beam size 4 and maximum generation length of 64. All experiments are carried out on a single NVIDIA Tesla V100 GPU, with time and memory usage reported in Table 2. Table 2: Comparison of step wall-time and memory usage for vanilla and Deep LoRA. Method Iteration Wall-Time (ms) Memory Usage (GB) Vanilla LoRA 102 12.526 Deep LoRA 106 12.648 20C Evaluation on Natural Language Generation In addition to the natural language understanding tasks evaluated in Section 4, we test the effec- tiveness of Deep LoRA compared to vanilla LoRA for few-shot fine-tuning for natural language generation (NLG), specifically on the E2E dataset (Novikova et al., 2017) with the T5 base model (Raffel et al., 2020). All hyperparameters are as reported in Section 4 and Appendix B. The results are shown in Table 3. We observe significant improvements using Deep LoRA in BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores, with marginally worse results with respect to METEOR (Banerjee and Lavie, 2005) score. Table 3:Improvement of Deep LoRA over vanilla LoRA for few-shot NLG fine-tuning.On the E2E dataset, we draw 16 samples at random over 10 trials with different seeds, and report the average performance gap on the validation split between Deep LoRA and vanilla LoRA for various metrics using the same train set. BLEU ROUGE-1 ROUGE-2 ROUGE-L ROUGE-LSUM METEOR ∆ +0 .033 +0 .032 +0 .056 +0 .061 +0 .047 −0.00036 D Ablating Compression Mechanism D.1 Deep Matrix Completion Figure 10: Comparing efficiency of compressed networks vs. randomly initialized narrow net- works for deep matrix completionwith different overestimated r and L = 3, d = 1000, r∗ = 5, ϵl = 10−3 and 20% of entries observed.Left: Number of iterations to converge.Right: Wall-time to converge. We compare the training efficiency of deep 2r-compressed factorizations (within a wide net- work of width d ≫ r) with randomly initialized deep factorizations of width 2r. As depicted in Figure 10 (left), the compressed factorization requires fewer iterations to reach convergence, and the number of iterations necessary is almost unaffected byr. Consequently, training compressed factorizations is considerably more time-efficient than training narrow networks of the same size, 21provided that r is not significantly larger than r∗. The distinction between compressed and nar- row factorizations underscores the benefits of wide factorizations, as previously demonstrated and discussed in Figure 1 (right), where increasing the width results in faster convergence. However, increasing the width alone also increases computational costs – by employing compression, we can achieve the best of both worlds. D.2 Deep LoRA Figure 11: Compression enables faster convergence of Deep LoRA.We compare full-width, com- pressed, and narrow deep factorizations for adapting to STS-B with 16 samples.Left: Batch train loss vs. iterations.Right: STS-B evaluation metric (Pearson correlation) vs. iterations. We verify that compression is crucial for the efficiency of Deep LoRA. We compare the perfor- mance of three different approaches: (i)Original, where we use a three-layer full-width factorization as in (12), (ii) Compressed, which is the rank-r compression of (12) (a.k.a. Deep LoRA), and (iii) Random, where the W(i) k in (12) are initialized randomly with W(2) k ∈ Rr×r. We can see that via compression, Deep LoRA can achieve similar convergence behavior to the original overparameter- ized factorization with much fewer parameters, while the randomly initialized version takes much longer to train, similar to the result for deep matrix completion in Appendix D.1. E Proofs The analytic form of the gradient∇Θ ℓ(Θ) is given by ∇Wlℓ(Θ) = W⊤ L:l+1EW ⊤ l−1:1, l∈ [L] (14) where E = f(Θ) − Φ, which when substituted into (4) gives the update rules Wl(t + 1) = (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t)E(t)W⊤ l−1:1(t), l∈ [L] (15) for t = 0, 1, 2, . . ., where E(t) = f(Θ(t)) − Φ. We first establish the following Theorem E.1 – the claim in Theorem 2.1 then follows in a rela- tively straightforward manner. We note that all statements quantified byi in this section implicity hold for all i ∈ [m] (as defined in Theorem 2.1) for the sake of notational brevity. 22E.1 Proof of Theorem 2.1 Lemma E.1. Under the setting of Theorem 2.1, there exist orthonormal sets{u(l) i }m i=1 ⊂ Rd and {v(l) i }m i=1 ⊂ Rd for l ∈ [L] satisfying v(l+1) i = u(l) i for all l ∈ [L − 1] such that the following hold for allt ≥ 0: A(t) : Wl(t)v(l) i = ρl(t)u(l) i ∀l ∈ [L], B(t) : W⊤ l (t)u(l) i = ρl(t)v(l) i ∀l ∈ [L], C(t) : Φ⊤WL:l+1(t)u(l) i = 0 ∀l ∈ [L], D(t) : ΦW⊤ l−1:1(t)v(l) i = 0 ∀l ∈ [L], where ρl(t) = ρl(t − 1) · (1 − ηλ − η · Q k̸=l ρk(t − 1)2) for all t ≥ 1 with ρl(0) = ϵl > 0. Proof. Define Ψ := W⊤ L:2(0)Φ. Since the rank ofΦ is at most r, we have that the rank ofΨ ∈ Rd×d is at most r, which implies thatdim N (Ψ) = dim N \u0000 Ψ⊤\u0001 ≥ d − r. We define the subspace S := N (Ψ) ∩ N \u0010 Ψ⊤W1(0) \u0011 ⊂ Rd. Since W1(0) ∈ Rd×d is nonsingular, we have dim(S) ≥ 2(d − r) − d = m. Let {v(1) i }m i=1 denote an orthonormal set contained inS and set u(1) i := W1(0)v(1) i /ϵ1, where ϵ1 > 0 is the scale of W1(0) – since W1(0)/ϵ1 is orthogonal, {u(1) i }m i=1 is also an orthonormal set. Then we trivially haveW1(0)v(1) i = ϵ1u(1) i , which implies W⊤ 1 (0)u(1) i = ϵ1v(1) i . It follows fromv(1) i ∈ Sthat Ψv(1) i = 0and Ψ⊤W1(0)v(1) i = 0,which is equivalent toW⊤ L:2(0)Φv(1) i = 0and Φ⊤WL:2(0)W1(0)v(1) i = ϵ1Φ⊤WL:2(0)u(1) i = 0 respectively. SinceW⊤ L:2(0) is full column rank, we further have thatΦv(1) i = 0. Now letE(l) denote that we have orthonormal sets{u(l) i }m i=1 and {v(l) i }m i=1 satisfying Wl(0)v(l) i = ϵlu(l) i , W⊤ l (0)u(l) i = ϵlv(l) i , Φ⊤WL:l+1(0)u(l) i = 0, and ΦW⊤ l−1:1(0)v(l) i = 0. From the above argu- ments, we have thatE(1) holds – now supposeE(k) holds for some1 ≤ k < L. Setv(k+1) i := u(k) i and u(k+1) i := Wk+1(0)v(k+1) i /ϵk+1. This implies thatWk+1(0)v(k+1) i = ϵk+1u(k+1) i and W⊤ k+1(0)u(k+1) i = ϵk+1v(k+1) i . Moreover, we have Φ⊤WL:(k+1)+1(0)u(k+1) i = Φ⊤WL:k+1(0)W⊤ k+1(0)u(k+1) i /ϵ2 k+1 = Φ⊤WL:k+1(0)v(k+1) i /ϵk+1 = Φ⊤WL:k+1(0)u(k) i /ϵk+1 = 0, where the first two equalities follow from orthogonality andu(k+1) i = Wk+1(0)v(k+1) i /ϵk+1, and the last equality is due tov(k+1) i = u(k) i . Similarly, we have ΦW⊤ (k+1)−1:1(0)v(k+1) i = ΦW⊤ k−1:1(0)W⊤ k (0)v(k+1) i = ΦW⊤ k−1:1(0)W⊤ k (0)u(k) i = ϵkΦW⊤ k−1:1(0)v(k) i = 0, where the second equality follows fromv(k+1) i = u(k) i and the third equality is due toW⊤ k (0)u(k) i = ϵkv(k) i . Therefore E(k + 1) holds, so we haveE(l) for all l ∈ [L]. As a result, we have shown the base cases A(0), B(0), C(0), and D(0). Now we proceed by induction ont ≥ 0. Suppose that A(t), B(t), C(t), and D(t) hold for some 23t ≥ 0. First, we showA(t + 1) and B(t + 1). We have Wl(t + 1)v(l) i = h (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t)E(t)W⊤ l−1:1(t) i v(l) i = h (1 − ηλ)Wl(t) − ηW⊤ L:l+1(t) (WL:1(t) − Φ) W⊤ l−1:1(t) i v(l) i = (1 − ηλ)Wl(t)v(l) i − ηW⊤ L:l+1(t)WL:1(t)W⊤ l−1:1(t)v(l) i = (1 − ηλ)Wl(t)v(l) i − η · ( Y k̸=l ρ2 k(t))Wl(t)v(l) i = ρl(t) · (1 − ηλ − η · Y k̸=l ρ2 k(t))u(l) i = ρl(t + 1)u(l) i for alll ∈ [L], where the first equality follows from(15), the second equality follows from definition of E(t), the third equality follows from D(t), and the fourth equality follows from A(t) and B(t) applied repeatedly along withv(l+1) i = u(l) i for all l ∈ [L − 1], proving A(t + 1). Similarly, we have W⊤ l (t + 1)u(l) i = h (1 − ηλ)W⊤ l (t) − ηWl−1:1(t)E⊤(t)WL:l+1(t) i u(l) i = h (1 − ηλ)W⊤ l (t) − ηWl−1:1(t) \u0010 W⊤ L:1(t) − Φ⊤ \u0011 WL:l+1(t) i u(l) i = (1 − ηλ)W⊤ l (t)u(l) i − ηWl−1:1(t)W⊤ L:1(t)WL:l+1(t)u(l) i = (1 − ηλ)W⊤ l (t)u(l) i − η · ( Y k̸=l ρ2 k(t))W⊤ l (t)u(l) i = ρl(t) · (1 − ηλ − η · Y k̸=l ρ2 k(t))v(l) i = ρl(t + 1)v(l) i for all l ∈ [L], where the third equality follows fromC(t), and the fourth equality follows fromA(t) and B(t) applied repeatedly along withv(l+1) i = u(l) i for all l ∈ [L − 1], proving B(t + 1). Now, we show C(t + 1). For anyk ∈ [L − 1], it follows fromv(k+1) i = u(k) i and A(t + 1) that Wk+1(t + 1)u(k) i = Wk+1(t + 1)v(k+1) i = ρk+1(t + 1)u(k+1) i . Repeatedly applying the above equality fork = l, l+ 1, . . . , L− 1, we obtain Φ⊤WL:l+1(t)u(l) i = \"L−1Y k=l ρk+1(t) # · Φ⊤u(L) i = 0 which follows from C(t), proving C(t + 1). Finally, we show D(t + 1). For any k ∈ {2, . . . , L}, it follows from v(k) i = u(k−1) i and B(t + 1) that W⊤ k−1(t + 1)v(k) i = W⊤ k−1(t + 1)u(k−1) i = ρk−1(t + 1)v(k−1) i . Repeatedly applying the above equality fork = l, l− 1, . . . ,2, we obtain ΦW⊤ l−1:1(t)v(l) i = \" lY k=2 ρk−1(t) # · Φv(1) i = 0 which follows fromD(t). Thus we have provenD(t + 1), concluding the proof. Proof of Theorem 2.1. By A(t) and B(t) of Theorem E.1, there exists orthonormal matrices{Ul,2}L l=1 ⊂ Rd×m and {Vl,2}L l=1 ⊂ Rd×m for l ∈ [L] satisfying Vl+1,2 = Ul,2 for all l ∈ [L − 1] as well as Wl(t)Vl,2 = ρl(t)Ul,2 and Wl(t)⊤Ul,2 = ρl(t)Vl,2 (16) for all l ∈ [L] and t ≥ 0, where ρl(t) satisfies (6) for t ≥ 1 with ρl(0) = ϵl. First, completeV1,2 to an 24orthonormal basis for Rd as V1 = [V1,1 V1,2]. Then for each l ∈ [L − 1], set Ul = [Ul,1 Ul,2] where Ul,1 = Wl(0)Vl,1/ϵl and Vl+1 = [Vl+1,1 Vl+1,2] where Vl+1,1 = Ul,1, and finally setUL = [UL,1 UL,2] where UL,1 = WL(0)VL,1/ϵL. We note thatVl+1 = Ul for each l ∈ [L − 1] and Ul, Vl are orthogonal since Wl(0)/ϵl is orthogonal for alll ∈ [L]. Then we have U⊤ l,1Wl(t)Vl,2 = ρl(t)U⊤ l,1Ul,2 = 0 (17) for all l ∈ [L] and t ≥ 0, where the first equality follows from (16). Similarly, we also have U⊤ l,2Wl(t)Vl,1 = ρ(t)V ⊤ l,2Vl,1 = 0 (18) for all l ∈ [L] and t ≥ 0, where the first equality also follows from(16). Therefore, combining (16), (17), and (18) yields U⊤ l Wl(t)Vl = \u0002 Ul,1 Ul,2 \u0003⊤ Wl(t) \u0002 Vl,1 Vl,2 \u0003 = \u0014fWl(t) 0 0 ρl(t)Im \u0015 for all l ∈ [L], where fWl(0) = ϵlI2r by construction of Ul,1. This directly implies (5), completing the proof. E.2 Low-rank bias in Theorem 2.1 Here, we verify the claims following Theorem 2.1 and give a precise characterization of the rate of decay of ρl as given by(6) and the conditions on learning rateη needed to achieve such behavior. These are given in the following lemma. Lemma E.2. In the setting of Theorem 2.1, suppose0 < ϵl = ϵ ≤ 1 for all l ∈ [L] and 0 < η≤ 1 λ+ϵ. Then for all t ≥ 0, the updates ofρl(t) in (6) satisfy ρl(t) = ρ(t) for some ρ, and ϵ · (1 − η · (λ + ϵ))t ≤ ρ(t) ≤ ϵ · (1 − ηλ)t. (19) Since λ and ϵ are often chose to be small, the above lemma implies that a small learning rate is not required to achieve a low-rank solution. Moreover, by choice ofη, when weight decay is employed (i.e., λ >0) the above inequality implies thatρ(t) → 0 as t → ∞. When λ = 0, we instead have that ρ is bounded byϵ. Proof of Theorem E.2. If ρl(0) = ϵ for all l ∈ [L], it is clear that ρl(t) = ρ(t) for some ρ for all t ≥ 0, and the updates take the form ρ(t) = ρ(t − 1) · h 1 − η · \u0010 λ + ρ(t − 1)2(L−1) \u0011i for each t ≥ 0. We proceed by induction. Fort = 0, since ρ(0) = ϵ, the claim holds trivially. Now suppose (19) holds for some t ≥ 0. By choice of η, we have that1 − η · (λ + ϵ) ≥ 0, so ρ(t) ≥ 0. It then follows that ρ(t + 1) = ρ(t) · h 1 − η · \u0010 λ + ρ(t)2(L−1) \u0011i ≤ ρ(t) · (1 − ηλ) ≤ ϵ · (1 − ηλ)t+1 by the fact thatρ(t) ≤ ϵ · (1 − ηλ)t. Next, by choice ofη and initial condition, we have thatρ(t) ≤ ϵ, so that ρ(t + 1) = ρ(t) · h 1 − η · \u0010 λ + ρ(t)2(L−1) \u0011i ≥ ρ(t) · (1 − η · (λ + ϵ)) ≥ ϵ · (1 − η · (λ + ϵ))t+1 since ϵ2(L−1) ≤ ϵ by ϵ ≤ 1. The claim follows. 25E.3 Proof of Theorem 2.2 Proof. First, it follows from Theorem 2.1 that for any1 ≤ i ≤ j ≤ L we have Wj:i(t) = Uj,1 fWj:i(t)V ⊤ i,1 + ( jY k=i ρk(t)) · Uj,2V ⊤ i,2 (20) for all t ≥ 0, where Ul,1, Vl,1 ∈ Rd×2r and Ul,2, Vl,2 ∈ Rd×m are the first 2r and last m columns of Ul, Vl ∈ Rd×d respectively. The key claim to be shown here is that cWl(t) = fWl(t) for all l ∈ [L] and t ≥ 0. Afterwards, it follows straightforwardly from (20) that \r\r\rf(Θ(t)) − bf( bΘ(t), UL,1, V1,1) \r\r\r 2 F = \r\r\r\r\rUL,1 fWL:1(t)V ⊤ 1,1 + ( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 − UL,1 cWL:1(t)V ⊤ L,1 \r\r\r\r\r 2 F = \r\r\r\r\rUL,1(fWL:1(t) − cWL:1(t))V ⊤ 1,1 + ( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 \r\r\r\r\r 2 F = \r\r\r\r\r( LY l=1 ρl(t)) · UL,2V ⊤ 1,2 \r\r\r\r\r 2 F ≤ m · LY l=1 ϵ2 l . We proceed by induction. Fort = 0, we have that cWl(0) = U⊤ l,1Wl(0)Vl,1 = fWl(0) for all l ∈ [L] by (20) and choice of initialization. Now suppose cWl(t) = fWl(t) for all l ∈ [L]. Comparing cWl(t + 1) = (1 − ηλ)cWl(t) − η∇cWl bℓ( bΘ(t)) with fWl(t + 1) = U⊤ l,1Wl(t + 1)Vl,1 = U⊤ l,1 [(1 − ηλ)Wl(t) − η∇Wlℓ(Θ(t))] Vl,1 = (1 − ηλ)fWl(t) − ηU⊤ l,1∇Wlℓ(Θ(t))Vl,1 it suffices to show that ∇cWl bℓ( bΘ(t)) = U⊤ l,1∇Wlℓ(Θ(t))Vl,1, ∀l ∈ [L] (21) to yield cWl(t + 1) = fWl(t + 1) for all l ∈ [L]. Computing the right hand side of (21), we have U⊤ l,1∇Wlℓ(Θ(t))Vl,1 = U⊤ l,1W⊤ L:l+1(t)(WL:1(t) − Φ)W⊤ l−1:1(t)Vl,1 = (WL:l+1(t)Ul,1)⊤(WL:1(t) − Φ)(V ⊤ l,1Wl−1:1(t))⊤ where WL:l+1(t)Ul,1 =   UL,1 fWL:l+1(t)V ⊤ l+1,1 + ( LY k=l+1 ρk(t)) · UL,2V ⊤ l+1,2 ! Ul,1 = UL,1 fWL:l+1(t) by (20) and the fact thatUl = Vl+1, and similarly V ⊤ l,1Wl−1:1(t) = V ⊤ l,1   Ul−1,1 fWl−1:1(t)V ⊤ 1,1 + ( l−1Y k=1 ρk(t)) · Ul−1,2V ⊤ 1,2 ! = fWl−1:1(t)V ⊤ 1,1. 26We also have that U⊤ L,1(WL:1(t) − Φ)V1,1 = U⊤ L,1   UL,1 fWL:1(t)V ⊤ 1,1 + ( LY k=1 ρk(t)) · UL,2V ⊤ 1,2 − Φ ! V1,1 = fWL:1(t) − U⊤ L,1ΦV1,1 so putting together the previous four equalities yields U⊤ l,1∇Wlℓ(Θ(t))Vl,1 = (WL:l+1(t)Ul,1)⊤(WL:1(t) − Φ)(V ⊤ l,1Wl−1:1(t))⊤ = fW⊤ L:l+1(t)U⊤ L,1(WL:1(t) − Φ)V1,1 fW⊤ l−1:1(t) = fW⊤ L:l+1(t)(fWL:1(t) − U⊤ L,1ΦV1,1)fW⊤ l−1:1(t). On the other hand, the left hand side of (21) gives ∇cWl bℓ( bΘ(t)) = cWL:l+1(t)⊤U⊤ L,1(UL,1 cWL:1(t)V ⊤ 1,1 − Φ)V1,1 cWl−1:1(t)⊤ = cWL:l+1(t)⊤(cWL:1(t) − U⊤ L,1ΦV1,1)cWl−1:1(t)⊤ so (21) holds by the fact thatcWl(t) = fWl(t) for all l ∈ [L], completing the proof. 27",
      "meta_data": {
        "arxiv_id": "2406.04112v2",
        "authors": [
          "Can Yaras",
          "Peng Wang",
          "Laura Balzano",
          "Qing Qu"
        ],
        "published_date": "2024-06-06T14:29:49Z",
        "pdf_url": "https://arxiv.org/pdf/2406.04112v2.pdf",
        "github_url": "https://github.com/cjyaras/deep-lora-transformers"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that leveraging inherent low-dimensional data structures and compressible dynamics within model parameters allows for enjoying the benefits of overparameterization without its computational burdens. Theoretically, it proves that learning dynamics in deep overparameterized low-rank matrix recovery are confined to invariant low-dimensional subspaces, enabling the construction and training of compact, compressed factorizations. Practically, the work develops efficient compression methods for overparameterized factorizations, improving efficiency for deep low-rank matrix completion and introducing \"Deep LoRA\" for language model fine-tuning. Deep LoRA reduces overfitting, simplifies hyperparameter setup, and maintains efficiency, particularly with limited data, by finding lower-rank solutions and being robust to rank choice.",
        "methodology": "The approach is grounded in theoretical findings for deep matrix factorization, where the existence of invariant low-dimensional subspaces for each weight matrix during gradient descent is rigorously proven. This allows for the construction and training of compact factorizations. For deep matrix completion, the method is generalized by continuously, albeit slowly, updating the compressed subspaces (UL,1, V1,1) during training using a small, discrepant learning rate (γ). For language model fine-tuning, the paper proposes \"Deep LoRA,\" which employs a deep (three-layer) overparameterized factorization for the trainable update (∆Wk) for each pretrained weight, applying the aforementioned compression technique to the learning dynamics of individual weights. The initial factors for compression are derived by analyzing the None spaces of initial gradients.",
        "experimental_setup": "For deep matrix factorization, the setup involved L-layer overparameterized factorizations with a squared Frobenius norm loss, scaled orthogonal initialization, and gradient descent. Simulations visualized SVD dynamics (L=3, d=30, r*=3, ϵl=1) and compared original vs. compressed trajectories (L=3, d=1000, r=r*=5, ϵl=10^-3). For deep matrix completion, the problem was defined with a masked Frobenius norm loss, comparing original and compressed methods with varying discrepant update rates for UL,1, V1,1 (γ=0.01 or γ=0), using L=3, d=1000, r=r*=5, ϵl=10^-3, and 20% observed entries. For model fine-tuning with Deep LoRA, a pretrained BERT base model and T5 base model (for NLG) from HuggingFace were used, adapting all 72 attention and feedforward weights. Both vanilla LoRA and Deep LoRA were tested with r=8 (unless specified), L=3 for Deep LoRA, optimized with Adam, and specific learning rates (Vanilla LoRA: η=10^-4, α=8; Deep LoRA: η=10^-2, γ=10^-2). Evaluation was performed on the GLUE benchmark and STS-B for NLU (with 1024, 16, 64, 256 samples) and the E2E dataset for NLG (16 samples), across multiple trials with different seeds, using metrics like performance gap, Pearson correlation, BLEU, ROUGE, and METEOR. All experiments were conducted on a single NVIDIA Tesla V100 GPU.",
        "limitations": "The theoretical results on network compression primarily exploit the specific gradient structure of deep matrix factorizations, implying that direct generalization to fully non-linear settings is a current challenge. Initial direct application of the compression method to deep matrix completion did not work well, as the compressed subspaces computed at initialization could be misaligned due to perturbation by the observation mask, necessitating the introduction of slow updates for the subspaces during training.",
        "future_research_directions": "Future work includes extending the analysis of compressibility to non-linear settings (e.g., ReLU activated networks) by identifying low-dimensional subspaces for similar dynamics. For Deep LoRA, potential extensions involve evaluating its efficacy in other modalities like diffusion models (especially with limited data), exploring the use of SGD for outer factors to reduce memory costs given high initial subspace alignment, and investigating second-order methods to accelerate fine-tuning along the rank-r subspace. Additionally, the research suggests exploring implications for representation learning, connecting low-rank bias to phenomena such as deep neural collapse and progressive neural collapse.",
        "experimental_code": "from dataclasses import dataclass, field\nfrom enum import StrEnum\nfrom typing import Optional, Tuple, Union\n\nfrom dataclasses_json import dataclass_json\n\n\nclass ExtendedEnum(StrEnum):\n\n    @classmethod\n    def values(cls):\n        return [e.value for e in cls]\n\n\nclass LoraAdaptType(ExtendedEnum):\n    ONLY_QUERY_VALUE = \"only-query-value\"\n    ATTENTION_MLP = \"attention-mlp\"\n    ALL_DENSE = \"all-dense\"\n\n\n@dataclass_json\n@dataclass\nclass TaskConfig:\n    # Lora hparams\n    lora_adapt_type: LoraAdaptType = LoraAdaptType.ONLY_QUERY_VALUE\n    lora_depth: int = 3\n    lora_init_scale: float = 1e-3\n    lora_rank: Optional[int] = None\n    lora_compress: bool = False\n    lora_gamma: float = 0\n\n\nimport flax.linen as nn\nimport jax.numpy as jnp\nfrom chex import ArrayTree\n\nfrom . import misc_utils, model_utils\n\n\nclass MatrixFactorization(nn.Module):\n    shape: Tuple[int, int]\n    init_scale: float\n    depth: int\n    rank: Optional[int]\n\n    def setup(self):\n        assert self.depth >= 2, \"Depth must be at least 2\"\n        set_width = self.rank if self.rank else max(self.shape)\n        misc_utils.check_rank(set_width, self.shape)\n\n        if self.depth == 2:\n            init_fn = nn.initializers.normal(stddev=1)\n            last_init_fn = nn.zeros_init()\n        else:\n            init_fn = nn.initializers.orthogonal(scale=self.init_scale)\n            last_init_fn = init_fn\n\n        layers = []\n        layers.append(\n            self.param(\n                \"W1\",\n                init_fn,\n                (set_width, self.shape[1]),\n            )\n        )\n        for i in range(2, self.depth):\n            layers.append(\n                self.param(\n                    f\"W{i}\",\n                    init_fn,\n                    (set_width, set_width),\n                )\n            )\n        layers.append(\n            self.param(\n                f\"W{self.depth}\",\n                last_init_fn,\n                (self.shape[0], set_width),\n            )\n        )\n        self.layers = layers\n\n    def __call__(self):\n        return jnp.linalg.multi_dot(self.layers[::-1])\n\n\nclass CompressedMatrixFactorization(nn.Module):\n    shape: Tuple[int, int]\n    init_scale: float\n    depth: int\n    rank: int\n\n    def setup(self):\n        self.left_factor = self.param(\n            \"left\", nn.initializers.orthogonal(), (self.shape[0], self.rank)\n        )\n        self.right_factor = self.param(\n            \"right\", nn.initializers.orthogonal(), (self.shape[1], self.rank)\n        )\n        self.mf = MatrixFactorization(\n            (self.rank, self.rank), self.init_scale, self.depth, None\n        )\n\n    def __call__(self):\n        return jnp.linalg.multi_dot([self.left_factor, self.mf(), self.right_factor.T])\n\n\nclass Lora(nn.Module):\n    flat_params_shape_dict: dict\n    init_scale: float\n    depth: int\n    rank: Optional[int]\n    compressed: bool\n\n    def setup(self):\n        mfs = {}\n        for flat_param_path, shape in self.flat_params_shape_dict.items():\n            if self.compressed:\n                assert (\n                    self.rank is not None\n                ), \"Rank must be specified for compressed LoRA\"\n                mf = CompressedMatrixFactorization(\n                    shape=shape,\n                    init_scale=self.init_scale,\n                    depth=self.depth,\n                    rank=self.rank,\n                    name=flat_param_path,\n                )\n                assert self.depth >= 3, \"Depth must be at least 3 for compressed LoRA\"\n            else:\n                mf = MatrixFactorization(\n                    shape=shape,\n                    init_scale=self.init_scale,\n                    depth=self.depth,\n                    rank=self.rank,\n                    name=flat_param_path,\n                )\n            mfs[flat_param_path] = mf\n        self.mfs = mfs\n\n    def __call__(self):\n        return {k: v() for k, v in self.mfs.items()}\n\n    def adapt(self, model_params: ArrayTree) -> ArrayTree:\n        updates = self()\n\n        def f(k, v):\n            flat_k = \"/\".join(k)\n            if flat_k in updates.keys():\n                return v + updates[flat_k]\n            else:\n                return v\n\n        return flax.traverse_util.path_aware_map(\n            f,\n            model_params,\n        )\n\n\ndef create_lora_model_from_config(\n    task_config: TaskConfig, model_params: ArrayTree\n) -> Lora:\n    \"Creates LoRA model from task config and pretrain model parameters.\"\n\n    filtered_flat_model_params_shape_dict = (\n        model_utils.get_filtered_flat_params_shape_dict(\n            model_params,\n            task_config.lora_adapt_type,\n        )\n    )\n\n    lora_model = Lora(\n        flat_params_shape_dict=filtered_flat_model_params_shape_dict,\n        depth=task_config.lora_depth,\n        init_scale=task_config.lora_init_scale,\n        rank=task_config.lora_rank if not task_config.lora_compress else None,\n        compressed=False,\n    )\n\n    return lora_model\n\n\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nfrom chex import ArrayTree\nfrom flax.training.train_state import TrainState\nfrom jax import Array\nfrom optax import Schedule\n\nfrom . import metrics, model_utils, models\nfrom .configs import TaskConfig\nfrom .models import Lora\n\n\nclass LoraState(TrainState):\n    dropout_rng: Array\n\n\ndef create_lora_loss_fn(\n    model_state: TrainState,\n    lora_state: LoraState,\n    is_regression: bool,\n    is_train: bool,\n) -> Callable:\n    \"Returns function that computes loss for model/lora state.\"\n\n    def loss_fn(lora_params: ArrayTree, batch: dict[str, np.ndarray]) -> Array:\n        labels = batch.pop(\"labels\")\n        adapted_model_params = lora_state.apply_fn(\n            {\"params\": lora_params}, model_state.params\n        )\n        logits = model_state.apply_fn(\n            **batch,\n            params=adapted_model_params,\n            dropout_rng=lora_state.dropout_rng,\n            train=is_train,\n        )[0]\n        if is_regression:\n            loss = metrics.mse_loss(logits, labels)\n        else:\n            if \"decoder_attention_mask\" in batch:\n                loss = metrics.ce_loss(\n                    logits, labels, padding=batch[\"decoder_attention_mask\"]\n                )\n            else:\n                loss = metrics.ce_loss(logits, labels)\n        return loss\n\n    return loss_fn\n\n\ndef create_train_step_fn(\n    task_config: TaskConfig, learning_rate_fn: optax.Schedule\n) -> Callable:\n\n    is_regression = task_config.finetune_task_name == \"stsb\"\n\n    @partial(jax.jit, donate_argnums=(1,))\n    def train_step_fn(\n        model_state: TrainState, lora_state: LoraState, batch: dict[str, np.ndarray]\n    ) -> tuple[LoraState, dict[str, Array]]:\n        _, new_dropout_rng = jax.random.split(lora_state.dropout_rng)\n        loss_and_grad_fn = jax.value_and_grad(\n            create_lora_loss_fn(model_state, lora_state, is_regression, is_train=True)\n        )\n        loss, grads = loss_and_grad_fn(lora_state.params, batch)\n        new_lora_state = lora_state.apply_gradients(grads=grads)\n        new_lora_state = new_lora_state.replace(dropout_rng=new_dropout_rng)\n        metrics = {\"loss\": loss, \"learning_rate\": learning_rate_fn(lora_state.step)}\n        return new_lora_state, metrics\n\n    return train_step_fn\n\n\ndef create_lora_state(\n    task_config: TaskConfig,\n    model_params: ArrayTree,\n    learning_rate_fn: Schedule,\n    lora_rng: Array,\n    dropout_rng: Array,\n) -> tuple[LoraState, Lora]:\n    lora_model = models.create_lora_model_from_config(task_config, model_params)\n    lora_variables = lora_model.init(lora_rng)\n    lora_params = lora_variables[\"params\"]\n    tx = create_optimizer(learning_rate_fn, task_config.weight_decay)\n    return (\n        LoraState.create(\n            apply_fn=partial(lora_model.apply, method=lora_model.adapt),\n            params=lora_params,\n            tx=tx,\n            dropout_rng=dropout_rng,\n        ),\n        lora_model,\n    )\n\n\ndef create_compressed_lora_train_state(\n    uncompressed_lora_state: LoraState,\n    uncompressed_lora_model: Lora,\n    model_state: TrainState,\n    batch: dict[str, np.ndarray],\n    task_config: TaskConfig,\n):\n    assert task_config.lora_compress, \"Lora compression is not enabled.\"\n    rank = task_config.lora_rank\n    assert rank is not None, \"Rank must be specified.\"\n    assert rank % 2 == 0, \"Rank must be even.\"\n    compressed_lora_model = models.Lora(\n        flat_params_shape_dict=model_utils.get_filtered_flat_params_shape_dict(\n            model_state.params, task_config.lora_adapt_type\n        ),\n        depth=task_config.lora_depth,\n        init_scale=task_config.lora_init_scale,\n        rank=rank,\n        compressed=True,\n    )\n\n    uncompressed_e2e = uncompressed_lora_model.apply(\n        {\"params\": uncompressed_lora_state.params}\n    )\n\n    # Get gradient of uncompressed factors\n    loss_fn = create_lora_loss_fn(\n        model_state,\n        uncompressed_lora_state,\n        task_config.finetune_task_name == \"stsb\",\n        False,\n    )\n\n    uncompressed_grads = jax.grad(loss_fn)(uncompressed_lora_state.params, batch)\n\n    # Move to numpy (do compression on CPU to save GPU memory)\n    uncompressed_lora_params_numpy = jax.tree_map(\n        np.array, uncompressed_lora_state.params\n    )\n    uncompressed_grads_numpy = jax.tree_map(np.array, uncompressed_grads)\n    uncompressed_e2e_numpy = jax.tree_map(np.array, uncompressed_e2e)\n\n    def svd(A):\n        U, s, VT = np.linalg.svd(A, full_matrices=True)\n        return U, s, VT.T\n\n    def get_left_right_factors(W1, W1_grad, e2e):\n\n        m, n = W1.shape\n        assert m == n, \"Need square matrix at this point\"\n\n        half_rank = rank // 2\n        Ugrad, _, Vgrad = svd(W1_grad)\n        Va = W1.T @ Ugrad[:, half_rank:] / task_config.lora_init_scale\n        Vb = Vgrad[:, half_rank:]\n        V0 = Va @ svd(np.concatenate([Va, -Vb], axis=1))[2][: Va.shape[1], n:]\n        V = svd(V0)[0][:, ::-1]\n        right = V[:, :rank]\n        left = e2e @ right / (task_config.lora_init_scale**task_config.lora_depth)\n\n        return left, right\n\n    compressed_lora_params_numpy = {}\n\n    pbar = tqdm(uncompressed_grads_numpy.items())\n    pbar.set_description(\"Compressing LoRA parameters\")\n\n    for k, g in pbar:\n        comp_mf_params = {}\n        m, n = uncompressed_lora_params_numpy[k][\"W1\"].shape\n        if m != n:\n            # WL.T will act like W1\n            right, left = get_left_right_factors(\n                uncompressed_lora_params_numpy[k][f\"W{task_config.lora_depth}\"].T,\n                g[f\"W{task_config.lora_depth}\"].T,\n                uncompressed_e2e_numpy[k].T,\n            )\n        else:\n            left, right = get_left_right_factors(\n                uncompressed_lora_params_numpy[k][\"W1\"],\n                g[\"W1\"],\n                uncompressed_e2e_numpy[k],\n            )\n        comp_mf_params[\"left\"] = left\n        comp_mf_params[\"right\"] = right\n        mf_params = {}\n        for w in g.keys():\n            mf_params[w] = task_config.lora_init_scale * jnp.eye(rank)\n        comp_mf_params[\"mf\"] = mf_params\n        compressed_lora_params_numpy[k] = comp_mf_params\n\n    compressed_lora_params = jax.tree_map(jnp.array, compressed_lora_params_numpy)\n\n    inner_tx = uncompressed_lora_state.tx\n    outer_tx = create_optimizer(\n        create_learning_rate_fn(\n            task_config.num_train_steps,\n            task_config.num_warmup_steps,\n            task_config.lora_gamma * task_config.learning_rate,\n            task_config.decay_ratio,\n        ),\n        task_config.weight_decay,\n    )\n    tx = optax.multi_transform(\n        {\"inner\": inner_tx, \"outer\": outer_tx},\n        flax.traverse_util.path_aware_map(\n            lambda p, _: \"outer\" if p[-1] == \"left\" or p[-1] == \"right\" else \"inner\",\n            compressed_lora_params,\n        ),\n    )\n\n    return LoraState.create(\n        apply_fn=partial(\n            compressed_lora_model.apply, method=compressed_lora_model.adapt\n        ),\n        params=compressed_lora_params,\n        tx=tx,\n        dropout_rng=uncompressed_lora_state.dropout_rng,\n    )",
        "experimental_info": "The Deep LoRA method is configured through the `TaskConfig` dataclass in `dlt/configs.py`. Key experimental settings related to the method are:\n\n-   `lora_depth`: Integer specifying the number of layers in the overparameterized factorization for the trainable update. The method proposes a \"deep (three-layer)\" factorization, so `lora_depth` is typically set to `3` in experiments comparing against standard LoRA (which often uses `depth=2`).\n-   `lora_rank`: Optional integer defining the rank of the factorization. When `lora_compress` is `True`, `lora_rank` must be specified. Typical values observed in experiments include `8`, `16`, `32`, `64`.\n-   `lora_init_scale`: Floating-point value for the initialization scale of the LoRA factors, usually `1e-3`.\n-   `lora_adapt_type`: Enum `LoraAdaptType` (`ONLY_QUERY_VALUE`, `ATTENTION_MLP`, `ALL_DENSE`) determines which weight matrices in the pretrained model are adapted by LoRA. `ALL_DENSE` is frequently used in the provided scripts.\n-   `lora_compress`: Boolean flag (`True`/`False`) to enable the compression technique for the LoRA factors. When `True`, `CompressedMatrixFactorization` is used, and the system updates compressed subspaces.\n-   `lora_gamma`: Floating-point value representing the small, discrepant learning rate (γ) used to continuously update the `left` and `right` compressed subspaces (`UL,1`, `V1,1`) during training. When `lora_compress` is `True`, this rate is multiplied by the base `learning_rate` for the outer optimization loop. Observed values include `1` (which means `gamma * learning_rate` is `learning_rate`) and `1e-2`. A default value of `0` means these factors are not updated with a separate learning rate.\n-   `learning_rate`: Base learning rate for the inner optimization loop, typically `1e-4`."
      }
    },
    {
      "title": "Black-Box Tuning for Language-Model-as-a-Service",
      "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.",
      "full_text": "Black-Box Tuning for Language-Model-as-a-Service Tianxiang Sun 1 Yunfan Shao1 Hong Qian 2 Xuanjing Huang 1 Xipeng Qiu 1 3 Abstract Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-speciﬁc prompts to query the PTMs through some black- box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gra- dients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the con- tinuous prompt prepended to the input text via derivative-free optimization. Instead of optimiz- ing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a ran- domly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimen- tal results show that the black-box tuning with RoBERTa on a few labeled samples not only sig- niﬁcantly outperforms manual prompt and GPT- 3’s in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning. 1. Introduction Scaling pre-trained language models (PTMs) has shown increasing power on a wide range of NLP tasks (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2020; 2021b; Zeng et al., 2021; Sun et al., 2021; Qiu et al., 2020). Extremely large PTMs can easily generalize to various downstream tasks with a few labeled samples (Brown et al., 2020). However, making these large PTMs beneﬁt everyone is a challenge. On the one hand, running such models can be very expensive or even infeasible for most users. On the other hand, the model parameters are often not open-sourced due to commercial 1Fudan University 2East China Normal University 3Peng Cheng Laboratory. Correspondence to: Tianxiang Sun <tx- sun19@fudan.edu.cn>, Xipeng Qiu <xpqiu@fudan.edu.cn>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Users Query  Response Server Mixed-Task Batch PTM Inference (Black-Box)  Task Prompt (Tunable) Samples Update Prompt Figure 1.Illustration of Language-Model-as-a-Service (LMaaS). Users can query the PTM deployed on the server through a black- box API. In each query, users can input a task prompt and a batch of texts. On the server side, the samples can be mixed in a large batch to be fed into the PTM. By iteratively querying the PTM through the black-box API, users can optimize and ﬁnally obtain good prompts to solve the language tasks of interest. considerations and the potential risk of misuse.1 Therefore, large PTMs such as GPT-3 (Brown et al., 2020), ERNIE 3.0 (Sun et al., 2021) and Yuan 1.0 (Wu et al., 2021) are usually released as a service, allowing users to access these powerful models through black-box APIs. In this scenario, called Language-Model-as-a-Service (LMaaS), users can solve the language tasks of interest using the black-box APIs by crafting task-speciﬁc text prompts or including training samples in the input texts (a.k.a. in- context learning (Brown et al., 2020)). Due to the great power of the general-purpose PTMs underlying the APIs, such approaches can achieve considerable performance on simple language tasks, and therefore have powered many interesting applications2. However, querying large PTMs through hand-crafted text prompts cannot fully exploit la- beled data, resulting in unsatisfactory performance in many use cases. Instead of designing discrete text prompts, recently much effort has been devoted to continuous prompt tuning (Li & Liang, 2021; Hambardzumyan et al., 2021; Liu et al., 1https://openai.com/blog/openai-api/ 2See https://gpt3demo.com/ for examples. arXiv:2201.03514v4  [cs.CL]  27 Jun 2022Black-Box Tuning for Language-Model-as-a-Service 2021b), which is to optimize the continuous prompt injected to the text while keeping the PTM parameters frozen. Such methods only require storing a small continuous prompt for each task, and therefore are highly deployment-efﬁcient. Besides, tuning the continuous prompt can be as effective as ﬁne-tuning the entire model when the PTM becomes large (Lester et al., 2021). However, in all the previous methods, the continuous prompts are learned through back- propagation, which is unavailable in the scenario of LMaaS. Can we optimize the task-speciﬁc continuous prompts when we only have access to the PTM inference API? Since gradients are unavailable, we can only invoke derivative-free optimization (DFO) 3 (Kolda et al., 2003; Conn et al., 2009; Rios & Sahinidis, 2013). DFO involves a kind of optimization algorithms that do not depend on gra- dients, but only relies on function values (or ﬁtness values) of sampled solutions. However, DFO algorithms are known to suffer from slow convergence rate when the dimension- ality of the search space is high. Thus, it is intractable to optimize even only the continuous prompts, which can be tens of thousands of parameters, using DFO algorithms. Fortunately, recent work found that common PTMs, despite their large numbers of parameters, have a very low intrinsic dimensionality (Aghajanyan et al., 2021; Qin et al., 2021). That means, there exists a low-dimensional reparameteriza- tion that is as effective for ﬁne-tuning as the full parameter space. It has been demonstrated that optimizing only hun- dreds (Aghajanyan et al., 2021) or even dozens (Qin et al., 2021) of parameters can achieve non-trivial performance. Given that the intrinsic dimensionality of the objective func- tion (in our case is the forward computation of PTMs) is low, the optimization can be effectively solved via DFO al- gorithms with random embedding (Wang et al., 2016; Qian et al., 2016; Letham et al., 2020). Based on the these insights, this paper proposes the Black- Box Tuning (BBT) to solve various language understand- ing tasks by only accessing the PTM inference API. In particular, we manage to optimize the continuous prompt prepended to the input text by iteratively querying the PTM inference API, as brieﬂy depicted in Figure 1. To han- dle the high dimensionality of the continuous prompt, we project the original prompt space using a random linear projection onto a much smaller subspace and solve this optimization problem with some derivative-free optimizer in that smaller subsapce. In contrast to conventional ﬁne- tuning methods that can only be performed by the service side, black-box tuning allows users to optimize their task- speciﬁc prompts locally on resource-limited devices (even without GPUs). Our experimental results demonstrate that prompting RoBERTaLARGE (Liu et al., 2019) using BBT on 3Also termed as black-box, zeroth-order or gradient-free opti- mization. a few labeled samples not only outperforms manual prompt and in-context learning (Brown et al., 2020), but also out- performs its gradient-based counterparts, namely prompt tuning (Lester et al., 2021) and full model tuning. The contribution of this paper is three folds:4 • This paper proposes a novel scenario (LMaaS) where one should learn to prompt the PTMs by only accessing their inference APIs. • This paper offers a solution (BBT) for such a scenario to accomplish common language understanding tasks without access to model parameters and gradients, such that large-scale PTMs can better beneﬁt users. • Empirical results show that DFO can successfully deal with real-world language tasks by learning to prompt large-scale PTMs with more than millions of parame- ters. Thus, this work pioneers the work of optimizing large-scale PTMs through DFO methods. 2. Background Large-Scale PTMs as APIs. It is a promising way to de- ploy large-scale PTMs to serve downstream applications by providing general-purpose APIs. For the service side, wrapping the computation of the PTM into an easy-to-use API has become a common practice (Brown et al., 2020; Sun et al., 2021; Wu et al., 2021). In contrast to training, the in- ference speed of large-scale PTMs can be highly optimized with acceleration techniques such as ORT and TensorRT. In addition, large-scale PTMs are often not open-sourced due to the commercial reasons and the potential risk of mis- use. For the user side , even if the large-scale PTMs are available, it is expensive or even infeasible to locally run them. Thus, how to exploit the PTM inference API to solve conventional language tasks is a promising direction. Intrinsic Dimensionality of PTMs. The intrinsic dimen- sionality of an objective function is the minimum number of parameters needed to obtain satisfactory solutions (Li et al., 2018). In particular, the intrinsic dimensionality in- dicates the lowest dimensional reparameterization that is as effective for optimizing as the full parameter space. Li et al. (2018) propose to measure the intrinsic dimensionality of neural networks by ﬁnding the minimal dimensionality of the subspace that is randomly projected from the full trainable parameters, in which they can optimize the neural networks to achieve satisfactory solutions. Aghajanyan et al. (2021) empirically show that large-scale pre-training implic- itly compresses the intrinsic dimensionality of downstream NLP tasks. By tuning only hundreds of parameters that 4Our code is publicly available at https://github.com/ txsun1997/Black-Box-TuningBlack-Box Tuning for Language-Model-as-a-Service are then randomly projected onto the full parameter space of RoBERTa, they can achieve 90% performance relative to full model tuning. Qin et al. (2021) show that intrinsic subspace on various tasks can be compressed to less than 100 dimensions with multi-task supervision. This line of research, along with the work of parameter-efﬁcient tun- ing (Houlsby et al., 2019; Li & Liang, 2021; Lester et al., 2021; Sun et al., 2022; Hu et al., 2021a; He et al., 2021), demonstrate that PTMs can well adapt to downstream tasks by tuning a very small proportion of parameters, which im- plies the possibility of optimizing large-scale PTMs with derivative-free algorithms. Prompt-Based Learning. Prompt-based learning is to formulate downstream tasks as a (masked) language mod- eling task, and therefore reduces the gap between PTM pre-training and ﬁne-tuning (Brown et al., 2020; Schick & Sch¨utze, 2021a;b; Gao et al., 2021; Sun et al., 2022). For instance, one can use BERT (Devlin et al., 2019) to predict whether the sentence ”This is a fantastic movie” is positive or negative by appending the prompt ”It was [MASK]” and see if BERT predicts ”great” or ”terrible” at the masked position. Note that the prompt is not necessarily discrete, it can also be optimized efﬁciently in continuous space with gradient descent (Li & Liang, 2021; Hambardzumyan et al., 2021; Qin & Eisner, 2021; Liu et al., 2021b; Zhong et al., 2021). In the case of only tuning the continuous prompt while keeping the parameters of large PTMs untouched, one can retain the efﬁcient serving beneﬁts while matching the performance of full model tuning (Lester et al., 2021). Our work also proposes to optimize the continuous prompt while keeping the PTM parameters unchanged, but without gradient descent. Derivative-Free Optimization. Derivative-free optimiza- tion (DFO) realizes optimization only via the function val- ues f(x) on the sampled solutions x. Most DFO algorithms share a common structure of sampling-and-updating to en- hance the quality of solutions. Representative DFO algo- rithms include evolutionary algorithms (Hansen et al., 2003), Bayesian optimization (Shahriari et al., 2016), etc. Due to their ability of addressing complex optimization tasks, DFO algorithms have achieved many impressive applica- tions in automatic machine learning (Snoek et al., 2012), reinforcement learning (Salimans et al., 2017; Hu et al., 2017), objective detection (Zhang et al., 2015b), etc. 3. Approach 3.1. Problem Formulation Common language understanding tasks can be formulated as a classiﬁcation task, which is to predict for a batch of input texts X the labels Y. To solve the target language understanding task with a general-purpose PTM, we should modify X with some template (e.g., adding some trigger words and a special token [MASK] for BERT-like PTMs) and map the labels Y to some words in the PTM vocab- ulary (e.g., the sentiment label ”positive” can be mapped to ”great”). The modiﬁed inputs and labels are denoted as ˜X and ˜Y. Assume the BERT-like PTM inference API f takes a continuous prompt p and a batch of modiﬁed texts ˜X as input, and outputs the logits on the masked positions, i.e., ˆY = f(p; ˜X). With the output logits, we can calculate the loss on this batch of data, which is not necessarily to be differentiable. Our goal is to ﬁnd the optimal prompt p⋆ = arg minp∈PL(f(p; ˜X),˜Y), where Pis some search space of interest and Lis some loss function such as nega- tive accuracy. The black-box function f is not available to the optimizer in closed form, but can be evaluated at a query point (p; ˜X). 3.2. Black-Box Tuning As demonstrated by Lester et al. (2021), dozens of prompt to- kens are required to obtain a competitive performance when only tuning continuous prompts. Given that the embedding dimensionality of large-scale PTMs is usually larger than one thousand (e.g., the word embeddings of RoBERTaLARGE are 1024-dimensional), the dimensionality of the continuous prompt p ∈RD that we are interested to optimize can be tens of thousands, which makes derivative-free optimization intractable. To handle this high-dimensional optimization, since large-scale PTMs have a low intrinsic dimensional- ity (Aghajanyan et al., 2021; Qin et al., 2021), we manage to optimize z ∈Rd in a much smaller subspace (d≪D), and use a random projection matrix A ∈RD×d to project z on the original prompt space P. Note that directly projecting z onto the prompt space that is compatible with the PTM is non-trivial. To ease the optimization, we instead optimize the increment of some initial prompt p0. For simplicity, we randomly sample n tokens from the PTM vocabulary as initialization. Thus, our objective becomes z⋆ = arg min z∈Z L(f(Az + p0; ˜X),˜Y) , (1) where Zis the search space. Previous work (Wang et al., 2016; Qian et al., 2016; Letham et al., 2020) in derivative- free optimization usually sets each entry in the random matrix A by sampling from some normal distribution. How- ever, this sampling strategy does not perform well in our scenario. Instead, we set values of the random matrix A by sampling from a uniform distribution adopted in He et al. (2015) (cf. Appendix A for the comparison). We restrict the search space to Z= [−5,5]d. For the loss functionL, a straightforward alternative is using negative accuracy. However, the reward of accuracy can beBlack-Box Tuning for Language-Model-as-a-Service Best film ever . It was <MASK> . A totally boring movie ! It was <MASK> . You 'll probably love it . It was <MASK> . great terrible great ෩𝒀 throne arrow apple 𝒛 𝑨 ∈ ℝ𝐷×𝑑 𝑨𝒛 𝒑𝟎 𝒑 Copy Pre-Trained Language Model Inference (Black-Box API) good:10.2 great:7.9 movie:7.1 … terrible:11.2 bad:9.9 boring:8.0 … great:9.8 love:5.2 film:3.3 … ෡𝒀𝓛(෩𝒀,෡𝒀) Derivative-Free Optimizer Best film ever . It was <MASK> . A totally boring movie ! It was <MASK> . You 'll probably love it . It was <MASK> . ෩𝑿 Server User Labeled Data Figure 2.A single iteration of the optimization. Given z ∈Rd provided by the derivative-free optimizer, we project it to the prompt space by a random matrix A ∈RD×d. By adding the projected prompt embeddings Az with some initial prompt embeddings p0 (in this illustration are the embeddings of tokens randomly sampled from the PTM’s vocabulary), we obtain the ﬁnal prompt embeddings that are then concatenated with the input texts ˜X. By calling the black-box API f, which implements the forward computation of the PTM, the predictions on the masked positions are obtained, i.e., ˆY = f(p; ˜X). With the prediction ˆY and the golden labels ˜Y at hand, we can calculate the loss that is used by the derivative-free optimizer to suggest a new z. sparse and less informative, especially when training data is limited. Thus, we also consider two loss functions that are more sensitive to predictions, i.e., cross entropy and hinge loss. Given the output logits ˆ yover a candidate set of label words, and the golden label word ˜yof a certain sample, the cross entropy is deﬁned as LCE(ˆ y,˜y) =−log Softmax˜y(ˆ y). (2) For hinge loss, we adopt a multi-class extension (Weston & Watkins, 1999), LHinge(ˆ y,˜y) = ∑ i̸=˜y max(0,γ + ˆ yi −ˆ y˜y). (3) In this work we set the margin γ = 2. The performances of using cross entropy, hinge loss, and negative accuracy are compared in Figure 3. 3.3. The CMA Evolution Strategy As demonstrated in Aghajanyan et al. (2021), the intrinsic dimensionality of PTMs like RoBERTa LARGE on various tasks can be hundreds. To handle optimization of such scale, we adopt the CMA-ES (Covariance Matrix Adaptation Evo- lution Strategy) (Hansen & Ostermeier, 2001; Hansen et al., 2003), which is a widely used evolutionary algorithm for non-convex black-box optimization in continuous domain. In particular, CMA-ES maintains a parameterized search distribution model, i.e., multivariate normal distribution. In each iteration, CMA-ES samples a population of new query solutions (also referred to as individuals or offspring) from the multivariate normal distribution model z(t+1) i ∼m(t) + σ(t)N(0,C(t)) , (4) where i= 1,...,λ and λis the population size. m(t) ∈Rd is the mean vector of the search distribution at iteration step t, σ(t) ∈R+ is the overall standard deviation that controls the step length, and C(t) ∈Rd×d is the covariance matrix that determines the shape of the distribution ellipsoid. By maximizing the likelihood of successful steps, m(t), σ(t), C(t) are updated (cf. Hansen (2016) for more details). 3.4. Pre-Training Prompt Embedding Considering that sentence-pair tasks can share the same template and label words, as shown in Table 1, we can pre- train a prompt embedding p0 on some publicly available NLI task (in our experiments we use the MNLI (Williams et al., 2018) training set) for a better initialization. For other classiﬁcation tasks we set p0 as word embeddings randomly drawn from the vocabulary of RoBERTaLARGE. 4. Experiments 4.1. Setup Dataset. We conduct experiments on several common language understanding tasks including sentiment analy- sis, topic classiﬁcation, natural language inference (NLI),Black-Box Tuning for Language-Model-as-a-Service Table 1.Statistics, manual templates, and label words used in our experiments. |Y| : number of classes. Category Dataset |Y| |Train| |Test| Type Template Label words single- sentence SST-2 2 67k 0.9k sentiment ⟨S⟩. It was[MASK]. great, bad Yelp P. 2 560k 38k sentiment ⟨S⟩. It was[MASK]. great, bad AG’s News 4 120k 7.6k topic [MASK]News:⟨S⟩ World, Sports, Business, Tech DBPedia 14 560k 70k topic [Category: [MASK]] ⟨S⟩ Company, Education, Artist, Athlete, Ofﬁce, Transportation, Building, Natural, Village, Animal, Plant, Album, Film, Written sentence- pair MRPC 2 3.7k 0.4k paraphrase ⟨S1⟩? [MASK], ⟨S2⟩ Yes, No RTE 2 2.5k 0.3k NLI ⟨S1⟩? [MASK], ⟨S2⟩ Yes, No SNLI 3 549k 9.8k NLI ⟨S1⟩? [MASK], ⟨S2⟩ Yes, Maybe, No and paraphrase. For sentiment analysis, we choose SST- 2 (Socher et al., 2013) and Yelp polarity (Zhang et al., 2015a). For topic classiﬁcation, we choose AG’s News and DBPedia (Zhang et al., 2015a). For NLI, we choose SNLI (Bowman et al., 2015) and RTE (Wang et al., 2019). For paraphrase, we choose MRPC (Dolan & Brockett, 2005). The statistics, manual templates and label words of these datasets are shown in Table 1. Few-Shot Setting. For a broad range of users, the amount of labeled data can be limited, in which case they can resort to the deployed large PTMs due to their great power of few- shot learning (Brown et al., 2020). Hence, in this paper we conduct experiments in the few-shot setting. We randomly select ksamples for each class to construct a k-shot training set Dtrain, and compose a development set Ddev by randomly drawing another ksamples from the original training set and ensure that |Dtrain|= |Ddev|to simulate the true few-shot learning setting (Perez et al., 2021). Following Zhang et al. (2021a), Gao et al. (2021), and Gu et al. (2021), we use the original development sets as the test sets. For datasets with- out development sets, we use the original test sets. Hence, in our experiments |Dtest|≫|D train|= |Ddev|. Backbone Model. We choose RoBERTaLARGE (Liu et al., 2019) as our backbone model because: (1) We mainly fo- cus on language understanding tasks; (2) Aghajanyan et al. (2021) have demonstrated that RoBERTaLARGE has a very small intrinsic dimensionality (about hundreds) on many tasks. It is worth noting that generative PTMs such as GPT (Brown et al., 2020), T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are also compatible with our framework if we convert downstream tasks into a uniﬁed text-to-text format. We leave for future work the applica- tions of generative PTMs. Baselines. We compare our proposed black-box tuning with two kinds of methods: gradient-based methods and gradient-free methods. For gradient-based methods, we consider three baselines: (1) Prompt Tuning: Following Lester et al. (2021), we only train the continuous prompts Table 2.Default conﬁguration of hyper-parameters. Hyper-parameter Default Prompt length (L) 50 Subspace dimension (d) 500 Population size (λ) 20 Random projection (A) Uniform Loss functionL Cross Entropy Budget (# of API calls) 8000 prepended to the input texts while keeping the PTM frozen. We use an Adam optimizer (Kingma & Ba, 2015) with learn- ing rate of 5e-4 and batch size of 16 for 1000 epochs. For fair comparison, we use the same prompt length, manual template, label words, and the same pre-trained prompt em- bedding for initialization on sentence-pair tasks as black-box tuning. (2) P-Tuning v2 (Liu et al., 2021a) is an improved variant of prompt tuning. Instead of injecting continuous prompts merely into the input layer, P-Tuning v2 prepends and optimizes continuous prompts at every layer of the PTM. We optimize the prompts of length 128 at each layer using an Adam optimizer with learning rate of 5e-4 and batch size of 32 for 2000 epochs. (3) Model Tuning: We ﬁne-tune the entire PTM on each task using an Adam optimizer with learning rate of 1e-5 and batch size of 16 for 200 epochs. For gradient-free methods, we consider three baselines: (1) Manual Prompt: We directly use the templates and label words in Table 1 to conduct zero-shot evaluation. The re- sults of manual prompt can be seen as initial points of our method. (2) In-context Learning: Following Brown et al. (2020), we randomly select up to 32 training samples and concatenate them with the input texts. (3) Feature-based Methods: Feature-based methods (Peters et al., 2019) is also a competitive baseline for LMaaS, where one can re- quest the features encoded by the large PTM and locally train a classiﬁer to accomplish the task of interest. Here we consider two implementations: (a) Feature-MLP: We train a two-layered MLP classiﬁer on the [CLS] representation of the PTM. (b) Feature-BiLSTM: We train a bidirectional LSTM (Hochreiter & Schmidhuber, 1997) on the repre- sentations of the sequence of tokens, followed by a linear classiﬁer on the top. For both implementations of feature-Black-Box Tuning for Language-Model-as-a-Service based methods, we use an Adam optimizer with learning rate of 3e-4 and batch size of 16 to train the attached clas- siﬁers for 1000 epochs. For black-box tuning, we give in Table 2 the default conﬁguration of hyper-parameters used in our experiments. The effect of each hyper-parameter is explored in § 4.3. 4.2. Results Overall Comparison. We ﬁrst demonstrate the experi- mental results of black-box tuning and the baselines across 7 datasets in Table 3. The proposed black-box tuning sig- niﬁcantly outperforms the other four gradient-free methods. We observe that in-context learning performs even worse than manual prompt on some tasks, and suffers from high variance. That means, in-context learning cannot effectively utilize labeled samples included in the context. Feature- based methods perform slightly better than manual prompt and in-context learning. Meanwhile, Feature-BiLSTM out- performs Feature-MLP due to its advantage of using more informative features. Surprisingly, black-box tuning also outperforms its gradient-based counterparts, namely prompt tuning, p-tuning v2, and model tuning, on average perfor- mance of the 7 tasks. Note that the only difference between prompt tuning and black-box tuning is whether we use gra- dient descent (i.e., Adam optimizer) or DFO algorithm (i.e., CMA-ES). Based on the experimental results, we suspect that gradient-based optimization tends to overﬁt the small training data while DFO tends to ﬁnd better solutions due to its exploration mechanism. In addition, we ﬁnd that model tuning performs much better than prompt tuning and black- box tuning when number of classes is large (e.g., DBPedia). On NLI tasks (i.e., SNLI and RTE), when using pre-trained prompt embedding (§ 3.4), prompt tuning and black-box tuning signiﬁcantly outperform model tuning, which also conﬁrms the effectiveness of prompt pre-training (Gu et al., 2021) in the context of black-box tuning. Detailed Comparison. In the scenario of LMaaS, there are many other factors to be considered. In Table 4 we com- pare black-box tuning and the baseline methods in terms of deployment efﬁciency, viability of as-a-service, training time, memory usage on the user side and the server side, and the amount of data to be uploaded and downloaded. Model tuning is not deployment-efﬁcient because it needs to main- tain a copy of the entire model for each user. Gradient-based methods cannot make the PTM serve as a service due to the requirement of gradients. Feature-based methods and black-box tuning are suitable for LMaaS. However, feature- based methods cannot achieve competitive results when labeled data is limited. Therefore, among all the considered methods, only black-box tuning can achieve satisfactory performance while maintaining reasonable training time, memory footprint, and network load. Unlike gradient-based methods, in which the optimization cost is proportional to the size of the PTM, the optimization cost of black-box tuning is decoupled from the scale of the PTM, and only relies on the subspace dimensionality. For fair compari- son of training time, we perform early stopping for all the compared methods, i.e., we stop learning if the development accuracy does not increase after 1000 steps. All the methods are implemented with PyTorch (Paszke et al., 2019) and ex- perimented on a single NVIDIA GTX 3090 GPU. Note that the process of model inference can be further accelerated via better implementations (e.g., using ONNX and TensorRT). In Table 4 we also report the training time of black-box tuning using ONNX Runtime. Detailed calculation of the amount of data to be uploaded/downloaded can be found in Appendix C. 4.3. Ablation Study In this section, we conduct ablation experiments on various hyper-parameters. To control experimental variables, we explore the effect of each hyper-parameter while keeping the other hyper-parameters as default as listed in Table 2. To stablize the experimental results and reduce the variance over different runs, we conduct ablation experiments in 64- shot setting. Each run is performed on the same data split with different random seeds. Experimental results of abla- tions on loss functions L, subspace dimensionality d, and prompt length Lare demonstrated in Figure 3. Additional ablation studies on the effect of the random projection A, the effect of the population size λ, and the ablations in the 16-shot setting are in Appendix A. For each ablation, we show results under different budget, which is measured by the number of PTM inference API calls. In each API call, one can provide a continuous prompt p and query the results of the PTM forward computation on a batch of training data. In our few-shot setting, we can put all the training data into one batch, and therefore the objective function to be optimized is deterministic instead of stochastic. CMA-ES vs. Adam. We compare our used derivative- free optimizer, CMA-ES, with a competitive ﬁrst-order opti- mizer, Adam (Kingma & Ba, 2015). For fair comparison, we update the continuous prompt using Adam with the gra- dients over the entire training data (i.e., batch size equals to |Dtrain|). We use learning rate of 1e-3 for Adam opti- mizer. As shown in the top row of Figure 3, Adam optimizer achieves faster convergence on both SST-2 and AG’s News due to the gradients it used. On the development sets, Adam performs slight worse than CMA-ES with cross entropy on SST-2 but better on AG’s News. But as demonstrated in Ta- ble 3, using Adam optimizer performs worse than CMA-ES on the average performance across seven task test sets.Black-Box Tuning for Language-Model-as-a-Service Table 3.Overall comparison on various language understanding tasks. We report mean and standard deviation of performance over 3 different splits (§ 4.1). All of the results are obtained with pre-trained RoBERTaLARGE in 16-shot (per class) setting. Method SST-2 Yelp P. AG’s News DBPedia MRPC SNLI RTE Avg.acc acc acc acc F1 acc acc Gradient-Based Methods Prompt Tuning 68.23 ±3.78 61.02±6.65 84.81±0.66 87.75±1.48 51.61±8.67 36.13±1.51 54.69±3.79 63.46 + Pre-trained prompt / / / / 77.48 ±4.85 64.55±2.43 77.13±0.83 74.42 P-Tuning v2 64.33 ±3.05 92.63±1.39 83.46±1.01 97.05±0.41 68.14±3.89 36.89±0.79 50.78±2.28 70.47 Model Tuning 85.39 ±2.84 91.82±0.79 86.36±1.85 97.98±0.14 77.35±5.70 54.64±5.29 58.60±6.21 78.88 Gradient-Free Methods Manual Prompt 79.82 89.65 76.96 41.33 67.40 31.11 51.62 62.56 In-Context Learning 79.79±3.06 85.38±3.92 62.21±13.46 34.83±7.59 45.81±6.67 47.11±0.63 60.36±1.56 59.36 Feature-MLP 64.80 ±1.78 79.20±2.26 70.77±0.67 87.78±0.61 68.40±0.86 42.01±0.33 53.43±1.57 66.63 Feature-BiLSTM 65.95 ±0.99 74.68±0.10 77.28±2.83 90.37±3.10 71.55±7.10 46.02±0.38 52.17±0.25 68.29 Black-Box Tuning 89.56±0.25 91.50±0.16 81.51±0.79 87.80±1.53 61.56±4.34 46.58±1.33 52.59±2.21 73.01 + Pre-trained prompt / / / / 75.51 ±5.54 83.83±0.21 77.62±1.30 83.90 Table 4.Comparison of deployment efﬁciency, viability of as-a-service, test accuracy, training time, memory footprint, and the amount of data to be uploaded/downloaded. ⋆ indicates the training time of the implementation with ONNX Runtime. All the compared methods are performed on the same 16-shot splits of SST-2 and AG’s News. Deployment- As-A- Test Training Memory Footprint Upload Download Efﬁcient Service Accuracy Time User Server per query per query SST-2 (max sequence length: 47) Prompt Tuning √ × 72.6 15.9 mins - 5.3 GB - - Model Tuning × × 87.8 9.8 mins - 7.3 GB - - Feature-MLP √ √ 63.8 7.0 mins 20 MB 2.8 GB 4 KB 128 KB Feature-BiLSTM √ √ 66.2 9.3 mins 410 MB 2.8 GB 4 KB 6016 KB Black-Box Tuning √ √ 89.4 10.1 (6.1 ⋆) mins 30 MB 3.0 GB 6 KB 0.25 KB AG’s News (max sequence length: 107) Prompt Tuning √ × 84.0 30.2 mins - 7.7 GB - - Model Tuning × × 88.4 13.1 mins - 7.3 GB - - Feature-MLP √ √ 71.0 13.5 mins 20 MB 3.6 GB 20 KB 256 KB Feature-BiLSTM √ √ 73.1 19.7 mins 500 MB 3.6 GB 20 KB 27392 KB Black-Box Tuning √ √ 82.6 21.0 (17.7 ⋆) mins 30 MB 4.6 GB 22 KB 1 KB Loss Functions. We consider three loss functions: cross entropy, hinge loss, and negative accuracy. As depicted in the top row of Figure 3, cross entropy and hinge loss signif- icantly outperform the negative accuracy. In the few-shot setting, the accuracy as a reward can be sparse, and cannot provide informative directions for optimization. On SST- 2 and AG’s News, we obtain that cross entropy performs slightly better than hinge loss. Subspace Dimensionality. The subspace of dimensional- ity dis the space where the optimization actually performs. According to the intrinsic dimensionality found in Agha- janyan et al. (2021), we explore the subspace dimensionality of {100, 200, 500, 1000}within the budget of {2k, 4k, 6k, 8k}. Accordingly, we set population size λ= 4 + 3 log(d). As shown in the middle row of Figure 3, the best subspace dimensionality can be different on different tasks (d= 200 performs the best on SST-2 development set and d= 500 performs the best on AG’s News development set), which is related to the observation that intrinsic dimensionality varies across different tasks (Aghajanyan et al., 2021). In general, a small subspace (e.g., d= 100) is hard to cover a good solution, while a large subspace (e.g., d= 1000) may lead to poor generalization. Prompt Length. Prompt length L determines the di- mensionality of the original parameter space (in our case D= L×1024). We evaluate black-box tuning under each budget in {2k, 4k, 6k, 8k}while varying the prompt length in {10, 20, 50, 100}. As shown in the bottom row of Fig- ure 3, shorter prompt confers faster convergence on the training sets but does not yield better generalization on the development sets. L = 50achieves the best accuracy on both SST-2 and AG’s News development sets.Black-Box Tuning for Language-Model-as-a-Service 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 77.5 80.0 82.5 85.0 87.5 90.0 92.5 95.0Dev accuracy (current best) SST-2 (dev) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 75 80 85 90 95 100Train accuracy (current best) AG's News (train) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 78 80 82 84 86 88 90Dev accuracy (current best) AG's News (dev) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 2000 4000 6000 8000 Budget (number of API calls) 94 95 96 97 98 99 100Train accuracy (current best) SST-2 (train) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 89 90 91 92 93 94Dev accuracy (current best) SST-2 (dev) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 84 86 88 90 92Train accuracy (current best) AG's News (train) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 83 84 85 86 87 88Dev accuracy (current best) AG's News (dev) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 95 96 97 98 99 100Train accuracy (current best) SST-2 (train) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 88 89 90 91 92 93 94Dev accuracy (current best) SST-2 (dev) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 82 84 86 88 90 92 94Train accuracy (current best) AG's News (train) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 83 84 85 86 87 88Dev accuracy (current best) AG's News (dev) L = 10 L = 20 L = 50 L = 100 Figure 3.Ablations of loss function, subspace dimensionality, and prompt length. We show mean and standard deviation of performance over 3 runs with different random seeds. Ablations of the random projection and the population size can be found in Appendix A. 5. Discussion and Future Work In this section we discuss our proposed method in the con- text of (1) derivative-free optimization and (2) prompt-based learning, respectively. By drawing comparisons with these two lines of research, we highlight some directions that could improve this work in future. Comparison with Previous Derivative-Free Approaches. Our proposed method lies in the same framework of previ- ous work that solves high-dimensional derivative-free op- timization problems via random embedding (Wang et al., 2016). In contrast, we set the random embedding A by sampling from a uniform distribution instead of normal dis- tributions, and use the CMA-ES to perform optimization in the generated subspace. In previous work, the target black- box functions are usually synthetic functions where only a few dimensions can affect the function values, and therefore most of the dimensions are strictly non-effective. In our real- world scenario, the intrinsic dimension can be approximate. In the context of PTMs, a more appropriate substitution for the term intrinsic dimensionality can be ϵ-effective dimen- sionality (Qian et al., 2016). Considering the relaxation to the intrinsic dimensionality of PTMs, more suitable ap- proaches such as sequential random embedding (Qian et al., 2016) and other more advanced methods of constructing the random projection matrix (Letham et al., 2020) should be explored in future work. Besides, the subspace generated by random projection can be sub-optimal. As demonstrated in Qin et al. (2021), training the projection A with multi- task supervision can result in better and smaller subspace. Besides, larger PTMs generally have lower intrinsic dimen- sionalities (Aghajanyan et al., 2021), as a result, we can use smaller subspace and more efﬁcient DFO algorithms such as Bayesian optimization on larger PTMs. Comparison with Previous Prompt-Based Learning Ap- proaches. From the perspective of prompt-based learning, our method is similar to prompt-tuning (Lester et al., 2021), where only the continuous prompt prepended to the input text is tuned, so our method also retains the beneﬁts of efﬁ- cient serving and mixed-task inference. In addition to the continuous prompt, we also insert some hard prompt tokens (e.g., ”It was [MASK]”) in the input text, which has been demonstrated to be effective in previous work (Gu et al., 2021) in the name of hybrid prompt tuning. Different from previous prompt-based learning approaches, our prompt tun-Black-Box Tuning for Language-Model-as-a-Service ing does not require backpropagation and gradient descent. Considering our used templates and label words are hand- crafted without trial-and-error, the performance reported in this paper is just a lower bound. More advanced techniques such as prompt engineering (Gao et al., 2021), label words engineering (Schick et al., 2020; Shin et al., 2020; Hu et al., 2021b), prompt pre-training (Gu et al., 2021), and prompt ensembling (Lester et al., 2021) are orthogonal to this work and therefore can further improve the performance. For simplicity, we do not integrate these methods and leave for future work. Acknowledgements The authors would like to thank Yang Yu for the valu- able suggestions of the methods and presentation of the paper, and the anonymous reviewers for their construc- tive comments. This work was supported by the National Key Research and Development Program of China (No. 2020AAA0108702), the National Natural Science Founda- tion of China (No. 62022027), the major key project of PCL (No. PCL2021A12), and the Natural Science Foundation of Shanghai (No. 21ZR1420300). References Aghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrin- sic dimensionality explains the effectiveness of language model ﬁne-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 7319–7328, 2021. Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language infer- ence. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 632– 642, 2015. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural In- formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Conn, A. R., Scheinberg, K., and Vicente, L. N.Introduction to Derivative-Free Optimization. SIAM, Philadelphia, PA, 2009. Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019. Dolan, W. B. and Brockett, C. Automatically construct- ing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005, 2005. Fedus, W., Zoph, B., and Shazeer, N. Switch transform- ers: Scaling to trillion parameter models with simple and efﬁcient sparsity. arXiv:2101.03961, 2021. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Con- ference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 3816–3830, 2021. Gu, Y ., Han, X., Liu, Z., and Huang, M. PPT: pre-trained prompt tuning for few-shot learning. arXiv:2109.04332, 2021. Hambardzumyan, K., Khachatrian, H., and May, J. W ARP: word-level adversarial reprogramming. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Con- ference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4921–4933, 2021. Hansen, N. The CMA evolution strategy: A tutorial. arXiv:1604.00772, 2016. Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evol. Comput., 9 (2):159–195, 2001. Hansen, N., M¨uller, S. D., and Koumoutsakos, P. Reducing the time complexity of the derandomized evolution strat- egy with covariance matrix adaptation (CMA-ES). Evol. Comput., 11(1):1–18, 2003. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a uniﬁed view of parameter-efﬁcient transfer learning. arXiv:2110.04366, 2021.Black-Box Tuning for Language-Model-as-a-Service He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In 2015 IEEE International Con- ference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026–1034, 2015. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pp. 2790–2799, 2019. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021a. Hu, S., Ding, N., Wang, H., Liu, Z., Li, J., and Sun, M. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classiﬁcation. arXiv:2108.02035, 2021b. Hu, Y .-Q., Qian, H., and Yu, Y . Sequential classiﬁcation- based optimization for direct policy search. In Proceed- ings of the 31st AAAI Conference on Artiﬁcial Intelli- gence, pp. 2029–2035, San Francisco, CA, 2017. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International Conference on Learn- ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Kolda, T. G., Lewis, R. M., and Torczon, V . Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385–482, 2003. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 3045–3059, 2021. Letham, B., Calandra, R., Rai, A., and Bakshy, E. Re-examining linear embeddings for high-dimensional Bayesian optimization. In Advances in Neural Informa- tion Processing Systems 33, virtual, 2020. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871–7880, 2020. Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscapes. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continu- ous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Vol- ume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597, 2021. Liu, X., Ji, K., Fu, Y ., Du, Z., Yang, Z., and Tang, J. P-tuning v2: Prompt tuning can be comparable to ﬁne-tuning universally across scales and tasks. arXiv:2110.07602, 2021a. Liu, X., Zheng, Y ., Du, Z., Ding, M., Qian, Y ., Yang, Z., and Tang, J. GPT understands, too. arXiv:2103.10385, 2021b. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨opf, A., Yang, E., DeVito, Z., Rai- son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. InAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 11054–11070, 2021. Peters, M. E., Ruder, S., and Smith, N. A. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representa- tion Learning for NLP , RepL4NLP@ACL 2019, Florence, Italy, August 2, 2019, pp. 7–14, 2019. Qian, H., Hu, Y ., and Yu, Y . Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016 , pp. 1946–1952, 2016.Black-Box Tuning for Language-Model-as-a-Service Qin, G. and Eisner, J. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 5203–5212, 2021. Qin, Y ., Wang, X., Su, Y ., Lin, Y ., Ding, N., Liu, Z., Li, J., Hou, L., Li, P., Sun, M., and Zhou, J. Exploring low- dimensional intrinsic task subspace via prompt tuning. arXiv:2110.07867, 2021. Qiu, X., Sun, T., Xu, Y ., Shao, Y ., Dai, N., and Huang, X. Pre-trained models for natural language processing: A survey. SCIENCE CHINA Technological Sciences, 2020. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text trans- former. J. Mach. Learn. Res., 21:140:1–140:67, 2020. Rios, L. M. and Sahinidis, N. V . Derivative-free optimiza- tion: A review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3): 1247–1293, 2013. Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. Evolution strategies as a scalable alternative to reinforce- ment learning. arXiv:1703.03864, 2017. Schick, T. and Sch¨utze, H. Exploiting cloze-questions for few-shot text classiﬁcation and natural language infer- ence. In Proceedings of the 16th Conference of the Eu- ropean Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 255–269, 2021a. Schick, T. and Sch ¨utze, H. It’s not just size that matters: Small language models are also few-shot learners. In Pro- ceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 2339–2352, 2021b. Schick, T., Schmid, H., and Sch ¨utze, H. Automatically identifying words that can serve as labels for few-shot text classiﬁcation. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020 , pp. 5569–5578, 2020. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and de Freitas, N. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104 (1):148–175, 2016. Shin, T., Razeghi, Y ., IV , R. L. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 4222–4235, 2020. Snoek, J., Larochelle, H., and Adams, R. P. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, pp. 2960–2968, Lake Tahoe, NV , 2012. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y ., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Wash- ington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1631–1642, 2013. Sun, T., Liu, X., Qiu, X., and Huang, X. Paradigm shift in natural language processing. Machine Intelligence Research, 2022. Sun, Y ., Wang, S., Feng, S., Ding, S., Pang, C., Shang, J., Liu, J., Chen, X., Zhao, Y ., Lu, Y ., Liu, W., Wu, Z., Gong, W., Liang, J., Shang, Z., Sun, P., Liu, W., Ouyang, X., Yu, D., Tian, H., Wu, H., and Wang, H. ERNIE 3.0: Large- scale knowledge enhanced pre-training for language un- derstanding and generation. arXiv:2107.02137, 2021. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and anal- ysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Wang, Z., Hutter, F., Zoghi, M., Matheson, D., and de Fre- itas, N. Bayesian optimization in a billion dimensions via random embeddings. J. Artif. Intell. Res., 55:361–387, 2016. Weston, J. and Watkins, C. Support vector machines for multi-class pattern recognition. In ESANN 1999, 7th Eu- ropean Symposium on Artiﬁcial Neural Networks, Bruges, Belgium, April 21-23, 1999, Proceedings, pp. 219–224, 1999. Williams, A., Nangia, N., and Bowman, S. R. A broad- coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 1112– 1122, 2018.Black-Box Tuning for Language-Model-as-a-Service Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li, F., Zhu, H., Luo, J., Xu, L., and Zhang, X. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv:2110.04725, 2021. Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y ., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., Li, C., Gong, Z., Yao, Y ., Huang, X., Wang, J., Yu, J., Guo, Q., Yu, Y ., Zhang, Y ., Wang, J., Tao, H., Yan, D., Yi, Z., Peng, F., Jiang, F., Zhang, H., Deng, L., Zhang, Y ., Lin, Z., Zhang, C., Zhang, S., Guo, M., Gu, S., Fan, G., Wang, Y ., Jin, X., Liu, Q., and Tian, Y . Pangu-α: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv:2104.12369, 2021. Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q., and Artzi, Y . Revisiting few-sample BERT ﬁne-tuning. In9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a. Zhang, X., Zhao, J. J., and LeCun, Y . Character-level con- volutional networks for text classiﬁcation. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 649–657, 2015a. Zhang, Y ., Sohn, K., Villegas, R., Pan, G., and Lee, H. Improving object detection with deep convolutional net- works via Bayesian optimization and structured predic- tion. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 249–258, Boston, MA, 2015b. Zhang, Z., Han, X., Zhou, H., Ke, P., Gu, Y ., Ye, D., Qin, Y ., Su, Y ., Ji, H., Guan, J., Qi, F., Wang, X., Zheng, Y ., Zeng, G., Cao, H., Chen, S., Li, D., Sun, Z., Liu, Z., Huang, M., Han, W., Tang, J., Li, J., Zhu, X., and Sun, M. CPM: A large-scale generative chinese pre-trained language model. arXiv:2012.00413, 2020. Zhang, Z., Gu, Y ., Han, X., Chen, S., Xiao, C., Sun, Z., Yao, Y ., Qi, F., Guan, J., Ke, P., Cai, Y ., Zeng, G., Tan, Z., Liu, Z., Huang, M., Han, W., Liu, Y ., Zhu, X., and Sun, M. CPM-2: large-scale cost-effective pre-trained language models. arXiv:2106.10715, 2021b. Zhong, Z., Friedman, D., and Chen, D. Factual probing is [MASK]: learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 5017–5033, 2021.Black-Box Tuning for Language-Model-as-a-Service A. Additional Experimental Results Random Projection. The random projection matrix A ∈RD×d is a key factor that determines whether and how hard it is to ﬁnd a good solution in the generated subspace. Here we compare two design choices of setting A: The ﬁrst choice is commonly used in previous high-dimensional derivative-free optimization work (Wang et al., 2016; Qian et al., 2016), that is setting each entry of A by sampling from a normal distribution. Following Qian et al. (2016), we use N(0,1/d) where d is the subspace dimensionality5. The second choice is setting each entry of A by sampling from a uniform distribution, which is widely used for initializing linear layers in modern neural networks. Here we use the uniform distribution proposed in He et al. (2015). As shown in Figure 4, both random projections can achieve a considerable cross entropy loss on SST-2 and AG’s News within reasonable budgets but faster convergence is obtained using uniform distribution. 0 2000 4000 6000 8000 Number of API calls 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Cross Entropy Loss SST-2 Normal Uniform 0 2000 4000 6000 8000 Number of API calls 0.3 0.4 0.5 0.6 0.7 0.8Cross Entropy Loss AG's News Normal Uniform Figure 4.Effect of random projection A. Population Size. In each iteration of the CMA-ES, a population of solutions are sampled from a multivariate normal distribution model. The evaluation of the population is then used to update the parameters of the multivariate normal distribution model. Here we study the effect of the population size on SST-2. In our experiments, we sequentially evaluate each solution in a population, and therefore larger population size will result in more API calls given the same CMA-ES iterations. As shown in Figure 5, smaller population size confers faster convergence in terms of number of API calls. We also demonstrate the comparison in terms of the CMA-ES iterations, which can be found in the following section. 0 2000 4000 6000 8000 Number of API calls 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 0 2000 4000 6000 8000 Number of API calls 84 86 88 90 92 94Dev accuracy (current best) SST-2 (dev) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 Figure 5.Effect of population size λ. 5We also tried N(0,1) as used in Wang et al. (2016), which does not work in our case. For N(0,1/d), we adopt a larger search space Zinstead of [−5,5]d to get it work.Black-Box Tuning for Language-Model-as-a-Service 0 250 500 750 1000 Subspace dimension d 84 86 88 90T est Accuracy SST-2 25 50 75 100 Prompt Length 84 86 88 90T est Accuracy SST-2 0 250 500 750 1000 Subspace dimension d 35 40 45 50T est Accuracy SNLI 25 50 75 100 Prompt Length 35 40 45 50T est Accuracy SNLI Figure 6.Ablation of subspace dimensionality and prompt length in 16-shot setting. 0 5000 10000 TFLOPs 0.1 0.2 0.3 0.4 0.5 0.6cross entropy loss d = 200 0 5000 10000 TFLOPs d = 400 0 5000 10000 TFLOPs d = 600 0 2500 5000 7500 10000 TFLOPs d = 800 0 5000 10000 TFLOPs d = 1000 CMA-ES Adam (lr=0.01) Adam (lr=0.1) Figure 7.Optimization in low-dimensional subspaces using CMA-ES and Adam. Ablation of Subspace Dimensionality and Prompt Length in 16-shot Setting. In § 4.3, we conduct ablation experi- ments in the 64-shot setting to reduce the variance over different runs. To keep consistent with the experimental setting in Table 3, we demonstrate in Figure 6 the ablation results on subspace dimensionality and prompt length in the 16-shot setting. CMA-ES vs. Adam in Subspaces. In Figure 3, we compare the convergence of prompt tuning (with Adam optimizer) and black-box tuning (with CMA-ES), where Adam performs optimization in the original prompt space (P) while CMA-ES performs in the generated subsapce (Z). Here we also compare the effectiveness and efﬁciency of Adam and CMA-ES in subspaces. As shown in Figure 7, CMA-ES is more efﬁcient and stable than Adam in low-dimensional subspaces. When the dimensionality of the subsapce becomes large (e.g., d= 1000), Adam with a appropriate learning rate can perform on par with CMA-ES. Note that CMA-ES does not require back-propagation, so the computation cost of one iteration for CMA-ES and Adam can be very different. For fair comparison, we convert the number of iterations into FLOPs. The FLOPs of one iteration of Adam is estimated to be three times greater than CMA-ES. B. Parallel Evaluation If the training data is smaller, or the server allows larger batches, a promising way to improve training efﬁciency is to use parallel evaluation. That is, we can evaluate the entire population in parallel, as depicted in Figure 8(a). As demonstrated in Figure 8(b), we can achieve 100% accuracy on the SST-2 training set with population size of 20 and 25 in 300 iterations (API calls). In case of the batch size per API call is limited, we can also use asynchronous queries to simulate the parallel evaluation. C. Estimation of Uploaded/Downloaded Data Size In this section we describe how we estimate the amount of data to be uploaded and downloaded (Table 4). For black-box tuning, there are two kinds of data to be uploaded: (1) training samples, and (2) continuous prompt. A training sample is comprised of two parts: input ids and attention mask. We can use the unsigned short (representation range: 0∼65535, 2 bytes per value) for input ids and use the bool type (1 byte per value) for attention mask. For continuous prompt, which contains hundreds of values, we can use the ﬂoat type (4 bytes per value) for representation.Black-Box Tuning for Language-Model-as-a-Service Serial Evaluation Population Logits Data Parallel Evaluation (a) 0 100 200 300 Iterations of CMA-ES 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 (b) Figure 8.(a) Illustration of the parallel evaluation. (b) Comparison of the convergence rate with different population sizes using parallel evaluation. Take SST-2 16-shot split as an example, the input ids and attention mask are in shape of 32 ×47, where 32 is the batch size and 47 is the maximum sequence length, so there are ∼2.9KB data for input ids and ∼1.5KB data for attention mask. Assume the prompt is 500-dimensional, we need to upload additional ∼2KB data for prompt. The data to be downloaded is the output logits of the candidate words, which is a dictionary containing |Y| ﬂoat values. Take SST-2 16-shot split as an example, the size of data to be downloaded is32 ×2 ×4bytes = 0.25KB. For feature-based methods we use similar estimation methods. The data size for upload is the same for Feature-MLP and Feature-BiLSTM. The data to be downloaded for Feature-MLP is the representation of the [CLS] token while the data to be downloaded for Feature-BiLSTM is the representation of all the tokens. Note that this estimation, without any data compression, is an upper bound of the real scenario.",
      "meta_data": {
        "arxiv_id": "2201.03514v4",
        "authors": [
          "Tianxiang Sun",
          "Yunfan Shao",
          "Hong Qian",
          "Xuanjing Huang",
          "Xipeng Qiu"
        ],
        "published_date": "2022-01-10T18:17:05Z",
        "pdf_url": "https://arxiv.org/pdf/2201.03514v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes Black-Box Tuning (BBT), a novel framework to optimize continuous prompts for Language-Model-as-a-Service (LMaaS) scenarios where gradients of large pre-trained language models (PTMs) are unavailable. It introduces a derivative-free optimization approach that operates in a randomly projected low-dimensional subspace, leveraging the low intrinsic dimensionality of large PTMs. The experimental results demonstrate that BBT, using RoBERTaLARGE with few-shot labeled samples, significantly outperforms manual prompting, GPT-3's in-context learning, and even gradient-based methods like prompt tuning and full model tuning on various language understanding tasks. This work pioneers the optimization of large-scale PTMs via derivative-free methods for user-side prompt tuning on resource-limited devices.",
        "methodology": "The methodology addresses the challenge of optimizing continuous prompts in high-dimensional spaces without gradient access. It formulates common language understanding tasks as classification, mapping inputs and labels to PTM-compatible formats. To overcome the intractability of high-dimensional derivative-free optimization, BBT projects the original prompt space (p ∈ RD) onto a much smaller subspace (z ∈ Rd, d≪D) using a random linear projection matrix A ∈ RD×d. The optimization is performed on z in this low-dimensional subspace. The objective function minimizes a loss L (cross entropy or hinge loss) over predictions from the black-box PTM inference API f(Az + p0; ˜X), where p0 is an initial prompt embedding. The random matrix A is initialized by sampling from a uniform distribution. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a derivative-free optimizer, is employed to update the prompt embeddings. For sentence-pair tasks, p0 can be pre-trained on NLI datasets (e.g., MNLI); otherwise, it's initialized with random word embeddings from the PTM's vocabulary.",
        "experimental_setup": "Experiments were conducted on 7 common language understanding tasks: sentiment analysis (SST-2, Yelp Polarity), topic classification (AG’s News, DBPedia), natural language inference (SNLI, RTE), and paraphrase (MRPC). RoBERTaLARGE was chosen as the backbone PTM. A few-shot learning setting (16-shot per class) was used, with development and test sets constructed from original datasets. BBT was compared against gradient-based baselines (Prompt Tuning, P-Tuning v2, Model Tuning) and gradient-free baselines (Manual Prompt, In-context Learning, Feature-based methods like Feature-MLP and Feature-BiLSTM). Default hyperparameters for BBT included a prompt length of 50, subspace dimension of 500, population size of 20, uniform random projection, cross entropy loss, and an API call budget of 8000. Ablation studies were performed on loss functions, subspace dimensionality, prompt length, random projection type, and population size, typically in a 64-shot setting to reduce variance, with some 16-shot results also provided. All methods were implemented in PyTorch on a single NVIDIA GTX 3090 GPU, with training time measured with and without ONNX Runtime acceleration.",
        "limitations": "The intrinsic dimensionality of PTMs is approximate in real-world scenarios, potentially leading to sub-optimal performance in the randomly generated subspace. The current approach uses hand-crafted templates and label words, which might not be optimal, suggesting the reported performance is a lower bound. The study focuses primarily on language understanding tasks with RoBERTaLARGE, leaving the application to generative PTMs (e.g., GPT, T5, BART) for future work. While CMA-ES generally performs well, Adam with an appropriate learning rate can achieve comparable results in larger low-dimensional subspaces (e.g., d=1000), implying CMA-ES is not universally superior in all subspace settings.",
        "future_research_directions": "Future work includes exploring more advanced methods for constructing the random projection matrix, such as sequential random embedding and other cutting-edge techniques, to better account for the ϵ-effective dimensionality of PTMs. Training the projection matrix 'A' with multi-task supervision to discover better and smaller subspaces is another promising direction. Applying the framework to larger PTMs (which typically exhibit lower intrinsic dimensionalities) to enable the use of even smaller subspaces and more efficient DFO algorithms like Bayesian optimization is suggested. Integrating advanced prompt-based learning techniques, such as prompt engineering, label words engineering, more extensive prompt pre-training, and prompt ensembling, is expected to further enhance performance. Finally, extending the framework to generative PTMs by converting downstream tasks into a unified text-to-text format is also a key future direction."
      }
    },
    {
      "title": "Black-Box Tuning for Language-Model-as-a-Service",
      "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.",
      "full_text": "Black-Box Tuning for Language-Model-as-a-Service Tianxiang Sun 1 Yunfan Shao1 Hong Qian 2 Xuanjing Huang 1 Xipeng Qiu 1 3 Abstract Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-speciﬁc prompts to query the PTMs through some black- box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gra- dients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the con- tinuous prompt prepended to the input text via derivative-free optimization. Instead of optimiz- ing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a ran- domly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimen- tal results show that the black-box tuning with RoBERTa on a few labeled samples not only sig- niﬁcantly outperforms manual prompt and GPT- 3’s in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning. 1. Introduction Scaling pre-trained language models (PTMs) has shown increasing power on a wide range of NLP tasks (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021; Zhang et al., 2020; 2021b; Zeng et al., 2021; Sun et al., 2021; Qiu et al., 2020). Extremely large PTMs can easily generalize to various downstream tasks with a few labeled samples (Brown et al., 2020). However, making these large PTMs beneﬁt everyone is a challenge. On the one hand, running such models can be very expensive or even infeasible for most users. On the other hand, the model parameters are often not open-sourced due to commercial 1Fudan University 2East China Normal University 3Peng Cheng Laboratory. Correspondence to: Tianxiang Sun <tx- sun19@fudan.edu.cn>, Xipeng Qiu <xpqiu@fudan.edu.cn>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Users Query  Response Server Mixed-Task Batch PTM Inference (Black-Box)  Task Prompt (Tunable) Samples Update Prompt Figure 1.Illustration of Language-Model-as-a-Service (LMaaS). Users can query the PTM deployed on the server through a black- box API. In each query, users can input a task prompt and a batch of texts. On the server side, the samples can be mixed in a large batch to be fed into the PTM. By iteratively querying the PTM through the black-box API, users can optimize and ﬁnally obtain good prompts to solve the language tasks of interest. considerations and the potential risk of misuse.1 Therefore, large PTMs such as GPT-3 (Brown et al., 2020), ERNIE 3.0 (Sun et al., 2021) and Yuan 1.0 (Wu et al., 2021) are usually released as a service, allowing users to access these powerful models through black-box APIs. In this scenario, called Language-Model-as-a-Service (LMaaS), users can solve the language tasks of interest using the black-box APIs by crafting task-speciﬁc text prompts or including training samples in the input texts (a.k.a. in- context learning (Brown et al., 2020)). Due to the great power of the general-purpose PTMs underlying the APIs, such approaches can achieve considerable performance on simple language tasks, and therefore have powered many interesting applications2. However, querying large PTMs through hand-crafted text prompts cannot fully exploit la- beled data, resulting in unsatisfactory performance in many use cases. Instead of designing discrete text prompts, recently much effort has been devoted to continuous prompt tuning (Li & Liang, 2021; Hambardzumyan et al., 2021; Liu et al., 1https://openai.com/blog/openai-api/ 2See https://gpt3demo.com/ for examples. arXiv:2201.03514v4  [cs.CL]  27 Jun 2022Black-Box Tuning for Language-Model-as-a-Service 2021b), which is to optimize the continuous prompt injected to the text while keeping the PTM parameters frozen. Such methods only require storing a small continuous prompt for each task, and therefore are highly deployment-efﬁcient. Besides, tuning the continuous prompt can be as effective as ﬁne-tuning the entire model when the PTM becomes large (Lester et al., 2021). However, in all the previous methods, the continuous prompts are learned through back- propagation, which is unavailable in the scenario of LMaaS. Can we optimize the task-speciﬁc continuous prompts when we only have access to the PTM inference API? Since gradients are unavailable, we can only invoke derivative-free optimization (DFO) 3 (Kolda et al., 2003; Conn et al., 2009; Rios & Sahinidis, 2013). DFO involves a kind of optimization algorithms that do not depend on gra- dients, but only relies on function values (or ﬁtness values) of sampled solutions. However, DFO algorithms are known to suffer from slow convergence rate when the dimension- ality of the search space is high. Thus, it is intractable to optimize even only the continuous prompts, which can be tens of thousands of parameters, using DFO algorithms. Fortunately, recent work found that common PTMs, despite their large numbers of parameters, have a very low intrinsic dimensionality (Aghajanyan et al., 2021; Qin et al., 2021). That means, there exists a low-dimensional reparameteriza- tion that is as effective for ﬁne-tuning as the full parameter space. It has been demonstrated that optimizing only hun- dreds (Aghajanyan et al., 2021) or even dozens (Qin et al., 2021) of parameters can achieve non-trivial performance. Given that the intrinsic dimensionality of the objective func- tion (in our case is the forward computation of PTMs) is low, the optimization can be effectively solved via DFO al- gorithms with random embedding (Wang et al., 2016; Qian et al., 2016; Letham et al., 2020). Based on the these insights, this paper proposes the Black- Box Tuning (BBT) to solve various language understand- ing tasks by only accessing the PTM inference API. In particular, we manage to optimize the continuous prompt prepended to the input text by iteratively querying the PTM inference API, as brieﬂy depicted in Figure 1. To han- dle the high dimensionality of the continuous prompt, we project the original prompt space using a random linear projection onto a much smaller subspace and solve this optimization problem with some derivative-free optimizer in that smaller subsapce. In contrast to conventional ﬁne- tuning methods that can only be performed by the service side, black-box tuning allows users to optimize their task- speciﬁc prompts locally on resource-limited devices (even without GPUs). Our experimental results demonstrate that prompting RoBERTaLARGE (Liu et al., 2019) using BBT on 3Also termed as black-box, zeroth-order or gradient-free opti- mization. a few labeled samples not only outperforms manual prompt and in-context learning (Brown et al., 2020), but also out- performs its gradient-based counterparts, namely prompt tuning (Lester et al., 2021) and full model tuning. The contribution of this paper is three folds:4 • This paper proposes a novel scenario (LMaaS) where one should learn to prompt the PTMs by only accessing their inference APIs. • This paper offers a solution (BBT) for such a scenario to accomplish common language understanding tasks without access to model parameters and gradients, such that large-scale PTMs can better beneﬁt users. • Empirical results show that DFO can successfully deal with real-world language tasks by learning to prompt large-scale PTMs with more than millions of parame- ters. Thus, this work pioneers the work of optimizing large-scale PTMs through DFO methods. 2. Background Large-Scale PTMs as APIs. It is a promising way to de- ploy large-scale PTMs to serve downstream applications by providing general-purpose APIs. For the service side, wrapping the computation of the PTM into an easy-to-use API has become a common practice (Brown et al., 2020; Sun et al., 2021; Wu et al., 2021). In contrast to training, the in- ference speed of large-scale PTMs can be highly optimized with acceleration techniques such as ORT and TensorRT. In addition, large-scale PTMs are often not open-sourced due to the commercial reasons and the potential risk of mis- use. For the user side , even if the large-scale PTMs are available, it is expensive or even infeasible to locally run them. Thus, how to exploit the PTM inference API to solve conventional language tasks is a promising direction. Intrinsic Dimensionality of PTMs. The intrinsic dimen- sionality of an objective function is the minimum number of parameters needed to obtain satisfactory solutions (Li et al., 2018). In particular, the intrinsic dimensionality in- dicates the lowest dimensional reparameterization that is as effective for optimizing as the full parameter space. Li et al. (2018) propose to measure the intrinsic dimensionality of neural networks by ﬁnding the minimal dimensionality of the subspace that is randomly projected from the full trainable parameters, in which they can optimize the neural networks to achieve satisfactory solutions. Aghajanyan et al. (2021) empirically show that large-scale pre-training implic- itly compresses the intrinsic dimensionality of downstream NLP tasks. By tuning only hundreds of parameters that 4Our code is publicly available at https://github.com/ txsun1997/Black-Box-TuningBlack-Box Tuning for Language-Model-as-a-Service are then randomly projected onto the full parameter space of RoBERTa, they can achieve 90% performance relative to full model tuning. Qin et al. (2021) show that intrinsic subspace on various tasks can be compressed to less than 100 dimensions with multi-task supervision. This line of research, along with the work of parameter-efﬁcient tun- ing (Houlsby et al., 2019; Li & Liang, 2021; Lester et al., 2021; Sun et al., 2022; Hu et al., 2021a; He et al., 2021), demonstrate that PTMs can well adapt to downstream tasks by tuning a very small proportion of parameters, which im- plies the possibility of optimizing large-scale PTMs with derivative-free algorithms. Prompt-Based Learning. Prompt-based learning is to formulate downstream tasks as a (masked) language mod- eling task, and therefore reduces the gap between PTM pre-training and ﬁne-tuning (Brown et al., 2020; Schick & Sch¨utze, 2021a;b; Gao et al., 2021; Sun et al., 2022). For instance, one can use BERT (Devlin et al., 2019) to predict whether the sentence ”This is a fantastic movie” is positive or negative by appending the prompt ”It was [MASK]” and see if BERT predicts ”great” or ”terrible” at the masked position. Note that the prompt is not necessarily discrete, it can also be optimized efﬁciently in continuous space with gradient descent (Li & Liang, 2021; Hambardzumyan et al., 2021; Qin & Eisner, 2021; Liu et al., 2021b; Zhong et al., 2021). In the case of only tuning the continuous prompt while keeping the parameters of large PTMs untouched, one can retain the efﬁcient serving beneﬁts while matching the performance of full model tuning (Lester et al., 2021). Our work also proposes to optimize the continuous prompt while keeping the PTM parameters unchanged, but without gradient descent. Derivative-Free Optimization. Derivative-free optimiza- tion (DFO) realizes optimization only via the function val- ues f(x) on the sampled solutions x. Most DFO algorithms share a common structure of sampling-and-updating to en- hance the quality of solutions. Representative DFO algo- rithms include evolutionary algorithms (Hansen et al., 2003), Bayesian optimization (Shahriari et al., 2016), etc. Due to their ability of addressing complex optimization tasks, DFO algorithms have achieved many impressive applica- tions in automatic machine learning (Snoek et al., 2012), reinforcement learning (Salimans et al., 2017; Hu et al., 2017), objective detection (Zhang et al., 2015b), etc. 3. Approach 3.1. Problem Formulation Common language understanding tasks can be formulated as a classiﬁcation task, which is to predict for a batch of input texts X the labels Y. To solve the target language understanding task with a general-purpose PTM, we should modify X with some template (e.g., adding some trigger words and a special token [MASK] for BERT-like PTMs) and map the labels Y to some words in the PTM vocab- ulary (e.g., the sentiment label ”positive” can be mapped to ”great”). The modiﬁed inputs and labels are denoted as ˜X and ˜Y. Assume the BERT-like PTM inference API f takes a continuous prompt p and a batch of modiﬁed texts ˜X as input, and outputs the logits on the masked positions, i.e., ˆY = f(p; ˜X). With the output logits, we can calculate the loss on this batch of data, which is not necessarily to be differentiable. Our goal is to ﬁnd the optimal prompt p⋆ = arg minp∈PL(f(p; ˜X),˜Y), where Pis some search space of interest and Lis some loss function such as nega- tive accuracy. The black-box function f is not available to the optimizer in closed form, but can be evaluated at a query point (p; ˜X). 3.2. Black-Box Tuning As demonstrated by Lester et al. (2021), dozens of prompt to- kens are required to obtain a competitive performance when only tuning continuous prompts. Given that the embedding dimensionality of large-scale PTMs is usually larger than one thousand (e.g., the word embeddings of RoBERTaLARGE are 1024-dimensional), the dimensionality of the continuous prompt p ∈RD that we are interested to optimize can be tens of thousands, which makes derivative-free optimization intractable. To handle this high-dimensional optimization, since large-scale PTMs have a low intrinsic dimensional- ity (Aghajanyan et al., 2021; Qin et al., 2021), we manage to optimize z ∈Rd in a much smaller subspace (d≪D), and use a random projection matrix A ∈RD×d to project z on the original prompt space P. Note that directly projecting z onto the prompt space that is compatible with the PTM is non-trivial. To ease the optimization, we instead optimize the increment of some initial prompt p0. For simplicity, we randomly sample n tokens from the PTM vocabulary as initialization. Thus, our objective becomes z⋆ = arg min z∈Z L(f(Az + p0; ˜X),˜Y) , (1) where Zis the search space. Previous work (Wang et al., 2016; Qian et al., 2016; Letham et al., 2020) in derivative- free optimization usually sets each entry in the random matrix A by sampling from some normal distribution. How- ever, this sampling strategy does not perform well in our scenario. Instead, we set values of the random matrix A by sampling from a uniform distribution adopted in He et al. (2015) (cf. Appendix A for the comparison). We restrict the search space to Z= [−5,5]d. For the loss functionL, a straightforward alternative is using negative accuracy. However, the reward of accuracy can beBlack-Box Tuning for Language-Model-as-a-Service Best film ever . It was <MASK> . A totally boring movie ! It was <MASK> . You 'll probably love it . It was <MASK> . great terrible great ෩𝒀 throne arrow apple 𝒛 𝑨 ∈ ℝ𝐷×𝑑 𝑨𝒛 𝒑𝟎 𝒑 Copy Pre-Trained Language Model Inference (Black-Box API) good:10.2 great:7.9 movie:7.1 … terrible:11.2 bad:9.9 boring:8.0 … great:9.8 love:5.2 film:3.3 … ෡𝒀𝓛(෩𝒀,෡𝒀) Derivative-Free Optimizer Best film ever . It was <MASK> . A totally boring movie ! It was <MASK> . You 'll probably love it . It was <MASK> . ෩𝑿 Server User Labeled Data Figure 2.A single iteration of the optimization. Given z ∈Rd provided by the derivative-free optimizer, we project it to the prompt space by a random matrix A ∈RD×d. By adding the projected prompt embeddings Az with some initial prompt embeddings p0 (in this illustration are the embeddings of tokens randomly sampled from the PTM’s vocabulary), we obtain the ﬁnal prompt embeddings that are then concatenated with the input texts ˜X. By calling the black-box API f, which implements the forward computation of the PTM, the predictions on the masked positions are obtained, i.e., ˆY = f(p; ˜X). With the prediction ˆY and the golden labels ˜Y at hand, we can calculate the loss that is used by the derivative-free optimizer to suggest a new z. sparse and less informative, especially when training data is limited. Thus, we also consider two loss functions that are more sensitive to predictions, i.e., cross entropy and hinge loss. Given the output logits ˆ yover a candidate set of label words, and the golden label word ˜yof a certain sample, the cross entropy is deﬁned as LCE(ˆ y,˜y) =−log Softmax˜y(ˆ y). (2) For hinge loss, we adopt a multi-class extension (Weston & Watkins, 1999), LHinge(ˆ y,˜y) = ∑ i̸=˜y max(0,γ + ˆ yi −ˆ y˜y). (3) In this work we set the margin γ = 2. The performances of using cross entropy, hinge loss, and negative accuracy are compared in Figure 3. 3.3. The CMA Evolution Strategy As demonstrated in Aghajanyan et al. (2021), the intrinsic dimensionality of PTMs like RoBERTa LARGE on various tasks can be hundreds. To handle optimization of such scale, we adopt the CMA-ES (Covariance Matrix Adaptation Evo- lution Strategy) (Hansen & Ostermeier, 2001; Hansen et al., 2003), which is a widely used evolutionary algorithm for non-convex black-box optimization in continuous domain. In particular, CMA-ES maintains a parameterized search distribution model, i.e., multivariate normal distribution. In each iteration, CMA-ES samples a population of new query solutions (also referred to as individuals or offspring) from the multivariate normal distribution model z(t+1) i ∼m(t) + σ(t)N(0,C(t)) , (4) where i= 1,...,λ and λis the population size. m(t) ∈Rd is the mean vector of the search distribution at iteration step t, σ(t) ∈R+ is the overall standard deviation that controls the step length, and C(t) ∈Rd×d is the covariance matrix that determines the shape of the distribution ellipsoid. By maximizing the likelihood of successful steps, m(t), σ(t), C(t) are updated (cf. Hansen (2016) for more details). 3.4. Pre-Training Prompt Embedding Considering that sentence-pair tasks can share the same template and label words, as shown in Table 1, we can pre- train a prompt embedding p0 on some publicly available NLI task (in our experiments we use the MNLI (Williams et al., 2018) training set) for a better initialization. For other classiﬁcation tasks we set p0 as word embeddings randomly drawn from the vocabulary of RoBERTaLARGE. 4. Experiments 4.1. Setup Dataset. We conduct experiments on several common language understanding tasks including sentiment analy- sis, topic classiﬁcation, natural language inference (NLI),Black-Box Tuning for Language-Model-as-a-Service Table 1.Statistics, manual templates, and label words used in our experiments. |Y| : number of classes. Category Dataset |Y| |Train| |Test| Type Template Label words single- sentence SST-2 2 67k 0.9k sentiment ⟨S⟩. It was[MASK]. great, bad Yelp P. 2 560k 38k sentiment ⟨S⟩. It was[MASK]. great, bad AG’s News 4 120k 7.6k topic [MASK]News:⟨S⟩ World, Sports, Business, Tech DBPedia 14 560k 70k topic [Category: [MASK]] ⟨S⟩ Company, Education, Artist, Athlete, Ofﬁce, Transportation, Building, Natural, Village, Animal, Plant, Album, Film, Written sentence- pair MRPC 2 3.7k 0.4k paraphrase ⟨S1⟩? [MASK], ⟨S2⟩ Yes, No RTE 2 2.5k 0.3k NLI ⟨S1⟩? [MASK], ⟨S2⟩ Yes, No SNLI 3 549k 9.8k NLI ⟨S1⟩? [MASK], ⟨S2⟩ Yes, Maybe, No and paraphrase. For sentiment analysis, we choose SST- 2 (Socher et al., 2013) and Yelp polarity (Zhang et al., 2015a). For topic classiﬁcation, we choose AG’s News and DBPedia (Zhang et al., 2015a). For NLI, we choose SNLI (Bowman et al., 2015) and RTE (Wang et al., 2019). For paraphrase, we choose MRPC (Dolan & Brockett, 2005). The statistics, manual templates and label words of these datasets are shown in Table 1. Few-Shot Setting. For a broad range of users, the amount of labeled data can be limited, in which case they can resort to the deployed large PTMs due to their great power of few- shot learning (Brown et al., 2020). Hence, in this paper we conduct experiments in the few-shot setting. We randomly select ksamples for each class to construct a k-shot training set Dtrain, and compose a development set Ddev by randomly drawing another ksamples from the original training set and ensure that |Dtrain|= |Ddev|to simulate the true few-shot learning setting (Perez et al., 2021). Following Zhang et al. (2021a), Gao et al. (2021), and Gu et al. (2021), we use the original development sets as the test sets. For datasets with- out development sets, we use the original test sets. Hence, in our experiments |Dtest|≫|D train|= |Ddev|. Backbone Model. We choose RoBERTaLARGE (Liu et al., 2019) as our backbone model because: (1) We mainly fo- cus on language understanding tasks; (2) Aghajanyan et al. (2021) have demonstrated that RoBERTaLARGE has a very small intrinsic dimensionality (about hundreds) on many tasks. It is worth noting that generative PTMs such as GPT (Brown et al., 2020), T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are also compatible with our framework if we convert downstream tasks into a uniﬁed text-to-text format. We leave for future work the applica- tions of generative PTMs. Baselines. We compare our proposed black-box tuning with two kinds of methods: gradient-based methods and gradient-free methods. For gradient-based methods, we consider three baselines: (1) Prompt Tuning: Following Lester et al. (2021), we only train the continuous prompts Table 2.Default conﬁguration of hyper-parameters. Hyper-parameter Default Prompt length (L) 50 Subspace dimension (d) 500 Population size (λ) 20 Random projection (A) Uniform Loss functionL Cross Entropy Budget (# of API calls) 8000 prepended to the input texts while keeping the PTM frozen. We use an Adam optimizer (Kingma & Ba, 2015) with learn- ing rate of 5e-4 and batch size of 16 for 1000 epochs. For fair comparison, we use the same prompt length, manual template, label words, and the same pre-trained prompt em- bedding for initialization on sentence-pair tasks as black-box tuning. (2) P-Tuning v2 (Liu et al., 2021a) is an improved variant of prompt tuning. Instead of injecting continuous prompts merely into the input layer, P-Tuning v2 prepends and optimizes continuous prompts at every layer of the PTM. We optimize the prompts of length 128 at each layer using an Adam optimizer with learning rate of 5e-4 and batch size of 32 for 2000 epochs. (3) Model Tuning: We ﬁne-tune the entire PTM on each task using an Adam optimizer with learning rate of 1e-5 and batch size of 16 for 200 epochs. For gradient-free methods, we consider three baselines: (1) Manual Prompt: We directly use the templates and label words in Table 1 to conduct zero-shot evaluation. The re- sults of manual prompt can be seen as initial points of our method. (2) In-context Learning: Following Brown et al. (2020), we randomly select up to 32 training samples and concatenate them with the input texts. (3) Feature-based Methods: Feature-based methods (Peters et al., 2019) is also a competitive baseline for LMaaS, where one can re- quest the features encoded by the large PTM and locally train a classiﬁer to accomplish the task of interest. Here we consider two implementations: (a) Feature-MLP: We train a two-layered MLP classiﬁer on the [CLS] representation of the PTM. (b) Feature-BiLSTM: We train a bidirectional LSTM (Hochreiter & Schmidhuber, 1997) on the repre- sentations of the sequence of tokens, followed by a linear classiﬁer on the top. For both implementations of feature-Black-Box Tuning for Language-Model-as-a-Service based methods, we use an Adam optimizer with learning rate of 3e-4 and batch size of 16 to train the attached clas- siﬁers for 1000 epochs. For black-box tuning, we give in Table 2 the default conﬁguration of hyper-parameters used in our experiments. The effect of each hyper-parameter is explored in § 4.3. 4.2. Results Overall Comparison. We ﬁrst demonstrate the experi- mental results of black-box tuning and the baselines across 7 datasets in Table 3. The proposed black-box tuning sig- niﬁcantly outperforms the other four gradient-free methods. We observe that in-context learning performs even worse than manual prompt on some tasks, and suffers from high variance. That means, in-context learning cannot effectively utilize labeled samples included in the context. Feature- based methods perform slightly better than manual prompt and in-context learning. Meanwhile, Feature-BiLSTM out- performs Feature-MLP due to its advantage of using more informative features. Surprisingly, black-box tuning also outperforms its gradient-based counterparts, namely prompt tuning, p-tuning v2, and model tuning, on average perfor- mance of the 7 tasks. Note that the only difference between prompt tuning and black-box tuning is whether we use gra- dient descent (i.e., Adam optimizer) or DFO algorithm (i.e., CMA-ES). Based on the experimental results, we suspect that gradient-based optimization tends to overﬁt the small training data while DFO tends to ﬁnd better solutions due to its exploration mechanism. In addition, we ﬁnd that model tuning performs much better than prompt tuning and black- box tuning when number of classes is large (e.g., DBPedia). On NLI tasks (i.e., SNLI and RTE), when using pre-trained prompt embedding (§ 3.4), prompt tuning and black-box tuning signiﬁcantly outperform model tuning, which also conﬁrms the effectiveness of prompt pre-training (Gu et al., 2021) in the context of black-box tuning. Detailed Comparison. In the scenario of LMaaS, there are many other factors to be considered. In Table 4 we com- pare black-box tuning and the baseline methods in terms of deployment efﬁciency, viability of as-a-service, training time, memory usage on the user side and the server side, and the amount of data to be uploaded and downloaded. Model tuning is not deployment-efﬁcient because it needs to main- tain a copy of the entire model for each user. Gradient-based methods cannot make the PTM serve as a service due to the requirement of gradients. Feature-based methods and black-box tuning are suitable for LMaaS. However, feature- based methods cannot achieve competitive results when labeled data is limited. Therefore, among all the considered methods, only black-box tuning can achieve satisfactory performance while maintaining reasonable training time, memory footprint, and network load. Unlike gradient-based methods, in which the optimization cost is proportional to the size of the PTM, the optimization cost of black-box tuning is decoupled from the scale of the PTM, and only relies on the subspace dimensionality. For fair compari- son of training time, we perform early stopping for all the compared methods, i.e., we stop learning if the development accuracy does not increase after 1000 steps. All the methods are implemented with PyTorch (Paszke et al., 2019) and ex- perimented on a single NVIDIA GTX 3090 GPU. Note that the process of model inference can be further accelerated via better implementations (e.g., using ONNX and TensorRT). In Table 4 we also report the training time of black-box tuning using ONNX Runtime. Detailed calculation of the amount of data to be uploaded/downloaded can be found in Appendix C. 4.3. Ablation Study In this section, we conduct ablation experiments on various hyper-parameters. To control experimental variables, we explore the effect of each hyper-parameter while keeping the other hyper-parameters as default as listed in Table 2. To stablize the experimental results and reduce the variance over different runs, we conduct ablation experiments in 64- shot setting. Each run is performed on the same data split with different random seeds. Experimental results of abla- tions on loss functions L, subspace dimensionality d, and prompt length Lare demonstrated in Figure 3. Additional ablation studies on the effect of the random projection A, the effect of the population size λ, and the ablations in the 16-shot setting are in Appendix A. For each ablation, we show results under different budget, which is measured by the number of PTM inference API calls. In each API call, one can provide a continuous prompt p and query the results of the PTM forward computation on a batch of training data. In our few-shot setting, we can put all the training data into one batch, and therefore the objective function to be optimized is deterministic instead of stochastic. CMA-ES vs. Adam. We compare our used derivative- free optimizer, CMA-ES, with a competitive ﬁrst-order opti- mizer, Adam (Kingma & Ba, 2015). For fair comparison, we update the continuous prompt using Adam with the gra- dients over the entire training data (i.e., batch size equals to |Dtrain|). We use learning rate of 1e-3 for Adam opti- mizer. As shown in the top row of Figure 3, Adam optimizer achieves faster convergence on both SST-2 and AG’s News due to the gradients it used. On the development sets, Adam performs slight worse than CMA-ES with cross entropy on SST-2 but better on AG’s News. But as demonstrated in Ta- ble 3, using Adam optimizer performs worse than CMA-ES on the average performance across seven task test sets.Black-Box Tuning for Language-Model-as-a-Service Table 3.Overall comparison on various language understanding tasks. We report mean and standard deviation of performance over 3 different splits (§ 4.1). All of the results are obtained with pre-trained RoBERTaLARGE in 16-shot (per class) setting. Method SST-2 Yelp P. AG’s News DBPedia MRPC SNLI RTE Avg.acc acc acc acc F1 acc acc Gradient-Based Methods Prompt Tuning 68.23 ±3.78 61.02±6.65 84.81±0.66 87.75±1.48 51.61±8.67 36.13±1.51 54.69±3.79 63.46 + Pre-trained prompt / / / / 77.48 ±4.85 64.55±2.43 77.13±0.83 74.42 P-Tuning v2 64.33 ±3.05 92.63±1.39 83.46±1.01 97.05±0.41 68.14±3.89 36.89±0.79 50.78±2.28 70.47 Model Tuning 85.39 ±2.84 91.82±0.79 86.36±1.85 97.98±0.14 77.35±5.70 54.64±5.29 58.60±6.21 78.88 Gradient-Free Methods Manual Prompt 79.82 89.65 76.96 41.33 67.40 31.11 51.62 62.56 In-Context Learning 79.79±3.06 85.38±3.92 62.21±13.46 34.83±7.59 45.81±6.67 47.11±0.63 60.36±1.56 59.36 Feature-MLP 64.80 ±1.78 79.20±2.26 70.77±0.67 87.78±0.61 68.40±0.86 42.01±0.33 53.43±1.57 66.63 Feature-BiLSTM 65.95 ±0.99 74.68±0.10 77.28±2.83 90.37±3.10 71.55±7.10 46.02±0.38 52.17±0.25 68.29 Black-Box Tuning 89.56±0.25 91.50±0.16 81.51±0.79 87.80±1.53 61.56±4.34 46.58±1.33 52.59±2.21 73.01 + Pre-trained prompt / / / / 75.51 ±5.54 83.83±0.21 77.62±1.30 83.90 Table 4.Comparison of deployment efﬁciency, viability of as-a-service, test accuracy, training time, memory footprint, and the amount of data to be uploaded/downloaded. ⋆ indicates the training time of the implementation with ONNX Runtime. All the compared methods are performed on the same 16-shot splits of SST-2 and AG’s News. Deployment- As-A- Test Training Memory Footprint Upload Download Efﬁcient Service Accuracy Time User Server per query per query SST-2 (max sequence length: 47) Prompt Tuning √ × 72.6 15.9 mins - 5.3 GB - - Model Tuning × × 87.8 9.8 mins - 7.3 GB - - Feature-MLP √ √ 63.8 7.0 mins 20 MB 2.8 GB 4 KB 128 KB Feature-BiLSTM √ √ 66.2 9.3 mins 410 MB 2.8 GB 4 KB 6016 KB Black-Box Tuning √ √ 89.4 10.1 (6.1 ⋆) mins 30 MB 3.0 GB 6 KB 0.25 KB AG’s News (max sequence length: 107) Prompt Tuning √ × 84.0 30.2 mins - 7.7 GB - - Model Tuning × × 88.4 13.1 mins - 7.3 GB - - Feature-MLP √ √ 71.0 13.5 mins 20 MB 3.6 GB 20 KB 256 KB Feature-BiLSTM √ √ 73.1 19.7 mins 500 MB 3.6 GB 20 KB 27392 KB Black-Box Tuning √ √ 82.6 21.0 (17.7 ⋆) mins 30 MB 4.6 GB 22 KB 1 KB Loss Functions. We consider three loss functions: cross entropy, hinge loss, and negative accuracy. As depicted in the top row of Figure 3, cross entropy and hinge loss signif- icantly outperform the negative accuracy. In the few-shot setting, the accuracy as a reward can be sparse, and cannot provide informative directions for optimization. On SST- 2 and AG’s News, we obtain that cross entropy performs slightly better than hinge loss. Subspace Dimensionality. The subspace of dimensional- ity dis the space where the optimization actually performs. According to the intrinsic dimensionality found in Agha- janyan et al. (2021), we explore the subspace dimensionality of {100, 200, 500, 1000}within the budget of {2k, 4k, 6k, 8k}. Accordingly, we set population size λ= 4 + 3 log(d). As shown in the middle row of Figure 3, the best subspace dimensionality can be different on different tasks (d= 200 performs the best on SST-2 development set and d= 500 performs the best on AG’s News development set), which is related to the observation that intrinsic dimensionality varies across different tasks (Aghajanyan et al., 2021). In general, a small subspace (e.g., d= 100) is hard to cover a good solution, while a large subspace (e.g., d= 1000) may lead to poor generalization. Prompt Length. Prompt length L determines the di- mensionality of the original parameter space (in our case D= L×1024). We evaluate black-box tuning under each budget in {2k, 4k, 6k, 8k}while varying the prompt length in {10, 20, 50, 100}. As shown in the bottom row of Fig- ure 3, shorter prompt confers faster convergence on the training sets but does not yield better generalization on the development sets. L = 50achieves the best accuracy on both SST-2 and AG’s News development sets.Black-Box Tuning for Language-Model-as-a-Service 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 77.5 80.0 82.5 85.0 87.5 90.0 92.5 95.0Dev accuracy (current best) SST-2 (dev) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 75 80 85 90 95 100Train accuracy (current best) AG's News (train) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 0 2000 4000 6000 8000 Iterations over entire training data (| train|=128) 78 80 82 84 86 88 90Dev accuracy (current best) AG's News (dev) Adam (cross entropy) CMA-ES (cross entropy) CMA-ES (hinge loss) CMA-ES (-accuracy) 2000 4000 6000 8000 Budget (number of API calls) 94 95 96 97 98 99 100Train accuracy (current best) SST-2 (train) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 89 90 91 92 93 94Dev accuracy (current best) SST-2 (dev) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 84 86 88 90 92Train accuracy (current best) AG's News (train) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 83 84 85 86 87 88Dev accuracy (current best) AG's News (dev) d = 1000 d = 500 d = 200 d = 100 2000 4000 6000 8000 Budget (number of API calls) 95 96 97 98 99 100Train accuracy (current best) SST-2 (train) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 88 89 90 91 92 93 94Dev accuracy (current best) SST-2 (dev) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 82 84 86 88 90 92 94Train accuracy (current best) AG's News (train) L = 10 L = 20 L = 50 L = 100 2000 4000 6000 8000 Budget (number of API calls) 83 84 85 86 87 88Dev accuracy (current best) AG's News (dev) L = 10 L = 20 L = 50 L = 100 Figure 3.Ablations of loss function, subspace dimensionality, and prompt length. We show mean and standard deviation of performance over 3 runs with different random seeds. Ablations of the random projection and the population size can be found in Appendix A. 5. Discussion and Future Work In this section we discuss our proposed method in the con- text of (1) derivative-free optimization and (2) prompt-based learning, respectively. By drawing comparisons with these two lines of research, we highlight some directions that could improve this work in future. Comparison with Previous Derivative-Free Approaches. Our proposed method lies in the same framework of previ- ous work that solves high-dimensional derivative-free op- timization problems via random embedding (Wang et al., 2016). In contrast, we set the random embedding A by sampling from a uniform distribution instead of normal dis- tributions, and use the CMA-ES to perform optimization in the generated subspace. In previous work, the target black- box functions are usually synthetic functions where only a few dimensions can affect the function values, and therefore most of the dimensions are strictly non-effective. In our real- world scenario, the intrinsic dimension can be approximate. In the context of PTMs, a more appropriate substitution for the term intrinsic dimensionality can be ϵ-effective dimen- sionality (Qian et al., 2016). Considering the relaxation to the intrinsic dimensionality of PTMs, more suitable ap- proaches such as sequential random embedding (Qian et al., 2016) and other more advanced methods of constructing the random projection matrix (Letham et al., 2020) should be explored in future work. Besides, the subspace generated by random projection can be sub-optimal. As demonstrated in Qin et al. (2021), training the projection A with multi- task supervision can result in better and smaller subspace. Besides, larger PTMs generally have lower intrinsic dimen- sionalities (Aghajanyan et al., 2021), as a result, we can use smaller subspace and more efﬁcient DFO algorithms such as Bayesian optimization on larger PTMs. Comparison with Previous Prompt-Based Learning Ap- proaches. From the perspective of prompt-based learning, our method is similar to prompt-tuning (Lester et al., 2021), where only the continuous prompt prepended to the input text is tuned, so our method also retains the beneﬁts of efﬁ- cient serving and mixed-task inference. In addition to the continuous prompt, we also insert some hard prompt tokens (e.g., ”It was [MASK]”) in the input text, which has been demonstrated to be effective in previous work (Gu et al., 2021) in the name of hybrid prompt tuning. Different from previous prompt-based learning approaches, our prompt tun-Black-Box Tuning for Language-Model-as-a-Service ing does not require backpropagation and gradient descent. Considering our used templates and label words are hand- crafted without trial-and-error, the performance reported in this paper is just a lower bound. More advanced techniques such as prompt engineering (Gao et al., 2021), label words engineering (Schick et al., 2020; Shin et al., 2020; Hu et al., 2021b), prompt pre-training (Gu et al., 2021), and prompt ensembling (Lester et al., 2021) are orthogonal to this work and therefore can further improve the performance. For simplicity, we do not integrate these methods and leave for future work. Acknowledgements The authors would like to thank Yang Yu for the valu- able suggestions of the methods and presentation of the paper, and the anonymous reviewers for their construc- tive comments. This work was supported by the National Key Research and Development Program of China (No. 2020AAA0108702), the National Natural Science Founda- tion of China (No. 62022027), the major key project of PCL (No. PCL2021A12), and the Natural Science Foundation of Shanghai (No. 21ZR1420300). References Aghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrin- sic dimensionality explains the effectiveness of language model ﬁne-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 7319–7328, 2021. Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language infer- ence. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 632– 642, 2015. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural In- formation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Conn, A. R., Scheinberg, K., and Vicente, L. N.Introduction to Derivative-Free Optimization. SIAM, Philadelphia, PA, 2009. Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Con- ference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019. Dolan, W. B. and Brockett, C. Automatically construct- ing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005, 2005. Fedus, W., Zoph, B., and Shazeer, N. Switch transform- ers: Scaling to trillion parameter models with simple and efﬁcient sparsity. arXiv:2101.03961, 2021. Gao, T., Fisch, A., and Chen, D. Making pre-trained lan- guage models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Con- ference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 3816–3830, 2021. Gu, Y ., Han, X., Liu, Z., and Huang, M. PPT: pre-trained prompt tuning for few-shot learning. arXiv:2109.04332, 2021. Hambardzumyan, K., Khachatrian, H., and May, J. W ARP: word-level adversarial reprogramming. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Con- ference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4921–4933, 2021. Hansen, N. The CMA evolution strategy: A tutorial. arXiv:1604.00772, 2016. Hansen, N. and Ostermeier, A. Completely derandomized self-adaptation in evolution strategies. Evol. Comput., 9 (2):159–195, 2001. Hansen, N., M¨uller, S. D., and Koumoutsakos, P. Reducing the time complexity of the derandomized evolution strat- egy with covariance matrix adaptation (CMA-ES). Evol. Comput., 11(1):1–18, 2003. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a uniﬁed view of parameter-efﬁcient transfer learning. arXiv:2110.04366, 2021.Black-Box Tuning for Language-Model-as-a-Service He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In 2015 IEEE International Con- ference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026–1034, 2015. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efﬁcient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pp. 2790–2799, 2019. Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021a. Hu, S., Ding, N., Wang, H., Liu, Z., Li, J., and Sun, M. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classiﬁcation. arXiv:2108.02035, 2021b. Hu, Y .-Q., Qian, H., and Yu, Y . Sequential classiﬁcation- based optimization for direct policy search. In Proceed- ings of the 31st AAAI Conference on Artiﬁcial Intelli- gence, pp. 2029–2035, San Francisco, CA, 2017. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International Conference on Learn- ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Kolda, T. G., Lewis, R. M., and Torczon, V . Optimization by direct search: New perspectives on some classical and modern methods. SIAM Review, 45(3):385–482, 2003. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 3045–3059, 2021. Letham, B., Calandra, R., Rai, A., and Bakshy, E. Re-examining linear embeddings for high-dimensional Bayesian optimization. In Advances in Neural Informa- tion Processing Systems 33, virtual, 2020. Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo- hamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen- sion. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871–7880, 2020. Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscapes. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continu- ous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Vol- ume 1: Long Papers), Virtual Event, August 1-6, 2021 , pp. 4582–4597, 2021. Liu, X., Ji, K., Fu, Y ., Du, Z., Yang, Z., and Tang, J. P-tuning v2: Prompt tuning can be comparable to ﬁne-tuning universally across scales and tasks. arXiv:2110.07602, 2021a. Liu, X., Zheng, Y ., Du, Z., Ding, M., Qian, Y ., Yang, Z., and Tang, J. GPT understands, too. arXiv:2103.10385, 2021b. Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V . Roberta: A robustly optimized BERT pretraining approach. arXiv:1907.11692, 2019. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K¨opf, A., Yang, E., DeVito, Z., Rai- son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 8024–8035, 2019. Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. InAdvances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 11054–11070, 2021. Peters, M. E., Ruder, S., and Smith, N. A. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representa- tion Learning for NLP , RepL4NLP@ACL 2019, Florence, Italy, August 2, 2019, pp. 7–14, 2019. Qian, H., Hu, Y ., and Yu, Y . Derivative-free optimization of high-dimensional non-convex functions by sequential random embeddings. In Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016 , pp. 1946–1952, 2016.Black-Box Tuning for Language-Model-as-a-Service Qin, G. and Eisner, J. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 5203–5212, 2021. Qin, Y ., Wang, X., Su, Y ., Lin, Y ., Ding, N., Liu, Z., Li, J., Hou, L., Li, P., Sun, M., and Zhou, J. Exploring low- dimensional intrinsic task subspace via prompt tuning. arXiv:2110.07867, 2021. Qiu, X., Sun, T., Xu, Y ., Shao, Y ., Dai, N., and Huang, X. Pre-trained models for natural language processing: A survey. SCIENCE CHINA Technological Sciences, 2020. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text trans- former. J. Mach. Learn. Res., 21:140:1–140:67, 2020. Rios, L. M. and Sahinidis, N. V . Derivative-free optimiza- tion: A review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3): 1247–1293, 2013. Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I. Evolution strategies as a scalable alternative to reinforce- ment learning. arXiv:1703.03864, 2017. Schick, T. and Sch¨utze, H. Exploiting cloze-questions for few-shot text classiﬁcation and natural language infer- ence. In Proceedings of the 16th Conference of the Eu- ropean Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pp. 255–269, 2021a. Schick, T. and Sch ¨utze, H. It’s not just size that matters: Small language models are also few-shot learners. In Pro- ceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 2339–2352, 2021b. Schick, T., Schmid, H., and Sch ¨utze, H. Automatically identifying words that can serve as labels for few-shot text classiﬁcation. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020 , pp. 5569–5578, 2020. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and de Freitas, N. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104 (1):148–175, 2016. Shin, T., Razeghi, Y ., IV , R. L. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 4222–4235, 2020. Snoek, J., Larochelle, H., and Adams, R. P. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, pp. 2960–2968, Lake Tahoe, NV , 2012. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y ., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Wash- ington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1631–1642, 2013. Sun, T., Liu, X., Qiu, X., and Huang, X. Paradigm shift in natural language processing. Machine Intelligence Research, 2022. Sun, Y ., Wang, S., Feng, S., Ding, S., Pang, C., Shang, J., Liu, J., Chen, X., Zhao, Y ., Lu, Y ., Liu, W., Wu, Z., Gong, W., Liang, J., Shang, Z., Sun, P., Liu, W., Ouyang, X., Yu, D., Tian, H., Wu, H., and Wang, H. ERNIE 3.0: Large- scale knowledge enhanced pre-training for language un- derstanding and generation. arXiv:2107.02137, 2021. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and anal- ysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Wang, Z., Hutter, F., Zoghi, M., Matheson, D., and de Fre- itas, N. Bayesian optimization in a billion dimensions via random embeddings. J. Artif. Intell. Res., 55:361–387, 2016. Weston, J. and Watkins, C. Support vector machines for multi-class pattern recognition. In ESANN 1999, 7th Eu- ropean Symposium on Artiﬁcial Neural Networks, Bruges, Belgium, April 21-23, 1999, Proceedings, pp. 219–224, 1999. Williams, A., Nangia, N., and Bowman, S. R. A broad- coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 1112– 1122, 2018.Black-Box Tuning for Language-Model-as-a-Service Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li, F., Zhu, H., Luo, J., Xu, L., and Zhang, X. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv:2110.04725, 2021. Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y ., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., Li, C., Gong, Z., Yao, Y ., Huang, X., Wang, J., Yu, J., Guo, Q., Yu, Y ., Zhang, Y ., Wang, J., Tao, H., Yan, D., Yi, Z., Peng, F., Jiang, F., Zhang, H., Deng, L., Zhang, Y ., Lin, Z., Zhang, C., Zhang, S., Guo, M., Gu, S., Fan, G., Wang, Y ., Jin, X., Liu, Q., and Tian, Y . Pangu-α: Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv:2104.12369, 2021. Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q., and Artzi, Y . Revisiting few-sample BERT ﬁne-tuning. In9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a. Zhang, X., Zhao, J. J., and LeCun, Y . Character-level con- volutional networks for text classiﬁcation. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 649–657, 2015a. Zhang, Y ., Sohn, K., Villegas, R., Pan, G., and Lee, H. Improving object detection with deep convolutional net- works via Bayesian optimization and structured predic- tion. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 249–258, Boston, MA, 2015b. Zhang, Z., Han, X., Zhou, H., Ke, P., Gu, Y ., Ye, D., Qin, Y ., Su, Y ., Ji, H., Guan, J., Qi, F., Wang, X., Zheng, Y ., Zeng, G., Cao, H., Chen, S., Li, D., Sun, Z., Liu, Z., Huang, M., Han, W., Tang, J., Li, J., Zhu, X., and Sun, M. CPM: A large-scale generative chinese pre-trained language model. arXiv:2012.00413, 2020. Zhang, Z., Gu, Y ., Han, X., Chen, S., Xiao, C., Sun, Z., Yao, Y ., Qi, F., Guan, J., Ke, P., Cai, Y ., Zeng, G., Tan, Z., Liu, Z., Huang, M., Han, W., Liu, Y ., Zhu, X., and Sun, M. CPM-2: large-scale cost-effective pre-trained language models. arXiv:2106.10715, 2021b. Zhong, Z., Friedman, D., and Chen, D. Factual probing is [MASK]: learning vs. learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 5017–5033, 2021.Black-Box Tuning for Language-Model-as-a-Service A. Additional Experimental Results Random Projection. The random projection matrix A ∈RD×d is a key factor that determines whether and how hard it is to ﬁnd a good solution in the generated subspace. Here we compare two design choices of setting A: The ﬁrst choice is commonly used in previous high-dimensional derivative-free optimization work (Wang et al., 2016; Qian et al., 2016), that is setting each entry of A by sampling from a normal distribution. Following Qian et al. (2016), we use N(0,1/d) where d is the subspace dimensionality5. The second choice is setting each entry of A by sampling from a uniform distribution, which is widely used for initializing linear layers in modern neural networks. Here we use the uniform distribution proposed in He et al. (2015). As shown in Figure 4, both random projections can achieve a considerable cross entropy loss on SST-2 and AG’s News within reasonable budgets but faster convergence is obtained using uniform distribution. 0 2000 4000 6000 8000 Number of API calls 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40Cross Entropy Loss SST-2 Normal Uniform 0 2000 4000 6000 8000 Number of API calls 0.3 0.4 0.5 0.6 0.7 0.8Cross Entropy Loss AG's News Normal Uniform Figure 4.Effect of random projection A. Population Size. In each iteration of the CMA-ES, a population of solutions are sampled from a multivariate normal distribution model. The evaluation of the population is then used to update the parameters of the multivariate normal distribution model. Here we study the effect of the population size on SST-2. In our experiments, we sequentially evaluate each solution in a population, and therefore larger population size will result in more API calls given the same CMA-ES iterations. As shown in Figure 5, smaller population size confers faster convergence in terms of number of API calls. We also demonstrate the comparison in terms of the CMA-ES iterations, which can be found in the following section. 0 2000 4000 6000 8000 Number of API calls 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 0 2000 4000 6000 8000 Number of API calls 84 86 88 90 92 94Dev accuracy (current best) SST-2 (dev) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 Figure 5.Effect of population size λ. 5We also tried N(0,1) as used in Wang et al. (2016), which does not work in our case. For N(0,1/d), we adopt a larger search space Zinstead of [−5,5]d to get it work.Black-Box Tuning for Language-Model-as-a-Service 0 250 500 750 1000 Subspace dimension d 84 86 88 90T est Accuracy SST-2 25 50 75 100 Prompt Length 84 86 88 90T est Accuracy SST-2 0 250 500 750 1000 Subspace dimension d 35 40 45 50T est Accuracy SNLI 25 50 75 100 Prompt Length 35 40 45 50T est Accuracy SNLI Figure 6.Ablation of subspace dimensionality and prompt length in 16-shot setting. 0 5000 10000 TFLOPs 0.1 0.2 0.3 0.4 0.5 0.6cross entropy loss d = 200 0 5000 10000 TFLOPs d = 400 0 5000 10000 TFLOPs d = 600 0 2500 5000 7500 10000 TFLOPs d = 800 0 5000 10000 TFLOPs d = 1000 CMA-ES Adam (lr=0.01) Adam (lr=0.1) Figure 7.Optimization in low-dimensional subspaces using CMA-ES and Adam. Ablation of Subspace Dimensionality and Prompt Length in 16-shot Setting. In § 4.3, we conduct ablation experi- ments in the 64-shot setting to reduce the variance over different runs. To keep consistent with the experimental setting in Table 3, we demonstrate in Figure 6 the ablation results on subspace dimensionality and prompt length in the 16-shot setting. CMA-ES vs. Adam in Subspaces. In Figure 3, we compare the convergence of prompt tuning (with Adam optimizer) and black-box tuning (with CMA-ES), where Adam performs optimization in the original prompt space (P) while CMA-ES performs in the generated subsapce (Z). Here we also compare the effectiveness and efﬁciency of Adam and CMA-ES in subspaces. As shown in Figure 7, CMA-ES is more efﬁcient and stable than Adam in low-dimensional subspaces. When the dimensionality of the subsapce becomes large (e.g., d= 1000), Adam with a appropriate learning rate can perform on par with CMA-ES. Note that CMA-ES does not require back-propagation, so the computation cost of one iteration for CMA-ES and Adam can be very different. For fair comparison, we convert the number of iterations into FLOPs. The FLOPs of one iteration of Adam is estimated to be three times greater than CMA-ES. B. Parallel Evaluation If the training data is smaller, or the server allows larger batches, a promising way to improve training efﬁciency is to use parallel evaluation. That is, we can evaluate the entire population in parallel, as depicted in Figure 8(a). As demonstrated in Figure 8(b), we can achieve 100% accuracy on the SST-2 training set with population size of 20 and 25 in 300 iterations (API calls). In case of the batch size per API call is limited, we can also use asynchronous queries to simulate the parallel evaluation. C. Estimation of Uploaded/Downloaded Data Size In this section we describe how we estimate the amount of data to be uploaded and downloaded (Table 4). For black-box tuning, there are two kinds of data to be uploaded: (1) training samples, and (2) continuous prompt. A training sample is comprised of two parts: input ids and attention mask. We can use the unsigned short (representation range: 0∼65535, 2 bytes per value) for input ids and use the bool type (1 byte per value) for attention mask. For continuous prompt, which contains hundreds of values, we can use the ﬂoat type (4 bytes per value) for representation.Black-Box Tuning for Language-Model-as-a-Service Serial Evaluation Population Logits Data Parallel Evaluation (a) 0 100 200 300 Iterations of CMA-ES 82.5 85.0 87.5 90.0 92.5 95.0 97.5 100.0Train accuracy (current best) SST-2 (train) Pop. size=5 Pop. size=10 Pop. size=15 Pop. size=20 Pop. size=25 (b) Figure 8.(a) Illustration of the parallel evaluation. (b) Comparison of the convergence rate with different population sizes using parallel evaluation. Take SST-2 16-shot split as an example, the input ids and attention mask are in shape of 32 ×47, where 32 is the batch size and 47 is the maximum sequence length, so there are ∼2.9KB data for input ids and ∼1.5KB data for attention mask. Assume the prompt is 500-dimensional, we need to upload additional ∼2KB data for prompt. The data to be downloaded is the output logits of the candidate words, which is a dictionary containing |Y| ﬂoat values. Take SST-2 16-shot split as an example, the size of data to be downloaded is32 ×2 ×4bytes = 0.25KB. For feature-based methods we use similar estimation methods. The data size for upload is the same for Feature-MLP and Feature-BiLSTM. The data to be downloaded for Feature-MLP is the representation of the [CLS] token while the data to be downloaded for Feature-BiLSTM is the representation of all the tokens. Note that this estimation, without any data compression, is an upper bound of the real scenario.",
      "meta_data": {
        "arxiv_id": "2201.03514v4",
        "authors": [
          "Tianxiang Sun",
          "Yunfan Shao",
          "Hong Qian",
          "Xuanjing Huang",
          "Xipeng Qiu"
        ],
        "published_date": "2022-01-10T18:17:05Z",
        "pdf_url": "https://arxiv.org/pdf/2201.03514v4.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes Black-Box Tuning (BBT), a novel framework to optimize continuous prompts for Language-Model-as-a-Service (LMaaS) scenarios where gradients of large pre-trained language models (PTMs) are unavailable. The key contribution is enabling task-specific prompt optimization by only accessing PTM inference APIs. BBT achieves this by performing derivative-free optimization (DFO) in a randomly generated low-dimensional subspace, leveraging the low intrinsic dimensionality of large PTMs. Empirical results demonstrate that BBT significantly outperforms manual prompt, GPT-3's in-context learning, and even gradient-based methods like prompt tuning and full model tuning on various language understanding tasks in a few-shot setting.",
        "methodology": "The methodology addresses the optimization of continuous prompts (p) for a black-box PTM inference API (f) using derivative-free optimization. To overcome the high dimensionality of continuous prompts, BBT projects the original high-dimensional prompt space (D-dimensional) onto a much smaller subspace (d-dimensional) using a random linear projection matrix (A). Optimization is then performed on a low-dimensional vector (z) in this subspace, where the final prompt is derived as Az + p0 (p0 being an initial prompt embedding). The values of the random matrix A are sampled from a uniform distribution. The CMA-ES (Covariance Matrix Adaptation Evolution Strategy), a widely used evolutionary algorithm, is employed for the derivative-free optimization in this continuous, non-convex, low-dimensional space. Loss functions considered include Cross Entropy, Hinge Loss, and Negative Accuracy, with Cross Entropy performing slightly better. For initialization, prompt embeddings p0 can be pre-trained on publicly available NLI tasks for sentence-pair tasks, or randomly drawn from the PTM's vocabulary for other tasks.",
        "experimental_setup": "Experiments were conducted on RoBERTaLARGE as the backbone model, due to its suitability for language understanding tasks and proven small intrinsic dimensionality. Seven common language understanding datasets were used, including SST-2, Yelp polarity (sentiment analysis), AG’s News, DBPedia (topic classification), SNLI, RTE (NLI), and MRPC (paraphrase). A few-shot setting (16-shot per class for main results, 64-shot for ablations) was employed, constructing training and development sets of k samples per class, and using original development/test sets as test sets. Baselines included gradient-based methods (Prompt Tuning, P-Tuning v2, Model Tuning) and gradient-free methods (Manual Prompt, In-context Learning, Feature-based methods like Feature-MLP and Feature-BiLSTM). Hyper-parameters for BBT were set with a default prompt length of 50, subspace dimension of 500, population size of 20, uniform random projection, Cross Entropy loss, and a budget of 8000 API calls. Performance was measured by mean and standard deviation of accuracy or F1-score over 3 different data splits. All methods were implemented with PyTorch and run on a single NVIDIA GTX 3090 GPU, with some training times measured with ONNX Runtime.",
        "limitations": "The intrinsic dimension can be approximate, and the subspace generated by random projection might be sub-optimal. The current work uses hand-crafted templates and label words, suggesting that the reported performance represents a lower bound, and more sophisticated prompt engineering could yield further improvements. Although CMA-ES, a derivative-free optimizer, tends to find better solutions due to its exploration mechanism compared to Adam on small training data, Adam shows faster convergence on the training set. The study focuses solely on language understanding tasks, not generative PTMs.",
        "future_research_directions": "Future research can explore more suitable derivative-free optimization approaches, such as sequential random embedding, or advanced methods for constructing the random projection matrix (e.g., training the projection matrix with multi-task supervision). Applying Black-Box Tuning to generative PTMs (like GPT, T5, or BART) by converting downstream tasks into a unified text-to-text format is another promising direction. Furthermore, integrating advanced techniques like prompt engineering, label words engineering, prompt pre-training, and prompt ensembling with BBT could significantly enhance performance. Investigation into using even smaller subspaces and more efficient DFO algorithms like Bayesian optimization for larger PTMs is also suggested, leveraging their generally lower intrinsic dimensionalities."
      }
    },
    {
      "title": "Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models",
      "abstract": "Prompt tuning in natural language processing (NLP) has become an increasingly\npopular method for adapting large language models to specific tasks. However,\nthe transferability of these prompts, especially continuous prompts, between\ndifferent models remains a challenge. In this work, we propose a zero-shot\ncontinuous prompt transfer method, where source prompts are encoded into\nrelative space and the corresponding target prompts are searched for\ntransferring to target models. Experimental results confirm the effectiveness\nof our method, showing that 'task semantics' in continuous prompts can be\ngeneralized across various language models. Moreover, we find that combining\n'task semantics' from multiple source models can further enhance the\ngeneralizability of transfer.",
      "full_text": "Published as a conference paper at ICLR 2024 ZERO -SHOT CONTINUOUS PROMPT TRANSFER : G EN- ERALIZING TASK SEMANTICS ACROSS LANGUAGE MODELS Zijun Wu1, Yongkang Wu2, Lili Mou1,3 1Dept. Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta 2Huawei Poisson Lab 3Canada CIFAR AI Chair zijun4@ualberta.ca, wuyongkang7@huawei.com, doublepower.mou@gmail.com ABSTRACT Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into a relative space and the corresponding target prompts are searched for transferring to target mod- els. Experimental results confirm the effectiveness of our method, showing that “task semantics” in continuous prompts can be generalized across various lan- guage models. Moreover, we find that combining “task semantics” from multiple source models can further enhance the performance of transfer.1 1 I NTRODUCTION Recently, natural language processing (NLP) has witnessed a paradigm shift from the finetuning of full language models to the optimization of a small subset of prompt tokens (Shin et al., 2020; Lester et al., 2021; Li & Liang, 2021; Zhong et al., 2021). As language models have dramatically increased in size and may contain billions of parameters (Brown et al., 2020), the strategy of freezing language models while optimizing the learnable prompt parameters becomes the most affordable and efficient alternative for downstream tasks. This technique, referred to as prompt tuning, has gained substantial recognition for its effectiveness across a range of language models (Shin et al., 2020; Lester et al., 2021; Li & Liang, 2021; Zhong et al., 2021). Various prompt tuning methods have been explored, which can be generally categorized into discrete and continuous cases. Discrete prompt tuning, such as AutoPrompt (Shin et al., 2020), primarily fo- cuses on the selection and optimization of a predetermined set of tokens within a language model’s vocabulary. By contrast, continuous prompt tuning (Zhong et al., 2021) allows the modification of continuous prompt embeddings by gradient descent. The latter typically offers better performance on downstream tasks due to its greater flexibility in the prompt space. However, existing prompt tuning often requires accessing the model’s internal states, as the gradient needs to be backpropa- gated to the first layer of token embeddings (Shin et al., 2020; Zhong et al., 2021), which contradicts the goal of avoiding gradient computation for large language models. Therefore, it would be ideal if we can perform prompt tuning on a small model (which is compu- tationally inexpensive) and transfer the prompt to large models. We thus question: How transfer- able are these prompts between different language models? Prior research on transferring prompts mainly focuses on discrete prompts (Rakotonirina et al., 2023). Such transfer is often straightfor- ward, as discrete prompt tokens usually carry semantic meanings by their nature and can be directly accepted by different language models. For continuous prompts, however, prompt transfer becomes less straightforward because they are unexplainable and sparsely distributed in a high-dimensional space (Khashabi et al., 2022; Su et al., 2022). Moreover, different models might learn the embedding 1Our code is available at https://github.com/MANGA-UOFA/PTfer 1 arXiv:2310.01691v2  [cs.CL]  12 Jul 2024Published as a conference paper at ICLR 2024 space differently, due to their designs, sizes, training paradigms, as well as parameter random initial- izations. Therefore, transferring a continuous prompt tailored for one model to another, especially with different dimensions, remains a challenge. Attempts to bridge this gap have centered around introducing a neural projector that aligns continu- ous prompts across different models. However, the learned projectors are specific to unique model pairs. More importantly, it introduces extra computational cost because the training requires task supervision on the target model and the source prompt embeddings, or even the need to utilize the parallel prompt embeddings for both models (Su et al., 2022). These approaches cannot be applied in a zero-shot transfer scenario, and are undesired in real applications. In this work, we propose a novel approach to zero-shot continuous prompt transfer without the need for task supervision or additional training of neural projectors. We introduce an encode-then-search strategy, where we encode the source prompts into a relative space (Norelli et al., 2022; Moschella et al., 2023) and then search for the corresponding target prompt embeddings. Our intuition is that the induced continuous prompt contains implicit information for a task (Vu et al., 2022; Wang et al., 2022), referred to as “task semantics”, which may be carried over from the source embedding space to the target. We suggest that, although direct transfer of prompt embeddings is problematic because different language models have their own embedding spaces, the position of the continuous prompt embedding relative to the embeddings of known words is more likely to share the same structure in different language models, inspired by the evidence of representation learning literature in other domains, such as word embeddings (Faruqui & Dyer, 2014; Lazaridou et al., 2015; Artetxe et al., 2018), synthetic structure discovery (Wu et al., 2023), unsupervised neural translation (Lample et al., 2018), and cognitive science (Levakov et al., 2021; Chersoni et al., 2021). In our approach, the transfer of prompts only requires a shared vocabulary of common tokens, which serve as the anchors of the relative embedding space. For the target model, we search for its prompt embeddings that preserve the same relative structure as that of the source language model. Our experiments confirm that, with our proposed zero-shot approach, continuous prompts are trans- ferable to different language models, largely outperforming baseline approaches such as training neural projectors. We also discover that utilizing continuous prompts from multiple distinct source models enhances the generalizability on target models. This is because the semantics of the prompts induced from a single source might be model-specific, whereas the multi-source method provides a more robust view of the task semantics, therefore achieving higher performance of prompt transfer. In short, our contributions are summarized as follows: • We address a novel setting of zero-shot continuous prompt transfer, which allows for the reuse of continuous prompts across different language models. • We propose an encode-then-search strategy that maps a continuous prompt into a relative space for transfer between language models. Our approach facilitates multi-source transfer, which cannot be easily done by previous work. • We provide detailed experimental analysis on a factual-probing suite of 41 types of ques- tions to show the effectiveness of our approach. 2 M ETHODOLOGY In this section, we begin with an overview of continuous prompt tuning in §2.1. We then introduce our encoding (§2.2) and decoding (§2.3) methods for transferring continuous prompt embeddings between language models. Finally in §2.4, we discuss our multi-source transfer approach for im- proving the performance of transfer. 2.1 C ONTINUOUS PROMPT TUNING Continuous prompt tuning (Zhong et al., 2021) optimizes the embeddings of a prompt in the con- tinuous space for a downstream task, which essentially introduces virtual tokens to the language model. During this process, the continuous prompt, which is a set of learnable embedding vec- tors, can capture the implicit information for the task of interest (Vu et al., 2022). Different from full-model finetuning methods (Zhou & Srikumar, 2022), the gradient from the final loss function backpropagates through the model but only updates the initial embedding layers. 2Published as a conference paper at ICLR 2024 Source LM Target LM [ X ][ MASK ] [ X ][ MASK ] (a) Source anchors Target anchors Induced source prompts Target prompts Relative SpaceTarget Space Source Space (b) Figure 1: (a) The goal of transferring the induced continuous prompts on a source model to a target model. (b) Our proposed method for this transfer in a zero-shot manner, where the target prompts should be aligned with the induced source prompts in the relative space. Consider prompting a language model for some task. A continuous prompt has the following format: Prompt(x) = x v1 v2 ··· vm (1) where x is an input data sample, and m is a pre-defined prompt length, i.e., the number of learnable vectors. The configuration of vi as either a prefix or postfix tox is an aspect of prompt design. In our implementation, we append these tokens as a postfix to x. For each virtual token vi, its embedding is a learnable vector vi ∈ Rd that has the same dimension d as the embedding layer of the language model. The soft prompt tuning objective is to maximize the likelihood of the outputy of the training sample, given by the source model Ps(·) as arg max v1,···,vm X (x,y)∈D log P(y | v1, ··· , vm, x) (2) After the continuous prompts v1, . . . ,vm are optimized, they are usually used to make inference on the same model for the same task (Lester et al., 2021; Li & Liang, 2021; Zhong et al., 2021). Prompt tuning is more efficient than full-model finetuning because only a small number of param- eters, i.e., the embeddings of the virtual tokens, are updated for a downstream task. Meanwhile, it maintains substantial flexibility and achieves similar performance to full-model finetuning (Lester et al., 2021). However, learning these virtual tokens may still be expensive for large language mod- els because we need to perform backpropagation through the entire model structure. Our goal is to explore the feasibility of learning a continuous prompt with a small language model and transferring it to larger ones, therefore avoiding excessive gradient computations of prompt tuning for different large language models. 2.2 E NCODING TO A RELATIVE SPACE We propose to transfer continuous prompts from a source language model to target models. The process involves two key phases: (1) encoding the source prompt embeddings into a relative repre- sentation and (2) searching for target prompt embeddings whose corresponding relative representa- tion aligns with those of the source. This two-phase method facilitates the effective transfer of task semantics between different embedding spaces, allowing the transferred prompts to accomplish the task on the target model. The concept of relative representation involves encoding a data point based on its similarities to certain reference points, known as anchors (Norelli et al., 2022; Moschella et al., 2023). In our work, the relative space, where these encoded representations reside, serves as a shared semantic space across different models and facilitates the transfer process of continuous prompts. Consider a continuous prompt v1, v2, ··· , vm ∈ Rds , where ds indicates the embedding dimension of the source language model. We aim to transform it into a relative representation. This can be shown in Figure 1b as transferring orange stars to orange triangles. To encode the relative embeddings of the prompt, we need a set of common tokens, serving as anchors, that are shared between both source and target language models. We simply choose the 3Published as a conference paper at ICLR 2024 shared tokens as the set of anchors, as they provide a common ground for expressing a prompt in relative terms, regardless of differently learned embedding spaces. Specifically, the anchors’ embeddings in the source model can be represented by a matrix As = [as 1, as 2, ··· , as k] ∈ Rds×k, where k is the number of anchors, and [, ] concatenates column vectors into a matrix. We then encode a prompt embedding vi in a relative space with respect to these anchors, where we compute the cosine similarity between a prompt embedding and each anchor’s embedding, given by rAs (vi) = (cos(vi, as 1), ··· , cos(vi, as k))⊤ (3) This encoding step translates a continuous prompt from the source language model into a language using the relationship among common tokens so that other models can potentially understand. It bridges the source and target language models and passes implicit task information contained in the source continuous prompt. 2.3 S EARCH IN THE TARGET SPACE We search a continuous prompt for the target language model, based on the intuition that the relative embeddings are model-agnostic and can be aligned across different language models, i.e., the orange and green triangles in Figure 1 should have the same structure. In this way, we can search the (absolute) target prompt embeddings by maximizing the alignment of the source and target relative spaces. Concretely, the target embeddings vt 1, vt 2, ··· , vt m ∈ Rdt are randomly initialized, where dt is the target embedding dimension and may not be the same as ds. They are represented by green stars in Figure 1b. These target embeddings are then encoded using the target anchor embeddings At = [at 1, at 2, ··· , at k] ∈ Rdt×k, shown by green squares in Figure 1b. These target anchors are the same as source anchors, and their embeddings are given by the target language model. Similar to encoding the source prompt, the target embeddings can be represented in the relative space by rAt (vt i) = (cos(vt i, at 1), ··· , cos(vt i, at k))⊤ (4) To align source and target relative embeddings, i.e., Eqns. (3) and (4), we seek a target embedding vt i that maximizes their similarity. The objective is maximize vt i cos(rAs (vi), rAt (vt i)). (5) which can be accomplished by gradient descent. This procedure is repeated for i = 1, ··· , mto obtain all the embeddings of a length-m prompt for the target language model. It is noted that such searched prompt embeddings may not have the same scale as the target language model’s word embeddings, partially because the cos measure used in (3) and (4) is insensitive to vector magnitude. Therefore, we normalize them as evt i = vt i − µv σv · σ + µ (6) with µv, σv ∈ R being the mean and standard deviation of all searched prompt embedding values, and µ, σ∈ R being those of pretrained word embeddings of the target model. The transferred continuous prompt, after the normalization, is directly used to query the target model Pt(·) for inference, given by Pt(y | evt 1, ··· , evt m, x). 2.4 M ULTI -SOURCE TRANSFER We argue that the induced prompt embeddings from a single model might be model-specific, which is supported by the evidence in Khashabi et al. (2022) that a language model can generate numerous continuous prompts capable of performing the same task. Such flexibility is believed to arise from the high expressiveness of the model’s lower layers (Telgarsky, 2016; Raghu et al., 2017). There- fore, the induced continuous prompt from a specific model may be one of the many plausible ones, carrying model-specific information in addition to task semantics and limiting the transferability to other language models. 4Published as a conference paper at ICLR 2024 To enhance the generalization of the induced continuous prompt, we propose a multi-source transfer approach. Specifically, we search for the target embeddings vt whose encoded embeddings rAt (vt) align closely with the encoded embeddings rAsi (vsi ) from multiple source models si in the relative space. In other words, the goal is to search for vt such that the sum of similarities between rAt (vt) and each source prompt rAsi (vsi ) is maximized. Given S-many source models, the objective of searching a target embedding vt i is: maximize vt i SX j=1 cos(rAsj (vsj i ), rAt (vt i)). (7) We follow Eqn. (6) to normalizevt i to target model’s embedding space, and use the resulting vectors evt i for inference. 3 E XPERIMENTS 3.1 D ATASET We utilized a widely used factual probing dataset, LAMA (Petroni et al., 2019), to evaluate the effectiveness of our continuous prompt transfer approach. We followed recent factual probing stud- ies (Shin et al., 2020; Zhong et al., 2021) that focus on the TREx split of LAMA. Specifically, LAMA-TREx presents a factual knowledge piece as a triple⟨subject, relation, object⟩. For example, the fact that “Dante was born in Florence” is represented as ⟨Dante, place of birth, Florence⟩, where “place of birth” is a pre-defined relation in the dataset. In total, there are 41 distinct relations as subtasks, each of which contains up to 1, 000 tuples. We chose factual probing for two key reasons: First, induced prompts represent different task semantics from 41 sub-tasks (distinct pre-defined re- lations), providing a robust way to evaluate the generalizability of our transfer approach in various scenarios. Second, the factual probing task requires the model to precisely predict the correct entities from its vocabulary. This makes it easier to judge the performance of a prompt. On the source pretrained language model, we adopted OptiPrompt (Zhong et al., 2021) for inducing a continuous prompt for each of the 41 sub-tasks. Given a sub-task of a certain relation, the source language model is queried using the prompt defined in Eqn. (1), where the prompt embeddings are randomly initialized and then optimized according to Eqn. (2). 3.2 C HOICE OF MODELS AND OTHER IMPLEMENTATION DETAILS We investigated our transferring approach across a range of language models, namely, BERT (De- vlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2020), including base and large variants. It should be noted that ALBERT utilizes parameter sharing across layers and reduced embedding dimensions, which, to some extent, ties the semantics of embeddings and hidden lay- ers. Therefore, we only used BERT and RoBERTa as the source language models while excluding ALBERTA due to its unique architecture that does not support full compatibility with BERT and RoBERTa. All these models are considered as target models for transfer. In the main experiments, we set the default number of prompt embeddings m to 5, and the number of anchors k to 8192. We report the standard evaluation metric, micro-average accuracy, which is calculated by averaging the accuracy of 41 sub-tasks of distinct relations in the LAMA dataset (Shin et al., 2020; Zhong et al., 2021). These settings are applied to both our approach and baselines. More implementation details are shown in Appendix A.1. 3.3 M AIN RESULTS Direct transfer.We first analyze a na¨ıve method, direct transfer, which directly applies the source- induced continuous prompt to the target model. This provides us with a general understanding of whether continuous prompts are directly transferable. We show the results of direct transfer in Table 1 as a standalone experiment, as it does not fit our main table because direct transfer is only feasible when the source and target embedding dimensions match. 5Published as a conference paper at ICLR 2024 Table 1: Accuracy of direct transfer between models with the same embedding dimension. Results are in percentage. Source Target dembedding Transfer acc (%) BERTbase RoBERTabase 768 0.11 RoBERTabase BERTbase 768 0.12 BERTlarge RoBERTalarge 1024 6.27 RoBERTalarge BERTlarge 1024 0.49 The results reveal that continuous prompts induced from the base mod- els of BERT and RoBERTa (both with 768 dimensions) perform poorly when transferred to each other (around 0.1% accuracy). For their large variants, the transfer performance from RoBERTa to BERT improves marginally, achiev- ing around 0.5% accuracy. Transfer- ring from BERT to RoBERTa achieves nearly6.3% accuracy, but is still far from ideal. Overall, this experiment verifies that continuous prompts are not directly transferable, signifying the importance of prompt transfer research. Baselines and single-source transfer.Table 2 represents the results of non-transfer baselines for reference. In the random method, prompt embeddings are randomly sampled from a normal dis- tribution fitted to the target models’ word embeddings. Its all-zero performance implies that factual probing is a challenging task that requires non-trivial efforts from a machine learning model. In direct tuning, the continuous prompt is directly tuned with the target model. As expected, direct tuning achieves high performance, but is undesired as it requires backpropagation through the target model; it serves as an “upper bound” of prompt transfer in the factual probing task. We also present the performance of manual prompts, provided by the LAMA dataset (Petroni et al., 2019), serving as another reference score for evaluating prompt transfer methods. Table 3 shows the main results of our proposed method along with several continuous prompt trans- fer baselines. We experimented with a straightforward method, called discretization (Khashabi et al., 2022), for continuous prompt transfer. Specifically, each prompt embedding is projected to its nearest-neighbor token embedding, and these discrete tokens are transferred to the target model. As analyzed in Khashabi et al. (2022), such a method yields poor transferability, probably due to the Table 2: Results of the non-transfer baselines, serving as reference scores for transfer. Method Target BERTbase BERTlarge RoBERTabase RoBERTalarge ALBERTbase ALBERTlarge Random 0.00 0.00 0.00 0.00 0.00 0.00 Manual 30.64 32.22 20.48 23.59 18.63 24.44 Direct tuning 50.56 51.97 46.24 41.06 42.98 43.78 Table 3: Main results. Best performance is highlighted in bold, while second-best performance is underlined. The numbers in gray are the self-transfer performance. Source Target BERTbase BERTlarge RoBERTabase RoBERTalarge ALBERTbase ALBERTlarge Discretization BERTbase 12.93 10.76 10.88 11.96 11.44 11.10 RoBERTabase 12.31 10.35 13.51 11.01 11.67 12.33 BERTlarge 9.00 11.02 5.64 11.93 7.35 6.66 RoBERTalarge 1.35 0.70 2.28 6.22 3.15 2.64 Neural projector BERTbase 26.82 12.49 14.36 9.78 10.99 18.77 RoBERTabase 23.46 17.46 35.37 20.16 11.63 14.44 BERTlarge 3.15 4.77 5.64 4.66 8.18 14.55 RoBERTalarge 2.62 3.20 5.54 12.80 7.45 8.25 Single source (ours) BERTbase 49.82 31.40 17.68 21.07 20.83 16.80 RoBERTabase 31.33 27.52 45.17 25.09 26.11 24.72 BERTlarge 26.78 50.21 7.64 16.91 15.10 13.44 RoBERTalarge 3.81 12.45 3.63 40.91 4.48 2.94 Dual sources (ours) BERTbase+ BERTlarge 49.21 47.78 27.60 23.21 23.67 22.32 BERTbase+ RoBERTabase 48.79 32.83 43.83 25.26 27.13 26.54 6Published as a conference paper at ICLR 2024 Figure 2: Validation accuracy vs. matching loss, with the curves showing the performance of various target models. expressive power in the neighbor of discrete token embeddings. Our results also demonstrate the low performance of discretization, which is consistent with previous work and indicates that a more nuanced approach is needed for effective prompt transfer. In addition, we included an intuitive baseline method, the neural projector(Su et al., 2022), for comparison. Specifically, we first trained a two-layer projector to map the source embedding space to the target one based on anchor words. Then, we projected the source-induced prompt embed- dings to the target model using the trained projector. Detailed settings and results are provided in Appendix A.2. As seen, transferring the induced prompt embeddings through the projector provides better results but still falls short of manual prompting. Now, we consider our proposed prompt transfer method with a single source. As seen, our method yields consistent improvement compared with the neural projector, which is a compelling result as our method does not require learning a mapping from the source embedding space to the target. This verifies that our proposal of working with a relative space is more effective than the original embedding space for continuous prompt transfer. More profoundly, the prompts transferred from the base models of BERT and RoBERTa surpass the manual prompting baseline, manifesting the practical value of our proposed prompt transfer method. Multi-source transfer. Finally, we evaluate our proposed multi-source prompt transfer method as described in Section 2.4. We consider two dual-source settings: BERT base+BERTlarge and BERTbase+RoBERTabase. The results are also shown in Table 3. As seen, using multiple sources generally improves transferability. For example, the BERTbase+BERTlarge dual-source setting outper- forms BERTbase by 2–10 percentage points, although BERTlarge alone is not a strong source model. Compared with the best single source, the BERT base+RoBERTabase dual-source setting yields an improvement of 1–2 points on the target models of ALBERTbase and ALBERTlarge, which are not in- volved in the prompt tuning process. Overall, this experiment verifies that multiple sources improve the transferability of continuous prompts. Transferability and expressive power.We observed in Table 3 that a larger source model (ei- ther BERTlarge or RoBERTalarge) has lower transfer performance. This aligns with the intuition in Khashabi et al. (2022) that there could be a large number of similarly performing continuous prompts, residing in a large (and also deep) model’s expressive embedding space (Telgarsky, 2016; Raghu et al., 2017). Therefore, the induced prompt may carry model specificity in addition to task semantics, limiting its transferability to other models. Fortunately, the low transferability of large source models does not affect the value of our work, because our typical application scenario is to tune a continuous prompt on a small source model and use it for a large model. This is precisely the setting where our approach is especially effective. 3.4 A NALYSIS Matching in relative space vs. transferability.Our approach is based on the intuition that the task semantic is carried in a relative space, which can be aligned for different language models. We analyze whether a closer matching of the relative space yields a higher transferability of the continuous prompts. In Figure 2, we show the trend of validation accuracy versus matching loss along the search process in Eqn. (5), where for each source–target combination, we averaged the performance of all sub-tasks. 7Published as a conference paper at ICLR 2024 Figure 3: The effect of normalization. Figure 4: The effect of the anchor number and prompt length. Each value (dot) was computed by averaging the accuracy from all source–target combinations. In Figure 2, we observe that, as the matching loss decreases (towards right in the plots), the validation accuracy of target models increases. In the special case where source and target models are identical, the matching loss of the relative space is able to approach zero, and the accuracy of the transferred prompt is close to that of the source prompt. This demonstrates that our approach is able to recover the original embeddings from the relative space, even if a normalization is performed to the target embeddings in Eqn. (6). When source and target models are different, the matching loss does not approach zero, which is reasonable because the different language models’ embedding spaces may not be perfectly aligned. Nevertheless, the correlation between matching loss and validation accuracy is highly consistent across all source–target combinations, convincingly showing that a better matching in the relative space leads to a more transferable continuous prompt. Effect of normalization to target embedding space.We further provide an ablation study on the normalization of target embeddings introduced in Eqn. (6). Specifically, we compare the perfor- mance of the target prompts with or without the normalization treatment. From Figure 3, we observe that, when the source and target models are identical (non-transfer set- tings), the normalization hurts the performance, as it distorts the original word embedding space. However, the gap is minimal, which provides additional evidence that we are able to recover the original embeddings through our relative space even with the normalization. When the source and target models are different, however, the normalization significantly improves the performance. The results confirm that the normalization treatment can better cast the relative embedding of a source-induced continuous prompt into the target embedding space, directly under- stood by the target language model. Effect of the anchor numbers and prompt length.We analyze the number of anchors and the prompt length. We first varied the number of anchors from the set {512, 1024, 2048, 4096, 17230}, where 17, 230 is the number of the shared tokens in different language models considered in this study. The anchor number decides the feature dimensionality of the relative representations, shown in Eqns. (3) and (4). Figure 4a reveals a general trend of improved transfer performance with an increasing number of anchors. When we have 512 anchors, the transfer performance is the lowest, which is due to the inadequate capacity of the low-dimensional relative space. On the other hand, using the entire shared vocabulary results in a marginal decrease in performance. This is reasonable because the embeddings of rare words may not be well trained, consequently introducing noise to the high-dimensional feature space. 8Published as a conference paper at ICLR 2024 We then investigate the effect of prompt length on transfer performance, where we chose the length from the set {1, 3, 5, 10}. We observe in Figure 4b that the performance of transfer improves until a certain point, namely, five virtual tokens in our case. With a prompt length of 10, the transfer performance decreases slightly. Overall, our approach is robust to these hyperparameters. Based on this analysis, we set the number of anchors to 8192 and the prompt length to 5 in our main experiments (see § 3.2). Additional results. We provide additional results in the appendices. These include a study of transferring induced prompts across different model architectures, such as from BERT to GPT-2, detailed in §B.1. Furthermore, we demonstrate the applicability of our method to classification tasks, discussed in §B.2. 4 R ELATED WORK In recent years, language models (LMs) have shown impressive few-shot learning capabilities that allow them to be adapted to a variety of downstream tasks through the design of textual prompts (Brown et al., 2020). Subsequent research improves the performance of NLP tasks by creating discrete prompts that are manually crafted, searched through gradient descent (Shin et al., 2020), or using reinforcement learning (Deng et al., 2022). Meanwhile, there has been a growing interest in continuous prompts tuning (Li & Liang, 2021; Lester et al., 2021; Zhong et al., 2021). These studies suggest that tuning a small number of parameters in prompt embeddings can match the performance of full-model finetuning (Houlsby et al., 2019; Lester et al., 2021), which shows the potential of continuous prompt tuning. Several previous studies have tried to utilize the induced continuous prompt to other tasks or other LMs. For example, Vu et al. (2022) show that prompt embeddings induced on a certain task can be used to initialize the prompts for similar tasks. This led to further research on retrieving and mixing continuous prompts for new tasks (Asai et al., 2022; Su et al., 2022; Wang et al., 2023). Su et al. (2022) further study the transferability of continuous prompts in cross-LM scenarios. They propose to train a projector between the embedding space of two LMs with parallel induced prompt embeddings or task signals, which contrasts with the zero-shot transfer approach in this work. Rakotonirina et al. (2023) and Wen et al. (2023) investigate zero-shot transferability between differ- ent LMs using the induced discrete prompts. Their work is orthogonal to ours as we focus on the cross-model transfer of continuous prompts. Although transferring discrete prompts offers greater simplicity compared with our proposed continuous prompt transfer, continuous prompts are more versatile and can be adapted to a broader range of applications. The concept of mapping different embeddings into a shared latent space has been well explored in the cross-lingual scenario (Faruqui & Dyer, 2014; Lazaridou et al., 2015; Artetxe et al., 2018), which further paves the way for unsupervised neural machine translation (Lample et al., 2018). We follow the assumption from these studies and assume the induced task embeddings (Vu et al., 2022) from different language models share similar latent structures. We employ relative represen- tation (Moschella et al., 2023) to encode a source-induced prompt, and decode it to the embedding space of the target model. 5 C ONCLUSION We introduced a zero-shot method for transferring continuous prompts between different language models through a relative space. Experiments confirm the effectiveness of our approach, and we further provide insights into the correlation between a model’s transferability and its expressive power. Moreover, we propose a simple method to improve the generalizability of prompt transfer by using multiple source models. Given the shift towards unified large language models (Brown et al., 2020), our method could enable smaller source models to act as effective “soft prompt engineers” that perform better than manual prompting. Additionally, it is a promising direction to explore direct human–model interactions that bypass the need for discrete language. This will involve prompting pretrained language models using continuous prompts transferred from sources like encoded brain signals (Zou et al., 2021). 9Published as a conference paper at ICLR 2024 REFERENCES Mikel Artetxe, Gorka Labaka, and Eneko Agirre. A robust self-learning method for fully un- supervised cross-lingual mappings of word embeddings. In Proceedings of the Annual Meet- ing of the Association for Computational Linguistics, pp. 789–798, 2018. URL https: //aclanthology.org/P18-1073. Akari Asai, Mohammadreza Salehi, Matthew Peters, and Hannaneh Hajishirzi. ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 6655–6672, 2022. URL https://aclanthology.org/2022.emnlp-main.446. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma- teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan- dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few- shot learners. In Advances in Neural Information Processing Systems , pp. 1877–1901, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Emmanuele Chersoni, Enrico Santus, Chu-Ren Huang, and Alessandro Lenci. Decoding word embeddings with brain-based semantic features. Computational Linguistics, 47:663–698, 2021. URL https://doi.org/10.1162/coli_a_00412. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing, pp. 3369–3391, 2022. URL https://aclanthology.org/2022.emnlp-main. 222. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186, 2019. URL https://aclanthology.org/N19-1423. Manaal Faruqui and Chris Dyer. Improving vector space word representations using multilin- gual correlation. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pp. 462–471, 2014. URL https://aclanthology.org/ E14-1049. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning, pp. 2790–2799, 2019. URL https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf. Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, and Yejin Choi. Prompt wayward- ness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pp. 3631–3643, 2022. URL https://aclanthology.org/ 2022.naacl-main.266. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. URL https://arxiv.org/abs/1412.6980. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rkYTTf-AZ. 10Published as a conference paper at ICLR 2024 Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A lite BERT for self-supervised learning of language representations. In Interna- tional Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=H1eA7AEtvS. Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni. Hubness and pollution: Delving into cross-space mapping for zero-shot learning. In Proceedings of the Annual Meeting of the Associ- ation for Computational Linguistics and the International Joint Conference on Natural Language Processing, pp. 270–280, 2015. URL https://aclanthology.org/P15-1027. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, 2021. URL https://aclanthology.org/2021.emnlp-main.243. Gidon Levakov, Joshua Faskowitz, Galia Avidan, and Olaf Sporns. Mapping individual differ- ences across brain network structure to function and behavior with connectome embedding. NeuroImage, 242:118469, 2021. URL https://www.sciencedirect.com/science/ article/pii/S1053811921007424. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing, pp. 4582–4597, 2021. URL https://aclanthology.org/2021.acl-long.353. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pre- training approach. arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/ abs/1907.11692. Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodol `a. Relative representations enable zero-shot latent space communication. In International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=SrC-nwieGJ. Antonio Norelli, Marco Fumero, Valentino Maiorca, Luca Moschella, Emanuele Rodol `a, and Francesco Locatello. ASIF: Coupled data turns unimodal models to multimodal without training. arXiv preprint arXiv:2210.01738, 2022. URL https://arxiv.org/pdf/2210.01738. Fabio Petroni, Tim Rockt¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing, pp. 2463–2473, 2019. URL https://aclanthology.org/ D19-1250. Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the ex- pressive power of deep neural networks. In Proceedings of the International Conference on Ma- chine Learning, pp. 2847–2854, 2017. URL https://proceedings.mlr.press/v70/ raghu17a.html. Nathana¨el Carraz Rakotonirina, Roberto Dessi, Fabio Petroni, Sebastian Riedel, and Marco Baroni. Can discrete information extraction prompts generalize across language models? InInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=sbWVtxq8-zE. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting knowledge from language models with automatically generated prompts. InProceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 4222–4235, 2020. URL https://aclanthology.org/2020.emnlp-main.346. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, and Jie Zhou. On transferability of prompt tuning for natural language processing. In Proceedings of the Conference of the North 11Published as a conference paper at ICLR 2024 American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, pp. 3949–3969, 2022. URL https://aclanthology.org/2022.naacl-main. 290. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning for language-model-as-a-service. In Proceedings of the International Conference on Machine Learning, pp. 20841–20855, 2022. URL https://proceedings.mlr.press/v162/ sun22e.html. Matus Telgarsky. Benefits of depth in neural networks. In Annual Conference on Learning Theory, pp. 1517–1539, 2016. URL https://proceedings.mlr.press/v49/telgarsky16. html. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’, and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 5039–5059, 2022. URL https://aclanthology.org/ 2022.acl-long.346. Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. Mul- titask prompt tuning enables parameter-efficient transfer learning. In International Confer- ence on Learning Representations, 2023. URL https://openreview.net/forum?id= Nk2pDtuhTq. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139–149, 2022. URL https://openaccess.thecvf.com/content/CVPR2022/ papers/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_ paper.pdf. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. In Advances in Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=VOstHxDdsN. Zijun Wu, Anup Anand Deshmukh, Yongkang Wu, Jimmy Lin, and Lili Mou. Unsupervised chunking with hierarchical RNN. arXiv preprint arXiv:2309.04919, 2023. URL https: //arxiv.org/abs/2309.04919. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015. URL https://arxiv.org/ abs/1505.00853. Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [MASK]: Learning vs. learning to recall. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5017–5033, 2021. URL https://aclanthology.org/2021.naacl-main.398. Yichu Zhou and Vivek Srikumar. A closer look at how fine-tuning changes BERT. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 1046–1061, 2022. URL https://aclanthology.org/2022.acl-long.75. Shuxian Zou, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. Towards brain-to-text genera- tion: Neural decoding with pre-trained encoder-decoder models. In NeurIPS 2021 AI for Science Workshop, 2021. URL https://openreview.net/forum?id=13IJlk221xG. 12Published as a conference paper at ICLR 2024 Table 4: Details of the pretrained language models considered in this study. MLM, NSP, SOP, and NTP stand for masked language modeling, next sentence prediction, sentence order prediction, and next token prediction, respectively. It should be noted that ALBERT employs weight sharing, and its memory consumption is similar to BERT and RoBERTa. Model #Parameters dhidden dembedding Pretraining task Pretraining data BERT base 110M 768 768 MLM & NSP BookCorpus, English Wikipedialarge 340M 1024 1024 RoBERTa base 125M 768 768 MLM BookCorpus, English Wikipedia, large 355M 1024 1024 CC-News, OpenWebText, Stories ALBERT base 12M 768 128 MLM & SOP BookCorpus, English Wikipedialarge 18M 1024 128 GPT-2 small 117M 768 768 NTP WebText base 345M 1024 1024 large 774M 1280 1280 A I MPLEMENTATION DETAILS A.1 D ETAILS OF THE MODELS Table 4 provides an overview of the language models used in this study, including base and large variants of BERT, RoBERTa, and ALBERT. Each model is trained with distinct pretraining tasks and datasets. In this study, we focus on transferring continuous prompts between masked language models, as this fill-in-the-blank mechanism is a natural way to probe knowledge (Shin et al., 2020). We also provide a preliminary empirical investigation of transferring continuous prompts between different model structures, e.g., from the encoder-only BERT model to the decoder-only GPT-2 model, which is discussed in §B.1. Due to the variations in pretraining datasets and tokenizing methods, the language models in differ- ent families (e.g., BERT vs. RoBERTa) have different vocabularies. We obtained a shared vocabu- lary of tokens by taking the intersection of these individual vocabularies. During the transfer, we first encode the source prompt embeddings to the entire relative space. Then, we pick top- k dimensions of highest values (k = 8192) and set the rest of zero, which follows Norelli et al. (2022). A.2 D ETAILS OF THE PROJECTOR BASELINE One of our baselines is a projector that maps the source embedding space to the target one. We trained a two-layer neural network as the projector based on the shared vocabulary. Specifically, we have Proj(es i) =W2(f(W1es i + b1)) +b2, (8) where f is the Leaky ReLU activation function (Xu et al., 2015). For some anchor wordi, we denote by es i and et i the word embeddings of the source model and target model, respectively. We train the projector by minimizing the mean squared error loss: LMSE = 1 k kX i=1 (Proj(es i) − et i), (9) where k is the size of shared vocabulary between two language models. We trained the neural network with 10 epochs using the Adam optimizer (Kingma & Ba, 2014). The learning rate was 5e-3 and the batch size was 16. The hidden dimension of this two-layer neural network was 768. We ran the validation on target models after each training epoch with the projected target prompt embeddings. We chose the projector with the highest validation performance and used it for test. B A DDITIONAL RESULTS In this appendix, we report preliminary results of the additional experiments conducted during the author response phase based on the reviewers’ suggestions. In particular, we show the adaptability 13Published as a conference paper at ICLR 2024 Table 5: Results on transferring prompts between encoder and decoder models. Method Target BERTbase RoBERTabase GPT2small GPT2medium GPT2large Direct tuning 50.56 46.24 31.62 32.23 34.44 Manual 30.64 20.48 4.73 8.01 10.23Source BERTbase - 17.68 10.46 11.52 5.50 RoBERTabase 31.33 - 14.06 13.70 14.33 GPT2small 6.58 0.39 - 13.72 2.34 GPT2medium 4.06 0.50 5.02 - 1.79 Table 6: Results of transferring prompts from source models to RoBERTa large on the SST-2 and DPpedia classification tasks. Method SST-2 (accuracy) DBpedia (accuracy) Direct tuning 90.94 84.92 Manual 69.95 72.28 Source: BERTbase 82.45 77.05 Source: RoBERTabase 84.63 80.81 of our method to different model architectures in §B.1, and experiment with classification tasks in §B.2. B.1 T RANSFER BETWEEN DIFFERENT MODEL ARCHITECTURES We first demonstrate the feasibility of transferring continuous prompts across different model archi- tectures. This experiment explores the transferability between encoder and decoder models, focusing on generative GPT-2 models of varying sizes: small, medium, and large, as detailed in Table 4. We selected BERTbase and RoBERTabase, two encoder models, for our primary experiment to examine the transferability of prompts to or from GPT-2 models. Table 5 shows the results of transferring continuous prompts across architectures on the LAMA dataset, including comparisons with the performance of directly tuned and manually prompted tar- get models for reference. We see that the prompts induced on the encoder models, BERT base and RoBERTabase, are transferable to the GPT-2 models with different sizes. Notably, RoBERTa base shows its best transferability, outperforming the manual prompting baseline across all target mod- els. However, we found that the GPT-2 models as the source cannot induce as meaningful prompts as the encoder models, often underperforming manual prompting. The underlying reason contributing to the poor transferability of the continuous prompts induced on GPT-2 models remains unexplored and merits further study. B.2 R ESULTS ON CLASSIFICATION TASKS Now we show our proposed transfer method is effective on other NLP tasks. Specifically, we include SST-2, a binary sentiment classification task, and DBpedia, a 14-category topic classification task. Unlike LAMA’s entity prediction which requires the model to consider the whole vocabulary, the classification task only requires prediction within the label words based on the prompt, for example, “great” or “bad” for the SST-2 dataset (Sun et al., 2022). As shown in Table 6, compared to using manual prompts on the target model directly, transferring prompts from both BERT base and RoBERTabase to the RoBERTalarge target model yields better re- sults. In line with our previous findings, RoBERTa base shows its superior transferability. Overall, our additional results present the potential of applying our approach to various tasks and model architectures. 14Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS We would like to thank Ning Shi for his insightful suggestion of the multi-source transfer ap- proach. We also thank all reviewers and chairs for their valuable and constructive comments. The research is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) under Grant No. RGPIN2020-04465, the Amii Fellow Program, the Canada CIFAR AI Chair Program, a UAHJIC project, an Alberta Innovates Program, and the Digital Research Alliance of Canada (alliancecan.ca). 15",
      "meta_data": {
        "arxiv_id": "2310.01691v2",
        "authors": [
          "Zijun Wu",
          "Yongkang Wu",
          "Lili Mou"
        ],
        "published_date": "2023-10-02T23:12:21Z",
        "pdf_url": "https://arxiv.org/pdf/2310.01691v2.pdf",
        "github_url": "https://github.com/MANGA-UOFA/PTfer"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the challenge of transferring continuous prompts between different language models in a zero-shot setting, aiming to avoid expensive gradient computations on large models. The main contributions include proposing a novel zero-shot continuous prompt transfer method that encodes source prompts into a relative space and searches for corresponding target prompts. Key findings confirm that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, the study demonstrates that combining 'task semantics' from multiple source models can further enhance transfer performance. The approach facilitates multi-source transfer, unlike previous methods.",
        "methodology": "The methodology involves an 'encode-then-search' strategy. First, continuous prompt tuning optimizes virtual token embeddings for a downstream task. To transfer, source prompt embeddings are encoded into a relative representation using a set of common tokens (anchors) shared between source and target models. This encoding is based on cosine similarity between prompt embeddings and anchor embeddings. Second, in the target space, target prompt embeddings are randomly initialized and then searched by maximizing the cosine similarity between their relative representations and those of the source prompts, using gradient descent. The searched target embeddings are subsequently normalized based on the mean and standard deviation of the target model's word embeddings. For multi-source transfer, the objective is extended to maximize the sum of similarities between the target relative embedding and the relative embeddings from multiple source models.",
        "experimental_setup": "The method was evaluated on the LAMA (Petroni et al., 2019) factual probing dataset, specifically the TREx split, which contains 41 distinct relations as sub-tasks. OptiPrompt (Zhong et al., 2021) was used to induce continuous prompts on source language models. Experiments utilized various BERT, RoBERTa, and ALBERT models (base and large variants) as source and target models. GPT-2 models (small, medium, large) were also included in an additional cross-architecture transfer study. The default prompt length was set to 5 virtual tokens, and 8192 anchors (common tokens) were used. The evaluation metric was micro-average accuracy across all 41 sub-tasks. Baselines included direct transfer, random prompts, manual prompts, direct tuning (as an upper bound), discretization, and a neural projector.",
        "limitations": "A notable limitation is that larger source models (e.g., BERTlarge, RoBERTalarge) exhibited lower transfer performance. This is attributed to the potential for model-specific information in their highly expressive embedding spaces, which limits generalizability beyond task semantics. Additionally, GPT-2 models, when used as source models, did not consistently induce meaningful prompts and often underperformed manual prompting, with the underlying reasons remaining unexplored. The cosine similarity measure used in encoding is insensitive to vector magnitude, necessitating a subsequent normalization step for target embeddings. Using the entire shared vocabulary as anchors was found to marginally decrease performance, likely due to noise from poorly trained rare word embeddings.",
        "future_research_directions": "Future research could explore the reasons behind the poor transferability of continuous prompts induced on GPT-2 (decoder-only) models when used as sources. A promising direction is to investigate direct human-model interactions that bypass discrete language, potentially by prompting pretrained language models using continuous prompts transferred from sources like encoded brain signals (e.g., from Zou et al., 2021).",
        "experimental_code": "File Path: rel2abs_util.py\nContent:\nfrom transformers import AutoConfig\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig\nfrom transformers import BertTokenizer, BertModel, BertConfig\nfrom transformers import AlbertConfig, AlbertTokenizer, AlbertModel\nfrom transformers import GPT2Config, GPT2Model, GPT2Tokenizer\nfrom transformers import BartConfig, BartTokenizer, BartModel\nfrom transformers import T5Config, T5Tokenizer, T5Model\nimport numpy as np\n\ndef get_model_tokenizer(model_name):\n    config = AutoConfig.from_pretrained(model_name)\n    if isinstance(config, RobertaConfig):\n        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n        base_model = RobertaModel.from_pretrained(model_name)\n        model_family = 'roberta'\n    elif isinstance(config, BertConfig):\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n        base_model = BertModel.from_pretrained(model_name)\n        model_family = 'bert'\n    elif isinstance(config, AlbertConfig):\n        tokenizer = AlbertTokenizer.from_pretrained(model_name)\n        base_model = AlbertModel.from_pretrained(model_name)\n        model_family = 'albert'\n    elif isinstance(config, GPT2Config):\n        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        base_model = GPT2Model.from_pretrained(model_name)\n        model_family = 'gpt2'\n    elif isinstance(config, BartConfig):\n        tokenizer = BartTokenizer.from_pretrained(model_name)\n        base_model = BartModel.from_pretrained(model_name)\n        model_family = 'bart'\n    elif isinstance(config, T5Config):\n        tokenizer = T5Tokenizer.from_pretrained(model_name)\n        base_model = T5Model.from_pretrained(model_name)\n        model_family = 'T5'\n    else:\n        raise ValueError('Model %s not supported yet!'%(model_name))\n    \n    return tokenizer, base_model, model_family\n\ndef build_vocab(tokenizer):\n    vocab = list(tokenizer.get_vocab())\n    inverse_vocab = {w: i for i, w in enumerate(vocab)}\n    return vocab, inverse_vocab\n\ndef select_random_samples(matrix, indices):\n    k = matrix.shape[1]\n    m = indices.shape[0]\n    selected_embeddings = np.empty((m, k))\n    \n    for i, idx in enumerate(indices):\n        selected_embeddings[i] = matrix[idx]\n    \n    return selected_embeddings\n\ndef vocab_cleaning(vocab):\n    vocab = [token.replace(\"Ġ\", \"\") for token in vocab] # for roberta\n    vocab = [token.replace(\" \", \"\") for token in vocab] # for albert\n    return vocab\n\ndef get_abs_anchors(model_name1, model_name2, num_anchor, common_vocab, seed, all_anchors, model_name3=None):\n    bert_tokenizer, bert_base_model, model_family1 = get_model_tokenizer(model_name1)\n    roberta_tokenizer, roberta_base_model, model_family2 = get_model_tokenizer(model_name2)\n    if model_name3 != None:\n        model_tokenizer3, model3, model_family3 = get_model_tokenizer(model_name3)\n\n    bert_UNK = bert_tokenizer.unk_token\n    roberta_UNK = roberta_tokenizer.unk_token\n    if model_name3 != None:\n        model3_UNK = model_tokenizer3.unk_token\n\n    if model_family1 in ['gpt2']:\n        bert_base_embeddings = bert_base_model.wte.weight.detach().cpu()\n    elif model_family1 in ['bart', 'T5']:\n        bert_base_embeddings = bert_base_model.shared.weight.detach().cpu()\n    else:\n        bert_base_embeddings = bert_base_model.embeddings.word_embeddings.weight.detach().cpu()\n\n    if model_family2 in ['gpt2']:\n        roberta_base_embeddings = roberta_base_model.wte.weight.detach().cpu()\n    elif model_family2 in ['bart', 'T5']:\n        roberta_base_embeddings = roberta_base_model.shared.weight.detach().cpu()\n    else:\n        roberta_base_embeddings = roberta_base_model.embeddings.word_embeddings.weight.detach().cpu()\n    \n    if model_name3 != None:\n        if model_family3 in ['gpt2']:\n            model3_embeddings = model3.wte.weight.detach().cpu()\n        elif model_family3 in ['bart', 'T5']:\n            model3_embeddings = model3.shared.weight.detach().cpu()\n        else:\n            model3_embeddings = model3.embeddings.word_embeddings.weight.detach().cpu()\n\n    if common_vocab != None:\n        with open(common_vocab, 'r') as f:\n            lines = f.readlines()\n            filtered_vocab = [x.strip() for x in lines]\n    else:\n        bert_vocab, _ = build_vocab(bert_tokenizer)\n        roberta_vocab, _ = build_vocab(roberta_tokenizer)\n\n        if model_name3 != None:\n            model_vocab3, _ = build_vocab(model_tokenizer3)\n\n\n        print('model 1 and 2 vocabs:', len(bert_vocab), len(roberta_vocab))\n        bert_vocab = vocab_cleaning(bert_vocab)\n        roberta_vocab = vocab_cleaning(roberta_vocab)\n\n        if model_name3 != None:\n            model_vocab3 = vocab_cleaning(model_vocab3)\n            filtered_vocab = list(set(bert_vocab) & set(roberta_vocab) & set(model_vocab3))\n        else:\n            filtered_vocab = list(set(bert_vocab) & set(roberta_vocab))\n\n    bert_filtered_indices = []\n    roberta_filtered_indices = []\n\n    if model_name3 != None:\n        model3_filtered_indices = []\n\n    # wf = open('aligned_vocab/%s_%s_aligned_vocab.txt' % (model_family1, model_family2), 'w')\n    for word in filtered_vocab:\n        # wf.write(word+'\\n')\n\n        bert_tokens = bert_tokenizer.tokenize(' ' + word)\n        if (len(bert_tokens) == 1) and (bert_tokens[0] != bert_UNK):\n            bert_index = bert_tokenizer.convert_tokens_to_ids(bert_tokens)[0]\n        else:\n            continue\n        \n        roberta_tokens = roberta_tokenizer.tokenize(' ' + word)\n        if (len(roberta_tokens) == 1) and (roberta_tokens[0] != roberta_UNK):\n            roberta_index = roberta_tokenizer.convert_tokens_to_ids(roberta_tokens)[0]\n        else:\n            continue\n        \n        if model_name3 != None:\n            model3_tokens = model_tokenizer3.tokenize(' ' + word)\n            if (len(model3_tokens) == 1) and (model3_tokens[0] != model3_UNK):\n                model3_index = model_tokenizer3.convert_tokens_to_ids(model3_tokens)[0]\n            else:\n                continue\n\n            model3_filtered_indices.append(model3_index)\n        \n        # wf.write('%s, %s\\n' % (bert_tokens[0], roberta_tokens[0]))\n        bert_filtered_indices.append(bert_index)\n        roberta_filtered_indices.append(roberta_index)\n            \n    assert len(bert_filtered_indices) == len(roberta_filtered_indices)\n    if model_name3 != None:\n        assert len(bert_filtered_indices) == len(model3_filtered_indices)\n\n    print('Number of filtered shared tokens %d' % len(bert_filtered_indices))\n\n    std = np.std(roberta_base_embeddings.reshape(-1).numpy())\n    mean = np.mean(roberta_base_embeddings.reshape(-1).numpy())\n\n    if all_anchors:\n        bert_base_embeddings = select_random_samples(bert_base_embeddings.numpy(), np.array(bert_filtered_indices))\n        roberta_base_embeddings = select_random_samples(roberta_base_embeddings.numpy(),np.array(roberta_filtered_indices))\n        if model_name3 != None:\n            model3_embeddings = select_random_samples(model3_embeddings.numpy(),np.array(model3_filtered_indices))\n        else:\n            model3_embeddings = None\n    else:\n        selected_indices = np.random.choice(list(range(0, len(bert_filtered_indices))), num_anchor, replace=False)\n        bert_base_embeddings = select_random_samples(bert_base_embeddings.numpy(), np.array([bert_filtered_indices[i] for i in selected_indices]))\n        roberta_base_embeddings = select_random_samples(roberta_base_embeddings.numpy(), np.array([roberta_filtered_indices[i] for i in selected_indices]))\n        if model_name3 != None:\n            model3_embeddings = select_random_samples(model3_embeddings.numpy(), np.array([model3_filtered_indices[i] for i in selected_indices]))\n        else:\n            model3_embeddings = None\n\n    return bert_base_embeddings, roberta_base_embeddings, (mean, std), model3_embeddings\n\nFile Path: evaluate.py\nContent:\nimport os\nimport torch\nfrom OptiPrompt.code.models import Prober\nfrom OptiPrompt.code.utils import evaluate, load_data, batchify, get_relation_meta, load_vocab\n\nMAX_NUM_VECTORS = 10\n\ndef get_new_token(vid):\n    assert(vid > 0 and vid <= MAX_NUM_VECTORS)\n    return '[V%d]'%(vid)\n\ndef convert_manual_to_dense(manual_template, model):\n    def assign_embedding(new_token, token):\n        \"\"\"\n        assign the embedding of token to new_token\n        \"\"\"\n        id_a = model.tokenizer.convert_tokens_to_ids([new_token])[0]\n        id_b = model.tokenizer.convert_tokens_to_ids([token])[0]\n        with torch.no_grad():\n            model.base_model.embeddings.word_embeddings.weight[id_a] = model.base_model.embeddings.word_embeddings.weight[id_b].detach().clone()\n\n    new_token_id = 0\n    template = []\n    for word in manual_template.split():\n        if word in ['[X]', '[Y]']:\n            template.append(word)\n        else:\n            tokens = model.tokenizer.tokenize(' ' + word)\n            for token in tokens:\n                new_token_id += 1\n                template.append(get_new_token(new_token_id))\n                assign_embedding(get_new_token(new_token_id), token)\n\n    return ' '.join(template)\n\ndef prepare_for_dense_prompt(model):\n    new_tokens = [get_new_token(i+1) for i in range(MAX_NUM_VECTORS)]\n    model.tokenizer.add_tokens(new_tokens)\n    model.mlm_model.resize_token_embeddings(len(model.tokenizer))\n\ndef init_template(args, model):\n    if args.init_manual_template:\n        relation = get_relation_meta(args)\n        template = convert_manual_to_dense(relation['template'], model)\n    else:\n        template = '[X] ' + ' '.join(['[V%d]'%(i+1) for i in range(args.num_vectors)]) + ' [Y] .'\n    return template\n\ndef load_optiprompt(model, original_vocab_size, vs):\n    # copy fine-tuned new_tokens to the pre-trained model\n    with torch.no_grad():\n        model.base_model.embeddings.word_embeddings.weight[original_vocab_size:] = torch.Tensor(vs)\n\n\nclass EvaluatePrompt:\n    def __init__(self, args):\n        args.model_name = args.tgt_model\n        args.model_dir = None\n        args.k = 5\n        args.eval_batch_size = 8\n\n        dev_data = os.path.join(args.data_path, args.relation, 'dev.jsonl')\n        test_data = os.path.join(args.data_path, args.relation, 'test.jsonl')\n\n        self.model = Prober(args)\n        self.original_vocab_size = len(list(self.model.tokenizer.get_vocab()))\n        prepare_for_dense_prompt(self.model)\n\n        if args.common_vocab is not None:\n            self.vocab_subset = load_vocab(args.common_vocab)\n            self.filter_indices, self.index_list = self.model.init_indices_for_filter_logprobs(self.vocab_subset)\n        else:\n            self.filter_indices = None\n            self.index_list = None\n\n        template = init_template(args, self.model)\n\n        self.valid_samples = load_data(dev_data, template, vocab_subset=self.vocab_subset, mask_token=self.model.MASK)\n        self.valid_samples_batches, self.valid_sentences_batches = batchify(self.valid_samples, args.eval_batch_size)\n\n        self.test_samples = load_data(test_data, template, vocab_subset=self.vocab_subset, mask_token=self.model.MASK)\n        self.test_samples_batches, self.test_sentences_batches = batchify(self.test_samples, args.eval_batch_size)\n\n        self.args = args\n        \n    def evaluate_valid(self, embeddings):\n        load_optiprompt(self.model, self.original_vocab_size, embeddings)\n        precision, _ = evaluate(self.model, self.valid_samples_batches, self.valid_sentences_batches, self.filter_indices, self.index_list)\n\n        return precision\n    \n    def evaluate_test(self, embeddings):\n        load_optiprompt(self.model, self.original_vocab_size, embeddings)\n        precision, _ = evaluate(self.model, self.test_samples_batches, self.test_sentences_batches, self.filter_indices, self.index_list)\n\n        return precision\n\n\nif __name__ == '__main__':\n    EvaluatePrompt().evaluate()\n\nFile Path: rel2abs_GO.py\nContent:\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom evaluate import EvaluatePrompt\n\n\nclass Rel2abs_Decoder:\n    def __init__(self, args, logger, target, src_anchors, anchors, tgt_distribution):\n        self.absolute = args.absolute\n        self.topk = args.topk\n\n        self.budget = args.budget\n        self.learning_rate = args.lr\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        target = torch.tensor(target).type(torch.float32).to(self.device)\n\n        src_anchors = torch.tensor(src_anchors).type(torch.float32).to(self.device)\n        self.target_rels = self.encode2rel(target, src_anchors)\n\n        if self.topk > 0:\n            self.target_rels, self.mask = self.zero_except_topk(self.target_rels)\n\n        self.mean = torch.tensor(tgt_distribution[0]).to(self.device)\n        self.std = torch.tensor(tgt_distribution[1]).to(self.device)\n\n        self.anchors = torch.tensor(anchors).type(torch.float32).to(self.device)\n        self.candidate = torch.empty((self.target_rels.shape[0], anchors.shape[1])).to(self.device)\n        self.candidate.requires_grad = True\n        torch.nn.init.xavier_normal_(self.candidate)\n\n        self.target_abss = None\n\n        self.eval_func = EvaluatePrompt(args)\n\n        self.cos_loss = nn.CosineEmbeddingLoss()\n        self.y = torch.ones(self.target_rels.shape[0]).to(self.device)\n\n        self.logger = logger\n\n        non_zero_indices = self.target_rels.nonzero(as_tuple=True)\n        non_zero_values = self.target_rels[non_zero_indices]\n        mean = non_zero_values.mean().item()\n        std = non_zero_values.std().item()\n        self.logger.info('Relative representations stat: mean %.4f, std %.4f' % (mean, std))\n    \n    def zero_except_topk(self, input_tensor):\n        if self.absolute:\n            _, topk_indices = torch.topk(torch.abs(input_tensor), self.topk)\n        else:\n            _, topk_indices = torch.topk(input_tensor, self.topk)\n\n        mask = torch.zeros_like(input_tensor).to(self.device)\n        mask.scatter_(-1, topk_indices, 1)\n        masked_tensor = input_tensor * mask\n        return masked_tensor, mask\n    \n    def regularize_tensor(self, tensor):\n        current_mean = torch.mean(tensor).to(self.device)\n        current_std = torch.std(tensor).to(self.device)\n        \n        normalized_tensor = (tensor - current_mean) / current_std\n        regularized_tensor = normalized_tensor * self.std + self.mean\n        \n        return regularized_tensor\n\n    def encode2rel(self, x, anchors):\n        A = F.normalize(x, dim=-1)\n        B = F.normalize(anchors, dim=-1)\n        return torch.matmul(A, B.T)\n\n    def set_target_abs(self, y):\n        self.target_abss = torch.tensor(y).type(torch.float32)\n        self.target_abss.requires_grad = False\n\n    def eval(self, x):\n        if self.target_abss == None:\n            raise AssertionError('No target abs embedding defined?')\n        cosine = nn.functional.cosine_similarity(x, self.target_abss, dim=-1)\n        return torch.mean(cosine).item()\n\n    def search(self):\n        optimizer = optim.Adam([self.candidate], lr=self.learning_rate)\n\n        best_precision = -1\n        best_candidate = None\n\n        pbar = tqdm(range(self.budget))\n        for i in pbar:\n            regularized_candidate = self.regularize_tensor(self.candidate)\n            x_rel = self.encode2rel(regularized_candidate, self.anchors)\n            if self.topk > 0:\n                x_rel = x_rel * self.mask\n\n            loss = self.cos_loss(x_rel, self.target_rels, self.y)\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            if (i+1) % 50 == 0:\n                with torch.no_grad():\n                    this_candidate = regularized_candidate.detach().cpu().numpy()\n                    precision = self.eval_func.evaluate_valid(this_candidate)\n                if precision > best_precision:\n                    best_candidate = this_candidate\n                    best_precision = precision\n                    self.logger.info('Get best precision: %.4f at step %d! loss: %.4f' % (best_precision, i+1, loss.item()))\n            pbar.set_description('best precision: %.4f, loss: %.4f' % (best_precision, loss.item()))\n\n        with torch.no_grad():\n            test_precision = self.eval_func.evaluate_test(best_candidate)\n        self.logger.info('Test precision: %.4f' % test_precision)\n\n        return best_candidate\n\nFile Path: rel2abs_GO_mix.py\nContent:\nimport torch\nfrom torch import optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom evaluate import EvaluatePrompt\n\nclass Rel2abs_Decoder:\n    def __init__(self, args, logger, target1, src_anchors1, target2, src_anchors2, anchors, tgt_distribution):\n        self.budget = args.budget\n        self.learning_rate = args.lr\n        self.absolute = args.absolute\n        self.topk = args.topk\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        target1 = torch.tensor(target1).type(torch.float32).to(self.device)\n        src_anchors1 = torch.tensor(src_anchors1).type(torch.float32).to(self.device)\n        self.target_rels1 = self.encode2rel(target1, src_anchors1)\n\n        target2 = torch.tensor(target2).type(torch.float32).to(self.device)\n        src_anchors2 = torch.tensor(src_anchors2).type(torch.float32).to(self.device)\n        self.target_rels2 = self.encode2rel(target2, src_anchors2)\n\n        if self.topk > 0:\n            self.target_rels2, self.mask = self.zero_except_topk(self.target_rels2)\n            self.target_rels1 = self.target_rels1 * self.mask\n\n        assert self.target_rels1.shape == self.target_rels2.shape\n\n        self.mean = torch.tensor(tgt_distribution[0]).to(self.device)\n        self.std = torch.tensor(tgt_distribution[1]).to(self.device)\n\n        self.anchors = torch.tensor(anchors).type(torch.float32).to(self.device)\n        self.candidate = torch.empty((self.target_rels1.shape[0], anchors.shape[1])).to(self.device)\n        self.candidate.requires_grad = True\n        torch.nn.init.xavier_normal_(self.candidate)\n\n        self.target_abss = None\n\n        self.eval_func = EvaluatePrompt(args)\n\n        self.cos_loss = nn.CosineEmbeddingLoss()\n        self.y = torch.ones(self.target_rels1.shape[0]).to(self.device)\n\n        self.logger = logger\n    \n    def zero_except_topk(self, input_tensor):\n        if self.absolute:\n            _, topk_indices = torch.topk(torch.abs(input_tensor), self.topk)\n        else:\n            _, topk_indices = torch.topk(input_tensor, self.topk)\n \n        mask = torch.zeros_like(input_tensor).to(self.device)\n        mask.scatter_(-1, topk_indices, 1)\n        masked_tensor = input_tensor * mask\n        return masked_tensor, mask\n    \n    def regularize_tensor(self, tensor):\n        current_mean = torch.mean(tensor).to(self.device)\n        current_std = torch.std(tensor).to(self.device)\n        \n        normalized_tensor = (tensor - current_mean) / current_std\n        regularized_tensor = normalized_tensor * self.std + self.mean\n        \n        return regularized_tensor\n\n    def encode2rel(self, x, anchors):\n        A = F.normalize(x, dim=-1)\n        B = F.normalize(anchors, dim=-1)\n        return torch.matmul(A, B.T)\n\n    def set_target_abs(self, y):\n        self.target_abss = torch.tensor(y).type(torch.float32)\n        self.target_abss.requires_grad = False\n\n    def eval(self, x):\n        if self.target_abss == None:\n            raise AssertionError('No target abs embedding defined?')\n        cosine = nn.functional.cosine_similarity(x, self.target_abss, dim=-1)\n        return torch.mean(cosine).item()\n\n    def search(self):\n        optimizer = optim.Adam([self.candidate], lr=self.learning_rate)\n\n        best_precision = -1\n        best_candidate = None\n\n        pbar = tqdm(range(self.budget))\n        for i in pbar:\n\n            regularized_candidate = self.regularize_tensor(self.candidate)\n            x_rel = self.encode2rel(regularized_candidate, self.anchors)\n            if self.topk > 0:\n                x_rel = x_rel * self.mask\n            \n            loss = (self.cos_loss(x_rel, self.target_rels1, self.y) + self.cos_loss(x_rel, self.target_rels2, self.y)) / 2\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            if (i+1) % 50 == 0:\n                with torch.no_grad():\n                    this_candidate = regularized_candidate.detach().cpu().numpy()\n                    precision = self.eval_func.evaluate_valid(this_candidate)\n                if precision > best_precision:\n                    best_candidate = this_candidate\n                    best_precision = precision\n                    self.logger.info('Get best precision: %.4f at step %d! loss: %.4f' % (best_precision, i+1, loss.item()))\n\n            pbar.set_description('best precision: %.4f, loss: %.4f' % (best_precision, loss.item()))\n\n        with torch.no_grad():\n            test_precision = self.eval_func.evaluate_test(best_candidate)\n        self.logger.info('Test precision: %.4f' % test_precision)\n        \n        return best_candidate\n\nFile Path: get_emb.py\nContent:\nimport numpy as np\nimport torch\nimport random\nimport argparse\nfrom rel2abs_util import get_abs_anchors\nimport os\nimport logging\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                datefmt='%m/%d/%Y %H:%M:%S',\n                level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef save_embeddings(args, embeddings):\n    save_path = os.path.join(args.tgt_prompt_path, args.relation, args.transferred_prompt_filename)\n    with open(save_path, 'wb') as f:\n        logger.info('Saving transferred prompt embeddings to %s' % save_path)\n        np.save(f, embeddings)\n\ndef fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef transfer(args):\n    v_base = np.load(os.path.join(args.src_prompt_path, args.relation, args.prompt_filename))\n\n    bert_base_anchors, bert_large_anchors, tgt_distribution, _ = get_abs_anchors(\n        args.src_model, \n        args.tgt_model, \n        args.num_anchor, \n        args.common_vocab, \n        args.seed,\n        args.all_anchors\n        )\n\n    logger.info('Start searching for prompt embeddings for relation %s' % args.relation)\n\n    from rel2abs_GO import Rel2abs_Decoder\n    decoder = Rel2abs_Decoder(args, logger, v_base, bert_base_anchors, bert_large_anchors, tgt_distribution)\n    x = decoder.search()\n\n    save_embeddings(args, x)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--relation', type=str, default='P17')\n    parser.add_argument('--src_prompt_path', type=str, default='OptiPrompt/optiprompt-outputs/bert-base-cased')\n    parser.add_argument('--tgt_prompt_path', type=str, default='OptiPrompt/optiprompt-outputs/bert-large-cased')\n    parser.add_argument('--prompt_filename', type=str, default='prompt_vecs.npy')\n    parser.add_argument('--transferred_prompt_filename', type=str, default='transfered_prompt_vecs.npy')\n    parser.add_argument('--src_model', type=str, default='bert-base-cased')\n    parser.add_argument('--tgt_model', type=str, default='bert-large-cased')\n    parser.add_argument('--num_anchor', type=int, default=8192)\n    parser.add_argument('--common_vocab', type=str, default='OptiPrompt/common_vocabs/common_vocab_cased_be_ro_al.txt')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--num_vectors', type=int, default=5)\n    parser.add_argument('--log_path', type=str, default='OptiPrompt/output')\n    parser.add_argument('--log_tag', type=str, default='')\n\n    # args for validation\n    parser.add_argument('--relation_profile', type=str, default='OptiPrompt/relation_metainfo/LAMA_relations.jsonl')\n    parser.add_argument('--data_path', type=str, default='OptiPrompt/data/autoprompt_data')\n    parser.add_argument('--init_manual_template', action='store_true')\n    parser.add_argument('--absolute', action='store_true')\n    parser.add_argument('--topk', type=int, default=0)\n    parser.add_argument('--budget', type=int, default=2000)\n    parser.add_argument('--lr', type=float, default=5e-3)\n    parser.add_argument('--all_anchors', action='store_true')\n\n    args = parser.parse_args()\n\n    if args.init_manual_template:\n        if args.topk > 0:\n            log_file = os.path.join(args.log_path, 'training_top%d_manual%s.log' % (args.topk, args.log_tag))\n        else:\n            log_file = os.path.join(args.log_path, 'training_%d_manual%s.log' % (args.num_anchor, args.log_tag))\n    else:\n        if args.topk > 0:\n            log_file = os.path.join(args.log_path, 'training_%dtokens_top%d%s.log' % (args.num_vectors, args.topk, args.log_tag))\n        else:\n            log_file = os.path.join(args.log_path, 'training_%dtokens_%d%s.log' % (args.num_vectors, args.num_anchor, args.log_tag))\n\n\n    logger.addHandler(logging.FileHandler(log_file))\n\n    logger.info(args)\n\n    fix_seed(args.seed)\n    transfer(args)\n\nFile Path: get_emb_mix.py\nContent:\nimport numpy as np\nimport torch\nimport random\nimport argparse\nfrom rel2abs_util import get_abs_anchors\nimport os\nimport logging\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n                datefmt='%m/%d/%Y %H:%M:%S',\n                level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef save_embeddings(args, embeddings):\n    save_path = os.path.join(args.tgt_prompt_path, args.relation, args.transferred_prompt_filename)\n    with open(save_path, 'wb') as f:\n        logger.info('Saving transferred prompt embeddings to %s' % save_path)\n        np.save(f, embeddings)\n\ndef fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    # Pytorch\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef transfer(args):\n    v_base1 = np.load(os.path.join(args.src_prompt_path1, args.relation, args.prompt_filename))\n    v_base2 = np.load(os.path.join(args.src_prompt_path2, args.relation, args.prompt_filename))\n\n    src_anchors1, tgt_anchors, tgt_distribution, src_anchors2 = get_abs_anchors(\n        args.src_model1, \n        args.tgt_model, \n        args.num_anchor, \n        args.common_vocab, \n        args.seed,\n        args.all_anchors,\n        model_name3= args.src_model2, \n        )\n    \n    logger.info('Start searching for prompt embeddings for relation %s' % args.relation)\n    \n    from rel2abs_GO_mix import Rel2abs_Decoder\n    decoder = Rel2abs_Decoder(args, \n                              logger, \n                              v_base1, \n                              src_anchors1, \n                              v_base2, \n                              src_anchors2, \n                              tgt_anchors, \n                              tgt_distribution\n                              )\n    x = decoder.search()\n\n    save_embeddings(args, x)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--relation', type=str, default='P17')\n    parser.add_argument('--prompt_path', type=str, default='OptiPrompt/optiprompt-outputs')\n    parser.add_argument('--prompt_filename', type=str, default='prompt_vecs.npy')\n    parser.add_argument('--transferred_prompt_filename', type=str, default='transfered_prompt_vecs.npy')\n    parser.add_argument('--src_model1', type=str, default='bert-base-cased')\n    parser.add_argument('--src_model2', type=str, default='roberta-base')\n    parser.add_argument('--tgt_model', type=str, default='bert-large-cased')\n    parser.add_argument('--num_anchor', type=int, default=8192)\n    parser.add_argument('--common_vocab', type=str, default='OptiPrompt/common_vocabs/common_vocab_cased_be_ro_al.txt')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--num_vectors', type=int, default=5)\n    parser.add_argument('--log_path', type=str, default='OptiPrompt/analyze_embed/output/log')\n    parser.add_argument('--log_tag', type=str, default='')\n\n    # args for validation\n    parser.add_argument('--relation_profile', type=str, default='OptiPrompt/relation_metainfo/LAMA_relations.jsonl')\n    parser.add_argument('--data_path', type=str, default='OptiPrompt/data/autoprompt_data')\n    parser.add_argument('--init_manual_template', action='store_true')\n    parser.add_argument('--budget', type=int, default=2000)\n    parser.add_argument('--lr', type=float, default=5e-3)\n    parser.add_argument('--all_anchors', action='store_true')\n    parser.add_argument('--topk', type=int, default=0)\n    parser.add_argument('--absolute', action='store_true')\n\n    args = parser.parse_args()\n\n    args.src_prompt_path1 = os.path.join(args.prompt_path, args.src_model1)\n    args.src_prompt_path2 = os.path.join(args.prompt_path, args.src_model2)\n    args.tgt_prompt_path = os.path.join(args.prompt_path, args.tgt_model)\n\n    if args.init_manual_template:\n        if args.topk > 0:\n            log_file = os.path.join(args.log_path, 'mix_training_top%d_manual%s.log' % (args.topk, args.log_tag))\n        else:\n            log_file = os.path.join(args.log_path, 'mix_training_%d_manual%s.log' % (args.num_anchor, args.log_tag))\n    else:\n        if args.topk > 0:\n            log_file = os.path.join(args.log_path, 'mix_training_top%d%s.log' % (args.topk, args.log_tag))\n        else:\n            log_file = os.path.join(args.log_path, 'mix_training_%d%s.log' % (args.num_anchor, args.log_tag))\n        \n    logger.addHandler(logging.FileHandler(log_file))\n\n    logger.info(args)\n\n    fix_seed(args.seed)\n    transfer(args)",
        "experimental_info": "Common experimental settings:\n- `relation`: (str) Default 'P17'\n- `prompt_filename`: (str) Name of the source prompt embedding file, default 'prompt_vecs.npy'\n- `transferred_prompt_filename`: (str) Name for saving transferred prompt embeddings, default 'transfered_prompt_vecs.npy'\n- `tgt_model`: (str) Target model name (e.g., 'bert-large-cased')\n- `num_anchor`: (int) Number of anchor tokens to use for relative representation, default 8192\n- `common_vocab`: (str) Path to a file containing common vocabulary between models, default 'OptiPrompt/common_vocabs/common_vocab_cased_be_ro_al.txt'\n- `seed`: (int) Random seed for reproducibility, default 42\n- `num_vectors`: (int) Number of virtual tokens in the prompt, default 5\n- `budget`: (int) Number of optimization steps for searching target prompt embeddings, default 2000\n- `lr`: (float) Learning rate for the optimization, default 5e-3\n- `all_anchors`: (action='store_true') If set, use all common vocabulary words as anchors instead of sampling `num_anchor`\n- `topk`: (int) If > 0, consider only the top-k most similar anchor dimensions in the relative representation, default 0 (no top-k filtering)\n- `absolute`: (action='store_true') If set, use absolute values for `topk` selection (when `topk` > 0).\n\nSingle-source specific settings (from `get_emb.py`):\n- `src_prompt_path`: (str) Path to source prompt embeddings, default 'OptiPrompt/optiprompt-outputs/bert-base-cased'\n- `src_model`: (str) Source model name (e.g., 'bert-base-cased')\n- `tgt_prompt_path`: (str) Path to save target prompt embeddings, default 'OptiPrompt/optiprompt-outputs/bert-large-cased'\n\nMulti-source specific settings (from `get_emb_mix.py`):\n- `prompt_path`: (str) Base path for all prompt outputs, default 'OptiPrompt/optiprompt-outputs'\n- `src_model1`: (str) First source model name (e.g., 'bert-base-cased')\n- `src_model2`: (str) Second source model name (e.g., 'roberta-base')\n- `src_prompt_path1`: Derived from `prompt_path` and `src_model1`\n- `src_prompt_path2`: Derived from `prompt_path` and `src_model2`\n- `tgt_prompt_path`: Derived from `prompt_path` and `tgt_model`"
      }
    },
    {
      "title": "Co-training Improves Prompt-based Learning for Large Language Models",
      "abstract": "We demonstrate that co-training (Blum & Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.",
      "full_text": "Co-training Improves Prompt-based Learning for Large Language Models Hunter Lang MIT CSAIL hjl@mit.edu Monica Agrawal MIT CSAIL magrawal@mit.edu Yoon Kim MIT CSAIL yoonkim@mit.edu David Sontag MIT CSAIL dsontag@mit.edu Abstract We demonstrate that co-training (Blum & Mitchell, 1998) can improve the performance of prompt- based learning by using unlabeled data. While prompting has emerged as a promising paradigm for few-shot and zero-shot learning, it is often brit- tle and requires much larger models compared to the standard supervised setup. We ﬁnd that co- training makes it possible to improve the original prompt model and at the same time learn a smaller, downstream task-speciﬁc model. In the case where we only have partial access to a prompt model (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a calibration model over the prompt outputs. When we have full access to the prompt model’s gradients but full ﬁnetuning remains pro- hibitively expensive (e.g., T0 (Sanh et al., 2022)), we learn a set of soft prompt continuous vectors to iteratively update the prompt model. We ﬁnd that models trained in this manner can signiﬁcantly im- prove performance on challenging datasets where there is currently a large gap between prompt-based learning and fully-supervised models. 1. Introduction Prompt-based learning, in which a pretrained language model is adapted to various tasks by priming on natural language prompts, has emerged as a promising framework for few-shot and zero-shot learning (Brown et al., 2020; Liu et al., 2021; Wei et al., 2021; Sanh et al., 2022). While intriguing, these methods can be sensitive to trivial cosmetic artifacts, including variations in prompt wording and the ordering of examples (Lu et al., 2021; Zhao et al., 2021; Kumar & Talukdar, 2021). Further, the models used in prompt-based learning (e.g., GPT-3, T0) are much larger than those typically used for standard ﬁne-tuning. These fac- tors make prompt-based learning difﬁcult to use and deploy in practice. Given a small amount of labeled data, one could evaluate the performance of each prompt and re-calibrate the prompt outputs to improve performance. However, (i) this reliance on labeled data goes against the goal of few-shot learning, and (ii) even with oracle calibration, some prompts have sub-par accuracy. Recently, to address issue (i), Zhao et al. (2021) developed a data-free calibration method that can dramatically improve the accuracy of few-shot prompts for GPT-3. We build on their work by showing how to use unlabeled data to further improve performance. To leverage unlabeled data, we use co-training (Blum & Mitchell, 1998), which operates on two views of each data point X: φ0(X) and φ1(X). For example, in a clinical diagnosis system, φ0(X) could be laboratory test results and φ1(X) an X-ray image. A pair of models ( h0 and h1 respectively) takes turns labeling a large unlabeled training set, and each model is trained on the conﬁdent pseudo-labels from the other. Model h0 only uses φ0(X), and model h1 uses φ1(X). By using complementary information in the views φ0,φ1 and the different inductive biases from models h0, h1, co-training allows each model to learn from the other without the need for labeled data. The initial signal to start the co-training process is provided by a “guess” at a model h0. To combine co-training and prompt-based learning, we use outputs from a large prompt-based model as φ0(X) and the pre-trained representation from a much smaller language model (e.g., DeBERTa (He et al., 2021)) as φ1(X). We specify the models h0 and h1 based on whether we have partial access to the prompt model (querying GPT-3) or full access (locally training T0). In partial access, we only have access to the large model’s output probabilities. In this case, we use unlabeled data to learn a model h0 that both calibrates individual prompts and ensembles multiple prompts. We refer to this as the label model. We use Calibrate-Before-Use (Zhao et al., 2021) to initialize the calibration parameters of this model for each prompt, and we initialize the ensembling parameters to ap- proximate majority vote. We then reﬁne this initial guess for h0 with co-training. We use the pre-trained representation from DeBERTa (He et al., 2021) for φ1(X) and train the last few layers of that model as h1. The only labeled data used is the set of k examples used in the input prompts. Figure 1 (left) shows the co-training process in the partial access setting. We also study a full access setting using T0 (Sanh et al., 2022) instead of GPT-3, so we can introspect the large prompt model. We derive the viewφ0(X) and the model h0 from T0. However, instead of fully ﬁne-tuning T0 during co-training, we focus on soft prompt tuning, which trains several orders-of-magnitude fewer parameters while attain- ing similar performance (Li & Liang, 2021; Lester et al., 2021). The parameter space for model h0 is the set of soft prompts, which are matrices RL×d, where Lis a sequence 1 arXiv:2202.00828v1  [cs.CL]  2 Feb 2022GPT-3 Output labelsMLP (ℎ&)Label model (ℎ() Co-train Example formatted as !prompts {{premise}} Question: {{hypothesis}} True or False?Given {{premise}} and what you know about the world, does {{hypothesis}} follow? Yesor No? {{premise}} Question: {{hypothesis}} Yesor no? {{premise}} {{hypothesis}}Unlabeled example*BERT* Contextual embedding \"\"$Output probabilities \"!$ T0 embedding {{premise}} Question: {{hypothesis}} True or False?Example formatted as a hard prompt Input embedding \"!($) Output labelsMLP (ℎ&) Soft prompt (ℎ!) T0 model {{premise}} {{hypothesis}}Unlabeled example*BERT* Contextual embedding \"\"$ Co-train Figure 1: The setup for our two applications of co-training to prompting for a binary entailment classiﬁcation dataset (RTE). Parameters in blue are trainable; models in gray are ﬁxed. Left: training a “label model” for post-hoc calibration and ensembling of multiple prompts. Here the prompts and the model (GPT-3) are ﬁxed, and we co-train the calibration / ensembling parameters with the task-speciﬁc model (e.g., DeBERTa). Right: training a soft prompt. Here the input is encoded as a hard prompt and the embedding matrix of the input sequence is obtained. A L×dmatrix of trainable parameters (the “soft prompt”) is prepended to this embedding, and the combined embedding sequence is passed through T0 to get output predictions. We co-train the soft prompt with the view 1 model (e.g., DeBERTa). length hyperparameter and is dthe dimension of the pre- trained T0 embeddings. Each row of the soft prompt mimics the embedding of a token, but the soft prompt need not corre- spond to the embedding of any actual token sequence. This matrix is prepended to the input embedding and the output of h0 is computed with the frozen T0 model. The initial guess at h0 (i.e., the initial soft prompt vector for use in co-training) is the repeated embedding of the [PAD] token. Since T0 was trained to perform well at zero-shot learning with prompts, this provides a good initial hypothesis. We co-train this model with a pre-trained DeBERTa represen- tation as φ1(X) and the last few layers of DeBERTa ash1. This is is shown in Figure 1, right. We apply our approach to standard few-shot and zero- shot benchmarks and ﬁnd that (i) iteratively co-training both models using unlabeled data consistently improves perfor- mance, (ii) pseudo-labels from a prompted model are an effective signal for ﬁne-tuning smaller task-speciﬁc models, and (iii) this approach can signiﬁcantly improve results on datasets previously considered difﬁcult for prompt-based learning. We conclude with a brief analysis of success and failure cases and describe high-level criteria required for our method to work. 2. Related work Prompting and prompt tuning. Lu et al. (2021) ﬁnd op- timal orderings of prompt examples based on an artiﬁcially constructed development set. Given the variance in per- formance across different prompts, others have focused on engineering suitable prompts, manually or otherwise (Liu et al., 2021). Jiang et al. (2020), Shin et al. (2020), and Gao et al. (2021) use data-driven techniques and language models to automatically generate candidate prompts. Rather than being constrained to human-readable prompts, Li & Liang (2021) and Lester et al. (2021) instead learn a con- tinuous soft task-speciﬁc “prompt” to condition language models. While effective, these methods typically require nontrivial amounts of labeled data. Another line of work uses the outputs from a prompted language model as weak labels, as we do in this work. Wang et al. (2021) propose to train smaller models on labels from GPT-3 to reduce annotation cost, but they train from indi- vidual, uncalibrated prompts and do not attempt to reﬁne the prompt model alongside the smaller model. Schick & Sch¨utze (2021) ﬁne-tune a separate RoBERTa model for each prompt using a small amount of labeled data. They next aggregate the outputs of these individual ﬁne-tuned models as a soft pseudo-label and train a ﬁnal model to match the soft aggregation. In contrast, we train a single BERT-style model on the ensembled prompt output without any additional labeled data. We use this model to reﬁne the ensemble parameters (and vice-versa). In our approach we only use prompt outputs as training signal, and we consider different types of prompts (open-ended instead of cloze). Self-training for few-shot text classiﬁcation. Our work relies on access to a large amount of unlabeled data to it- eratively grow a conﬁdently-labeled training set for each model. Similarly, self-training ﬁrst trains a model on a small set of initial data, uses the trained model to produce pseudo-labels on a set of unlabeled data, and then iteratively includes the conﬁdently pseudo-labeled data as new train- ing labels (Scudder, 1965). In the context of few-shot text classiﬁcation, Mukherjee & Awadallah (2020) develop an uncertainty-aware technique for choosing which data points 2to include, which requires a small amount of labeled data. Karamanolakis et al. (2019, 2021) employ self-training and iterative co-training with weak supervision as the initial label signal, and they similarly use a neural network with pretrained embeddings as a downstream model. However, they explore hand-written or keyword-based rules as weak supervision, in contrast to the present work, where we derive our weak signals from prompted models. The parameter- ization of h0 in our partial access setting is similar to the weighting they use to combine rules. Co-training. Co-training dates back to Blum & Mitchell (1998), who assumed that φ0(X) and φ1(X) are two dis- tinct views and conditionally independent given the true label. Under this strict condition, they proved that the al- gorithm ﬁnds a good classiﬁer after just one step. Many subsequent analyses (e.g., Dasgupta et al., 2002; Balcan et al., 2005) relax this condition, showing that views can be dependent or even identical as long as certain relation- ships hold between the models being trained (essentially, they are “different enough”). In a similar vein, Wei et al. (2020) give a theoretical explanation of why (and when) models can learn to be more accurate than the pseudo-labels used to train them. We take implicit advantage of these results in our work. The views we use are highly dependent, and yet the models we train are often able to outperform the pseudo-labels we used to train them in each co-training iteration. 3. Co-training with prompting The skeleton of our approach is shown in Algorithm 1 (full detail is provided in Algorithms 4 and 5 in the supplement). First, a hypothesis h0 over view φ0 is initialized such that its initial predictions are reasonable. (We discuss initialization in depth in the following sections.) Next, we obtain the conﬁdently labeled training data L0 0, which is a subset of the unlabeled data points, together with pseudo-labels for those points from h0. In iteration t, we select a β + tβ′ fraction of the data. (We discuss techniques for selection of conﬁdent data in Section 4 and the choice of β and β′ in Section 5.) These conﬁdently-labeled points are then used to train a model h1 on view φ1, and h1’s conﬁdently- labeled data is extracted as L0 1. This is used to train a new h0, and the process continues for T steps. Train performs standard supervised training on the pseudo-labels for that iteration. In this section, we give details for how to construct the views φ0 and φ1, the hypothesis classes we use for the model h0, and the initialization schemes for h0 in both the partial access and full access settings. 3.1. Partial access: co-training a label model In the usual few-shot setting with prompting, k labeled examples ({xi,yi})k i=1 are converted into a single natural language prompt following a prompt template. We call this prompt k-shot, since it uses klabeled examples. Instead of using one k-shot prompt, we use kone-shot prompts, only including one example in the template at a time. This gives us koutputs. Separating out the signal from each labeled Algorithm 1 Co-training algorithm input U= {xn}U n=1 unlabeled examples input {(xj,yj)}k j=1 labeled examples (optional) input initial coverage β, coverage increase β′ h0 ←InitClassifier(φ0) for tin {0,...,T −1}do ˜β ←β+ tβ′ // GetConfData∗deﬁned in Algorithms 2, 3 Lt 0 ←GetConfData∗ ( U; h0,φ0,˜β ) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfData∗ ( U; h1,φ1,˜β ) h0 ←Train(φ0,Lt 1) end for return (h0,h1) example in this way allows us to combine the examples more effectively than the one k-shot prompt model. View. Let φ(i) 0 (x) ∈R|V|be the vector of probabilities output by GPT-3 on input xformatted in a one-shot prompt with labeled example (xi,yi). Here V is a subset of the full token vocabulary—the verbalizer tokens—and consists of the “label tokens” for the prompt as well as other tokens related to the label. For example, in sentiment analysis, if x1 is “this movie was great!”, φ(1) 0 (x) is GPT-3’s output on: Review: this movie was great! Positive or Negative? Positive Review: {{x.review}} Positive or Negative? and V might include the label tokens Positive / Negative and related tokens such as uncased label tokens or synonyms.1 To select the verbalizer tokens in a task- agnostic way, we obtain the top 10 predictions for GPT-3 on each prompt, sort tokens by the total probability assigned to them on the unlabeled training set, and choose the top 25%. This ensures that other frequent tokens appear in the feature set for φ0. For example, Date appears for TREC question classiﬁcation even though the closest true label category is Number. The label model automatically learns this associa- tion during the co-training iterations. Henceforth we assume that V is ordered and the ﬁrst elements of V correspond to the label tokens. By concatenating φ(i) 0 (x) for each of the k labeled examples, we obtain a matrix φ0(x) ∈Rk×|V|, the ﬁrst view for co-training. The second view, φ1(x), is the frozen representation of a pretrained model like DeBERTa (He et al., 2021). In our experiments, we use the representa- tion in the penultimate layer as φ1(x), and the hypothesis class over this view is the last layer and the linear classiﬁer. Hypothesis class. This leaves the hypothesis class for model h0: how do we combine kprompt signals into one pseudo-label? Probabilities from these models are often miscalibrated (Zhao et al., 2021), and thus averaging or majority voting does not yield good results. Instead, we 1In the running sentiment analysis example we might have V = {Negative, Positive, negative, positive, bad, good, ... }. 3propose to learn a label model that scales each prompt vec- tor φ(i) 0 (x) by a prompt-speciﬁc calibration matrix W(i) before averaging. The combined architecture for this model is given by h0(x; W,α):2 li = ReLU ( W(i)φ(i) 0 (x) ) ; h0(x; W,α) = softmax ( k∑ i=1 αili ) , (1) where α ∈Rk is a vector of weights for ensembling the scaled prompt outputs. The ReLU(·) allows the model to easily ignore particular prompt/label combinations. For ex- ample, if prompt j has very poor precision when it out- puts label z, setting W(j) zz to be negative causes ljz to be 0. Note that we directly calibrate probabilities rather than log-probabilities, following Zhao et al. (2021) (i.e., φ(i) 0 ∈ [0,1]|V| and ∥φ(i) 0 ∥1 = 1). In each iteration of co-training, we train this model using the standard cross- entropy loss on the conﬁdent data for that iteration. Initialization. The calibration matrices W(i) are initial- ized using Calibrate-Before-Use (CBU) (Zhao et al., 2021), which ﬁrst computes the probability vector φ(i) 0 (xcf) on content-free inputs xcf (e.g., N/A or the empty string), and then uses these as a scaling factor: W(i) = Diag ( 1 φ(i) 0 (xcf) ) . This initialization scheme ensures that the scaled prompt outputs are neutral on truly neutral inputs, improving cal- ibration. We also set αi = 1 for each i ∈{1,...,k }to initially weight each prompt equally in the ensemble. When V consists of more than just label tokens, we initialize W(i) in blocks: we use CBU for the label tokens, and initialize the rest of W to be 0. That is, for an l-way classiﬁcation task, where W(i) ∈Rl×|V|, we initialize the ﬁrst lcolumns of W(i) using CBU (since we assumedV was ordered, these columns correspond to the label tokens) and set the rest to 0. Hence only the label tokens (e.g., Negative, Positive) are used at initialization, but subsequent iterations of co- training can use nonzero weights on the extra tokens. 3.2. Full access: co-training a soft prompt In this setting, our prompt model is the T0 model (Sanh et al., 2022), which achieves zero-shot generalization by ﬁne-tuning T5 (Raffel et al., 2020) on multiple tasks whose labeled examples have been transformed into natural lan- guage question-answer pairs. Since T0 is publicly available and smaller than GPT-3, we can introspect the model and compute gradients in this case. View. We set φ0(X) to the initial word embeddings of T0 and leave φ1(X) and h1 unchanged (i.e., φ1 is the penulti- mate layer of a pretrained DeBERTa representation). 2Here we use W to refer to {W(i)}k i=1. Hypothesis class. The model h0 is parameterized by a continuous soft prompt (Li & Liang, 2021; Lester et al., 2021). Concretely, letting d= 2048be the dimension of the T0 word embeddings, a soft prompt is a matrix of parameters P ∈RL×d, where Lis a sequence length hyperparameter. Each row of the soft prompt acts like the embedding of a “token” (but needn’t correspond to the embedding of any real token—i.e., there are no constraints on P). The hypothesis h0(x; P) is thus given by prepending the soft prompt to the input word embedding sequence and using the concatena- tion (P; φ0(X)) as input to T0. The subsequent T0 layers are frozen and not updated during training. Given enough la- beled data, soft prompt tuning can match the performance of full-ﬁne-tuning with far fewer trainable parameters (Lester et al., 2021; Le Scao & Rush, 2021).3 Initialization. T0 is speciﬁcally trained to perform well at zero-shot tasks with a variety of hard prompts, so using a hard prompt out-of-the-box gives good initial performance. Hence, to initialize a soft prompt hypothesis, we encode the input using a hard prompt and then set the soft prompt to be the repeated embedding of the tokenizer’s padding token. Using the RTE dataset (Dagan et al., 2005) as a running example, we ﬁrst encode the input using a hard prompt, where each input example xis formatted as: {{x.premise}} Question: {{x.hypothesis}} True or False? We then set h0 to be the repeated embedding of the T0 padding token, i.e., at initialization the T0 model sees: [PAD]...[PAD]{{x.premise}} Question: {{x.hypothesis}} True or False? This combination of hard prompt encoding with soft prompt- ing differs from the usual soft prompting setup (Li & Liang, 2021; Lester et al., 2021). We discuss this issue in more depth in Section B.3. 4. Selecting conﬁdent data The key step in co-training is selecting conﬁdently-labeled data for use in the next training iteration. The literature on co-training has identiﬁed a large number of methods for per- forming this data selection (GetConfData, in Algorithm 1). We consider two simple approaches in this work: model conﬁdence and cut statistic. In both cases, we specify an ini- tial coverage fraction βand a coverage increase fraction β′. Given U unlabeled examples, the amount of pseudo-labeled data in round t≥0 is therefore U(β+ tβ′). Model conﬁdence. For model conﬁdence, we sort every example by the scores output by each model and select the top β+ tβ′fraction in iteration t. While simple, this can result in very imbalanced updates to the pseudo-labeled dataset if the model is only conﬁdent for one label or if one label is inherently more noisy than the others. If additional 3We use L = 20in our experiments, following Lester et al. (2021), so the soft prompt has 20 ×2048 = 40960parameters. 4knowledge regarding the marginal label distribution is avail- able (e.g., approximate label balance or a constraint on the minimum label frequency), we can imbue this knowledge into the data selection process by grouping examples by their predicted label and then performing the sort-and-select procedure for each label separately. Knowledge of the ap- proximate label balance is a standard assumption in weak supervision (e.g., Fu et al., 2020), but we make a much weaker assumption when using the model conﬁdence rank- ing: we assume we know a lower bound γsuch that for all labels y, P[Y = y] ≥γ. We set γ = 0.01, i.e., that every class accounts for at least 1% of the data. The detailed pro- cedure for conﬁdent data selection using model conﬁdence is shown in Algorithm 2 (supplement). Cut statistic. The cut statistic is a ranking heuristic that uses the view geometry more than the model conﬁdence approach (Muhlenbach et al., 2004; Zhang & Zhou, 2011). Suppose we want to select data conﬁdently labeled by a model over view φ(X) (we omit the subscript ifor clearer notation). First, we form a graph G = (V,E) with one vertex for each unlabeled training example and edges con- necting vertices who are K-nearest neighbors in φ(X) (or a representation related to φ(X)—for example, for T0 we can use a contextual representation from inside the model instead of the uncontextual embeddings φ0). Let ˆY(X) = argmaxh(φ(X)) be the hard pseudo-label assigned to input X by model h. We say an edge (xu,xv) is cut if ˆY(xu) ̸= ˆY(xv). Intuitively, we can feel conﬁdent about examples that have few cut edges, since they have the same label as most of their neighbors. Regions of Gwith high noise are less likely to be correctly labeled. The cut statistic heuristically quantiﬁes this idea to rank examples. Suppose (as a null hypothesis) that the labels ˆY were sampled i.i.d. from the marginal distribution P[ ˆY = y] (i.e., independently of X). For vertices uand vcorrespond- ing to examples xu, xv, deﬁne Iuv = I[ ˆY(xu) ̸= ˆY(xv)]. Consider the test statistic: Ju = ∑ v∈N(u) wuvIuv,where wuv = 1/(1 +∥φ(xu) −φ(xv)∥2) are edge weights that decrease as the distance between u and v increases, and N(u) are the neighbors of u. The mean of Ju under the null hypothesis is: µ= (1−P[ ˆY(xu)]) ∑ v∈N(u) wuv,and the variance is: σ2 = P[ ˆY(xu)](1 −P[ ˆY(xu)]) ∑ v∈N(u) w2 uv. Following Zhang & Zhou (2011), we approximate the dis- tribution of J with a normal distribution of mean µ and variance σ2. Then we can rank examples xu by the left- sided tail probability for Ju (lower is better). If Ju is much smaller than expected, then the total cut edge weight is much smaller than expected under the null hypothesis. To select conﬁdent data, we sort examples by Ju and choose the top β+ tβ′fraction in iteration t. The detailed procedure for conﬁdent data selection using the cut statistic is shown in Algorithm 3 (supplement). 4.1. Relabeling. Pseudo-labels from previous iterations can either be re-used or thrown out. If the initial hypothesis has high precision but low coverage, it is typically preferable to re-use the pseudo-labels from previous iterations, since as coverage increases the quality of the pseudo-labels is likely to go down. On the other hand, if the models being trained are capable of correcting incorrect pseudo-labels, it is preferable to relabel, since this can improve the quality of the training data in each iteration. We exclusively use the latter, since we found that the pseudo-label accuracy on the covered subset of data often increased with more iterations. The original co-training algorithm (Blum & Mitchell, 1998) builds L cumulatively, but subsequent co-training procedures also use relabeling (Zhang & Zhou, 2011). We discuss relabeling further in Section A.1. 5. Experiments Datasets. We investigate the beneﬁt of co-training on sev- eral standard natural language benchmarks, focusing on datasets with a large gap between the best prompt-based methods and fully-supervised learning (Wang et al., 2019b,a; Brown et al., 2020). We use the RTE (Dagan et al., 2005), CB (De Marneffe et al., 2019), TREC (V oorhees & Tice, 2000), and BoolQ (Clark et al., 2019) datasets. Full details for these datasets are in Appendix B. In the partial access set- ting, we do not evaluate on BoolQ due to the large amount of GPT-3 quota required for labeling. In the full access setting, we do not evaluate on TREC as T0 was pretrained on TREC. Training methods, partial access. In the few-shot setting we randomly select k = 4 training examples from each dataset until the initial label model assigns every pseudo- label at least γβU times (i.e., we resample prompts until we can initialize the label model in accordance with the constraint that P[Y = y] ≥γ for all y). While larger k might improve performance, k = 4gives a good balance between performance and GPT-3 quota usage. (Indeed, with our 4 one-shot prompts, we are able to beat the GPT-3 32- shot accuracy on CB). In each co-training iteration, we train the label model over view φ0 using Adam with learning 1e-4, weight de- cay 5e-3, and batch size 64 for 40 epochs. We train the DeBERTa-large model overφ1 for 20 epochs using Adam with learning rate 1e-5, weight decay 0.01, batch size 16. All parameters were frozen except the last language model layer, the pooler, and the linear classiﬁcation layer. In order to avoid indirect label leakage, we did not tune these hy- perparameters and instead chose common hyperparameters used for these types of models. For early stopping, each model was evaluated every epoch on a pseudo-labeled vali- dation set and the best model checkpoint was chosen based on balanced accuracy on the pseudo-labels at the end of each round. Using the balanced accuracy (average of the recall for each label) avoids collapsing to the majority class even when the pseudo-labels are relatively imbalanced. This validation set was sampled uniformly from the training set to give a training/validation split of 90%/10%. To determine β, β′,T for co-training, we performed a light hyperparameter search based on performance on a gold-labeled validation set of 500 examples sampled from 5the TREC training set.4 This resulted in the the following values: initial coverage of β = 0.5, per-step coverage in- crease of β′= 0.1, and total co-training steps T = 5. We emphasize that this gold validation set was not used during any co-training iteration (e.g. for model selection, early stopping, learning rate tuning, etc.) We set the minimum label frequency γ = 0.01 and did not tune this value. We used model conﬁdence to add conﬁdent data in view 0 and the cut statistic to add conﬁdent data in view 1.5 We used the [CLS] token embedding in the last layer of DeBERTa for cut statistic nearest neighbors in view 1. We used K = 20 nearest neighbors for the cut statistic and performed no tuning on this value. Training methods, full access. In the zero-shot setting, for training the soft prompt over view φ0(x) we mainly used the hyperparameters suggested by Lester et al. (2021), which were obtained by performing gold soft prompt tuning using T5 on SuperGLUE. We used Adafactor with constant learning rate 0.3, weight decay 1e-5, and batch size 24 for 30000 training steps. For DeBERTa-large, we used the same hyperparameters as in the partial access setting. As in the partial access setting, we used balanced pseudo-label accuracy to select the best model checkpoint at the end of each training round. We used the cut statistic for conﬁdent selection in both views, since with T0 we have access to the internal embeddings, unlike with GPT-3. We used the T0 decoder’s contextual embedding for the ﬁrst decoded token to compute nearest neighbors for the view 0 cut statistic. Training details (e.g., β,β′,T, etc.) are otherwise exactly the same as in the partial access setting. During training, the pseudo-label for each example is ﬁrst mapped to a token that matches the hard prompt (e.g. 0→True and 1→False for the RTE example above). These token labels are then mapped to embedding indices using the T0 tokenizer, and the soft prompt is trained via regular sequence-to-sequence training with the maximum likelihood objective. This is identical to the soft prompt training technique from Lester et al. (2021). Caveat. As noted by Perez et al. (2021), much current work on prompt-based learning does not constitute “true” few-shot/zero-shot learning as they often implicitly assume access to a small labeled set to select various model conﬁg- urations (e.g., prompts and hyperparameters). Insofar as we inherit such conﬁgurations from existing work, our work is similarly not few-shot/zero-shot in the strictest sense, al- though we tried to minimize such issues by using exactly the same co-training parameters (β,β′,T,γ ) and model hy- perparameters for all datasets. (We also did not perform an extensive tuning of these parameters.) While we are encour- aged by the observation that model conﬁgurations seem to work well across diverse datasets, investigating co-training in the context of true few-shot/zero-shot learning (Schick & 4Hence our few-shot experiments on TREC are not few-shot in the truest sense of the term. 5This works better than using the cut statistic in both views—since the cut statistic relies heavily on good nearest neighbors, it makes the most sense in a view that already has a good distance function for examples (the pretrained DeBERTa representation). Sch¨utze, 2021) is an important avenue for future work. Baselines. For baselines we compare against: • GPT-3 32-shot: From Brown et al. (2020). 32 examples combined in one prompt. Uncalibrated. • Calibrate Before Use: Performance of CBU using 4-shot prompts (from (Zhao et al., 2021)). • Prompt-based FT: Our reproduction of the ﬁne-tuning method from (Gao et al., 2021), using 2 labels per class. • Snorkel on GPT-3 output: Snorkel generative label model, which aggregates over the four GPT-3 1-shot outputs with- out using any labeled data. (Ratner et al., 2016, 2020). • Snorkel + DeBERTA-large: DeBERTA-large ﬁne-tuned on outputs from Snorkel label model using the same hy- perparameters as our co-training methods. • Label Model (no co-training): the label model after ini- tialization with (1). The baselines that use (roughly) the same amount of labeled data as our method are shown in the top section of Table 1 and Table 2. The bottom sections contain baselines that use more labeled data, including oracle upper-bounds based on full training data. Training details for the baselines are in Section B.2. 5.1. Results Table 1 shows the results for the partial access setting, where we co-train the label model, which calibrates and combines multiple GPT-3 outputs, with a smaller pretrained model (DeBERTa-large). For view 0, our co-trained label model (Label Model + co-training) improves over the initial label model (Label Model before co-training) and the average per- formance of GPT-3 4-shot before (GPT-3 4-shot) and after (Calibrate Before Use) calibration. It also improves over Snorkel on GPT-3, which, like our method, uses unlabeled data to combine the outputs of our four 1-shot prompts. For CB, the co-trained label model outperforms GPT-3 32-shot despite only using 4 labeled examples. This suggests that us- ing unlabeled data to learn to ensemblek1-shot prompts can be more label-efﬁcient than putting allklabeled examples in one prompt. For TREC and CB, the co-trained label model also outperforms prompt-based ﬁne-tuning (Prompt-based FT (Gao et al., 2021)) with the same amount of labeled data (Prompt-based FT also uses a gold-labeled validation set of kexamples per class, whereas our method only uses a pseudo-labeled validation set). For RTE and CB, we nearly match the fully-supervised performance on view 0 (Label Model on full train), suggesting that co-training is able to extract nearly all of the signal from the GPT-3 probabilities in these cases without using any extra labeled data. For view 1, the co-trained DeBERTa-large model outper- forms all of the baselines that use the same amount of label information. For RTE and TREC, it outperforms Prompt- based FT even when the latter uses 4x (for RTE) and 12x (for TREC) the number of labeled examples (Prompt-based FT with 8 labels per class). This suggests that the pseudo- 6Table 1: Few-shot learning results. (Top) Results against various baselines that use exactly 4 labels per dataset (except for Prompt-based FT, which uses 8 labels for CB and 12 labels for TREC, since this approach uses labels at the “per-class” level). GPT-3 and CBU results are copied from Zhao et al. (2021), while we train our own Prompt-based FT (Gao et al., 2021) and Snorkel (Ratner et al., 2020) models. (Bottom) Results from baselines trained on more data (for reference only). Standard deviation (when applicable) numbers are given by 4 runs over prompts (GPT-3, CBU) or random seeds (Snorkel, Prompt-based FT, Co-training). †rows show accuracy on the private SuperGLUE test set. Otherwise, the accuracies are on the public SuperGLUE validation sets, which we treated as a test set. Model View RTE (2-class) CB (3-class) TREC (6-class) GPT-3 4-shot (from Zhao et al. (2021)) * 58.7 (11.9) 45.2 (19.4) 60.2 (7.6) Calibrate Before Use (CBU) (Zhao et al., 2021) * 60.4 (8.1) 60.7 (6.7) 69.7 (1.4) Prompt-based FT (Gao et al., 2021) * 52.8 (0.9) 84.4 (3.2) 54.8 (2.9) Snorkel on GPT-3 (Ratner et al., 2020) φ0 59.6 (0.0) 70.2 (0.8) 65.2 (0.0) Snorkel on GPT-3 + DeBERTa-large φ1 67.2 (0.5) 81.6 (2.2) 63.3 (0.4) Label Model (no co-training) φ0 62.8 76.8 77.2 Label Model + co-training φ0 64.9 (1.1) 83.5 (2.3) 78.3 (1.2) DeBERTa-large +co-training φ1 67.4 (2.3) 86.2 (3.2) 80.6 (1.1) Label Model on full train φ0 67.8 (0.5) 82.7 (0.8) 91.9 (1.1) DeBERTa-large on full train φ1 93.3 95.2 96.7 GPT-3 32-shot†(Brown et al., 2020) * 69.0 75.6 * Prompt-based FT with 4 labels per class * 48.3 (3.3) 84.4 (4.4) 58.8 (5.9) Prompt-based FT with 8 labels per class * 56.1 (3.2) 87.5 (0.0) 80.0 (4.8) Table 2: Zero-shot learning results with T0 as the initial view 0 model and DeBERTa as the second model. We also show the results trained on the full dataset in the bottom two rows. For T0, we take the best prompts from Sanh et al. (2022) and replicate their results as exact numbers for each prompt were not provided in the original paper. Standard deviations are not provided in this case as even a single run takes a nontrivial amount of time. Model/Algorithm View RTE CB BoolQ T0-3B (best) (Sanh et al., 2022) φ0 68.9 66.1 59.1 T0-3B zero-shot (no co-training) φ0 68.9 58.9 56.4 T0-3B soft prompt + co-training φ0 87.0 67.9 49.1 DeBERTa-large +co-training φ1 86.3 67.9 48.9 T0-3B soft prompt on full train φ0 90.6 80.4 86.9 DeBERTa-large on full train φ1 93.3 95.2 86.1 labels provided by (a learned ensemble of) prompts are an effective training signal for smaller models. Table 2 shows the results for the full access setting with T0. For RTE and CB, co-training improves on the per- formance of the initial zero-shot prompt (T0-3B zero-shot (no co-training) ). For RTE, the co-trained view 0 and view 1 models nearly match the performance of their fully- supervised counterparts. The difference in co-training per- formance on RTE in Table 1 and Table 2 shows the beneﬁt of having full access to h0. Since we can introspect the prompt model, we can use the cut statistic in both views. In the ﬁrst step of co-training, the cut statistic on view 0 se- lects conﬁdent data with 90%-accurate pseudo-labels. The conﬁdent data selection on CB is similarly good: in the ﬁrst co-training step, the cut statistic selects pseudo-labels with 89% accuracy. The pseudo-labels extracted by the view 1 model after the ﬁrst step of co-training (L0 1) are 98% accu- rate, so after training on the initial pseudo-labels, the view 1 model is able to select a very high quality training set at coverage β = 0.5. However, the CB performance is worse than in Table 1 despite the strong initial signal in L0 0 and the near-perfect training data in L0 1. Similarly, for BoolQ, co-training makes the soft prompt worse than the initial zero-shot model. We explore the reasons behind this below. 5.2. When (and how) does co-training work? Figure 2 (left) shows the evolution of the test accuracy for h0 and h1 over co-training iterations on TREC (from Ta- ble 1). Multiple rounds of co-training increase performance for both views as the models become more precise and the coverage increases. Figure 2 (right) shows the precision of the conﬁdent data extracted by h0 for each iteration of co-training, broken down by label. This ﬁgure shows two distinct phenomena. For most labels, precision decreases as coverage goes up, as we expect from usual co-training theory (see e.g. Balcan et al., 2005). However, for label 1, precision actually increases over iterations. Model h1 (De- BERTa) is able to select new conﬁdent data for label 1 that is 70 1 2 3 4 Iterations 0.72 0.74 0.76 0.78 0.80Test Accuracy TREC Accuracy h0 h1 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Precision TREC Confident h0 Precision Label # 0 1 2 3 4 5 Figure 2: Partial access setting, TREC. Left: Test accuracy vs co-training iteration for the label model h0 and the De- BERTa modelh1. Right: precision per label vs co-training iteration, h0. 0 1 2 3 4 Iterations 0.60 0.65 0.70 0.75Test Accuracy CB Accuracy h0 h1 0 1 2 3 4 Iterations 0.6 0.7 0.8 0.9 1.0Balanced Train Accuracy CB Confident Balanced Accuracy h0 h1 Figure 3: Full access setting, CB. Left: Test accuracy vs co- training iteration for T0-3B (h0) and the DeBERTa model (h1). Right: balanced accuracy of conﬁdent pseudo-labels extracted from T0-3B (h0) and DeBERTa (h1). better than the weak labels used to train it, which improves the h0 precision for label 1 in subsequent iterations. For example, in iteration 2, h0’s conﬁdent precision for label 1 is 0.39, but after h1 is trained on that data, it proposes new conﬁdent data for label 1 with precision 0.58 (not shown in Figure 2). Pseudo-label correction is one of the beneﬁts of having two complementary models (though it can also happen with a single model with appropriate regularization (Wei et al., 2020)). Figure 3 shows what can happen when h0 and h1 are not complementary enough. The left display shows the accu- racy of each model over co-training iterations. The right display shows the balanced accuracy of the conﬁdent data extracted from each model. In the ﬁrst co-training step, h1 greatly improves over the initial h0, and selects extremely accurate conﬁdent data (nearly 100% accurate) at coverage β = 0.5. This improves the performance of the soft prompt for the next iteration (h0, left, iteration 1), but the conﬁdent balanced accuracy of h0 sharply decreases. Inspecting the training of h0 on L0 1, the soft prompting procedure appears to have overﬁt to the pseudo-labels on L0 1. Due to the small size of CB (250 training examples), coverage β = 0.5 and our 90/10 train/val split gives only 112 data points for train- ing h0 in the ﬁrst iteration, but the soft prompt is a very ﬂexible hypothesis class. This overﬁtting causes the accu- racy of conﬁdent data to decrease, which in turn degrades the performance of h0, and eventually the two models con- verge to almost identical predictors. This suggests that CB does not have enough unlabeled data for co-training to per- form well with T0, at least when β = 0.5. Using larger initial coverage (e.g. β = 1.0, β′ = 0) or a less ﬂexible hypothesis class for h0 might improve performance. Finally, Table 2 showed that co-trainingdecreased perfor- mance on BoolQ even though the initial soft prompt seemed to have reasonably strong signal (56.4% accuracy). How- ever, the ﬁner-grained statistics are less promising. After the ﬁrst training iteration, with β = 0.5, h1 assigns pseudolabel 0 to 3270 training examples with precision 0.4 and pseu- dolabel 1 to 1848 examples with precision 0.66. This means the “total noise” η= P[ ˆY = 1|Y = 0] +P[ ˆY = 0|Y = 1] in the pseudo-labels is 0.93. At the beginning of iteration t= 1, when ˜β = 0.6, the total noise in the conﬁdent data assigned by h0 is 0.98. For comparison, the total noise for the initial h0 on CB is 0.21. Even under ideal conditions on φ0 and φ1, η <1 is required for learning to work, and the sample complexity depends on 1/(1−η) (Blum & Mitchell, 1998). Unfortunately, the same issue persists on BoolQ for different βvalues. The negative result on BoolQ suggests that the initialization for h0 needs to have less total noise. A different prompt or a better initial hypothesis (e.g., full T0 instead of T0-3B) could be more amenable to co-training. 6. Conclusion Our results indicate that using unlabeled data to co-train a prompted model with a smaller model can boost the perfor- mance of prompt-based learning on few-shot and zero-shot classiﬁcation tasks. As a side effect, this procedure also produces a smaller performant model on the task of interest, distilling and reﬁning the knowledge in the large prompted model. Using two complementary models and views allows the models to learn from each other despite training on par- tially incorrect pseudo-labels. We showed that the beneﬁt of co-training is limited when the initial signal provided by the prompted model is too noisy (BoolQ, full access), when there is not enough unlabeled data to obtain good (pseudo-label) generalization performance (CB, full access), and when there is a large gap in fully-supervised accuracy on view 0 and view 1 (RTE, partial vs full access). Develop- ing methods to overcome these limitations in the context of prompting is an interesting direction for future work. Acknowledgments DS and HL were partially supported by NSF AiTF award CCF-1723344. MA was supported by the Takeda Fellow- ship. Thanks to Dr. Steven Horng of Beth Israel Deaconess Medical Center for providing access to an NVIDIA DGX machine (Horng, 2022), and thanks to NVIDIA Corporation for their donation of two NVIDIA A100 GPUs. Thanks to OpenAI and AI21 for providing quota to access their davinci and Jurassic-Jumbo models (respectively). Fi- nally, thanks to Rebecca Boiarsky and Catherine Wong for their feedback on drafts of this paper and to Aravindan Vija- yaraghavan for helpful discussions on co-training theory. 8References Balcan, M.-F., Blum, A., and Yang, K. Co-training and expansion: Towards bridging theory and practice. Ad- vances in neural information processing systems, 17:89– 96, 2005. Blum, A. and Mitchell, T. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 92–100, 1998. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surpris- ing difﬁculty of natural yes/no questions. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Min- nesota, June 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/N19-1300. URL https: //aclanthology.org/N19-1300. Dagan, I., Glickman, O., and Magnini, B. The pascal recog- nising textual entailment challenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005. Dasgupta, S., Littman, M. L., and McAllester, D. Pac generalization bounds for co-training. Advances in neural information processing systems, 1:375–382, 2002. De Marneffe, M.-C., Simons, M., and Tonhauser, J. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeu- tung, volume 23, pp. 107–124, 2019. Fu, D., Chen, M., Sala, F., Hooper, S., Fatahalian, K., and R´e, C. Fast and three-rious: Speeding up weak supervi- sion with triplet methods. In International Conference on Machine Learning, pp. 3280–3291. PMLR, 2020. Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers) , pp. 3816–3830. Association for Computational Linguistics, August 2021. URL https: //aclanthology.org/2021.acl-long.295. He, P., Liu, X., Gao, J., and Chen, W. Deberta: Decoding- enhanced bert with disentangled attention. In Proceed- ings of ICLR, 2021. Horng, S. Machine learning core. Feb 2022. doi: 10.6084/m9.ﬁgshare.19104917.v1. URL https: //figshare.com/articles/preprint/ Machine_Learning_Core/19104917/1. Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know? Transactions of the Association for Computational Linguistics , 8:423–438, 2020. Karamanolakis, G., Hsu, D., and Gravano, L. Leverag- ing just a few keywords for ﬁne-grained aspect detec- tion through weakly supervised co-training. In Pro- ceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th In- ternational Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pp. 4611–4621, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1468. URL https: //aclanthology.org/D19-1468. Karamanolakis, G., Mukherjee, S., Zheng, G., and Hassan, A. Self-training with weak supervision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 845–863, 2021. Kumar, S. and Talukdar, P. Reordering examples helps dur- ing priming-based few-shot learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, August 2021. Association for Computa- tional Linguistics. Le Scao, T. and Rush, A. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, pp. 2627–2636, Online, June 2021. Association for Compu- tational Linguistics. doi: 10.18653/v1/2021.naacl-main. 208. URL https://aclanthology.org/2021. naacl-main.208. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Associa- tion for Computational Linguistics. URL https:// aclanthology.org/2021.emnlp-main.243. Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continu- ous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pp. 4582–4597, Online, August 2021. Associa- tion for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.353. URL https://aclanthology. org/2021.acl-long.353. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of 9prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021. Lu, Y ., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Muhlenbach, F., Lallich, S., and Zighed, D. A. Identifying and handling mislabelled instances. Journal of Intelligent Information Systems, 22(1):89–109, 2004. Mukherjee, S. and Awadallah, A. Uncertainty-aware self- training for few-shot text classiﬁcation. Advances in Neural Information Processing Systems, 33, 2020. Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. arXiv preprint arXiv:2105.11447, 2021. Pilehvar, M. T. and Camacho-Collados, J. Wic: the word-in- context dataset for evaluating context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21: 1–67, 2020. Ratner, A., Bach, S. H., Ehrenberg, H., Fries, J., Wu, S., and R´e, C. Snorkel: Rapid training data creation with weak supervision. The VLDB Journal, 29(2):709–730, 2020. Ratner, A. J., De Sa, C. M., Wu, S., Selsam, D., and R´e, C. Data programming: Creating large training sets, quickly. Advances in neural information processing systems, 29: 3567–3575, 2016. Sanh, V ., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chafﬁn, A., Stiegler, A., Le Scao, T., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Confer- ence on Learning Representations, 2022. Schick, T. and Sch¨utze, H. Exploiting cloze-questions for few-shot text classiﬁcation and natural language infer- ence. In Proceedings of the 16th Conference of the Eu- ropean Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021. Schick, T. and Sch ¨utze, H. True few-shot learning with prompts – a real-world perspective. Computing Research Repository, arXiv:2111.13440, 2021. URL http:// arxiv.org/abs/2001.07676. Scudder, H. Probability of error of some adaptive pattern- recognition machines. IEEE Transactions on Information Theory, 11(3):363–371, 1965. Shin, T., Razeghi, Y ., Logan IV , R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Lan- guage Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP), Online, November 2020. Association for Computational Linguis- tics. URL https://aclanthology.org/2020. emnlp-main.346. V oorhees, E. M. and Tice, D. M. Building a question an- swering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 200–207, 2000. Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Super- glue: a stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd Inter- national Conference on Neural Information Processing Systems, pp. 3266–3280, 2019a. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analy- sis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, 2019b. Wang, S., Liu, Y ., Xu, Y ., Zhu, C., and Zeng, M. Want to reduce labeling cost? GPT-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4195–4205, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguis- tics. URL https://aclanthology.org/2021. findings-emnlp.354. Wei, C., Shen, K., Chen, Y ., and Ma, T. Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations, 2020. Wei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V . Finetuned language models are zero-shot learners. arXiv:2109.01652, 2021. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jer- nite, Y ., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State- of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing: System Demonstrations , pp. 38–45, Online, October 2020. Association for Compu- tational Linguistics. URL https://www.aclweb. org/anthology/2020.emnlp-demos.6. Zhang, M.-L. and Zhou, Z.-H. Cotrade: Conﬁdent co- training with data editing. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 41(6):1612– 1626, 2011. Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th Interna- tional Conference on Machine Learning, volume 139, pp. 12697–12706, 2021. 10A. Algorithm details A.1. Relabeling Pseudolabels from previous iterations can either be re-used or thrown out. If the initial hypothesis has high precision but low coverage, it is typically preferable to re-use the pseudolabels from previous iterations, since as coverage increases the quality of the pseudolabels is likely to go down. On the other hand, if the models being trained are capable of correcting incorrect pseudolabels, it is preferable to relabel, since this can improve the quality of the training data in each iteration. In iteration tof co-training, we extract a pseudolabeled dataset Lt i from model hi and use it to train model h1−i. Let N ⊂[U] be a set of indices that correspond to the data points conﬁdently pseudolabeled by model hi in this iteration (according to either the model conﬁdence or cut statistic rankings). Deﬁne S = {(xn,ˆyn) : n ∈N} as the set of these points together with their pseudolabels ˆyn := argmax hi(φi(xn)). Let Lt−1 i = {(xn,˜yn)}be the conﬁdent data used in the previous iteration to train model h1−i. For xn that appear in Lt−1 i but where n ∈N , we have a choice to make: do we use the old pseudolabel ˜yn, or the new one ˆyn? These need not agree, since hi has been updated. Let S′ = {(xn,ˆyn) ∈S |¬∃y : (xn,y) ∈Lt−1 i }be the set of newly pseudolabeled examples—points that do not appear in Lt−1 i with any pseudolabel. If we choose to re-use the pseudolabels from the previous iteration, the GetConfData update is: Lt i ←Lt−1 i ∪S′ On the other hand, if we throw out the previously pseudola- beled data, the update is simply: Lt i ←S We exclusively use the latter, since our models can learn to correct bad initial labels (see Figure 2, Label 1). We found that the pseudolabel accuracy on the covered subset of data often increased with more iterations. This relabeling technique is different from the original cotraining algorithm (Blum & Mitchell, 1998), which builds Lcumulatively, but subsequent cotraining procedures also use relabeling (Zhang & Zhou, 2011). A.2. Warm starting Instead of warm-starting the models h0 and h1 (initializing them from the output of the previous iteration), we initialize them from scratch each co-training iteration to reduce the effect of a “bad” training iteration and so that we can use the same training hyperparameters for every iteration. This takes advantage of the fact that there exists a robust set of initial hyperparameters that have been shown to work well for ﬁne-tuning large language models. However, further exploration of warm-starting is an interesting direction for future work, since it may yield signiﬁcant reduction in the computational burden of co-training. A.3. Conﬁdent data selection Algorithm 2 shows how to select conﬁdent data using model conﬁdence, and Algorithm 3 shows how to select conﬁdent data using the cut statistic. As mentioned in the main text, φ0 and φ1 themselves needn’t be the representations used to compute nearest neighbors for the cut statistic. For example, φ0(x) for T0 is the non-contextual T0 input embedding ofx. Instead of computing nearest neighbors in this view, we use the contextual embedding from much later in the T0 model: the ﬁnal decoder embedding of the ﬁrst decoded token. This is a function of both φ0(x) and the current hypothesis h0. Because the embeddings are contextual, this representation has better nearest neighbors than φ0(x); because it also takes h0 into account, these neighbors are adapted to the current task. Similarly, for DeBERTa-large in view 1, we use the [CLS] token embedding in the last layer of the DeBERTa representation rather than the penultimate layer, since this layer has been adapted to the task of interest by the time we select conﬁdent data. Validation dataset We use a pseudolabeled validation set to perform model selection during the co-training iterations. Since the conﬁdent-data-selection methods can pick out the most precisely pseudolabeled examples (w.r.t. the true label), we also use them to select a conﬁdent validation set from the larger val set for each Train step. In particular, when training model hi, we use model h1−i to select a ˜β = β + tβ′conﬁdent fraction of full validation data in each step (the same fraction used for the conﬁdent training set). This allows us to use a more precise validation set for model selection. A.4. Full algorithms The detailed algorithms for co-training in the partial access setting and full access setting are shown in Algorithms 4 and 5, respectively. Algorithm 4 uses model conﬁdence for view 0 and cut statistic for view 1. Algorithm 5 uses cut statistic for both views. The detailed procedures for model conﬁdence and the cut statistic are shown in Algorithms 2 and 3, respectively. As mentioned in the previous section, the view 1 cut statistic uses the [CLS] token embedding in the last layer of h1. In the full access case, the view 0 cut statistic uses the T0 decoder’s hidden state for the ﬁrst decoded token. 11B. Training and dataset details B.1. Datasets • RTE (Dagan et al., 2005): Binary textual entailment, 2490 training examples, 277 validation examples (our test set). P[Y] = (0.5,0.5) • CB (De Marneffe et al., 2019): Ternary textual entailment, 250 training examples, 56 validation examples (our test set). P[Y] = (0.41,0.5,0.09) • WiC (Pilehvar & Camacho-Collados, 2018): Binary word sense disambiguation, 5428 training examples, 638 vali- dation examples (our test set). P[Y] = (0.5,0.5) • TREC (V oorhees & Tice, 2000): 6-way question classiﬁ- cation, 5452 training examples, 500 test examples. Label balance: P[Y] = (0.2131,0.2293,0.0158,0.2243,0.1643,0.1532) • BoolQ (Clark et al., 2019): Binary reading comprehen- sion, 9427 training examples, 3270 validation examples (our test). P[Y] = (0.38,0.62) B.2. Training details Prompt-based FT. We ﬁne-tuned the MLM-pretrained RoBERTa-large model using Adam for 1000 steps with batch size 16, learning rate 1e-5 and weight decay 0.01. We sampled a validation set the same size as the train- ing set while ensuring that the validation set also had an equal number of examples per class. This small validation set was used to select the best model checkpoint in each run and the test results were averaged over four random seeds. This is similar to the “no Ddev” setting in Gao et al. (2021) in that we didn’t use the small validation set for hyperparameter tuning—we used the same hyperparame- ters as the “no Ddev” setting. However, we still allow the method to use the labeled validation set for model selection. We used the same prompt templates as Gao et al. (2021). For RTE, the label words were Yes, No. For CB, the la- bel words were Yes, No, Maybe. For TREC, the label words were Description, Entity, Abbreviation, Person, Number, Location. Calibrate Before Use (CBU). For xcf, we followed Zhao et al. (2021) and used “N/A”, the empty string, and “[MASK]”. We obtained the GPT-3 outputs for each of these xcf’s, renormalized the outputs over the label tokens, aver- aged the re-normalized outputs across the three xcf’s, and used the average result as the scaling factor for W(i). This is identical to Zhao et al. (2021). Co-training. Following RoBERTa and DeBERTa, we used an MNLI-pretrained checkpoint for RTE and CB ( microsoft/deberta-large-mnli on Hug- gingFaceHub). Otherwise, we used DeBERTa-large (microsoft/deberta-large). We did not experi- ment with DeBERTa V2 or V3. B.3. Soft prompt encoding As detailed in Section 3, we combine hard prompt encoding with soft prompting. That is, we format the input using a hard prompt, and combine this formatted input embedding with the soft prompt matrix. This differs from the usual soft prompting setup (Li & Liang, 2021; Lester et al., 2021), where the input is encoded more neutrally, without a natural- language hard prompt: sentence1: {{x.premise}} sentence2: {{x.hypothesis}} A priori, this difference in input encoding could affect the performance of soft prompt tuning and the zero-shot per- formance of the initial prompted model. However, the full- training-dataset soft-prompt tuning baseline in Table 2 (T0 soft prompts on full training set) uses our hard prompt en- coding + soft prompting, and it matches fully ﬁne-tuned DeBERTa-large. This suggests that the accuracy loss from choosing a hard prompt (at least for the prompts that we chose) is minimal. Using the hard prompt encoding might improve the la- bel efﬁciency of soft prompt tuning, since the soft prompt parameters can focus on “ﬁxing up” the given hard prompt instead of learning a prompt-like embedding from scratch. On the other hand, if the hard prompt performs poorly, the hard prompt encoding might put an unnecessary upper limit on the soft prompt tuning performance, since the soft prompt may not be able to “undo” the hard prompt performance. An in-depth comparison between the neutral encoding from the traditional soft-prompting setup and the hard prompt + soft prompt encoding we propose is an interesting direction for future work. B.4. Hardware All models were trained on two NVIDIA A100 80Gb GPUs using PyTorch and the Transformers library (Wolf et al., 2020). For the partial access setting, a full run of T = 5 co-training iterations with DeBERTa-large takes roughly two hours on this hardware. For the full access setting, a full run of T = 5 co-training iterations with T0-3B and DeBERTa-large takes roughly 40 hours. 120.5 1.00.0 0.5 1.0  Label 0 0.5 1.00.0 0.5 1.0  Label 1 0.5 1.00.0 0.5 1.0  Label 2 0.5 1.00.0 0.5 1.0  Label 3 0.5 1.00.0 0.5 1.0  Label 4 0.5 1.00.0 0.5 1.0  Label 5 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Initial Coverage 0.0 0.2 0.4 0.6 0.8 1.0 Confident Precision per Label Confident Precision per Label on Training 0.5 1.00.0 0.5 1.0  Label 0 0.5 1.00.0 0.5 1.0  Label 1 0.5 1.00.0 0.5 1.0  Label 2 0.5 1.00.0 0.5 1.0  Label 3 0.5 1.00.0 0.5 1.0  Label 4 0.5 1.00.0 0.5 1.0  Label 5 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Initial Coverage 0.0 0.2 0.4 0.6 0.8 1.0 Confident Recall per Label Confident Recall per Label on Training Figure 4: Precision (left) and Recall (right) versus βfor each label in the initial conﬁdent set L0 0, extracted from the initial label model using the model conﬁdence method (Algorithm 2). The precision of some labels (e.g., 2, 3) begins to decline more sharply after β = 0.5. This gives additional evidence for our choice of β = 0.5: it trades off between the initial precision of L0 0 and the coverage for each label. At smaller values of β, there are no pseudolabeled examples for label 1 and very few for label 0. At larger values of β, the precision of the other labels is worse. C. Additional co-training analysis In this section, we provide more information regarding the evolution of h0 and h1 over the co-training iterations for TREC. We focus on the TREC dataset since its 6 classes enable us to investigate more complex co-training dynamics. To see the effect ofβon the quality of the initial conﬁdent data L0 0, we plot the precision and recall for each label for different values of βin Figure 4. This ﬁgure indicates that the tradeoff when choosing βis between having high preci- sion for each label (lower β) and having enough coverage for each label to train on (high β). To show how co-training affects label balance across mul- tiple iterations, we plot the total variation distance between the true label balance and the balance estimated using the pseudolabels in each iteration’s conﬁdent data Lt 0. Figure 5 indicates that this distance decreases with co-training it- erations, so the label model automatically learns to have a balance closer to the unseen true balance. In Figures 6, 7, and 8, we plot the recall, normalized coverage, and precision for each label in Lt 0 and Lt 1. The normalized coverage for label jis the number of examples with pseudolabel jdivided by the number of examples with true label j; it separates coverage from the precision, un- like recall. By comparing the evolution of label curves in Figure 7 and 8, we can see that the models tend to add more conﬁdent data when they are more precise and add less conﬁdent data when they are less precise, which is the desired behavior. Additionally, these ﬁgures show two dif- ferent ways in which co-training works to improve models: “coverage-expansion” and “pseudolabel-correction.” In the coverage-expansion regime, the precision for a label slightly decreases as iterations increase, but the coverage improves; this regime was predicted by early work on co-training (Bal- can et al., 2005). In the pseudolabel-correction regime, both precision and coverage increase, because models are able to learn to be more accurate than the pseudolabels used to 0 1 2 3 4 Iteration 0.0 0.1 0.2 0.3 0.4Total Variation Distance TREC h0 Total Variation Distance Figure 5: Total variation distance between the true label bal- ance and the label balance estimated from the pseudolabels Lt 0 at each iteration. As co-training iterations proceed, the label model automatically learns a balance closer to the true unseen label balance. train them. Wei et al. (2020) give a theoretical explanation of this in the context of self-training, rather than co-training. 130 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Recall TREC h0 Confident Recall Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Recall TREC h1 Confident Recall Label # 0 1 2 3 4 5 Figure 6: Recall of the conﬁdent pseudolabel set Lt 0 (left, extracted from h0 using Algorithm 2) and Lt 1 (right, extracted from h1 using Algorithm 3) for each label versus co-training iteration t. 0 1 2 3 4 Iterations 0.0 0.5 1.0 1.5Normalized Coverage TREC h0 Normalized Coverage Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.5 1.0 1.5Normalized Coverage TREC h1 Normalized Coverage Label # 0 1 2 3 4 5 Figure 7: Normalized coverage of the conﬁdent pseudolabel set Lt 0 (extracted from h0 using Algorithm 2) for each label versus co-training iteration t. Normalized coverage for label jis computed as |{(x,ˆy) ∈Lt i : ˆy= j}|/|{x: y(x) =j}| (the number of examples with conﬁdent pseudolabel jdivided by the number of examples with true label j). This metric decouples the coverage from the precision. The increasing slope of label 1 (left) indicates that h0 adds more conﬁdent data for label 1 in the later iterations. Combining this with the label 1 precision versus iteration curve in Figure 8 (left) indicates that the model adds more conﬁdent data for label 1 as it gets more precise, which is the desired behavior. On the other hand, for other labels (e.g. label 4) the rate of conﬁdent data addition and the precision stay relatively constant. 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Precision TREC h0 Confident Precision Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Precision TREC h1 Confident Precision Label # 0 1 2 3 4 5 Figure 8: Precision per label vs co-training iteration, h0 (left—identical to right display of Figure 2) and h1 (right). Together with Figure 7, this indicates the two regimes of co-training. For labels 0 and 2-5, the precision decreases or remains the same while the coverage increases roughly linearly. This is the “coverage-expansion” regime, where the initial conﬁdent pseudolabels are high-precision and the model learns to imperfectly extend that initial signal to the uncovered data with some losses in precision. This regime is present in classical co-training results (Balcan et al., 2005). On the other hand, for label 1, both the coverage and the precision increase with the iteration t. This is the pseudolabel-correction regime, because the models are able to learn to be more accurate than the pseudolabels used to train them (compare h0 precision for label 1 to h1 precision for label 1 in the same iteration—the h1 model is trained on the labels from h0, but is able to select conﬁdent data with better precision than those labels). 14D. Prompts Here we list the prompts used for our experiments, largely taken from Sanh et al. (2022). D.1. Partial Access Setting RTE {example.premise} Question:{example.hypothesis} True, False, or Unknown? answer: {example.answer} {premise} Question: {hypothesis} True, False, or Unknown? answer: CB premise: {example.premise} hypothesis: {example.hypothesis} Does the premise imply the hypothesis? Yes, No, or Neither? answer: {example.answer} premise: {premise} hypothesis: {hypothesis} Does the premise imply the hypothesis? Yes, No, or Neither? answer: TREC Classify the questions based on whether their answer type is Unknown, Number, Location, Person, Description, Entity, or Abbreviation. Question: {example question} Answer Type: {example type} Question: {question} Answer Type: D.2. Full Access Setting RTE {premise} Question: {hypothesis} True, False, or Unknown? CB {premise} Question: {hypothesis} True, False, or Neither? BoolQ Text: {passage} Answer the following yes/no question: {question}? Yes or no? 15Algorithm 2 GetConfDataMC input {xn}U n=1 unlabeled examples input model hi, view φi input coverage fraction ˜β input minimum class percentage γ // compute pseudolabel and score for each example for nin {1,...,U }do on = hi(φi(xn)) (note on ∈Rnumlabels) ˆyn ←argmaxlonl sn ←maxlonl end for L←∅ // ﬁrst, select top γ% for each class by score ms←⌊γ˜βU⌋// min num points to select for lin {1,...,numlabels }do Il = {(xn,ˆyn,sn) : ˆyn = l} // sort Il by score sn (ascending) Sl ←Sort(Il, key=lambda q: q[2]) // add top α% to L(read off end of Sl) L←L∪Sl[-ms:] end for // now select the rest of the points rs←⌈˜βU⌉−|L|// num remaining points to select I= {(xn,ˆyn,sn)}U n=1 I←I\\ L// don’t select twice S ←Sort(I, key=lambda q: q[2]) L←L∪S[-rs:] // chop off score and return point + pseudolabel L←{(xn,ˆyn)|∃sn : (xn,ˆyn,sn) ∈L} return L Algorithm 3 GetConfDataCS input {xn}U n=1 unlabeled examples input model hi, view φi input coverage fraction ˜β // compute pseudolabel and repr. for each example for nin {1,...,U }do on = hi(φi(xn)) (note on ∈Rnumlabels) ˆyn ←argmaxlonl end for L←∅ for yin {1,...,numlabels }do ˆPy = |{n: ˆyn = y}|/U end for // compute 20 nearest neighbors for each ex. for uin {1,...,U }do N(u) = NN20(φi(xu),{φi(xv)}U v=1) for vin N(u) do wuv = 1/(1 +∥φ(xu) −φ(xv)∥2) Iuv = I[ˆyu ̸= ˆyv] end for // now compute cut statistic Ju = ∑ v∈N(u) wuvIuv µu = (1−ˆPˆyu ) ∑ v∈N(u) wuv σ2 = ˆPˆyu (1 −ˆPˆyu ) ∑ v∈N(u) w2 uv su = Ju−µu σ end for // now sort by statistic and return top data I←{ (xn,ˆyn,sn)}U n=1 S ←Sort(I, key=lambda q: q[2]) ns= ⌊˜βU⌋ L←S[:ns] L←{(xn,ˆyn)|∃sn : (xn,ˆyn,sn) ∈L} return L 16Algorithm 4 Co-training algorithm (detailed, GPT-3) input {(xj,yj)}k j=1 initial labeled examples input U= {xn}U n=1 unlabeled examples input initial coverage β, coverage increase β′ input minimum percentage per class γ // build view 0 for unlabeled examples for nin {1,...,U }do for jin {1,...,k }do φ(j) 0 (xn) ←GPT3(xn,(xj,yj)) end for φ0(xn) ← ( φ(1) 0 ; ... ; φ(k) 0 ) end for // build view 1 for unlabeled examples for nin {1,...,U }do // extract pre-trained DeBERTa representation forxn φ1(xn) ←DeBERTa(xn) end for // initialize h0 according to (1) for jin {1,...,k }do // get GPT-3 outputs on content-free input φ(j) 0 (xcf) ←GPT3(xcf,(xj,yj)) W(j) ←Diag ( 1 φ(j) 0 (xcf ) ) end for W ←{W(j)}k j=1 α←1 h0 ←h0( ·; W,α) // co-training loop for tin {0,...,T −1}do ˜β ←β+ tβ′ Lt 0 ←GetConfDataMC(U,h0,φ0,˜β,γ) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfDataCS(U,h1,φ1,˜β) h0 ←Train(φ0,Lt 1) end for return (h0,h1) Algorithm 5 Co-training algorithm (detailed, T0) input U= {xn}U n=1 unlabeled examples input initial coverage β, coverage increase β′ // format the input with a hard prompt template P, // then get T0 embedding to build view φ0 for nin {1,...,U }do ˜xn ←P(xn) φ0(xn) ←T0Emb(˜xn) end for // build view 1 for unlabeled examples for nin {1,...,U }do // extract pre-trained DeBERTa representation forxn φ1(xn) ←DeBERTa(xn) end for // initialize soft prompt with repeated pad token emb p←T0Emb([PAD]) h0(·) ←T0((p; p; ... ; p); ·) // co-training loop for tin {0,...,T −1}do ˜β ←β+ tβ′ Lt 0 ←GetConfDataCS(U,h0,φ0,˜β) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfDataCS(U,h1,φ1,˜β) h0 ←Train(φ0,Lt 1) end for return (h0,h1) 17",
      "meta_data": {
        "arxiv_id": "2202.00828v1",
        "authors": [
          "Hunter Lang",
          "Monica Agrawal",
          "Yoon Kim",
          "David Sontag"
        ],
        "published_date": "2022-02-02T00:48:26Z",
        "pdf_url": "https://arxiv.org/pdf/2202.00828v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that co-training can significantly improve the performance of prompt-based learning for Large Language Models (LLMs) by effectively utilizing unlabeled data. It shows that co-training can enhance the original prompt model and simultaneously facilitate the learning of a smaller, task-specific downstream model. The approach is effective in both partial access (e.g., GPT-3 output probabilities) and full access (e.g., T0 gradients for soft prompt tuning) settings, leading to improved results on challenging datasets where a notable performance gap existed between prompt-based and fully-supervised methods. It also finds that pseudo-labels from a prompted model serve as an effective signal for fine-tuning smaller task-specific models.",
        "methodology": "The methodology integrates co-training with prompt-based learning by defining two complementary views. View φ0(X) is derived from a large prompt-based model (GPT-3 output probabilities or T0's initial word embeddings), and View φ1(X) uses the frozen representation from a smaller pre-trained language model (e.g., DeBERTa's penultimate layer). Two settings are explored: 1. Partial Access (e.g., GPT-3): Model h0 is a learnable 'label model' that calibrates individual prompt outputs using prompt-specific matrices (initialized with Calibrate-Before-Use) and ensembles multiple prompts. Model h1 is the last few layers of DeBERTa. 2. Full Access (e.g., T0): Model h0 is a set of continuous soft prompt vectors (initialized with repeated [PAD] token embeddings and combined with hard prompt encoding) that are prepended to the input embedding for the frozen T0 model. Model h1 remains DeBERTa. During co-training iterations, models iteratively label a large unlabeled dataset, and each model is trained on confident pseudo-labels from the other. Confident data selection is performed using either 'model confidence' (sorting by scores, potentially with minimum class frequency) or a 'cut statistic' (graph-based heuristic based on K-nearest neighbors and agreement of pseudo-labels). The process uses relabeling, where pseudo-labels are regenerated in each iteration.",
        "experimental_setup": "The research evaluated the approach on several standard natural language processing benchmarks known for a large gap between prompt-based and fully-supervised learning: RTE (binary textual entailment), CB (ternary textual entailment), TREC (6-way question classification), and BoolQ (binary reading comprehension). In the partial access setting, GPT-3 was used as the large prompt model (h0) with DeBERTa-large as the smaller model (h1), typically with k=4 few-shot examples. In the full access setting, T0-3B was used for h0 (via soft prompt tuning) and DeBERTa-large for h1, in a zero-shot context. Co-training involved an initial coverage β=0.5, per-step increase β'=0.1, and T=5 total steps, with a minimum label frequency γ=0.01. Hyperparameters for training h0 (label model/soft prompt) and h1 (DeBERTa) were based on common practices or existing work, with early stopping based on balanced accuracy on a pseudo-labeled validation set. The cut statistic was used for confident data selection in view 1 (and view 0 for full access), and model confidence for view 0 in partial access. Baselines included GPT-3 32-shot, Calibrate Before Use (CBU), Prompt-based FT (Gao etal., 2021), and Snorkel-based methods. All models were trained on two NVIDIA A100 80GB GPUs.",
        "limitations": "The study identified several limitations: 1. **Noisy Initial Signal:** Co-training performance decreases if the initial signal provided by the prompted model is too noisy (e.g., BoolQ dataset with T0, where initial pseudo-labels had very high total noise). 2. **Insufficient Unlabeled Data:** The method can overfit when there is not enough unlabeled data to obtain good generalization performance, especially with highly flexible hypothesis classes (e.g., CB dataset with T0, where the soft prompt overfit the small pseudo-labeled training set). 3. **Gap in View Signal Quality:** The benefit is limited when there's a large gap in fully-supervised accuracy between the two views, suggesting that the weaker view might not provide enough signal for effective co-training. 4. **Hyperparameter Tuning:** While an effort was made to minimize this, the approach, like much prompt-based learning, implicitly assumes access to a small labeled set for selecting various model configurations (e.g., prompts and hyperparameters), making it not 'true' few-shot/zero-shot in the strictest sense.",
        "future_research_directions": "Future research directions include: 1. Developing methods to overcome the identified limitations, specifically addressing issues with noisy initial signals from prompted models and situations with insufficient unlabeled data. 2. Investigating co-training in the context of 'true' few-shot/zero-shot learning, where no labeled data is used for hyperparameter selection or model configuration. 3. Exploring the use of warm-starting for models in co-training iterations to potentially reduce computational burden. 4. Conducting an in-depth comparison between the proposed hard prompt encoding combined with soft prompting and the traditional 'neutral encoding' soft-prompting setup to understand their impact on performance and label efficiency."
      }
    },
    {
      "title": "Co-training Improves Prompt-based Learning for Large Language Models",
      "abstract": "We demonstrate that co-training (Blum & Mitchell, 1998) can improve the\nperformance of prompt-based learning by using unlabeled data. While prompting\nhas emerged as a promising paradigm for few-shot and zero-shot learning, it is\noften brittle and requires much larger models compared to the standard\nsupervised setup. We find that co-training makes it possible to improve the\noriginal prompt model and at the same time learn a smaller, downstream\ntask-specific model. In the case where we only have partial access to a prompt\nmodel (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a\ncalibration model over the prompt outputs. When we have full access to the\nprompt model's gradients but full finetuning remains prohibitively expensive\n(e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous\nvectors to iteratively update the prompt model. We find that models trained in\nthis manner can significantly improve performance on challenging datasets where\nthere is currently a large gap between prompt-based learning and\nfully-supervised models.",
      "full_text": "Co-training Improves Prompt-based Learning for Large Language Models Hunter Lang MIT CSAIL hjl@mit.edu Monica Agrawal MIT CSAIL magrawal@mit.edu Yoon Kim MIT CSAIL yoonkim@mit.edu David Sontag MIT CSAIL dsontag@mit.edu Abstract We demonstrate that co-training (Blum & Mitchell, 1998) can improve the performance of prompt- based learning by using unlabeled data. While prompting has emerged as a promising paradigm for few-shot and zero-shot learning, it is often brit- tle and requires much larger models compared to the standard supervised setup. We ﬁnd that co- training makes it possible to improve the original prompt model and at the same time learn a smaller, downstream task-speciﬁc model. In the case where we only have partial access to a prompt model (e.g., output probabilities from GPT-3 (Brown et al., 2020)) we learn a calibration model over the prompt outputs. When we have full access to the prompt model’s gradients but full ﬁnetuning remains pro- hibitively expensive (e.g., T0 (Sanh et al., 2022)), we learn a set of soft prompt continuous vectors to iteratively update the prompt model. We ﬁnd that models trained in this manner can signiﬁcantly im- prove performance on challenging datasets where there is currently a large gap between prompt-based learning and fully-supervised models. 1. Introduction Prompt-based learning, in which a pretrained language model is adapted to various tasks by priming on natural language prompts, has emerged as a promising framework for few-shot and zero-shot learning (Brown et al., 2020; Liu et al., 2021; Wei et al., 2021; Sanh et al., 2022). While intriguing, these methods can be sensitive to trivial cosmetic artifacts, including variations in prompt wording and the ordering of examples (Lu et al., 2021; Zhao et al., 2021; Kumar & Talukdar, 2021). Further, the models used in prompt-based learning (e.g., GPT-3, T0) are much larger than those typically used for standard ﬁne-tuning. These fac- tors make prompt-based learning difﬁcult to use and deploy in practice. Given a small amount of labeled data, one could evaluate the performance of each prompt and re-calibrate the prompt outputs to improve performance. However, (i) this reliance on labeled data goes against the goal of few-shot learning, and (ii) even with oracle calibration, some prompts have sub-par accuracy. Recently, to address issue (i), Zhao et al. (2021) developed a data-free calibration method that can dramatically improve the accuracy of few-shot prompts for GPT-3. We build on their work by showing how to use unlabeled data to further improve performance. To leverage unlabeled data, we use co-training (Blum & Mitchell, 1998), which operates on two views of each data point X: φ0(X) and φ1(X). For example, in a clinical diagnosis system, φ0(X) could be laboratory test results and φ1(X) an X-ray image. A pair of models ( h0 and h1 respectively) takes turns labeling a large unlabeled training set, and each model is trained on the conﬁdent pseudo-labels from the other. Model h0 only uses φ0(X), and model h1 uses φ1(X). By using complementary information in the views φ0,φ1 and the different inductive biases from models h0, h1, co-training allows each model to learn from the other without the need for labeled data. The initial signal to start the co-training process is provided by a “guess” at a model h0. To combine co-training and prompt-based learning, we use outputs from a large prompt-based model as φ0(X) and the pre-trained representation from a much smaller language model (e.g., DeBERTa (He et al., 2021)) as φ1(X). We specify the models h0 and h1 based on whether we have partial access to the prompt model (querying GPT-3) or full access (locally training T0). In partial access, we only have access to the large model’s output probabilities. In this case, we use unlabeled data to learn a model h0 that both calibrates individual prompts and ensembles multiple prompts. We refer to this as the label model. We use Calibrate-Before-Use (Zhao et al., 2021) to initialize the calibration parameters of this model for each prompt, and we initialize the ensembling parameters to ap- proximate majority vote. We then reﬁne this initial guess for h0 with co-training. We use the pre-trained representation from DeBERTa (He et al., 2021) for φ1(X) and train the last few layers of that model as h1. The only labeled data used is the set of k examples used in the input prompts. Figure 1 (left) shows the co-training process in the partial access setting. We also study a full access setting using T0 (Sanh et al., 2022) instead of GPT-3, so we can introspect the large prompt model. We derive the viewφ0(X) and the model h0 from T0. However, instead of fully ﬁne-tuning T0 during co-training, we focus on soft prompt tuning, which trains several orders-of-magnitude fewer parameters while attain- ing similar performance (Li & Liang, 2021; Lester et al., 2021). The parameter space for model h0 is the set of soft prompts, which are matrices RL×d, where Lis a sequence 1 arXiv:2202.00828v1  [cs.CL]  2 Feb 2022GPT-3 Output labelsMLP (ℎ&)Label model (ℎ() Co-train Example formatted as !prompts {{premise}} Question: {{hypothesis}} True or False?Given {{premise}} and what you know about the world, does {{hypothesis}} follow? Yesor No? {{premise}} Question: {{hypothesis}} Yesor no? {{premise}} {{hypothesis}}Unlabeled example*BERT* Contextual embedding \"\"$Output probabilities \"!$ T0 embedding {{premise}} Question: {{hypothesis}} True or False?Example formatted as a hard prompt Input embedding \"!($) Output labelsMLP (ℎ&) Soft prompt (ℎ!) T0 model {{premise}} {{hypothesis}}Unlabeled example*BERT* Contextual embedding \"\"$ Co-train Figure 1: The setup for our two applications of co-training to prompting for a binary entailment classiﬁcation dataset (RTE). Parameters in blue are trainable; models in gray are ﬁxed. Left: training a “label model” for post-hoc calibration and ensembling of multiple prompts. Here the prompts and the model (GPT-3) are ﬁxed, and we co-train the calibration / ensembling parameters with the task-speciﬁc model (e.g., DeBERTa). Right: training a soft prompt. Here the input is encoded as a hard prompt and the embedding matrix of the input sequence is obtained. A L×dmatrix of trainable parameters (the “soft prompt”) is prepended to this embedding, and the combined embedding sequence is passed through T0 to get output predictions. We co-train the soft prompt with the view 1 model (e.g., DeBERTa). length hyperparameter and is dthe dimension of the pre- trained T0 embeddings. Each row of the soft prompt mimics the embedding of a token, but the soft prompt need not corre- spond to the embedding of any actual token sequence. This matrix is prepended to the input embedding and the output of h0 is computed with the frozen T0 model. The initial guess at h0 (i.e., the initial soft prompt vector for use in co-training) is the repeated embedding of the [PAD] token. Since T0 was trained to perform well at zero-shot learning with prompts, this provides a good initial hypothesis. We co-train this model with a pre-trained DeBERTa represen- tation as φ1(X) and the last few layers of DeBERTa ash1. This is is shown in Figure 1, right. We apply our approach to standard few-shot and zero- shot benchmarks and ﬁnd that (i) iteratively co-training both models using unlabeled data consistently improves perfor- mance, (ii) pseudo-labels from a prompted model are an effective signal for ﬁne-tuning smaller task-speciﬁc models, and (iii) this approach can signiﬁcantly improve results on datasets previously considered difﬁcult for prompt-based learning. We conclude with a brief analysis of success and failure cases and describe high-level criteria required for our method to work. 2. Related work Prompting and prompt tuning. Lu et al. (2021) ﬁnd op- timal orderings of prompt examples based on an artiﬁcially constructed development set. Given the variance in per- formance across different prompts, others have focused on engineering suitable prompts, manually or otherwise (Liu et al., 2021). Jiang et al. (2020), Shin et al. (2020), and Gao et al. (2021) use data-driven techniques and language models to automatically generate candidate prompts. Rather than being constrained to human-readable prompts, Li & Liang (2021) and Lester et al. (2021) instead learn a con- tinuous soft task-speciﬁc “prompt” to condition language models. While effective, these methods typically require nontrivial amounts of labeled data. Another line of work uses the outputs from a prompted language model as weak labels, as we do in this work. Wang et al. (2021) propose to train smaller models on labels from GPT-3 to reduce annotation cost, but they train from indi- vidual, uncalibrated prompts and do not attempt to reﬁne the prompt model alongside the smaller model. Schick & Sch¨utze (2021) ﬁne-tune a separate RoBERTa model for each prompt using a small amount of labeled data. They next aggregate the outputs of these individual ﬁne-tuned models as a soft pseudo-label and train a ﬁnal model to match the soft aggregation. In contrast, we train a single BERT-style model on the ensembled prompt output without any additional labeled data. We use this model to reﬁne the ensemble parameters (and vice-versa). In our approach we only use prompt outputs as training signal, and we consider different types of prompts (open-ended instead of cloze). Self-training for few-shot text classiﬁcation. Our work relies on access to a large amount of unlabeled data to it- eratively grow a conﬁdently-labeled training set for each model. Similarly, self-training ﬁrst trains a model on a small set of initial data, uses the trained model to produce pseudo-labels on a set of unlabeled data, and then iteratively includes the conﬁdently pseudo-labeled data as new train- ing labels (Scudder, 1965). In the context of few-shot text classiﬁcation, Mukherjee & Awadallah (2020) develop an uncertainty-aware technique for choosing which data points 2to include, which requires a small amount of labeled data. Karamanolakis et al. (2019, 2021) employ self-training and iterative co-training with weak supervision as the initial label signal, and they similarly use a neural network with pretrained embeddings as a downstream model. However, they explore hand-written or keyword-based rules as weak supervision, in contrast to the present work, where we derive our weak signals from prompted models. The parameter- ization of h0 in our partial access setting is similar to the weighting they use to combine rules. Co-training. Co-training dates back to Blum & Mitchell (1998), who assumed that φ0(X) and φ1(X) are two dis- tinct views and conditionally independent given the true label. Under this strict condition, they proved that the al- gorithm ﬁnds a good classiﬁer after just one step. Many subsequent analyses (e.g., Dasgupta et al., 2002; Balcan et al., 2005) relax this condition, showing that views can be dependent or even identical as long as certain relation- ships hold between the models being trained (essentially, they are “different enough”). In a similar vein, Wei et al. (2020) give a theoretical explanation of why (and when) models can learn to be more accurate than the pseudo-labels used to train them. We take implicit advantage of these results in our work. The views we use are highly dependent, and yet the models we train are often able to outperform the pseudo-labels we used to train them in each co-training iteration. 3. Co-training with prompting The skeleton of our approach is shown in Algorithm 1 (full detail is provided in Algorithms 4 and 5 in the supplement). First, a hypothesis h0 over view φ0 is initialized such that its initial predictions are reasonable. (We discuss initialization in depth in the following sections.) Next, we obtain the conﬁdently labeled training data L0 0, which is a subset of the unlabeled data points, together with pseudo-labels for those points from h0. In iteration t, we select a β + tβ′ fraction of the data. (We discuss techniques for selection of conﬁdent data in Section 4 and the choice of β and β′ in Section 5.) These conﬁdently-labeled points are then used to train a model h1 on view φ1, and h1’s conﬁdently- labeled data is extracted as L0 1. This is used to train a new h0, and the process continues for T steps. Train performs standard supervised training on the pseudo-labels for that iteration. In this section, we give details for how to construct the views φ0 and φ1, the hypothesis classes we use for the model h0, and the initialization schemes for h0 in both the partial access and full access settings. 3.1. Partial access: co-training a label model In the usual few-shot setting with prompting, k labeled examples ({xi,yi})k i=1 are converted into a single natural language prompt following a prompt template. We call this prompt k-shot, since it uses klabeled examples. Instead of using one k-shot prompt, we use kone-shot prompts, only including one example in the template at a time. This gives us koutputs. Separating out the signal from each labeled Algorithm 1 Co-training algorithm input U= {xn}U n=1 unlabeled examples input {(xj,yj)}k j=1 labeled examples (optional) input initial coverage β, coverage increase β′ h0 ←InitClassifier(φ0) for tin {0,...,T −1}do ˜β ←β+ tβ′ // GetConfData∗deﬁned in Algorithms 2, 3 Lt 0 ←GetConfData∗ ( U; h0,φ0,˜β ) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfData∗ ( U; h1,φ1,˜β ) h0 ←Train(φ0,Lt 1) end for return (h0,h1) example in this way allows us to combine the examples more effectively than the one k-shot prompt model. View. Let φ(i) 0 (x) ∈R|V|be the vector of probabilities output by GPT-3 on input xformatted in a one-shot prompt with labeled example (xi,yi). Here V is a subset of the full token vocabulary—the verbalizer tokens—and consists of the “label tokens” for the prompt as well as other tokens related to the label. For example, in sentiment analysis, if x1 is “this movie was great!”, φ(1) 0 (x) is GPT-3’s output on: Review: this movie was great! Positive or Negative? Positive Review: {{x.review}} Positive or Negative? and V might include the label tokens Positive / Negative and related tokens such as uncased label tokens or synonyms.1 To select the verbalizer tokens in a task- agnostic way, we obtain the top 10 predictions for GPT-3 on each prompt, sort tokens by the total probability assigned to them on the unlabeled training set, and choose the top 25%. This ensures that other frequent tokens appear in the feature set for φ0. For example, Date appears for TREC question classiﬁcation even though the closest true label category is Number. The label model automatically learns this associa- tion during the co-training iterations. Henceforth we assume that V is ordered and the ﬁrst elements of V correspond to the label tokens. By concatenating φ(i) 0 (x) for each of the k labeled examples, we obtain a matrix φ0(x) ∈Rk×|V|, the ﬁrst view for co-training. The second view, φ1(x), is the frozen representation of a pretrained model like DeBERTa (He et al., 2021). In our experiments, we use the representa- tion in the penultimate layer as φ1(x), and the hypothesis class over this view is the last layer and the linear classiﬁer. Hypothesis class. This leaves the hypothesis class for model h0: how do we combine kprompt signals into one pseudo-label? Probabilities from these models are often miscalibrated (Zhao et al., 2021), and thus averaging or majority voting does not yield good results. Instead, we 1In the running sentiment analysis example we might have V = {Negative, Positive, negative, positive, bad, good, ... }. 3propose to learn a label model that scales each prompt vec- tor φ(i) 0 (x) by a prompt-speciﬁc calibration matrix W(i) before averaging. The combined architecture for this model is given by h0(x; W,α):2 li = ReLU ( W(i)φ(i) 0 (x) ) ; h0(x; W,α) = softmax ( k∑ i=1 αili ) , (1) where α ∈Rk is a vector of weights for ensembling the scaled prompt outputs. The ReLU(·) allows the model to easily ignore particular prompt/label combinations. For ex- ample, if prompt j has very poor precision when it out- puts label z, setting W(j) zz to be negative causes ljz to be 0. Note that we directly calibrate probabilities rather than log-probabilities, following Zhao et al. (2021) (i.e., φ(i) 0 ∈ [0,1]|V| and ∥φ(i) 0 ∥1 = 1). In each iteration of co-training, we train this model using the standard cross- entropy loss on the conﬁdent data for that iteration. Initialization. The calibration matrices W(i) are initial- ized using Calibrate-Before-Use (CBU) (Zhao et al., 2021), which ﬁrst computes the probability vector φ(i) 0 (xcf) on content-free inputs xcf (e.g., N/A or the empty string), and then uses these as a scaling factor: W(i) = Diag ( 1 φ(i) 0 (xcf) ) . This initialization scheme ensures that the scaled prompt outputs are neutral on truly neutral inputs, improving cal- ibration. We also set αi = 1 for each i ∈{1,...,k }to initially weight each prompt equally in the ensemble. When V consists of more than just label tokens, we initialize W(i) in blocks: we use CBU for the label tokens, and initialize the rest of W to be 0. That is, for an l-way classiﬁcation task, where W(i) ∈Rl×|V|, we initialize the ﬁrst lcolumns of W(i) using CBU (since we assumedV was ordered, these columns correspond to the label tokens) and set the rest to 0. Hence only the label tokens (e.g., Negative, Positive) are used at initialization, but subsequent iterations of co- training can use nonzero weights on the extra tokens. 3.2. Full access: co-training a soft prompt In this setting, our prompt model is the T0 model (Sanh et al., 2022), which achieves zero-shot generalization by ﬁne-tuning T5 (Raffel et al., 2020) on multiple tasks whose labeled examples have been transformed into natural lan- guage question-answer pairs. Since T0 is publicly available and smaller than GPT-3, we can introspect the model and compute gradients in this case. View. We set φ0(X) to the initial word embeddings of T0 and leave φ1(X) and h1 unchanged (i.e., φ1 is the penulti- mate layer of a pretrained DeBERTa representation). 2Here we use W to refer to {W(i)}k i=1. Hypothesis class. The model h0 is parameterized by a continuous soft prompt (Li & Liang, 2021; Lester et al., 2021). Concretely, letting d= 2048be the dimension of the T0 word embeddings, a soft prompt is a matrix of parameters P ∈RL×d, where Lis a sequence length hyperparameter. Each row of the soft prompt acts like the embedding of a “token” (but needn’t correspond to the embedding of any real token—i.e., there are no constraints on P). The hypothesis h0(x; P) is thus given by prepending the soft prompt to the input word embedding sequence and using the concatena- tion (P; φ0(X)) as input to T0. The subsequent T0 layers are frozen and not updated during training. Given enough la- beled data, soft prompt tuning can match the performance of full-ﬁne-tuning with far fewer trainable parameters (Lester et al., 2021; Le Scao & Rush, 2021).3 Initialization. T0 is speciﬁcally trained to perform well at zero-shot tasks with a variety of hard prompts, so using a hard prompt out-of-the-box gives good initial performance. Hence, to initialize a soft prompt hypothesis, we encode the input using a hard prompt and then set the soft prompt to be the repeated embedding of the tokenizer’s padding token. Using the RTE dataset (Dagan et al., 2005) as a running example, we ﬁrst encode the input using a hard prompt, where each input example xis formatted as: {{x.premise}} Question: {{x.hypothesis}} True or False? We then set h0 to be the repeated embedding of the T0 padding token, i.e., at initialization the T0 model sees: [PAD]...[PAD]{{x.premise}} Question: {{x.hypothesis}} True or False? This combination of hard prompt encoding with soft prompt- ing differs from the usual soft prompting setup (Li & Liang, 2021; Lester et al., 2021). We discuss this issue in more depth in Section B.3. 4. Selecting conﬁdent data The key step in co-training is selecting conﬁdently-labeled data for use in the next training iteration. The literature on co-training has identiﬁed a large number of methods for per- forming this data selection (GetConfData, in Algorithm 1). We consider two simple approaches in this work: model conﬁdence and cut statistic. In both cases, we specify an ini- tial coverage fraction βand a coverage increase fraction β′. Given U unlabeled examples, the amount of pseudo-labeled data in round t≥0 is therefore U(β+ tβ′). Model conﬁdence. For model conﬁdence, we sort every example by the scores output by each model and select the top β+ tβ′fraction in iteration t. While simple, this can result in very imbalanced updates to the pseudo-labeled dataset if the model is only conﬁdent for one label or if one label is inherently more noisy than the others. If additional 3We use L = 20in our experiments, following Lester et al. (2021), so the soft prompt has 20 ×2048 = 40960parameters. 4knowledge regarding the marginal label distribution is avail- able (e.g., approximate label balance or a constraint on the minimum label frequency), we can imbue this knowledge into the data selection process by grouping examples by their predicted label and then performing the sort-and-select procedure for each label separately. Knowledge of the ap- proximate label balance is a standard assumption in weak supervision (e.g., Fu et al., 2020), but we make a much weaker assumption when using the model conﬁdence rank- ing: we assume we know a lower bound γsuch that for all labels y, P[Y = y] ≥γ. We set γ = 0.01, i.e., that every class accounts for at least 1% of the data. The detailed pro- cedure for conﬁdent data selection using model conﬁdence is shown in Algorithm 2 (supplement). Cut statistic. The cut statistic is a ranking heuristic that uses the view geometry more than the model conﬁdence approach (Muhlenbach et al., 2004; Zhang & Zhou, 2011). Suppose we want to select data conﬁdently labeled by a model over view φ(X) (we omit the subscript ifor clearer notation). First, we form a graph G = (V,E) with one vertex for each unlabeled training example and edges con- necting vertices who are K-nearest neighbors in φ(X) (or a representation related to φ(X)—for example, for T0 we can use a contextual representation from inside the model instead of the uncontextual embeddings φ0). Let ˆY(X) = argmaxh(φ(X)) be the hard pseudo-label assigned to input X by model h. We say an edge (xu,xv) is cut if ˆY(xu) ̸= ˆY(xv). Intuitively, we can feel conﬁdent about examples that have few cut edges, since they have the same label as most of their neighbors. Regions of Gwith high noise are less likely to be correctly labeled. The cut statistic heuristically quantiﬁes this idea to rank examples. Suppose (as a null hypothesis) that the labels ˆY were sampled i.i.d. from the marginal distribution P[ ˆY = y] (i.e., independently of X). For vertices uand vcorrespond- ing to examples xu, xv, deﬁne Iuv = I[ ˆY(xu) ̸= ˆY(xv)]. Consider the test statistic: Ju = ∑ v∈N(u) wuvIuv,where wuv = 1/(1 +∥φ(xu) −φ(xv)∥2) are edge weights that decrease as the distance between u and v increases, and N(u) are the neighbors of u. The mean of Ju under the null hypothesis is: µ= (1−P[ ˆY(xu)]) ∑ v∈N(u) wuv,and the variance is: σ2 = P[ ˆY(xu)](1 −P[ ˆY(xu)]) ∑ v∈N(u) w2 uv. Following Zhang & Zhou (2011), we approximate the dis- tribution of J with a normal distribution of mean µ and variance σ2. Then we can rank examples xu by the left- sided tail probability for Ju (lower is better). If Ju is much smaller than expected, then the total cut edge weight is much smaller than expected under the null hypothesis. To select conﬁdent data, we sort examples by Ju and choose the top β+ tβ′fraction in iteration t. The detailed procedure for conﬁdent data selection using the cut statistic is shown in Algorithm 3 (supplement). 4.1. Relabeling. Pseudo-labels from previous iterations can either be re-used or thrown out. If the initial hypothesis has high precision but low coverage, it is typically preferable to re-use the pseudo-labels from previous iterations, since as coverage increases the quality of the pseudo-labels is likely to go down. On the other hand, if the models being trained are capable of correcting incorrect pseudo-labels, it is preferable to relabel, since this can improve the quality of the training data in each iteration. We exclusively use the latter, since we found that the pseudo-label accuracy on the covered subset of data often increased with more iterations. The original co-training algorithm (Blum & Mitchell, 1998) builds L cumulatively, but subsequent co-training procedures also use relabeling (Zhang & Zhou, 2011). We discuss relabeling further in Section A.1. 5. Experiments Datasets. We investigate the beneﬁt of co-training on sev- eral standard natural language benchmarks, focusing on datasets with a large gap between the best prompt-based methods and fully-supervised learning (Wang et al., 2019b,a; Brown et al., 2020). We use the RTE (Dagan et al., 2005), CB (De Marneffe et al., 2019), TREC (V oorhees & Tice, 2000), and BoolQ (Clark et al., 2019) datasets. Full details for these datasets are in Appendix B. In the partial access set- ting, we do not evaluate on BoolQ due to the large amount of GPT-3 quota required for labeling. In the full access setting, we do not evaluate on TREC as T0 was pretrained on TREC. Training methods, partial access. In the few-shot setting we randomly select k = 4 training examples from each dataset until the initial label model assigns every pseudo- label at least γβU times (i.e., we resample prompts until we can initialize the label model in accordance with the constraint that P[Y = y] ≥γ for all y). While larger k might improve performance, k = 4gives a good balance between performance and GPT-3 quota usage. (Indeed, with our 4 one-shot prompts, we are able to beat the GPT-3 32- shot accuracy on CB). In each co-training iteration, we train the label model over view φ0 using Adam with learning 1e-4, weight de- cay 5e-3, and batch size 64 for 40 epochs. We train the DeBERTa-large model overφ1 for 20 epochs using Adam with learning rate 1e-5, weight decay 0.01, batch size 16. All parameters were frozen except the last language model layer, the pooler, and the linear classiﬁcation layer. In order to avoid indirect label leakage, we did not tune these hy- perparameters and instead chose common hyperparameters used for these types of models. For early stopping, each model was evaluated every epoch on a pseudo-labeled vali- dation set and the best model checkpoint was chosen based on balanced accuracy on the pseudo-labels at the end of each round. Using the balanced accuracy (average of the recall for each label) avoids collapsing to the majority class even when the pseudo-labels are relatively imbalanced. This validation set was sampled uniformly from the training set to give a training/validation split of 90%/10%. To determine β, β′,T for co-training, we performed a light hyperparameter search based on performance on a gold-labeled validation set of 500 examples sampled from 5the TREC training set.4 This resulted in the the following values: initial coverage of β = 0.5, per-step coverage in- crease of β′= 0.1, and total co-training steps T = 5. We emphasize that this gold validation set was not used during any co-training iteration (e.g. for model selection, early stopping, learning rate tuning, etc.) We set the minimum label frequency γ = 0.01 and did not tune this value. We used model conﬁdence to add conﬁdent data in view 0 and the cut statistic to add conﬁdent data in view 1.5 We used the [CLS] token embedding in the last layer of DeBERTa for cut statistic nearest neighbors in view 1. We used K = 20 nearest neighbors for the cut statistic and performed no tuning on this value. Training methods, full access. In the zero-shot setting, for training the soft prompt over view φ0(x) we mainly used the hyperparameters suggested by Lester et al. (2021), which were obtained by performing gold soft prompt tuning using T5 on SuperGLUE. We used Adafactor with constant learning rate 0.3, weight decay 1e-5, and batch size 24 for 30000 training steps. For DeBERTa-large, we used the same hyperparameters as in the partial access setting. As in the partial access setting, we used balanced pseudo-label accuracy to select the best model checkpoint at the end of each training round. We used the cut statistic for conﬁdent selection in both views, since with T0 we have access to the internal embeddings, unlike with GPT-3. We used the T0 decoder’s contextual embedding for the ﬁrst decoded token to compute nearest neighbors for the view 0 cut statistic. Training details (e.g., β,β′,T, etc.) are otherwise exactly the same as in the partial access setting. During training, the pseudo-label for each example is ﬁrst mapped to a token that matches the hard prompt (e.g. 0→True and 1→False for the RTE example above). These token labels are then mapped to embedding indices using the T0 tokenizer, and the soft prompt is trained via regular sequence-to-sequence training with the maximum likelihood objective. This is identical to the soft prompt training technique from Lester et al. (2021). Caveat. As noted by Perez et al. (2021), much current work on prompt-based learning does not constitute “true” few-shot/zero-shot learning as they often implicitly assume access to a small labeled set to select various model conﬁg- urations (e.g., prompts and hyperparameters). Insofar as we inherit such conﬁgurations from existing work, our work is similarly not few-shot/zero-shot in the strictest sense, al- though we tried to minimize such issues by using exactly the same co-training parameters (β,β′,T,γ ) and model hy- perparameters for all datasets. (We also did not perform an extensive tuning of these parameters.) While we are encour- aged by the observation that model conﬁgurations seem to work well across diverse datasets, investigating co-training in the context of true few-shot/zero-shot learning (Schick & 4Hence our few-shot experiments on TREC are not few-shot in the truest sense of the term. 5This works better than using the cut statistic in both views—since the cut statistic relies heavily on good nearest neighbors, it makes the most sense in a view that already has a good distance function for examples (the pretrained DeBERTa representation). Sch¨utze, 2021) is an important avenue for future work. Baselines. For baselines we compare against: • GPT-3 32-shot: From Brown et al. (2020). 32 examples combined in one prompt. Uncalibrated. • Calibrate Before Use: Performance of CBU using 4-shot prompts (from (Zhao et al., 2021)). • Prompt-based FT: Our reproduction of the ﬁne-tuning method from (Gao et al., 2021), using 2 labels per class. • Snorkel on GPT-3 output: Snorkel generative label model, which aggregates over the four GPT-3 1-shot outputs with- out using any labeled data. (Ratner et al., 2016, 2020). • Snorkel + DeBERTA-large: DeBERTA-large ﬁne-tuned on outputs from Snorkel label model using the same hy- perparameters as our co-training methods. • Label Model (no co-training): the label model after ini- tialization with (1). The baselines that use (roughly) the same amount of labeled data as our method are shown in the top section of Table 1 and Table 2. The bottom sections contain baselines that use more labeled data, including oracle upper-bounds based on full training data. Training details for the baselines are in Section B.2. 5.1. Results Table 1 shows the results for the partial access setting, where we co-train the label model, which calibrates and combines multiple GPT-3 outputs, with a smaller pretrained model (DeBERTa-large). For view 0, our co-trained label model (Label Model + co-training) improves over the initial label model (Label Model before co-training) and the average per- formance of GPT-3 4-shot before (GPT-3 4-shot) and after (Calibrate Before Use) calibration. It also improves over Snorkel on GPT-3, which, like our method, uses unlabeled data to combine the outputs of our four 1-shot prompts. For CB, the co-trained label model outperforms GPT-3 32-shot despite only using 4 labeled examples. This suggests that us- ing unlabeled data to learn to ensemblek1-shot prompts can be more label-efﬁcient than putting allklabeled examples in one prompt. For TREC and CB, the co-trained label model also outperforms prompt-based ﬁne-tuning (Prompt-based FT (Gao et al., 2021)) with the same amount of labeled data (Prompt-based FT also uses a gold-labeled validation set of kexamples per class, whereas our method only uses a pseudo-labeled validation set). For RTE and CB, we nearly match the fully-supervised performance on view 0 (Label Model on full train), suggesting that co-training is able to extract nearly all of the signal from the GPT-3 probabilities in these cases without using any extra labeled data. For view 1, the co-trained DeBERTa-large model outper- forms all of the baselines that use the same amount of label information. For RTE and TREC, it outperforms Prompt- based FT even when the latter uses 4x (for RTE) and 12x (for TREC) the number of labeled examples (Prompt-based FT with 8 labels per class). This suggests that the pseudo- 6Table 1: Few-shot learning results. (Top) Results against various baselines that use exactly 4 labels per dataset (except for Prompt-based FT, which uses 8 labels for CB and 12 labels for TREC, since this approach uses labels at the “per-class” level). GPT-3 and CBU results are copied from Zhao et al. (2021), while we train our own Prompt-based FT (Gao et al., 2021) and Snorkel (Ratner et al., 2020) models. (Bottom) Results from baselines trained on more data (for reference only). Standard deviation (when applicable) numbers are given by 4 runs over prompts (GPT-3, CBU) or random seeds (Snorkel, Prompt-based FT, Co-training). †rows show accuracy on the private SuperGLUE test set. Otherwise, the accuracies are on the public SuperGLUE validation sets, which we treated as a test set. Model View RTE (2-class) CB (3-class) TREC (6-class) GPT-3 4-shot (from Zhao et al. (2021)) * 58.7 (11.9) 45.2 (19.4) 60.2 (7.6) Calibrate Before Use (CBU) (Zhao et al., 2021) * 60.4 (8.1) 60.7 (6.7) 69.7 (1.4) Prompt-based FT (Gao et al., 2021) * 52.8 (0.9) 84.4 (3.2) 54.8 (2.9) Snorkel on GPT-3 (Ratner et al., 2020) φ0 59.6 (0.0) 70.2 (0.8) 65.2 (0.0) Snorkel on GPT-3 + DeBERTa-large φ1 67.2 (0.5) 81.6 (2.2) 63.3 (0.4) Label Model (no co-training) φ0 62.8 76.8 77.2 Label Model + co-training φ0 64.9 (1.1) 83.5 (2.3) 78.3 (1.2) DeBERTa-large +co-training φ1 67.4 (2.3) 86.2 (3.2) 80.6 (1.1) Label Model on full train φ0 67.8 (0.5) 82.7 (0.8) 91.9 (1.1) DeBERTa-large on full train φ1 93.3 95.2 96.7 GPT-3 32-shot†(Brown et al., 2020) * 69.0 75.6 * Prompt-based FT with 4 labels per class * 48.3 (3.3) 84.4 (4.4) 58.8 (5.9) Prompt-based FT with 8 labels per class * 56.1 (3.2) 87.5 (0.0) 80.0 (4.8) Table 2: Zero-shot learning results with T0 as the initial view 0 model and DeBERTa as the second model. We also show the results trained on the full dataset in the bottom two rows. For T0, we take the best prompts from Sanh et al. (2022) and replicate their results as exact numbers for each prompt were not provided in the original paper. Standard deviations are not provided in this case as even a single run takes a nontrivial amount of time. Model/Algorithm View RTE CB BoolQ T0-3B (best) (Sanh et al., 2022) φ0 68.9 66.1 59.1 T0-3B zero-shot (no co-training) φ0 68.9 58.9 56.4 T0-3B soft prompt + co-training φ0 87.0 67.9 49.1 DeBERTa-large +co-training φ1 86.3 67.9 48.9 T0-3B soft prompt on full train φ0 90.6 80.4 86.9 DeBERTa-large on full train φ1 93.3 95.2 86.1 labels provided by (a learned ensemble of) prompts are an effective training signal for smaller models. Table 2 shows the results for the full access setting with T0. For RTE and CB, co-training improves on the per- formance of the initial zero-shot prompt (T0-3B zero-shot (no co-training) ). For RTE, the co-trained view 0 and view 1 models nearly match the performance of their fully- supervised counterparts. The difference in co-training per- formance on RTE in Table 1 and Table 2 shows the beneﬁt of having full access to h0. Since we can introspect the prompt model, we can use the cut statistic in both views. In the ﬁrst step of co-training, the cut statistic on view 0 se- lects conﬁdent data with 90%-accurate pseudo-labels. The conﬁdent data selection on CB is similarly good: in the ﬁrst co-training step, the cut statistic selects pseudo-labels with 89% accuracy. The pseudo-labels extracted by the view 1 model after the ﬁrst step of co-training (L0 1) are 98% accu- rate, so after training on the initial pseudo-labels, the view 1 model is able to select a very high quality training set at coverage β = 0.5. However, the CB performance is worse than in Table 1 despite the strong initial signal in L0 0 and the near-perfect training data in L0 1. Similarly, for BoolQ, co-training makes the soft prompt worse than the initial zero-shot model. We explore the reasons behind this below. 5.2. When (and how) does co-training work? Figure 2 (left) shows the evolution of the test accuracy for h0 and h1 over co-training iterations on TREC (from Ta- ble 1). Multiple rounds of co-training increase performance for both views as the models become more precise and the coverage increases. Figure 2 (right) shows the precision of the conﬁdent data extracted by h0 for each iteration of co-training, broken down by label. This ﬁgure shows two distinct phenomena. For most labels, precision decreases as coverage goes up, as we expect from usual co-training theory (see e.g. Balcan et al., 2005). However, for label 1, precision actually increases over iterations. Model h1 (De- BERTa) is able to select new conﬁdent data for label 1 that is 70 1 2 3 4 Iterations 0.72 0.74 0.76 0.78 0.80Test Accuracy TREC Accuracy h0 h1 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Precision TREC Confident h0 Precision Label # 0 1 2 3 4 5 Figure 2: Partial access setting, TREC. Left: Test accuracy vs co-training iteration for the label model h0 and the De- BERTa modelh1. Right: precision per label vs co-training iteration, h0. 0 1 2 3 4 Iterations 0.60 0.65 0.70 0.75Test Accuracy CB Accuracy h0 h1 0 1 2 3 4 Iterations 0.6 0.7 0.8 0.9 1.0Balanced Train Accuracy CB Confident Balanced Accuracy h0 h1 Figure 3: Full access setting, CB. Left: Test accuracy vs co- training iteration for T0-3B (h0) and the DeBERTa model (h1). Right: balanced accuracy of conﬁdent pseudo-labels extracted from T0-3B (h0) and DeBERTa (h1). better than the weak labels used to train it, which improves the h0 precision for label 1 in subsequent iterations. For example, in iteration 2, h0’s conﬁdent precision for label 1 is 0.39, but after h1 is trained on that data, it proposes new conﬁdent data for label 1 with precision 0.58 (not shown in Figure 2). Pseudo-label correction is one of the beneﬁts of having two complementary models (though it can also happen with a single model with appropriate regularization (Wei et al., 2020)). Figure 3 shows what can happen when h0 and h1 are not complementary enough. The left display shows the accu- racy of each model over co-training iterations. The right display shows the balanced accuracy of the conﬁdent data extracted from each model. In the ﬁrst co-training step, h1 greatly improves over the initial h0, and selects extremely accurate conﬁdent data (nearly 100% accurate) at coverage β = 0.5. This improves the performance of the soft prompt for the next iteration (h0, left, iteration 1), but the conﬁdent balanced accuracy of h0 sharply decreases. Inspecting the training of h0 on L0 1, the soft prompting procedure appears to have overﬁt to the pseudo-labels on L0 1. Due to the small size of CB (250 training examples), coverage β = 0.5 and our 90/10 train/val split gives only 112 data points for train- ing h0 in the ﬁrst iteration, but the soft prompt is a very ﬂexible hypothesis class. This overﬁtting causes the accu- racy of conﬁdent data to decrease, which in turn degrades the performance of h0, and eventually the two models con- verge to almost identical predictors. This suggests that CB does not have enough unlabeled data for co-training to per- form well with T0, at least when β = 0.5. Using larger initial coverage (e.g. β = 1.0, β′ = 0) or a less ﬂexible hypothesis class for h0 might improve performance. Finally, Table 2 showed that co-trainingdecreased perfor- mance on BoolQ even though the initial soft prompt seemed to have reasonably strong signal (56.4% accuracy). How- ever, the ﬁner-grained statistics are less promising. After the ﬁrst training iteration, with β = 0.5, h1 assigns pseudolabel 0 to 3270 training examples with precision 0.4 and pseu- dolabel 1 to 1848 examples with precision 0.66. This means the “total noise” η= P[ ˆY = 1|Y = 0] +P[ ˆY = 0|Y = 1] in the pseudo-labels is 0.93. At the beginning of iteration t= 1, when ˜β = 0.6, the total noise in the conﬁdent data assigned by h0 is 0.98. For comparison, the total noise for the initial h0 on CB is 0.21. Even under ideal conditions on φ0 and φ1, η <1 is required for learning to work, and the sample complexity depends on 1/(1−η) (Blum & Mitchell, 1998). Unfortunately, the same issue persists on BoolQ for different βvalues. The negative result on BoolQ suggests that the initialization for h0 needs to have less total noise. A different prompt or a better initial hypothesis (e.g., full T0 instead of T0-3B) could be more amenable to co-training. 6. Conclusion Our results indicate that using unlabeled data to co-train a prompted model with a smaller model can boost the perfor- mance of prompt-based learning on few-shot and zero-shot classiﬁcation tasks. As a side effect, this procedure also produces a smaller performant model on the task of interest, distilling and reﬁning the knowledge in the large prompted model. Using two complementary models and views allows the models to learn from each other despite training on par- tially incorrect pseudo-labels. We showed that the beneﬁt of co-training is limited when the initial signal provided by the prompted model is too noisy (BoolQ, full access), when there is not enough unlabeled data to obtain good (pseudo-label) generalization performance (CB, full access), and when there is a large gap in fully-supervised accuracy on view 0 and view 1 (RTE, partial vs full access). Develop- ing methods to overcome these limitations in the context of prompting is an interesting direction for future work. Acknowledgments DS and HL were partially supported by NSF AiTF award CCF-1723344. MA was supported by the Takeda Fellow- ship. Thanks to Dr. Steven Horng of Beth Israel Deaconess Medical Center for providing access to an NVIDIA DGX machine (Horng, 2022), and thanks to NVIDIA Corporation for their donation of two NVIDIA A100 GPUs. Thanks to OpenAI and AI21 for providing quota to access their davinci and Jurassic-Jumbo models (respectively). Fi- nally, thanks to Rebecca Boiarsky and Catherine Wong for their feedback on drafts of this paper and to Aravindan Vija- yaraghavan for helpful discussions on co-training theory. 8References Balcan, M.-F., Blum, A., and Yang, K. Co-training and expansion: Towards bridging theory and practice. Ad- vances in neural information processing systems, 17:89– 96, 2005. Blum, A. and Mitchell, T. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 92–100, 1998. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surpris- ing difﬁculty of natural yes/no questions. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936, Minneapolis, Min- nesota, June 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/N19-1300. URL https: //aclanthology.org/N19-1300. Dagan, I., Glickman, O., and Magnini, B. The pascal recog- nising textual entailment challenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005. Dasgupta, S., Littman, M. L., and McAllester, D. Pac generalization bounds for co-training. Advances in neural information processing systems, 1:375–382, 2002. De Marneffe, M.-C., Simons, M., and Tonhauser, J. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeu- tung, volume 23, pp. 107–124, 2019. Fu, D., Chen, M., Sala, F., Hooper, S., Fatahalian, K., and R´e, C. Fast and three-rious: Speeding up weak supervi- sion with triplet methods. In International Conference on Machine Learning, pp. 3280–3291. PMLR, 2020. Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers) , pp. 3816–3830. Association for Computational Linguistics, August 2021. URL https: //aclanthology.org/2021.acl-long.295. He, P., Liu, X., Gao, J., and Chen, W. Deberta: Decoding- enhanced bert with disentangled attention. In Proceed- ings of ICLR, 2021. Horng, S. Machine learning core. Feb 2022. doi: 10.6084/m9.ﬁgshare.19104917.v1. URL https: //figshare.com/articles/preprint/ Machine_Learning_Core/19104917/1. Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models know? Transactions of the Association for Computational Linguistics , 8:423–438, 2020. Karamanolakis, G., Hsu, D., and Gravano, L. Leverag- ing just a few keywords for ﬁne-grained aspect detec- tion through weakly supervised co-training. In Pro- ceedings of the 2019 Conference on Empirical Meth- ods in Natural Language Processing and the 9th In- ternational Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pp. 4611–4621, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1468. URL https: //aclanthology.org/D19-1468. Karamanolakis, G., Mukherjee, S., Zheng, G., and Hassan, A. Self-training with weak supervision. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 845–863, 2021. Kumar, S. and Talukdar, P. Reordering examples helps dur- ing priming-based few-shot learning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, August 2021. Association for Computa- tional Linguistics. Le Scao, T. and Rush, A. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, pp. 2627–2636, Online, June 2021. Association for Compu- tational Linguistics. doi: 10.18653/v1/2021.naacl-main. 208. URL https://aclanthology.org/2021. naacl-main.208. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and Punta Cana, Dominican Republic, November 2021. Associa- tion for Computational Linguistics. URL https:// aclanthology.org/2021.emnlp-main.243. Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continu- ous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Pa- pers), pp. 4582–4597, Online, August 2021. Associa- tion for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.353. URL https://aclanthology. org/2021.acl-long.353. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of 9prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021. Lu, Y ., Bartolo, M., Moore, A., Riedel, S., and Stenetorp, P. Fantastically ordered prompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021. Muhlenbach, F., Lallich, S., and Zighed, D. A. Identifying and handling mislabelled instances. Journal of Intelligent Information Systems, 22(1):89–109, 2004. Mukherjee, S. and Awadallah, A. Uncertainty-aware self- training for few-shot text classiﬁcation. Advances in Neural Information Processing Systems, 33, 2020. Perez, E., Kiela, D., and Cho, K. True few-shot learning with language models. arXiv preprint arXiv:2105.11447, 2021. Pilehvar, M. T. and Camacho-Collados, J. Wic: the word-in- context dataset for evaluating context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21: 1–67, 2020. Ratner, A., Bach, S. H., Ehrenberg, H., Fries, J., Wu, S., and R´e, C. Snorkel: Rapid training data creation with weak supervision. The VLDB Journal, 29(2):709–730, 2020. Ratner, A. J., De Sa, C. M., Wu, S., Selsam, D., and R´e, C. Data programming: Creating large training sets, quickly. Advances in neural information processing systems, 29: 3567–3575, 2016. Sanh, V ., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chafﬁn, A., Stiegler, A., Le Scao, T., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. In The Tenth International Confer- ence on Learning Representations, 2022. Schick, T. and Sch¨utze, H. Exploiting cloze-questions for few-shot text classiﬁcation and natural language infer- ence. In Proceedings of the 16th Conference of the Eu- ropean Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021. Schick, T. and Sch ¨utze, H. True few-shot learning with prompts – a real-world perspective. Computing Research Repository, arXiv:2111.13440, 2021. URL http:// arxiv.org/abs/2001.07676. Scudder, H. Probability of error of some adaptive pattern- recognition machines. IEEE Transactions on Information Theory, 11(3):363–371, 1965. Shin, T., Razeghi, Y ., Logan IV , R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Lan- guage Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP), Online, November 2020. Association for Computational Linguis- tics. URL https://aclanthology.org/2020. emnlp-main.346. V oorhees, E. M. and Tice, D. M. Building a question an- swering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 200–207, 2000. Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Super- glue: a stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd Inter- national Conference on Neural Information Processing Systems, pp. 3266–3280, 2019a. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analy- sis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, 2019b. Wang, S., Liu, Y ., Xu, Y ., Zhu, C., and Zeng, M. Want to reduce labeling cost? GPT-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4195–4205, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguis- tics. URL https://aclanthology.org/2021. findings-emnlp.354. Wei, C., Shen, K., Chen, Y ., and Ma, T. Theoretical analysis of self-training with deep networks on unlabeled data. In International Conference on Learning Representations, 2020. Wei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V . Finetuned language models are zero-shot learners. arXiv:2109.01652, 2021. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jer- nite, Y ., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State- of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing: System Demonstrations , pp. 38–45, Online, October 2020. Association for Compu- tational Linguistics. URL https://www.aclweb. org/anthology/2020.emnlp-demos.6. Zhang, M.-L. and Zhou, Z.-H. Cotrade: Conﬁdent co- training with data editing. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 41(6):1612– 1626, 2011. Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th Interna- tional Conference on Machine Learning, volume 139, pp. 12697–12706, 2021. 10A. Algorithm details A.1. Relabeling Pseudolabels from previous iterations can either be re-used or thrown out. If the initial hypothesis has high precision but low coverage, it is typically preferable to re-use the pseudolabels from previous iterations, since as coverage increases the quality of the pseudolabels is likely to go down. On the other hand, if the models being trained are capable of correcting incorrect pseudolabels, it is preferable to relabel, since this can improve the quality of the training data in each iteration. In iteration tof co-training, we extract a pseudolabeled dataset Lt i from model hi and use it to train model h1−i. Let N ⊂[U] be a set of indices that correspond to the data points conﬁdently pseudolabeled by model hi in this iteration (according to either the model conﬁdence or cut statistic rankings). Deﬁne S = {(xn,ˆyn) : n ∈N} as the set of these points together with their pseudolabels ˆyn := argmax hi(φi(xn)). Let Lt−1 i = {(xn,˜yn)}be the conﬁdent data used in the previous iteration to train model h1−i. For xn that appear in Lt−1 i but where n ∈N , we have a choice to make: do we use the old pseudolabel ˜yn, or the new one ˆyn? These need not agree, since hi has been updated. Let S′ = {(xn,ˆyn) ∈S |¬∃y : (xn,y) ∈Lt−1 i }be the set of newly pseudolabeled examples—points that do not appear in Lt−1 i with any pseudolabel. If we choose to re-use the pseudolabels from the previous iteration, the GetConfData update is: Lt i ←Lt−1 i ∪S′ On the other hand, if we throw out the previously pseudola- beled data, the update is simply: Lt i ←S We exclusively use the latter, since our models can learn to correct bad initial labels (see Figure 2, Label 1). We found that the pseudolabel accuracy on the covered subset of data often increased with more iterations. This relabeling technique is different from the original cotraining algorithm (Blum & Mitchell, 1998), which builds Lcumulatively, but subsequent cotraining procedures also use relabeling (Zhang & Zhou, 2011). A.2. Warm starting Instead of warm-starting the models h0 and h1 (initializing them from the output of the previous iteration), we initialize them from scratch each co-training iteration to reduce the effect of a “bad” training iteration and so that we can use the same training hyperparameters for every iteration. This takes advantage of the fact that there exists a robust set of initial hyperparameters that have been shown to work well for ﬁne-tuning large language models. However, further exploration of warm-starting is an interesting direction for future work, since it may yield signiﬁcant reduction in the computational burden of co-training. A.3. Conﬁdent data selection Algorithm 2 shows how to select conﬁdent data using model conﬁdence, and Algorithm 3 shows how to select conﬁdent data using the cut statistic. As mentioned in the main text, φ0 and φ1 themselves needn’t be the representations used to compute nearest neighbors for the cut statistic. For example, φ0(x) for T0 is the non-contextual T0 input embedding ofx. Instead of computing nearest neighbors in this view, we use the contextual embedding from much later in the T0 model: the ﬁnal decoder embedding of the ﬁrst decoded token. This is a function of both φ0(x) and the current hypothesis h0. Because the embeddings are contextual, this representation has better nearest neighbors than φ0(x); because it also takes h0 into account, these neighbors are adapted to the current task. Similarly, for DeBERTa-large in view 1, we use the [CLS] token embedding in the last layer of the DeBERTa representation rather than the penultimate layer, since this layer has been adapted to the task of interest by the time we select conﬁdent data. Validation dataset We use a pseudolabeled validation set to perform model selection during the co-training iterations. Since the conﬁdent-data-selection methods can pick out the most precisely pseudolabeled examples (w.r.t. the true label), we also use them to select a conﬁdent validation set from the larger val set for each Train step. In particular, when training model hi, we use model h1−i to select a ˜β = β + tβ′conﬁdent fraction of full validation data in each step (the same fraction used for the conﬁdent training set). This allows us to use a more precise validation set for model selection. A.4. Full algorithms The detailed algorithms for co-training in the partial access setting and full access setting are shown in Algorithms 4 and 5, respectively. Algorithm 4 uses model conﬁdence for view 0 and cut statistic for view 1. Algorithm 5 uses cut statistic for both views. The detailed procedures for model conﬁdence and the cut statistic are shown in Algorithms 2 and 3, respectively. As mentioned in the previous section, the view 1 cut statistic uses the [CLS] token embedding in the last layer of h1. In the full access case, the view 0 cut statistic uses the T0 decoder’s hidden state for the ﬁrst decoded token. 11B. Training and dataset details B.1. Datasets • RTE (Dagan et al., 2005): Binary textual entailment, 2490 training examples, 277 validation examples (our test set). P[Y] = (0.5,0.5) • CB (De Marneffe et al., 2019): Ternary textual entailment, 250 training examples, 56 validation examples (our test set). P[Y] = (0.41,0.5,0.09) • WiC (Pilehvar & Camacho-Collados, 2018): Binary word sense disambiguation, 5428 training examples, 638 vali- dation examples (our test set). P[Y] = (0.5,0.5) • TREC (V oorhees & Tice, 2000): 6-way question classiﬁ- cation, 5452 training examples, 500 test examples. Label balance: P[Y] = (0.2131,0.2293,0.0158,0.2243,0.1643,0.1532) • BoolQ (Clark et al., 2019): Binary reading comprehen- sion, 9427 training examples, 3270 validation examples (our test). P[Y] = (0.38,0.62) B.2. Training details Prompt-based FT. We ﬁne-tuned the MLM-pretrained RoBERTa-large model using Adam for 1000 steps with batch size 16, learning rate 1e-5 and weight decay 0.01. We sampled a validation set the same size as the train- ing set while ensuring that the validation set also had an equal number of examples per class. This small validation set was used to select the best model checkpoint in each run and the test results were averaged over four random seeds. This is similar to the “no Ddev” setting in Gao et al. (2021) in that we didn’t use the small validation set for hyperparameter tuning—we used the same hyperparame- ters as the “no Ddev” setting. However, we still allow the method to use the labeled validation set for model selection. We used the same prompt templates as Gao et al. (2021). For RTE, the label words were Yes, No. For CB, the la- bel words were Yes, No, Maybe. For TREC, the label words were Description, Entity, Abbreviation, Person, Number, Location. Calibrate Before Use (CBU). For xcf, we followed Zhao et al. (2021) and used “N/A”, the empty string, and “[MASK]”. We obtained the GPT-3 outputs for each of these xcf’s, renormalized the outputs over the label tokens, aver- aged the re-normalized outputs across the three xcf’s, and used the average result as the scaling factor for W(i). This is identical to Zhao et al. (2021). Co-training. Following RoBERTa and DeBERTa, we used an MNLI-pretrained checkpoint for RTE and CB ( microsoft/deberta-large-mnli on Hug- gingFaceHub). Otherwise, we used DeBERTa-large (microsoft/deberta-large). We did not experi- ment with DeBERTa V2 or V3. B.3. Soft prompt encoding As detailed in Section 3, we combine hard prompt encoding with soft prompting. That is, we format the input using a hard prompt, and combine this formatted input embedding with the soft prompt matrix. This differs from the usual soft prompting setup (Li & Liang, 2021; Lester et al., 2021), where the input is encoded more neutrally, without a natural- language hard prompt: sentence1: {{x.premise}} sentence2: {{x.hypothesis}} A priori, this difference in input encoding could affect the performance of soft prompt tuning and the zero-shot per- formance of the initial prompted model. However, the full- training-dataset soft-prompt tuning baseline in Table 2 (T0 soft prompts on full training set) uses our hard prompt en- coding + soft prompting, and it matches fully ﬁne-tuned DeBERTa-large. This suggests that the accuracy loss from choosing a hard prompt (at least for the prompts that we chose) is minimal. Using the hard prompt encoding might improve the la- bel efﬁciency of soft prompt tuning, since the soft prompt parameters can focus on “ﬁxing up” the given hard prompt instead of learning a prompt-like embedding from scratch. On the other hand, if the hard prompt performs poorly, the hard prompt encoding might put an unnecessary upper limit on the soft prompt tuning performance, since the soft prompt may not be able to “undo” the hard prompt performance. An in-depth comparison between the neutral encoding from the traditional soft-prompting setup and the hard prompt + soft prompt encoding we propose is an interesting direction for future work. B.4. Hardware All models were trained on two NVIDIA A100 80Gb GPUs using PyTorch and the Transformers library (Wolf et al., 2020). For the partial access setting, a full run of T = 5 co-training iterations with DeBERTa-large takes roughly two hours on this hardware. For the full access setting, a full run of T = 5 co-training iterations with T0-3B and DeBERTa-large takes roughly 40 hours. 120.5 1.00.0 0.5 1.0  Label 0 0.5 1.00.0 0.5 1.0  Label 1 0.5 1.00.0 0.5 1.0  Label 2 0.5 1.00.0 0.5 1.0  Label 3 0.5 1.00.0 0.5 1.0  Label 4 0.5 1.00.0 0.5 1.0  Label 5 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Initial Coverage 0.0 0.2 0.4 0.6 0.8 1.0 Confident Precision per Label Confident Precision per Label on Training 0.5 1.00.0 0.5 1.0  Label 0 0.5 1.00.0 0.5 1.0  Label 1 0.5 1.00.0 0.5 1.0  Label 2 0.5 1.00.0 0.5 1.0  Label 3 0.5 1.00.0 0.5 1.0  Label 4 0.5 1.00.0 0.5 1.0  Label 5 0.0 0.2 0.4 0.6 0.8 1.0 Fraction of Initial Coverage 0.0 0.2 0.4 0.6 0.8 1.0 Confident Recall per Label Confident Recall per Label on Training Figure 4: Precision (left) and Recall (right) versus βfor each label in the initial conﬁdent set L0 0, extracted from the initial label model using the model conﬁdence method (Algorithm 2). The precision of some labels (e.g., 2, 3) begins to decline more sharply after β = 0.5. This gives additional evidence for our choice of β = 0.5: it trades off between the initial precision of L0 0 and the coverage for each label. At smaller values of β, there are no pseudolabeled examples for label 1 and very few for label 0. At larger values of β, the precision of the other labels is worse. C. Additional co-training analysis In this section, we provide more information regarding the evolution of h0 and h1 over the co-training iterations for TREC. We focus on the TREC dataset since its 6 classes enable us to investigate more complex co-training dynamics. To see the effect ofβon the quality of the initial conﬁdent data L0 0, we plot the precision and recall for each label for different values of βin Figure 4. This ﬁgure indicates that the tradeoff when choosing βis between having high preci- sion for each label (lower β) and having enough coverage for each label to train on (high β). To show how co-training affects label balance across mul- tiple iterations, we plot the total variation distance between the true label balance and the balance estimated using the pseudolabels in each iteration’s conﬁdent data Lt 0. Figure 5 indicates that this distance decreases with co-training it- erations, so the label model automatically learns to have a balance closer to the unseen true balance. In Figures 6, 7, and 8, we plot the recall, normalized coverage, and precision for each label in Lt 0 and Lt 1. The normalized coverage for label jis the number of examples with pseudolabel jdivided by the number of examples with true label j; it separates coverage from the precision, un- like recall. By comparing the evolution of label curves in Figure 7 and 8, we can see that the models tend to add more conﬁdent data when they are more precise and add less conﬁdent data when they are less precise, which is the desired behavior. Additionally, these ﬁgures show two dif- ferent ways in which co-training works to improve models: “coverage-expansion” and “pseudolabel-correction.” In the coverage-expansion regime, the precision for a label slightly decreases as iterations increase, but the coverage improves; this regime was predicted by early work on co-training (Bal- can et al., 2005). In the pseudolabel-correction regime, both precision and coverage increase, because models are able to learn to be more accurate than the pseudolabels used to 0 1 2 3 4 Iteration 0.0 0.1 0.2 0.3 0.4Total Variation Distance TREC h0 Total Variation Distance Figure 5: Total variation distance between the true label bal- ance and the label balance estimated from the pseudolabels Lt 0 at each iteration. As co-training iterations proceed, the label model automatically learns a balance closer to the true unseen label balance. train them. Wei et al. (2020) give a theoretical explanation of this in the context of self-training, rather than co-training. 130 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Recall TREC h0 Confident Recall Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Recall TREC h1 Confident Recall Label # 0 1 2 3 4 5 Figure 6: Recall of the conﬁdent pseudolabel set Lt 0 (left, extracted from h0 using Algorithm 2) and Lt 1 (right, extracted from h1 using Algorithm 3) for each label versus co-training iteration t. 0 1 2 3 4 Iterations 0.0 0.5 1.0 1.5Normalized Coverage TREC h0 Normalized Coverage Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.5 1.0 1.5Normalized Coverage TREC h1 Normalized Coverage Label # 0 1 2 3 4 5 Figure 7: Normalized coverage of the conﬁdent pseudolabel set Lt 0 (extracted from h0 using Algorithm 2) for each label versus co-training iteration t. Normalized coverage for label jis computed as |{(x,ˆy) ∈Lt i : ˆy= j}|/|{x: y(x) =j}| (the number of examples with conﬁdent pseudolabel jdivided by the number of examples with true label j). This metric decouples the coverage from the precision. The increasing slope of label 1 (left) indicates that h0 adds more conﬁdent data for label 1 in the later iterations. Combining this with the label 1 precision versus iteration curve in Figure 8 (left) indicates that the model adds more conﬁdent data for label 1 as it gets more precise, which is the desired behavior. On the other hand, for other labels (e.g. label 4) the rate of conﬁdent data addition and the precision stay relatively constant. 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Precision TREC h0 Confident Precision Label # 0 1 2 3 4 5 0 1 2 3 4 Iterations 0.0 0.2 0.4 0.6 0.8 1.0Confident Precision TREC h1 Confident Precision Label # 0 1 2 3 4 5 Figure 8: Precision per label vs co-training iteration, h0 (left—identical to right display of Figure 2) and h1 (right). Together with Figure 7, this indicates the two regimes of co-training. For labels 0 and 2-5, the precision decreases or remains the same while the coverage increases roughly linearly. This is the “coverage-expansion” regime, where the initial conﬁdent pseudolabels are high-precision and the model learns to imperfectly extend that initial signal to the uncovered data with some losses in precision. This regime is present in classical co-training results (Balcan et al., 2005). On the other hand, for label 1, both the coverage and the precision increase with the iteration t. This is the pseudolabel-correction regime, because the models are able to learn to be more accurate than the pseudolabels used to train them (compare h0 precision for label 1 to h1 precision for label 1 in the same iteration—the h1 model is trained on the labels from h0, but is able to select conﬁdent data with better precision than those labels). 14D. Prompts Here we list the prompts used for our experiments, largely taken from Sanh et al. (2022). D.1. Partial Access Setting RTE {example.premise} Question:{example.hypothesis} True, False, or Unknown? answer: {example.answer} {premise} Question: {hypothesis} True, False, or Unknown? answer: CB premise: {example.premise} hypothesis: {example.hypothesis} Does the premise imply the hypothesis? Yes, No, or Neither? answer: {example.answer} premise: {premise} hypothesis: {hypothesis} Does the premise imply the hypothesis? Yes, No, or Neither? answer: TREC Classify the questions based on whether their answer type is Unknown, Number, Location, Person, Description, Entity, or Abbreviation. Question: {example question} Answer Type: {example type} Question: {question} Answer Type: D.2. Full Access Setting RTE {premise} Question: {hypothesis} True, False, or Unknown? CB {premise} Question: {hypothesis} True, False, or Neither? BoolQ Text: {passage} Answer the following yes/no question: {question}? Yes or no? 15Algorithm 2 GetConfDataMC input {xn}U n=1 unlabeled examples input model hi, view φi input coverage fraction ˜β input minimum class percentage γ // compute pseudolabel and score for each example for nin {1,...,U }do on = hi(φi(xn)) (note on ∈Rnumlabels) ˆyn ←argmaxlonl sn ←maxlonl end for L←∅ // ﬁrst, select top γ% for each class by score ms←⌊γ˜βU⌋// min num points to select for lin {1,...,numlabels }do Il = {(xn,ˆyn,sn) : ˆyn = l} // sort Il by score sn (ascending) Sl ←Sort(Il, key=lambda q: q[2]) // add top α% to L(read off end of Sl) L←L∪Sl[-ms:] end for // now select the rest of the points rs←⌈˜βU⌉−|L|// num remaining points to select I= {(xn,ˆyn,sn)}U n=1 I←I\\ L// don’t select twice S ←Sort(I, key=lambda q: q[2]) L←L∪S[-rs:] // chop off score and return point + pseudolabel L←{(xn,ˆyn)|∃sn : (xn,ˆyn,sn) ∈L} return L Algorithm 3 GetConfDataCS input {xn}U n=1 unlabeled examples input model hi, view φi input coverage fraction ˜β // compute pseudolabel and repr. for each example for nin {1,...,U }do on = hi(φi(xn)) (note on ∈Rnumlabels) ˆyn ←argmaxlonl end for L←∅ for yin {1,...,numlabels }do ˆPy = |{n: ˆyn = y}|/U end for // compute 20 nearest neighbors for each ex. for uin {1,...,U }do N(u) = NN20(φi(xu),{φi(xv)}U v=1) for vin N(u) do wuv = 1/(1 +∥φ(xu) −φ(xv)∥2) Iuv = I[ˆyu ̸= ˆyv] end for // now compute cut statistic Ju = ∑ v∈N(u) wuvIuv µu = (1−ˆPˆyu ) ∑ v∈N(u) wuv σ2 = ˆPˆyu (1 −ˆPˆyu ) ∑ v∈N(u) w2 uv su = Ju−µu σ end for // now sort by statistic and return top data I←{ (xn,ˆyn,sn)}U n=1 S ←Sort(I, key=lambda q: q[2]) ns= ⌊˜βU⌋ L←S[:ns] L←{(xn,ˆyn)|∃sn : (xn,ˆyn,sn) ∈L} return L 16Algorithm 4 Co-training algorithm (detailed, GPT-3) input {(xj,yj)}k j=1 initial labeled examples input U= {xn}U n=1 unlabeled examples input initial coverage β, coverage increase β′ input minimum percentage per class γ // build view 0 for unlabeled examples for nin {1,...,U }do for jin {1,...,k }do φ(j) 0 (xn) ←GPT3(xn,(xj,yj)) end for φ0(xn) ← ( φ(1) 0 ; ... ; φ(k) 0 ) end for // build view 1 for unlabeled examples for nin {1,...,U }do // extract pre-trained DeBERTa representation forxn φ1(xn) ←DeBERTa(xn) end for // initialize h0 according to (1) for jin {1,...,k }do // get GPT-3 outputs on content-free input φ(j) 0 (xcf) ←GPT3(xcf,(xj,yj)) W(j) ←Diag ( 1 φ(j) 0 (xcf ) ) end for W ←{W(j)}k j=1 α←1 h0 ←h0( ·; W,α) // co-training loop for tin {0,...,T −1}do ˜β ←β+ tβ′ Lt 0 ←GetConfDataMC(U,h0,φ0,˜β,γ) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfDataCS(U,h1,φ1,˜β) h0 ←Train(φ0,Lt 1) end for return (h0,h1) Algorithm 5 Co-training algorithm (detailed, T0) input U= {xn}U n=1 unlabeled examples input initial coverage β, coverage increase β′ // format the input with a hard prompt template P, // then get T0 embedding to build view φ0 for nin {1,...,U }do ˜xn ←P(xn) φ0(xn) ←T0Emb(˜xn) end for // build view 1 for unlabeled examples for nin {1,...,U }do // extract pre-trained DeBERTa representation forxn φ1(xn) ←DeBERTa(xn) end for // initialize soft prompt with repeated pad token emb p←T0Emb([PAD]) h0(·) ←T0((p; p; ... ; p); ·) // co-training loop for tin {0,...,T −1}do ˜β ←β+ tβ′ Lt 0 ←GetConfDataCS(U,h0,φ0,˜β) h1 ←Train(φ1,Lt 0) Lt 1 ←GetConfDataCS(U,h1,φ1,˜β) h0 ←Train(φ0,Lt 1) end for return (h0,h1) 17",
      "meta_data": {
        "arxiv_id": "2202.00828v1",
        "authors": [
          "Hunter Lang",
          "Monica Agrawal",
          "Yoon Kim",
          "David Sontag"
        ],
        "published_date": "2022-02-02T00:48:26Z",
        "pdf_url": "https://arxiv.org/pdf/2202.00828v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper demonstrates that co-training, leveraging unlabeled data, significantly improves prompt-based learning for Large Language Models (LLMs) in few-shot and zero-shot settings. It tackles the brittleness and high model size requirements of prompting. Key findings include: (i) consistent performance improvement through iterative co-training, (ii) effectiveness of pseudo-labels from prompted models for fine-tuning smaller task-specific models, and (iii) notable performance gains on datasets challenging for traditional prompt-based methods, often bridging the gap with fully-supervised models. The approach also distills and refines knowledge into a smaller, performant model.",
        "methodology": "The core methodology adapts co-training for prompt-based learning, using two views: φ0 from a large prompt model (GPT-3 or T0) and φ1 from a smaller pretrained model (DeBERTa). Two settings are explored: 1. Partial Access (e.g., GPT-3): h0 is a learnable 'label model' that calibrates and ensembles multiple one-shot GPT-3 prompts using a ReLU-scaled sum with learned weights; h1 is the last few layers of DeBERTa. 2. Full Access (e.g., T0): h0 is a continuous soft prompt (matrices prepended to input embeddings, with frozen T0 layers); h1 is again the last few layers of DeBERTa. The co-training algorithm iteratively trains h1 on h0's confident pseudo-labels and vice-versa, using either 'model confidence' or 'cut statistic' methods for confident data selection. Pseudo-labels are re-generated in each iteration ('relabeling'), and models are re-initialized from scratch each round.",
        "experimental_setup": "The research was evaluated on standard NLP benchmarks: RTE, CB, TREC (for partial access), and RTE, CB, BoolQ (for full access, excluding datasets T0 was pretrained on, and BoolQ for GPT-3 due to quota). For partial access, k=4 labeled examples were used to initialize one-shot prompts. For full access, a zero-shot setting was used. Training hyperparameters were largely adopted from existing work, with co-training specific parameters (initial coverage β=0.5, coverage increase β'=0.1, T=5 iterations, minimum label frequency γ=0.01) tuned on a small gold-labeled TREC validation set. Validation during co-training used balanced accuracy on pseudo-labeled validation sets. Model Confidence was used for φ0 and Cut Statistic for φ1 in partial access, while Cut Statistic was used for both in full access. Baselines included GPT-3 32-shot, Calibrate Before Use (CBU), Prompt-based Fine-Tuning, and Snorkel variants. All models were trained on two NVIDIA A100 80Gb GPUs.",
        "limitations": "The work shares a common limitation with much current prompt-based learning: it's not 'true' few-shot/zero-shot in the strictest sense, as it implicitly uses small labeled sets for selecting model configurations (e.g., prompts, hyperparameters). Co-training's benefits are limited when the initial signal from the prompted model is too noisy (e.g., BoolQ with high total noise), when there's insufficient unlabeled data to achieve good pseudo-label generalization (e.g., CB due to dataset size causing overfitting in soft prompting), or when there's a large discrepancy in fully-supervised accuracy between the two views (e.g., RTE partial vs. full access). The specific combination of hard prompt encoding with soft prompting is noted as a potential constraint that needs further investigation.",
        "future_research_directions": "Future work could focus on investigating co-training within truly few-shot/zero-shot learning paradigms, minimizing any implicit reliance on labeled data for configuration. Developing methods to address the identified limitations is crucial, specifically: improving robustness when the initial prompted model signal is noisy (e.g., exploring different prompts or better initial h0 hypotheses), enhancing performance with limited unlabeled data (e.g., adjusting initial coverage or using less flexible h0 hypothesis classes), and mitigating issues arising from large accuracy gaps between the two views. Further exploration of warm-starting models in co-training is suggested to reduce computational burden. Finally, an in-depth comparison between traditional neutral-encoded soft-prompting and the proposed hard prompt + soft prompt encoding is an interesting avenue."
      }
    },
    {
      "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
      "abstract": "Hyperparameter optimization, also known as hyperparameter tuning, is a widely\nrecognized technique for improving model performance. Regrettably, when\ntraining private ML models, many practitioners often overlook the privacy risks\nassociated with hyperparameter optimization, which could potentially expose\nsensitive information about the underlying dataset. Currently, the sole\nexisting approach to allow privacy-preserving hyperparameter optimization is to\nuniformly and randomly select hyperparameters for a number of runs,\nsubsequently reporting the best-performing hyperparameter. In contrast, in\nnon-private settings, practitioners commonly utilize ``adaptive''\nhyperparameter optimization methods such as Gaussian process-based\noptimization, which select the next candidate based on information gathered\nfrom previous outputs. This substantial contrast between private and\nnon-private hyperparameter optimization underscores a critical concern. In our\npaper, we introduce DP-HyPO, a pioneering framework for ``adaptive'' private\nhyperparameter optimization, aiming to bridge the gap between private and\nnon-private hyperparameter optimization. To accomplish this, we provide a\ncomprehensive differential privacy analysis of our framework. Furthermore, we\nempirically demonstrate the effectiveness of DP-HyPO on a diverse set of\nreal-world datasets.",
      "full_text": "DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework Hua Wang∗ Sheng Gao† Huanyu Zhang‡ Weijie J. Su§ Milan Shen¶ November 28, 2023 Abstract Hyperparameter optimization, also known as hyperparameter tuning, is a widely recognized technique for improving model performance. Regrettably, when training private ML models, many practitioners often overlook the privacy risks associated with hyperparameter optimization, which could potentially expose sensitive information about the underlying dataset. Currently, the sole existing approach to allow privacy-preserving hyperparameter optimization is to uniformly and randomly select hyperparameters for a number of runs, subsequently reporting the best- performing hyperparameter. In contrast, in non-private settings, practitioners commonly utilize “adaptive” hyperparameter optimization methods such as Gaussian process-based optimization, which select the next candidate based on information gathered from previous outputs. This substantial contrast between private and non-private hyperparameter optimization underscores a critical concern. In our paper, we introduce DP-HyPO, a pioneering framework for “adaptive” private hyperparameter optimization, aiming to bridge the gap between private and non-private hyperparameter optimization. To accomplish this, we provide a comprehensive differential privacy analysis of our framework. Furthermore, we empirically demonstrate the effectiveness of DP-HyPO on a diverse set of real-world datasets. 1 Introduction In recent decades, modern deep learning has demonstrated remarkable advancements in various applications. Nonetheless, numerous training tasks involve the utilization of sensitive information pertaining to individuals, giving rise to substantial concerns regarding privacy [31, 7]. To address these concerns, the concept of differential privacy (DP) was introduced by [13, 14]. DP provides a mathematically rigorous framework for quantifying privacy leakage, and it has gained widespread acceptance as the most reliable approach for formally evaluating the privacy guarantees of machine learning algorithms. ∗Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: wanghua@wharton.upenn.edu. †Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: shenggao@wharton.upenn.edu. ‡Meta Platforms, Inc., New York, NY 10003, USA. Email:huanyuzhang@meta.com. §Department of Statistics and Data Science, University of Pennsylvania, Philadelphia, PA 19104, USA. Email: suw@wharton.upenn.edu. ¶Meta Platforms, Inc., Menlo Park, CA 94025, USA. Email:milanshen@gmail.com. 1 arXiv:2306.05734v2  [cs.LG]  27 Nov 2023When training deep learning models, the most popular method to ensure privacy is noisy (stochastic) gradient descent (DP-SGD) [4, 37]. DP-SGD typically resembles non-private gradient- based methods; however, it incorporates gradient clipping and noise injection. More specifically, each individual gradient is clipped to ensure a boundedℓ2 norm. Gaussian noise is then added to the average gradient which is utilized to update the model parameters. These adjustments guarantee a bounded sensitivity of each update, thereby enforcing DP through the introduction of additional noise. In both non-private and private settings, hyperparameter optimization (HPO) plays a crucial role in achieving optimal model performance. Commonly used methods for HPO include grid search (GS), random search (RS), and Bayesian optimization (BO). GS and RS approaches are typically non-adaptive, as they select the best hyperparameter from a predetermined or randomly selected set. While these methods are straightforward to implement, they can be computationally expensive and inefficient when dealing with large search spaces. As the dimensionality of hyperparameters increases, the number of potential trials may grow exponentially. To address this challenge, adaptive HPO methods such as Bayesian optimization have been introduced [36, 15, 42]. BO leverages a probabilistic model that maps hyperparameters to objective metrics, striking a balance between exploration and exploitation. BO quickly emerged as the default method for complex HPO tasks, offering improved efficiency and effectiveness compared to non-adaptive methods. While HPO is a well-studied problem, the integration of a DP constraint into HPO has received little attention. Previous works on DP machine learning often neglect to account for the privacy cost associated with HPO [1, 41, 44, 44]. These works either assume that the best parameters are known in advance or rely on a supplementary public dataset that closely resembles the private dataset distribution, which is not feasible in most real-world scenarios. Only recently have researchers turned to the important concept of honest HPO [30], where the privacy cost during HPO cannot be overlooked. Private HPO poses greater challenges compared to the non-private case for two primary reasons. First, learning with DP-SGD introduces additional hyperparameters (e.g., clipping norm, the noise scale, and stopping time), which hugely adds complexity to the search for optimal hyperparameters. Second, DP-SGD is more sensitive to the selection of hyperparameter combinations, with its performance largely influenced by this choice [30, 11, 33]. To tackle this challenging question, previous studies such as [26, 34] propose running the base algorithm with different hyperparameters a random number of times. They demonstrate that this approach significantly benefits privacy accounting, contrary to the traditional scaling of privacy guarantees with the square root of the number of runs (based on the composition properties from [21]). While these papers make valuable contributions, their approaches only allow for uniformly random subsampling from a finite and pre-fixed set of candidate hyperparameters at each run. As a result, any advanced technique from HPO literature that requires adaptivity is either prohibited or necessitates a considerable privacy cost (polynomially dependent on the number of runs), creating a substantial gap between non-private and private HPO methods. Given these considerations, a natural question arises:Can private hyperparameter optimization be adaptive, without a huge privacy cost?In this paper, we provide an affirmative answer to this question. 1.1 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on 2potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm outperforms its uniform counterpart across several practical scenarios.Gener- ally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Further- more, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 1.2 Our Contributions • We introduce the pioneering adaptive private hyperparameter optimization frame- work, DP-HyPO, which enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters. DP-HyPO permits the flexible use of non-DP adaptive hyperparameter optimization methods, such as Gaussian process, for enhanced efficiency, while avoiding the substantial privacy costs due to composition. In contrast to the non-adaptive methods presented in [34, 26], our adaptive framework, DP-HyPO, effectively bridges the gap between private and non-private hyperparameter optimization. Importantly, our framework not only encompasses the aforementioned non-adaptive methods as special cases, but also seamlessly integrates virtually all conceivable adaptive methods into the framework. • We provide sharp DP guarantees for the adaptive private hyperparameter optimiza- tion. Specifically, when the training procedure is executed multiple times, with each iteration being DP on its own, outputting the best repetition is DP ensured by the composition property. However, applying composition results in excessively loose privacy guarantees. Prior work in [26, 34] presents bounds that are either independent of the number of repetitions or depend logarithmically on it. Nevertheless, these results require that the hyperparameter selection for each iteration follows a uniform sampling distribution. In contrast, DP-HyPO allows arbitrary 3adaptive sampling distributions based on previous runs. Utilizing the Rényi DP framework, we offer a strict generalization of those uniform results by providing an accurate characterization of the Rényi divergence between the adaptive sampling distributions of neighboring datasets, without any stability assumptions. • Empirically, we observe that the Gaussian process-based DP-HyPO algorithm out- performs its uniform counterpartacross several practical scenarios. Generally, practitioners can integrate any non-private adaptive HPO methods into the DP-HyPO framework, opening up a vast range of adaptive private HPO algorithm possibilities. Furthermore, DP-HyPO grants practitioners the flexibility to determine the privacy budget allocation for adaptivity, empowering them to balance between the adaptivity and privacy loss when confronting various hyperparameter optimization challenges. 2 Preliminaries 2.1 Differential Privacy and Hyperparameter Optimization Differential Privacy is a mathematically rigorous framework for quantifying privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Formally, forε >0, and0 ≤ δ <1, we consider a (randomized) algorithmM : Zn → Ythat takes as input a dataset. Definition 2.1(Differential privacy). A randomized algorithmM is (ε, δ)-DP if for any neighboring dataset D, D′ ∈ Zn differing by an arbitrary sample, and for any eventE, we have P[M(D) ∈ E] ⩽ eε · P \u0002 M \u0000 D′\u0001 ∈ E \u0003 + δ. Here, ε and δ are privacy parameters that characterize the privacy guarantee of algorithmM. One of the fundamental properties of DP is composition. When multiple DP algorithms are sequentially composed, the resulting algorithm remains private. The total privacy cost of the composition scales approximately with the square root of the number of compositions [21]. We now formalize the problem of hyperparameter optimization with DP guarantees, which builds upon the finite-candidate framework presented in [26, 34]. Specifically, we consider a set of base DP algorithms Mλ : Zn → Y, whereλ ∈ Λ represents a set of hyperparameters of interest,Zn is the domain of datasets, andY denotes the range of the algorithms. This setΛ may be any infinite set, e.g., the cross product of the learning rateη and clipping normR in DP-SGD. We require that the set Λ is a measure space with an associated measureµ. Common choices forµ include the counting measure or Lebesgue measure. We make a mild assumption thatµ(Λ) < ∞. Based on the previous research [34], we make two simplifying assumptions. First, we assume that there is a total ordering on the rangeY, which allows us to compare two selected models based on their “performance measure”, denoted byq. Second, we assume that, for hyperparameter optimization purposes, we output the trained model, the hyperparameter, and the performance measure. Specifically, for any input datasetD and hyperparameterλ, the return value ofMλ is (x, q) ∼ Mλ(D), wherex represents the combination of the model parameters and the hyperparameter λ, andq is the (noisy) performance measure of the model. 42.2 Related Work In this section, we focus on related work concerning private HPO, while deferring the discussion on non-private HPO to Appendix F. Historically, research in DP machine learning has neglected the privacy cost associated with HPO [1, 41, 44]. It is only recently that researchers have begun to consider the honest HPO setting [30], in which the cost is taken into account. A direct approach to addressing this issue involves composition-based analysis. If each training run of a hyperparameter satisfies DP, the entire HPO procedure also complies with DP through composition across all attempted hyperparameter values. However, the challenge with this method is that the privacy guarantee derived from accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied(3ε, 0)-DP as long as each training run of a hyperparameter was (ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 3 DP-HyPO: General Framework for Private Hyperparameter Op- timization The obvious approach to the problem of differentially private hyperparameter optimization would be to run each base algorithm and simply return the best one. However, running such an algorithm on large hyperparameter space is not feasible due to the privacy cost growing linearly in the worst case. While [26, 34] have successfully reduced the privacy cost for hyperparameter optimization from linear to constant, there are still two major drawbacks. First, none of the previous methods considers the case when the potential number of hyperparameter candidates is infinite, which is common in most hyperparameter optimization scenarios. In fact, we typically start with a range of hyperparameters that we are interested in, rather than a discrete set of candidates. Furthermore, prior methods are 5limited to the uniform sampling scheme over the hyperparameter domainΛ. In practice, this setting is unrealistic since we want to “adapt” the selection based on previous results. For instance, one could use Gaussian process to adaptively choose the next hyperparameter for evaluation, based on all the previous outputs. However, no adaptive hyperparameter optimization method has been proposed or analyzed under the DP constraint. In this paper, we bridge this gap by introducing the first DP adaptive hyperparameter optimization framework. 3.1 DP-HyPO Framework To achieve adaptive hyperparameter optimization with differential privacy, we propose the DP-HyPO framework. Our approach keeps an adaptive sampling distributionπ at each iteration that reflects accumulated information. Let Q(D, π) be the procedure that randomly draws a hyperparameterλ from the distribution1 π ∈ D(Λ) , and then returns the output fromMλ(D). We allow the sampling distribution to depend on both the dataset and previous outputs, and we denote asπ(j) the sampling distribution at thej-th iteration on datasetD. Similarly, the sampling distribution at thej-th iteration on the neighborhood dataset D′ is denoted asπ′(j). We now present the DP-HyPO framework, denoted asA(D, π(0), T , C, c), in Framework 1. The algorithm takes a prior distributionπ(0) ∈ D(Λ) as input, which reflects arbitrary prior knowledge about the hyperparameter space. Another input is the distributionT of the total repetitions of training runs. Importantly, we require it to be a random variable rather than a fixed number to preserve privacy. The last two inputs areC and c, which are upper and lower bounds of the density of any posterior sampling distributions. A finiteC and a positivec are required to bound the privacy cost of the entire framework. Framework 1DP-HyPO A(D, π(0), T , C, c) Initialize π(0), a prior distribution overΛ. Initialize the result setA = {} Draw T ∼ T for j = 0 to T − 1 do (x, q) ∼ Q(D, π(j)) A = A ∪ {(x, q)} Update π(j+1) based onA according to any adaptive algorithm such that for allλ ∈ Λ, c ≤ π(j+1)(λ) π(0)(λ) ≤ C Output (x, q) from A with the highestq Note that we intentionally leave the update rule forπ(j+1) unspecified in Framework 1 to reflect the fact that any adaptive update rule that leverages information from previous runs can be used. However, for a non-private adaptive HPO update rule, the requirement of bounded adaptive density c ≤ π(j+1)(λ) π(0)(λ) ≤ C may be easily violated. In Section 3.2, We provide a simple projection technique 1Here, D(Λ) represents the space of probability densities onΛ. 6to privatize any non-private update rules. In Section 4, we provide an instantiation of DP-HyPO using Gaussian process. We now state our main privacy results for this framework in terms of Rényi Differential Privacy (RDP) [29]. RDP is a privacy measure that is more general than the commonly used(ε, δ)-DP and provides tighter privacy bounds for composition. We defer its exact definition to Definition A.2 in the appendix. We note that different distributions of the number of selections (iterations),T , result in very different privacy guarantees. Here, we showcase the key idea for deriving the privacy guarantee of DP-HyPO framework by considering a special case whenT follows a truncated negative binomial distribution2 NegBin(θ, γ) (the same assumption as in [34]). In fact, as we show in the proof of Theorem 1 in Appendix A, the privacy bounds only depend onT directly through its probability generating function, and therefore one can adapt the proof to obtain the corresponding privacy guarantees for other probability families, for example, the Possion distribution considered in [34]. From here and on, unless otherwise specified, we will stick withT = NegBin(θ, γ) for simplicity. We also assume for simplicity that the prior distributionπ(0) is a uniform distribution overΛ. We provide more detailed discussion of handling informed prior other than uniform distribution in Appendix D. Theorem 1. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞), γ ∈ (0, 1), and 0 < c≤ C. Suppose for allMλ : Zn → Yover λ ∈ Λ, the base algorithms satisfy(α, ε)-RDP and(ˆα, ˆε)-RDP for someε, ˆε ≥ 0, α∈ (1, ∞), and ˆα ∈ [1, ∞). Then the DP-HyPO algorithmA(D, π(0), NegBin(θ, γ), C, c) satisfies (α, ε′)-RDP where ε′ = ε + (1 +θ) · \u0012 1 − 1 ˆα \u0013 ˆε + \u0012 α α − 1 + 1 +θ \u0013 log C c + (1 + θ) · log(1/γ) ˆα + log E[T] α − 1 . To prove Theorem 1, one of our main technical contributions is Lemma A.4, which quantifies the Rényi divergence of the sampling distribution at each iteration between the neighboring datasets. We then leverage this crucial result and the probability generating function ofT to bound the Rényi divergence in the output ofA. We defer the detailed proof to Appendix A. Next, we present the case with pure DP guarantees. Recall the fact that(ε, 0)-DP is equivalent to (∞, ε)-RDP [29]. When bothα and ˆα tend towards infinity, we easily obtain the following theorem in terms of(ε, 0)-DP. Theorem 2. Suppose thatT follows truncated negative Binomial distributionT ∼ NegBin(θ, γ). Let θ ∈ (−1, ∞) and γ ∈ (0, 1). If all the base algorithmsMλ satisfies (ε, 0)-DP, then the DP-HyPO algorithm A(D, π(0), NegBin(θ, γ), C, c) satisfies \u0000 (2 + θ) \u0000 ε + log C c \u0001 , 0 \u0001 -DP. Theorem 1 and Theorem 2 provide practitioners the freedom to trade off between allocating more DP budget to enhance the base algorithm or to improve adaptivity. In particular, a higher value ofC c signifies greater adaptivity, while a largerε improves the performance of base algorithms. 3.1.1 Uniform Optimization Method as a Special Case We present the uniform hyperparameter optimization method [34, 25] in Algorithm 2, which is a special case of our general DP-HyPO Framework withC = c = 1. Essentially, this algorithm never updates the sampling distributionπ. 2Truncated negative binomial distribution is a direct generalization of the geometric distribution. See Appendix B for its definition. 7Algorithm 2Uniform Hyperparameter OptimizationU(D, θ, γ,Λ) Let π = Unif({1, ...,|Λ|}), andA = {} Draw T ∼ NegBin(θ, γ) for j = 0 to T − 1 do (x, q) ∼ Q(D, π) A = A ∪ {(x, q)} Output (x, q) from A with the highestq Our results in Theorem 1 and Theorem 2 generalize the main technical results of [34, 26]. Specifically, whenC = c = 1 and Λ is a finite discrete set, our Theorem 1 precisely recovers Theorem 2 in [34]. Furthermore, when we setθ = 1, the truncated negative binomial distribution reduces to the geometric distribution, and our Theorem 2 recovers Theorem 3.2 in [26] . 3.2 Practical Recipe to Privatize HPO Algorithms In the DP-HyPO framework, we begin with a prior and adaptively update it based on the accumulated information. However, for privacy purposes, we require the densityπ(j) to be bounded by some constants c and C, which is due to the potential privacy leakage when updatingπ(j) based on the history. It is crucial to note that this distributionπ(j) can be significantly different from the distribution π′(j) if we were given a different input datasetD′. Therefore, we require the probability mass/density function to satisfy c µ(Λ) ≤ π(j)(λ) ≤ C µ(Λ) for allλ ∈ Λ to control the privacy loss due to adaptivity. This requirement is not automatically satisfied and typically necessitates modifications to current non-private HPO methods. To address this challenge, we propose a general recipe to modify any non-private method. The idea is quite straightforward: throughout the algorithm, we maintain a non-private version of the distribution densityπ(j). When sampling from the spaceΛ, we perform a projection from π(j) to the space consisting of bounded densities. Specifically, we define the space of essentially bounded density functions bySC,c = {f ∈ ΛR+ : ess supf ≤ C µ(Λ), ess inff ≥ c µ(Λ), R α∈Λ f(α)dα = 1}. For such a space to be non-empty, we require thatc ≤ 1 ≤ C, where µ is the measure onΛ. This condition is well-defined as we assumeµ(Λ) < ∞. To privatizeπ(j) at thej-th iteration, we project it into the spaceSC,c, by solving the following convex functional programming problem: min f ∥f − π(j)∥2, s.t. f ∈ SC,c. (3.1) Note that this is a convex program sinceSC,c is convex and closed. We denote the output from this optimization problem byPSC,c(π(j)). Theoretically, problem(3.1) allows the hyperparameter space Λ to be general measurable space with arbitrary topological structure. However, empirically, practitioners need to discretizeΛ to some extent to make the convex optimization computationally feasible. Compared to the previous work, our formulation provides the most general characterization of the problem and allows pratitioners toadaptively and iteratively choose a proper discretization as needed. Framework 1 tolerates a much finer level of discretization than the previous method, as the performance of latter degrades fast when the number of candidates increases. We also provide 8examples using CVX to solve this problem in Section 4.2. In Appendix C, we discuss about its practical implementation, and the connection to information projection. 4 Application: DP-HyPO with Gaussian Process In this section, we provide an instantiation of DP-HyPO using Gaussian process (GP) [40]. GPs are popular non-parametric Bayesian models frequently employed for hyperparameter optimization. At the meta-level, GPs are trained to generate surrogate models by establishing a probability distribution over the performance measureq. While traditional GP implementations are not private, we leverage the approach introduced in Section 3.2 to design a private version that adheres to the bounded density contraint. We provide the algorithmic description in Section 4.1 and the empircal evaluation in Section 4.2. 4.1 Algorithm Description The following Algorithm (AGP) is a private version of Gaussian process for hyperparameter tuning. In Algorithm 3, we utilize GP to construct a surrogate model that generates probability distributions Algorithm 3DP-HyPO with Gaussian processAGP(D, θ, γ, τ, β,Λ, C, c) Initialize π(0) = Unif(Λ), andA = {} Draw T ∼ NegBin(θ, γ) for t = 0 to T − 1 do Truncate the density of currentπ(t) to be bounded into the range of[c, C] by projecting to SC,c. ˜π(t) = PSC,c(π(t)). Sample (x, q) ∼ Q(D, ˜π(j)), and updateA = A ∪ {(x, q)} Update mean estimation and variance estimation of the Gaussian processµλ, σ2 λ, and get the score assλ = µλ + τσλ. Update true (untruncated) posteriorπ(t+1) with softmax, byπ(t+1)(λ) = exp(β·sλ)R λ′∈Λ exp(β·s′ λ). Output (x, q) from A with the highestq for the performance measureq. By estimating the mean and variance, we assign a “score” to each hyperparameter λ, known as the estimated upper confidence bound (UCB). The weight factorτ controls the balance between exploration and exploitation, where larger weights prioritize exploration by assigning higher scores to hyperparameters with greater uncertainty. To transform these scores into a sampling distribution, we apply the softmax function across all hyperparameters, incorporating the parameterβ as the inverse temperature. A higher value ofβ signifies increased confidence in the learned scores for each hyperparameter. 4.2 Empirical Evaluations We now evaluate the performance of our GP-based DP-HyPO (referred to as “GP”) in various settings. Since DP-HyPO is the first adaptive private hyperparameter optimization method of its kind, we compare it to the special case of Uniform DP-HyPO (Algorithm 2), referred to as 9“Uniform”, as proposed in [26, 34]. In this demonstration, we consider two pragmatic privacy configurations: the white-box setting and the black-box setting, contingent on whether adaptive HPO algorithms incur extra privacy cost. In the white-box scenario (Section 4.2.1 and 4.2.2), we conduct experiments involving training deep learning models on both the MNIST dataset and CIFAR-10 dataset. Conversely, when considering the black-box setting (Section 4.2.3), our attention shifts to a real-world Federated Learning (FL) task from the industry. These scenarios provide meaningful insights into the effectiveness and applicability of our GP-based DP-HyPO approach. 4.2.1 MNIST Simulation We begin with the white-box scenario, in which the data curator aims to provide overall protection to the published model. In this context, to accommodate adaptive HPO algorithms, it becomes necessary to reduce the budget allocated to the base algorithm. In this section, we consider the MNIST dataset, where we employ DP-SGD to train a standard CNN. The base algorithms in this case are different DP-SGD models with varying hyperparameters, and we evaluate each base algorithm based on its accuracy. Our objective is to identify the best hyperparameters that produce the most optimal model within a given total privacy budget. Specifically, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping the other parameters fixed. We ensure that both the GP algorithm and the Uniform algorithm operate under the same total privacy budget, guaranteeing a fair comparison. Due to constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. For both base algorithms (with different noise multipliers), we cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add a Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. Further details on the simulation and parameter configuration can be found in Appendix E.1. In the left panel of Figure 1, we demonstrated the comparison of performance of the Uniform and GP methods with total privacy budgetε = 153 and δ = 1e − 5. The accuracy reported is the actual accuracy of the output hyperparameter. From the figure, we see that whenT is very small(T <8), GP method is slightly worse than Uniform method as GP spendslog(C/c) budget less than Uniform method for each base algorithm (the cost of adaptivity). However, we see that after a short period of exploration, GP consistently outperform Uniform, mostly due to the power of being adaptive. The superiority of GP is further demonstrated in Table 1, aggregating over geometric distribution. 4.2.2 CIFAR-10 Simulation When examining the results from MNIST, a legitimate critique arises: our DP-Hypo exhibits only marginal superiority over its uniform counterpart, which questions the assertion that adaptivity holds significant value. Our conjecture is that the hyperparameter landscape of MNIST is relatively uncomplicated, which limits the potential benefits of adaptive algorithms. 3The ε values are seemingly very large. Nonetheless, the reported privacy budget encompasses the overall cost of the entire HPO, which is typically overlooked in the existing literature. Given that HPO roughly incurs three times the privacy cost of the base algorithm, anε as high as15 could be reported as only5 in many other works. 10Figure 1: Left: The accuracy of the output hyperparameter in MNIST semi-real simulation, with ε = 15, δ = 0.00001. Middle: The accuracy of the output hyperparameter in CIFAR-10, with ε = 12, δ = 0.00001. Right: The loss of the output hyperparameter in FL. Error bars stands for95% confidence. Curves for GP are calculated by averaging400 independent runs, and curves for Uniform are calculated by averaging10000 independent runs. For a clearer demonstration, we compare the performance for each fixed value ofT, and recognize that the actual performance is a weighted average across different values ofT. To test the hypothesis, we conduct experiments on the CIFAR-10 dataset, with a setup closely mirroring the previous experiment: we employ the same CNN model for training, and optimize the same set of hyperparameters, which are the learning rateη and clipping normR. The primary difference lies in how we generate the hyperparameter landscape. Given that a single run on CIFAR-10 is considerably more time-consuming than on MNIST, conducting multiple runs for every hyperparameter combination is unfeasible. To address this challenge, we leverage BoTorch [3], an open-sourced library for HPO, to generate the landscape. Since we operate in the white-box setting, where the base algorithms have distinct privacy budgets for the uniform and adaptive scenarios, we execute 50 runs and generate the landscape for each case, including the mean (µλ) and standard error (σλ) of accuracy for each hyperparameter combinationλ. When the algorithm (GP or Uniform) visits a specificλ, our oracle returns a noisy scoreq(λ) drawn from a normal distribution of N(µλ, σλ). A more detailed description of our landscapes and parameter configuration can be found in Appendix E.2. In the middle of Figure 1, we showcase a performance comparison between the Uniform and GP methods with a total privacy budget ofε = 12 and δ = 1e − 5. Clearly, GP consistently outperforms the Uniform method, with the largest performance gap occurring when the number of runs is around 10. 4.2.3 Federated Learning In this section, we move to the black-box setting, where the privacy budget allocated to the base algorithm remains fixed, while we allow extra privacy budget for HPO. That being said, the adaptivity can be achieved without compromising the utility of the base algorithm. We explore another real-world scenario: a Federated Learning (FL) task conducted on a propri- etary dataset4 from industry. Our aim is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we once again rely on the landscape generated by BoTorch [3], as shown in Figure 3 in Appendix E.3. 4We have to respect confidentiality constraints that limit our ability to provide extensive details about this dataset. 11Under the assumption that base algorithms are black-box models with fixed privacy costs, we proceed with HPO while varying the degree of adaptivity. The experiment results are visualized in the right panel of Figure 1, and Table 2 presents the aggregated performance data. We consistently observe that GP outperforms Uniform in the black-box setting. Furthermore, our findings suggest that allocating a larger privacy budget to the GP method facilitates the acquisition of adaptive information, resulting in improved performance in HPO. This highlights the flexibility of GP in utilizing privacy resources effectively. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP 0.946 0.948 0.948 0.947 0.943 0.937 0.934 0.932 Uniform 0.943 0.945 0.945 0.944 0.940 0.935 0.932 0.929 Table 1:Accuracy of MNIST using Geometric Distribution with various different values ofγ for Uniform and GP methods. Each number is the mean of200 runs. Geometric(γ) 0.001 0.002 0.003 0.005 0.01 0.02 0.025 0.03 GP (C = 1.25) 0.00853 0.0088 0.00906 0.00958 0.0108 0.0129 0.0138 0.0146 GP (C = 1.33) 0.00821 0.00847 0.00872 0.00921 0.0104 0.0123 0.0132 0.0140 GP (C = 1.5) 0.00822 0.00848 0.00872 0.00920 0.0103 0.0123 0.0131 0.0130 Uniform 0.0104 0.0106 0.0109 0.0113 0.0123 0.0141 0.0149 0.0156 Table 2:Loss of FL using Geometric Distribution with various different values ofγ for Uniform and GP methods with different choice ofC and c = 1/C. Each number is the mean of200 runs. 5 Conclusion In conclusion, this paper presents a novel framework, DP-HyPO. As the first adaptive HPO framework with sharp DP guarantees, DP-HyPO effectively bridges the gap between private and non-private HPO. Our work encompasses the random search method by [26, 34] as a special case, while also granting practitioners the ability to adaptively learn better sampling distributions based on previous runs. Importantly, DP-HyPO enables the conversion of any non-private adaptive HPO algorithm into a private one. Our framework proves to be a powerful tool for professionals seeking optimal model performance and robust DP guarantees. The DP-HyPO framework presents two interesting future directions. One prospect involves an alternative HPO specification which is practically more favorable. Considering the extensive literature on HPO, there is a significant potential to improve the empirical performance by leveraging more advanced HPO methods. Secondly, there is an interest in establishing a theoretical utility guarantee for DP-HyPO. By leveraging similar proof methodologies to those in Theorem 3.3 in [26], it is feasible to provide basic utility guarantees for the general DP-HyPO, or for some specific configurations within DP-HyPO. 126 Acknowledgements The authors would like to thank Max Balandat for his thoughtful comments and insights that helped us improve the paper. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. [2] Martin S Andersen, Joachim Dahl, Lieven Vandenberghe, et al. Cvxopt: A python package for convex optimization.Available at cvxopt. org, 54, 2013. [3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020. [4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. [5] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper- parameter optimization. Advances in neural information processing systems, 24, 2011. [6] Mark Bun, Gautam Kamath, Thomas Steinke, and Steven Z Wu. Private hypothesis selection. Advances in Neural Information Processing Systems, 32, 2019. [7] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. InUSENIX Security Symposium, volume 267, 2019. [8] Kamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differ- entially private machine learning.Advances in Neural Information Processing Systems, 26, 2013. [9] Edith Cohen, Xin Lyu, Jelani Nelson, Tamás Sarlós, and Uri Stemmer. Generalized private selection and testing with high confidence.arXiv preprint arXiv:2211.12063, 2022. [10] Imre Csiszár and Frantisek Matus. Information projections revisited.IEEE Transactions on Information Theory, 49(6):1474–1490, 2003. [11] Soham De, Leonard Berrada, Jamie Hayes, Samuel L Smith, and Borja Balle. Unlock- ing high-accuracy differentially private image classification through scale. arXiv preprint arXiv:2204.13650, 2022. [12] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy.Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):3–37, 2022. 13[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. InTheory of Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. [14] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N Rothblum, and Salil Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 381–390, 2009. [15] Matthias Feurer and Frank Hutter. Hyperparameter optimization.Automated machine learning: Methods, systems, challenges, pages 3–33, 2019. [16] Yonatan Geifman and Ran El-Yaniv. Deep active learning with a neural architecture search. Advances in Neural Information Processing Systems, 32, 2019. [17] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. InConference on Learning Theory, pages 1785–1816. PMLR, 2020. [18] Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art.Knowledge- Based Systems, 212:106622, 2021. [19] Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate differentiable architecture search.arXiv preprint arXiv:1903.09900, 2019. [20] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. InLearning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pages 507–523. Springer, 2011. [21] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. InInternational conference on machine learning, pages 1376–1385. PMLR, 2015. [22] Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport.Advances in neural information processing systems, 31, 2018. [23] Rajiv Khanna, Joydeep Ghosh, Rusell Poldrack, and Oluwasanmi Koyejo. Information projection and approximate inference for structured sparse variables. InArtificial Intelligence and Statistics, pages 1358–1366. PMLR, 2017. [24] Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. InUncertainty in artificial intelligence, pages 367–377. PMLR, 2020. [25] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper- band: A novel bandit-based approach to hyperparameter optimization.The Journal of Machine Learning Research, 18(1):6765–6816, 2017. [26] Jingcheng Liu and Kunal Talwar. Private selection from private candidates. InProceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 298–309, 2019. 14[27] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 94–103. IEEE, 2007. [28] Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Towards automatically-tuned neural networks. InWorkshop on automatic machine learning, pages 58–65. PMLR, 2016. [29] Ilya Mironov. Rényi differential privacy. In2017 IEEE 30th computer security foundations symposium (CSF), pages 263–275. IEEE, 2017. [30] Shubhankar Mohapatra, Sajin Sasy, Xi He, Gautam Kamath, and Om Thakkar. The role of adaptive optimizers for honest private hyperparameter selection. InProceedings of the aaai conference on artificial intelligence, volume 36, pages 7806–7813, 2022. [31] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages 739–753. IEEE, 2019. [32] Renato Negrinho, Matthew Gormley, Geoffrey J Gordon, Darshan Patil, Nghia Le, and Daniel Ferreira. Towards modular and programmable architecture search.Advances in neural informa- tion processing systems, 32, 2019. [33] Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, and Prateek Mittal. Dp-raft: A differentially private recipe for accelerated fine-tuning.arXiv preprint arXiv:2212.04486, 2022. [34] Nicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy. In International Conference on Learning Representations, 2021. [35] Carl Edward Rasmussen. Gaussian processes in machine learning. In Advanced Lectures on Machine Learning: ML Summer Schools 2003, Canberra, Australia, February 2-14, 2003, Tübingen, Germany, August 4-16, 2003, Revised Lectures, pages 63–71. Springer, 2004. [36] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization.Proceedings of the IEEE, 104(1):148–175, 2015. [37] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245–248. IEEE, 2013. [38] Salil Vadhan. The complexity of differential privacy.Tutorials on the Foundations of Cryptogra- phy: Dedicated to Oded Goldreich, pages 347–450, 2017. [39] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition of differential privacy via the edgeworth accountant.arXiv preprint arXiv:2206.04236, 2022. [40] Christopher KI Williams and Carl Edward Rasmussen.Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. 15[41] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via low-rank reparametrization. In International Conference on Machine Learning, pages 12208–12218. PMLR, 2021. [42] Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [43] Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search.arXiv preprint arXiv:1807.06906, 2018. [44] Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021. 16A Proofs of the technical results A.1 Proof of Main Results First, we define Rényi divergence as follows. Definition A.1(Rényi Divergences). Let P and Q be probability distributions on a common space Ω. Assume thatP is absolutely continuous with respect toQ - i.e., for all measurableE ⊂ Ω, if Q(E) = 0, thenP(E) = 0. Let P(x) and Q(x) denote the densities ofP and Q respectively. The KL divergence fromP to Q is defined as D1(P∥Q) := E X←P \u0014 log \u0012P(X) Q(X) \u0013\u0015 = Z Ω P(x) log \u0012P(x) Q(x) \u0013 dx. The max divergence fromP to Q is defined as D∞(P∥Q) := sup \u001a log \u0012P(E) Q(E) \u0013 : P(E) > 0 \u001b . For α ∈ (1, ∞), the Rényi divergence fromP to Q of orderα is defined as Dα(P∥Q) := 1 α − 1 log   E X←P \"\u0012P(X) Q(X) \u0013α−1#! = 1 α − 1 log \u0012 E X←Q \u0014\u0012P(X) Q(X) \u0013α\u0015\u0013 = 1 α − 1 log \u0012Z Q P(x)αQ(x)1−αdx \u0013 . We now present the definition of Rényi DP (RDP) in [29]. Definition A.2(Rényi Differential Privacy). A randomized algorithmM : Xn → Yis (α, ε)-Rényi differentially private if, for all neighbouring pairs of inputsD, D′ ∈ Xn, Dα (M(x)∥M (x′)) ≤ ε. We define some additional notations for the sake of the proofs. In algorithm 1, for any1 ≤ j ≤ T, and neighboring datasetD and D′, we define the following notations for anyy = (x, q) ∈ Y, the totally ordered range set. Pj(y) = P˜y∼Q(D,π(j))(˜y = y) and P′ j(y) = P˜y∼Q(D′,π′(j))(˜y = y) Pj(≤ y) = P˜y∼Q(D,π(j))(˜y ≤ y) and P′ j(≤ y) = P˜y∼Q(D′,π′(j))(˜y ≤ y) Pj(< y) = P˜y∼Q(D,π(j))(˜y < y) and P′ j(< y) = P˜y∼Q(D′,π′(j))(˜y < y). By these definitions, we havePj(≤ y) = Pj(< y) + Pj(y), andP′ j(≤ y) = P′ j(< y) + P′ j(y). And additionally, we have Pj(y) P′ j(y) = R λ∈Λ P(Mλ(D) = y)π(j)(λ)dλR λ∈Λ P(Mλ(D′) = y)π′(j)(λ)dλ ≤ sup λ∈Λ P(Mλ(D) = y)π(j)(λ) P(Mλ(D′) = y)π′(j)(λ) ≤ C c · sup λ∈Λ P(Mλ(D) = y) P(Mλ(D′) = y). (A.1) 17Here, the first inequality follows from the simple property of integration, and the second inequality follows from the fact thatπ(j) has bounded density betweenc and C. Similarly, we have Pj(≤ y) P′ j(≤ y) ≤ C c · sup λ∈Λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y), (A.2) and Pj(< y) P′ j(< y) ≤ C c · sup λ∈Λ P(Mλ(D) < y) P(Mλ(D′) < y). (A.3) Note thatD and D′ are neighboring datasets, andMλ satisfies some DP guarantees. So the ratio P(Mλ(D)∈E) P(Mλ(D′)∈E) for any eventE can be bounded. For simplicity, we define the inner product of a distribution π with the vector M(D) = (P(Mλ(D) = y) : λ ∈ Λ) as π · M(D) := Z λ∈Λ P(Mλ(D) = y)π(λ)dλ. (A.4) Now, we define additional notations to bound the probabilities. RecallSC,s is given by{f ∈ ΛR+ : ess supf ≤ C, ess inff ≥ c, R α∈Λ f(α)dα = 1.}. It is straightforward to see this is a compact set as it is the intersection of three compact sets. We define P+(y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) = y)π(j)(λ)dλ = π+ · M(D), (A.5) where π+ is the distribution that achieves the supreme in the compact setSC,c. Similarly, we define P′−(y) for D′ as given by P′−(y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) = y) · π′(j)(λ)dλ = π′− · M. (A.6) Similarly, we can defineP′+(y) and P−(y) accordingly. From the definition, we know that P−(y) ≤ Pj(y) ≤ P+(y) and P′−(y) ≤ P′ j(y) ≤ P′+(y). (A.7) We also have P+(y) P′−(y) = π∗ · M(D) π′− · M(D′) ≤ sup λ P(Mλ(D) = y) P(Mλ(D′) = y) · C c . (A.8) It is similar to define P+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′+(≤ y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) ≤ y) and P′−(≤ y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) ≤ y) P+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′+(< y) := sup π∈SC,c Z λ∈Λ P(Mλ(D′) < y) 18P−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D) < y) and P′−(< y) := inf π∈SC,c Z λ∈Λ P(Mλ(D′) < y). Following the exact same proof, we have P−(≤ y) ≤ Pj(≤ y) ≤ P+(≤ y) and P′−(≤ y) ≤ P′ j(≤ y) ≤ P′+(≤ y) (A.9) P−(< y) ≤ Pj(< y) ≤ P+(< y) and P′−(< y) ≤ P′ j(< y) ≤ P′+(< y) (A.10) P+(≤ y) P′−(≤ y) ≤ sup λ P(Mλ(D) ≤ y) P(Mλ(D′) ≤ y) · C c and P+(< y) P′−(< y) ≤ sup λ P(Mλ(D) < y) P(Mλ(D′) < y) · C c . (A.11) It is also straightforward to verify from the definition that P+(≤ y) = P+(< y) + P+(y) and P′+(≤ y) = P′+(< y) + P′+(y) (A.12) P+ − (≤ y) = P−(< y) + P−(y) and P′−(≤ y) = P′−(< y) + P′−(y). (A.13) Lemma A.3.Suppose ifaλ, bλ are non-negative andcλ, c′ λ are positive for allλ. Then we have P λ aλcλP λ bλc′ λ ≤ P λ aλP λ bλ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. Proof of Lemma A.3.This lemma is pretty straight forward by comparing the coefficient for each term in the full expansion. Specifically, we re-write the inequality as X λ aλcλ X λ′ b′ λ ≤ X λ aλ X λ′ b′ λc′ λ · sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f. (A.14) For each termaλb′ λ, its coefficient on the left hand side of(A.14) is cλ, but its coefficient on the right hand side of(A.14) is c′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f. Since we always havec′ λ · supλ,λ′ \f\f\fcλ c′ λ \f\f\f ≥ cλ, andaλb′ λ ≥ 0, we know the inequality (A.14) holds. Next, in order to present our results in terms of RDP guarantees, we prove the following lemma. Lemma A.4.The Rényi divergence betweenP+ and P− is be bounded as follows: Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 Proof of Lemma A.4.We write that e(α−1)Dα(P+∥P′−) = X y∈Y P+(y)α · P′−(y)1−α = X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 (A.15) Here, π+ and π′− are defined in(A.5) and (A.6), so they are essentiallyπ+ y and π′− y as they depend on the value ofy. Therefore, we need to “remove” this dependence ony to leverage the RDP guarantees for each base algorithmMλ. We accomplish this task by bridging viaπ, the uniform 19density onΛ (that isπ(λ) = π(λ′) for anyλ, λ′ ∈ Λ). Specifically, we defineaλ = π(λ)P(Mλ(D) = y), bλ = π(λ)P(Mλ(D′) = y), cλ = π+ y (λ) π(λ) , andc′ λ = π′− y (λ) π(λ) . We see that sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)/π(λ) π′−y (λ′)/π(λ′) \f\f\f\f\f = sup λ,λ′ \f\f\f\f\f π+ y (λ)) π′−y (λ′) \f\f\f\f\f ≤ C/c, (A.16) since π is the uniform, andπ+ y and π′− y belongs toSC,c. We now apply Lemma A.3 with the above notations for eachy to (A.15), and we have X y∈Y (P λ π+(λ)P(Mλ(D) = y))α (P λ π′−(λ)P(Mλ(D′) = y))α−1 = X y∈Y \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 \u0010P λ π(λ)P(Mλ(D′) = y) · π′−(λ) π(λ) \u0011α−1 = X y∈Y (P λ aλ · cλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ · c′ λ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 \u0010P λ π(λ)P(Mλ(D) = y) · π+(λ) π(λ) \u0011 (P λ bλ)α−1 = X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ · cλ) (P λ bλ)α−1 ≤ X y∈Y sup λ,λ′ \f\f\f\f cλ c′ λ \f\f\f\f α−1 (P λ aλ)α−1 (P λ aλ) · supλ cλ (P λ bλ)α−1 ≤ X y∈Y \u0012C c \u0013α−1 (P λ aλ)α−1 (P λ aλ) · \u0000C c \u0001 (P λ bλ)α−1 = X y∈Y \u0012C c \u0013α · (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 The first inequality is due to Lemma A.3, the second inequality is becauseaλ are non-negative, and the last inequality is because of(A.16) and the fact that bothπ+(λ) and π(λ) are defined inSC,c, and thus their ratio is upper bounded byC c for anyλ. Now we only need to prove that for any fixed distributionπ that doesn’t depend on valuey, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ sup λ∈Λ e(α−1)Dα(Mλ(D)∥Mλ(D′)). (A.17) With this result, we immediately know the result holds for uniform distributionπ as a special case. To prove this result, we first observe that the functionf(u, v) = uαv1−α is a convex function. This 20is because the Hessian off is \u0012α(α − 1)uα−2v1−α −α(α − 1)uα−1v−α −α(α − 1)uα−1v−α α(α − 1)uαv−α−1 \u0013 , which is easy to see to be positive semi-definite. And now, consider any distributionπ, denote u(λ) = P(Mλ(D) = y) and v(λ) = P(Mλ(D′) = y) by Jensen’s inequality, we have f( X λ π(λ)u(λ), X λ π(λ)v(λ)) ≤ X λ π(λ)f(u(λ), v(λ)). By adding the summation overy on both side of the above inequality, we have X y∈Y (P λ π(λ)P(Mλ(D) = y))α (P λ π(λ)P(Mλ(D′) = y))α−1 ≤ X y∈Y X λ π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = X λ X y∈Y π(λ) P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 ≤ sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 . The first equality is due to Fubini’s theorem. And the second inequality is straight forward as one observe π(λ) only depends onλ. This concludes the proof as we know that e(α−1)Dα(P+∥P′−) ≤ \u0012C c \u0013α sup λ X y∈Y P(Mλ(D) = y)α P(Mλ(D′) = y)α−1 = \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′) or equivalently, Dα(P+∥P′−) ≤ α α − 1 log C c + sup λ∈Λ Dα \u0000 Mλ(D)∥Mλ(D′) \u0001 . We now present our crucial technical lemma for adaptive hyperparameter tuing with any distribution on the number of repetitionsT. This is a generalization from [34]. Lemma A.5.Fix α >1. LetT be a random variable supported onN≥0. Letf : [0, 1] → R be the probability generating function ofK, that is,f(x) = P∞ k=0 P[T = k]xk. Let Mλ and M′ λ be the base algorithm forλ ∈ Λ on Y on D and D′ respectively. Define A1 := A(D, π(0), T , C, c), andA2 := A(D′, π(0), T , C, c). Then Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 , where applying the same postprocessing to the bounding probabilitiesP+ and P′− gives probabilitiesq and q′ respectively. This means that, there exist a function setg : Y →[0, 1] such thatq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. 21Proof of Lemma A.5.We consider the event thatA1 outputs y. By definition, we have A1(y) = ∞X k=1 P(T = k)[ kY j=1 Pj(≤ y) − kY i=1 Pj(< y)] = ∞X k=1 P(T = k)[ kX i=1 Pi(y) i−1Y j=1 Pj(< y) · kY j=i+1 Pj(≤ y)] ≤ ∞X k=1 P(T = k)[ kX i=1 P+(y) i−1Y j=1 P+(< y) · kY j=i+1 P+(≤ y)] = ∞X k=1 P(T = k)[ kX i=1 P+(y) · P+(< y)i−1 · P+(≤ y)k−i] = ∞X k=1 P(T = k)[P+(≤ y)k − P+(< y)k] = f(P+(≤ y)) − f(P+(< y)) = P+(y) · E X←Uniform([P+(<y),P+(≤y)]) [f′(X)]. The second equality is by partitioning on the events of the first time of gettingy, we usei to index such a time. The third inequality is using(A.7), (A.9), and(A.10). The third to last equality is by (A.12) and algebra. The second to last equality is by definition of the probability generating function f. The last equality follows from definition of integral. Similarly, we have A2(y) ≥ ∞X k=1 P(T = k)[P′−(≤ y)k − P′−(< y)k] = P′−(y) · E X←Uniform([P′−(<y),P′−(≤y)]) [f′(X)]. The rest part of the proof is standard and follows similarly as in [34]. Specifically, we have e(α−1)Dα(A1∥A2) = X y∈Y A1(y)α · A2(y)1−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] \u0002 f′(X) \u0003α · E X′←[P′−(<y),P′−(≤y)] \u0002 f′ \u0000 X′\u0001\u00031−α ≤ X y∈Y P+(y)α · P′−(y)1−α · E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ \u0012C c \u0013α sup λ e(α−1)Dα(Mλ(D)∥Mλ(D′)) · max y∈Y E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi . The last inequality follows from Lemma A.4. The second inequality follows from the fact that, for any α ∈ R, the functionh : (0, ∞)2 → (0, ∞) given byh(u, v) = uα · v1−α is convex. Therefore, E[U]αE[V ]1−α = h(E[(U, V)]) ≤ E[h(U, V)] = E \u0002 Uα · V 1−α\u0003 all positive random variables(U, V). Note that X and X′ are required to be uniform separately, but their joint distribution can be 22arbitrary. As in [34], we will couple them so thatX−P+(<y) P+(y) = X′−P′−(<y) P′−(y) . In particular, this implies that, for eachy ∈ Y, there exists somet ∈ [0, 1] such that E X←[P+(<y),P+(≤y)] X′←[P′−(<y),P′−(≤y)] h f′(X)α · f′ \u0000 X′\u00011−αi ≤ f′(P+(< y)+t·P+(y))α ·f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α Therefore, we have Dα (A1∥A2) ≤sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log  max y∈Y t∈[0,1] f′(P+(< y) + t · P+(y))α · f′ \u0000 P′−(< y) + t · P′−(y) \u00011−α  . To prove the result, we simply fixy∗ ∈ Yand t∗ ∈ [0, 1] achieving the maximum above and define g(y) :=    1 if y < y∗ t∗ if y = y∗ 0 if y > y∗ The result directly follows by settingq = E X←P+ [g(X)] and q′ = E X′←P′− [g (X′)]. Now we can prove Theorem 1, given the previous technical lemma. The proof share similarity to the proof of Theorem 2 in [34] with the key difference from the different form in Lemma A.5. We demonstrate this proof as follows for completeness. Proof of Theorem 1.We first specify the probability generating function of the truncated negative binomial distribution f(x) = E T∼NegBin(θ,γ) \u0002 xT \u0003 = ((1−(1−γ)x)−θ−1 γ−θ−1 if θ ̸= 0 log(1−1−γ)x) log(γ) if θ = 0 Therefore, f′(x) = (1 − (1 − γ)x)−θ−1 · (θ·(1−γ) γ−θ−1 if θ ̸= 0 1−γ log(1/γ) if θ = 0 = (1 − (1 − γ)x)−θ−1 · γθ+1 · E[T]. By Lemma A.5, for appropriate valuesq, q′ ∈ [0, 1] and for allα >1 and all ˆα >1, we have Dα (A1∥A2) ≤ sup λ Dα \u0000 Mλ∥M′ λ \u0001 + α α − 1 log C c + 1 α − 1 log \u0010 f′(q)α · f′ \u0000 q′\u00011−α\u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · (1 − (1 − γ)q)−α(θ+1) · \u0000 1 − (1 − γ)q′\u0001−(1−α)(θ+1)\u0011 = ε + α α − 1 log C c 23+ 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 (γ + (1 − γ)(1 − q))1−ˆα · \u0000 γ + (1 − γ) \u0000 1 − q′\u0001\u0001ˆα\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here, we letˆαν = (α − 1)(1 + θ) and (1 − ˆα)ν + u = −α(θ + 1)) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1)Dˆα(P+∥P−)\u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (Here,1 − q and 1 − q′ are postprocessings of someP+ and P′− respectively ande(ˆα−1)Dˆα(·∥·) is convex) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 (By Lemma A.4) ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · (γ + (1 − γ)(1 − q))u \u0011 ≤ ε + α α − 1 log C c + 1 α − 1 log \u0010 γθ+1 · E[T] · \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011ν · γu \u0011 (Here γ ≤ γ + (1 − γ)(1 − q) and u ≤ 0) = ε + α α − 1 log C c + ν α − 1 log \u0010 γ + (1 − γ) · e(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011 + 1 α − 1 log \u0010 γθ+1 · E[T] · γu \u0011 = ε + α α − 1 log C c + ν α − 1 \u0012 (ˆα − 1) sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + ˆα log C c + log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011\u0011 + 1 α − 1 log \u0010 γu+θ+1 · E[T] \u0011 = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u0010 1 − γ · \u0010 1 − e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)+ˆα log C c \u0011\u0011 + log(E[T]) α − 1 + 1 + θ ˆα log(1/γ) (Here we haveν = (α − 1)(1 + θ) ˆα and u = −(1 + θ) \u0012α − 1 ˆα + 1 \u0013 ) = ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 sup λ Dˆα \u0000 Mλ∥M′ λ \u0001 + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ − 1 + e−(ˆα−1) supλ Dˆα(Mλ∥M′ λ)−ˆα log C c \u0013 + log(E[T]) α − 1 ≤ ε + α α − 1 log C c + (1 +θ) \u0012 1 − 1 ˆα \u0013 ˆε + (1 +θ) log C c + 1 + θ ˆα log \u00121 γ \u0013 + log(E[T]) α − 1 , which completes the proof. B Truncated Negative Binomial Distribution We introduce the definition of truncated negative binomial distribution [34] in this section. Definition B.1.(Truncated Negative Binomial Distribution [34]). Let γ ∈ (0, 1) and θ ∈ (−1, ∞). Define a distribution NegBin(θ, γ) on N+ as follows: 24• If θ ̸= 0 and T is drawn from NegBin(θ, γ), then ∀k ∈ N P [T = k] = (1 − γ)k γ−θ − 1 · k−1Y ℓ=0 \u0012ℓ + θ ℓ + 1 \u0013 and E[T] = θ·(1−γ) γ·(1−γθ). Note that whenθ = 1, it reduces to the geometric distribution with parameter γ. • If θ = 0 and T is drawn from NegBin(0, γ), then P[T = k] = (1 − γ)k k · log(1/γ) and E[T] = 1/γ−1 log(1/γ). C Privatization of Sampling Distribution C.1 General Functional Projection Framework In section 3.2, we define the projection onto a convex setSC,c as an optimization in terms ofℓ2 loss. More generally, we can perform the following general projection at thej-th iteration by considering an additional penalty term, with a constantν: min f ∥f − π(j)∥2 + νKL(π(j), f) (C.1) s.t. f ∈ SC,c. When ν = 0, we recover the originalℓ2 projection. Moreover, it’s worth noting that our formulation has implications for the information projection literature [10, 23]. Specifically, as the penalty term parameterν approaches infinity, the optimization problem evolves into a minimization of KL divergence, recovering the objective function of information projection (in this instance, moment projection). However, the constraint sets in the literature of information projection are generally much simpler than our setSC,c, making it infeasible to directly borrow methods from its field. To the best of our knowledge, our framework is the first to address this specific problem in functional projection and establish a connection to information projection in the DP community. C.2 Practical Implementation of Functional Projection Optimization program (3.1) is essentially a functional programming sincef is a function onΛ. However, whenΛ represents a non-discrete parameter space, such functional minimization is typically difficulttosolveanalytically. Evenwithintheliteratureofinformationprojection, noneofthemethods considers our constraint setSC,c, which can be viewed as the intersections of uncountable single-point constraints onf. To obtain a feasible solution to the optimization problem, we leverage the idea of discretization. Instead of viewing(3.1) as a functional projection problem, we manually discretize Λ and solve(3.1) as a minimization problem over a discrete set. Note that such approximation is unavoidable in numerical computations since computers can only manage discrete functions, even when we solve the functional projection analytically. Moreover, we also have the freedom of choosing 25the discretization grid without incurring extra privacy loss since the privacy cost is independent of the size of parameter space. By convertingSC,c into a set of finite constraints, we are able to solve the discrete optimization problem efficiently using CVXOPT [2]. D DP-HyPO with General Prior Distribution In the main manuscript, we assumeπ(0) follows a uniform distribution over the parameter spaceΛ for simplicity. In practice, informed priors can be used when we want to integrate knowledge about the parameter space into sampling distribution, which is common in the Bayesian optimization framework. We now present the general DP-HyPO framework under the informed prior distribution. To begin with, we define the space of essentially bounded density functions with respect toπ(0) as SC,c(π(0)) = {f ∈ ΛR+ : ess supf/π(0) ≤ C, ess inff/π(0) ≥ c, Z α∈Λ f(α)dα = 1, f≪ π(0)}. When π(0) = 1 µ(λ), we recover the original definition ofSC,c. Note that heref ≪ π(0) means thatf is absolute continuous with respect to the prior distributionπ(0) and this ensures thatSC,c(π(0)) is non-empty. Note that such condition is automatically satisfied whenπ(0) is the uniform prior over the entire parameter space. To define the projection of a density at thej-th iteration, π(j), into the spaceSC,c(π(0)), we consider the following functional programming problem: min f ∥f − π(j)∥2 s.t. f ∈ SC,c(π(0)), which is a direct generalization of Equation (3.1). As before,SC,c(π(0)) is also convex and closed and the optimization program can be solved efficiently via discretization onΛ. E Experiment Details E.1 MNIST Simulation We now provide the detailed description of the experiment in Section 4.2.1. As specified therein, we consider two variable hyperparameters: the learning rateη and clipping normR, while keeping all the other hyperparameters fixed. We set the training batch size to be256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.71 for GP andσ = 0.64 for Uniform. For demonstration purposes, we set C to 2 andc to 0.75 in the GP method, so each base algorithm of Uniform haslog C/c more privacy budget than base algorithms in GP method. In Algorithm 3, we setτ to 0.1 andβ to 1. To facilitate the implementation of both methods, we discretize the learning rates and clipping norms as specified in the following setting to allow simple implementation of sampling and projection for Uniform and GP methods. Setting E.1.we set a log-spaced grid discretization onη in the range[0.0001, 10] with a multiplicative factor of 3√ 10, resulting in16 observations forη. We also set a linear-spaced grid discretization onR 26in the range[0.3, 6] with an increment of0.3, resulting in20 observations forR. This gives a total of 320 hyperparameters over the search region. We specify the network structure we used in the simulation as below. It is the standard CNN in Tensorflow Privacy and Opacus. class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3) self.conv2 = nn.Conv2d(16, 32, 4, 2) self.fc1 = nn.Linear(32 * 4 * 4, 32) self.fc2 = nn.Linear(32, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 1) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 1) x = x.view(-1, 32 * 4 * 4) x = F.relu(self.fc1(x)) x = self.fc2(x) return x Despite the simple nature of MNIST, the simulation of training CNN with the two methods over each different fixedT still take significant computation resources. Due to the constraints on computational resources, we conduct a semi-real simulation using the MNIST dataset. We cache the mean accuracy of5 independently trained models for each discretized hyperparameter and treat that as a proxy for the “actual accuracy” of the hyperparameter. Each time we sample the accuracy of a hyperparameter, we add Gaussian noise with a standard deviation of0.1 to the cached mean. We evaluate the performance of the output model based on the “actual accuracy” corresponding to the selected hyperparameter. E.2 CIFAR-10 Simulation We also provide a description of the experiment in Section 4.2.2. We set the training batch size to be 256, and the total number of epoch to be10. The value ofσ is determined based on the allocatedε budget for each base algorithm. Specifically,σ = 0.65 for GP andσ = 0.6 for Uniform. Regarding our GP method, we adopt the same set of hyperparameters as used in our MNIST experiments, which includeC = 2, c = 0.75, τ = 0.1, andβ = 1. As usual, we discretize the learning rates and clipping norms as specified in the following Setting. Setting E.2.we set a log-spaced grid discretization onη in the range[0.0001, 1] with a multiplicative factor of100.1, resulting in50 observations forη. We also set a linear-spaced grid discretization on R in the range[0, 100] with an increment of2, resulting in50 observations forR. This gives a total of 2500 hyperparameter combinations over the search region. We follow the same CNN model architecture with our MNIST experiments. 27In Figure 2, we provide the hyperparameter landscape forσ = 0.65, as generated by BoTorch [3]. Figure 2: Mean and standard error of the accuracy of DP-SGD over the two hyperparameters for σ = 0.65. The learning rate (log-scale) ranges from0.00001 (left) to 1 (right) while the clipping norm ranges from 0 (top) to 100 (bottom). The landscape forσ = 0.6 is similar, with a better accuracy. E.3 Federated Learning Simulation Figure 3: Mean and Standard Error of the loss of the FL over the two hyperparameters. We now provide the detailed description of the experiment in Section 4.2.3. As specified therein, we considered a FL task on a proprietary dataset5. Our objective is to determine the optimal learning rates for the central server (using AdaGrad) and the individual users (using SGD). To simulate this scenario, we utilize the landscape generated by BoTorch [3], as illustrated in Figure 3, and consider it as our reference landscape for both mean and standard deviation of the loss for each hyperparameter. When the algorithm (GP or Uniform) visits a specific hyperparameterλ, our oracle returns a noisy scoreq(λ) drawn from a normal distributionN(µλ, σλ). Figure 3 displays a heatmap that presents the mean (µλ) and standard error (σλ) structure of the loss over these two hyperparameters, providing insights into the landscape’s characteristics. 5We are unable to report a lot of detail about the proprietary dataset due to confidentiality. 28F Additional Related Work In this section, we delve into a more detailed review of the pertinent literature. We begin with non-private Hyperparameter Optimization, a critical topic in the realm of Auto- mated Machine Learning (AutoML) [18]. The fundamental inquiry revolves around the generation of high-performing models within a specific search space. In historical context, two types of optimiza- tions have proven significant in addressing this inquiry: architecture optimization and hyperparameter optimization. Architecture optimization pertains to model-specific parameters such as the number of neural network layers and their interconnectivity, while hyperparameter optimization concerns training-specific parameters, including the learning rate and minibatch size. In our paper, we incorpo- rate both types of optimizations within our HPO framework. Practically speaking,Λ can encompass various learning rates and network architectures for selection. For HPO, elementary methods include grid search and random search [24, 19, 16]. Progressing beyond non-adaptive random approaches, surrogate model-based optimization presents an adaptive method, leveraging information from preceding results to construct a surrogate model of the objective function [28, 43, 22, 32]. These methods predominantly employs Bayesian optimization techniques, including Gaussian process [35], Random Forest [20], and tree-structured Parzen estimator [5]. Another important topic in this paper is Differential Privacy (DP). DP offers a mathematically robust framework for measuring privacy leakage. A DP algorithm promises that an adversary with perfect information about the entire private dataset in use – except for a single individual – would find it hard to distinguish between its presence or absence based on the output of the algorithm [13]. Historically, DP machine learning research has overlooked the privacy cost associated with HPO [1, 41, 44]. The focus has only recently shifted to the “honest HPO” setting, where this cost is factored in [30]. Addressing this issue directly involves employing a composition-based analysis. If each training run of a hyperparameter upholds DP, then the overall HPO procedure adheres to DP through composition across all attempted hyperparameter values. A plethora of literature on the composition of DP mechanisms attempts to quantify a better DP guarantee of the composition. Vadhan et al. [38] demonstrated that though(ε, δ)-DP possesses a simple mathematical form, deriving the precise privacy parameters of a composition is #-P hard. Despite this obstacle, numerous advanced techniques are available to calculate a reasonably accurate approximation of the privacy parameters, such as Moments Accountant [1], GDP Accountant [12], and Edgeworth Accountant [39]. The efficacy of these accountants is attributed to the fact that it is easier to reason about the privacy guarantees of compositions within the framework of Rényi differential privacy [29] or f-differential privacy [12]. These methods have found widespread application in DP machine learning. For instance, when training deep learning models, one of the most commonly adopted methods to ensure DP is via noisy stochastic gradient descent (noisy SGD) [4, 37], which uses Moments Accountant to better quantify the privacy guarantee. Although using composition for HPO is a simple and straightforward approach, it carries with it a significant challenge. The privacy guarantee derived from composition accounting can be excessively loose, scaling polynomially with the number of runs. Chaudhuri et al. [8] were the first to enhance the DP bounds for HPO by introducing additional stability assumptions on the learning algorithms. [26] made significant progress in enhancing DP bounds for HPO without relying on any stability properties of the learning algorithms. They proposed a simple procedure where a hyperparameter was randomly selected from a uniform distribution for each training run. This selection process was repeated a random number of times according to a geometric distribution, and the best model obtained from these runs was outputted. They showed that this procedure satisfied 29(3ε, 0)-DP as long as each training run of a hyperparameter was(ε, 0)-DP. Building upon this, [34] extended the procedure to accommodate negative binomial or Poisson distributions for the repeated uniform selection. They also offered more precise Rényi DP guarantees for this extended procedure. Furthermore, [9] explored a generalization of the procedure for top-k selection, considering (ε, δ)-DP guarantees. In a related context, [30] explored a setting that appeared superficially similar to ours, as their title mentioned “adaptivity.” However, their primary focus was on improving adaptive optimizers such as DP-Adam, which aimed to reduce the necessity of hyperparameter tuning, rather than the adaptive HPO discussed in this paper. Notably, in terms of privacy accounting, their approach only involved composing the privacy cost of each run without proposing any new method. Another relevant area of research is DP selection, which encompasses well-known methods such as the exponential mechanism [27] and the sparse vector technique [14], along with subsequent studies (e.g., [6] and [17]). However, this line of research always assumes the existence of a low- sensitivity score function for each candidate, which is an unrealistic assumption for hyperparameter optimization. 30",
      "meta_data": {
        "arxiv_id": "2306.05734v2",
        "authors": [
          "Hua Wang",
          "Sheng Gao",
          "Huanyu Zhang",
          "Weijie J. Su",
          "Milan Shen"
        ],
        "published_date": "2023-06-09T07:55:46Z",
        "pdf_url": "https://arxiv.org/pdf/2306.05734v2.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces DP-HyPO, a pioneering adaptive private hyperparameter optimization (HPO) framework that bridges the gap between private and non-private HPO. It enables practitioners to adapt to previous runs and focus on potentially superior hyperparameters, allowing flexible use of non-DP adaptive HPO methods like Gaussian processes without substantial privacy costs from composition. The framework provides sharp Differential Privacy (DP) guarantees by utilizing the Rényi DP framework, strictly generalizing prior non-adaptive uniform sampling results without stability assumptions. Empirically, the Gaussian process-based DP-HyPO algorithm demonstrates superior performance compared to its uniform counterpart across diverse real-world datasets.",
        "methodology": "DP-HyPO maintains an adaptive sampling distribution `π` at each iteration, which evolves based on accumulated information. The framework necessitates that the sampling distribution `π(j)` (at iteration `j`) has bounded density, constrained by constants `c` and `C` relative to a prior `π(0)`. To achieve this, a general recipe is proposed: any non-private adaptive HPO method's distribution `π(j)` is projected into a space `SC,c` of bounded density functions by solving a convex functional programming problem (`min f ||f - π(j)||^2 s.t. f ∈ SC,c`). The total number of training runs `T` is drawn from a random distribution (e.g., truncated negative binomial) to preserve privacy. An instantiation of DP-HyPO with Gaussian Processes (GP) is provided, where GPs build a surrogate model for performance, assign scores (estimated Upper Confidence Bound, UCB) to hyperparameters, and use a softmax function with an inverse temperature `β` to derive the sampling distribution, which is then projected into `SC,c`.",
        "experimental_setup": "The DP-HyPO framework, specifically its Gaussian process-based instantiation (\"GP\"), was empirically evaluated against a Uniform DP-HyPO baseline (\"Uniform\") derived from prior work. Experiments were conducted in two privacy configurations: a white-box setting and a black-box setting. The white-box scenario involved training deep learning models on the MNIST and CIFAR-10 datasets. For MNIST, a standard CNN was trained with DP-SGD, optimizing learning rate `η` and clipping norm `R`. A semi-real simulation cached mean accuracies of five independently trained models, adding Gaussian noise (SD 0.1) upon sampling. The CIFAR-10 experiment used the same CNN and hyperparameters, but the landscape was generated by BoTorch, providing noisy scores from a normal distribution. The black-box setting involved a real-world Federated Learning (FL) task on a proprietary dataset from industry, optimizing learning rates for the central server (AdaGrad) and users (SGD), with the landscape also generated by BoTorch. Performance was measured by actual accuracy (MNIST, CIFAR-10) or loss (FL), with results aggregated over multiple runs (e.g., 400 for GP, 10000 for Uniform on MNIST). Key parameters included privacy budgets (`ε=15, δ=1e-5` for MNIST; `ε=12, δ=1e-5` for CIFAR-10), and GP-specific parameters like `C=2, c=0.75, τ=0.1, β=1` (varied `C` for FL). Hyperparameter spaces were discretized for practical implementation.",
        "limitations": "The framework requires posterior sampling distributions to have densities bounded by constants `c` and `C`, which may not be naturally satisfied by non-private adaptive HPO methods, necessitating a projection technique. Practically, discretizing the hyperparameter space `Λ` is necessary for computational feasibility of the convex optimization problem, even though the theoretical formulation allows for general measurable spaces. Additionally, empirical results on MNIST showed only marginal superiority of DP-HyPO over its uniform counterpart, which the authors attribute to the potentially \"uncomplicated\" hyperparameter landscape of MNIST limiting the benefits of adaptive algorithms.",
        "future_research_directions": "Two main future research directions are suggested: First, exploring alternative HPO specifications to improve empirical performance by leveraging more advanced HPO methods from the extensive existing literature. Second, establishing theoretical utility guarantees for the general DP-HyPO framework, or for specific configurations within it, by leveraging similar proof methodologies to those found in prior work like Theorem 3.3 in [26]."
      }
    },
    {
      "title": "Hyperparameter Optimization through Neural Network Partitioning",
      "abstract": "Well-tuned hyperparameters are crucial for obtaining good generalization\nbehavior in neural networks. They can enforce appropriate inductive biases,\nregularize the model and improve performance -- especially in the presence of\nlimited data. In this work, we propose a simple and efficient way for\noptimizing hyperparameters inspired by the marginal likelihood, an optimization\nobjective that requires no validation data. Our method partitions the training\ndata and a neural network model into $K$ data shards and parameter partitions,\nrespectively. Each partition is associated with and optimized only on specific\ndata shards. Combining these partitions into subnetworks allows us to define\nthe ``out-of-training-sample\" loss of a subnetwork, i.e., the loss on data\nshards unseen by the subnetwork, as the objective for hyperparameter\noptimization. We demonstrate that we can apply this objective to optimize a\nvariety of different hyperparameters in a single training run while being\nsignificantly computationally cheaper than alternative methods aiming to\noptimize the marginal likelihood for neural networks. Lastly, we also focus on\noptimizing hyperparameters in federated learning, where retraining and\ncross-validation are particularly challenging.",
      "full_text": "Published as a conference paper at ICLR 2023 HYPERPARAMETER OPTIMIZATION THROUGH NEURAL NETWORK PARTITIONING Bruno Mlodozeniec†∗, Matthias Reisser‡, Christos Louizos‡ †University of Cambridge, ‡Qualcomm AI Research bkm28@cam.ac.uk, {mreisser,clouizos}@qti.qualcomm.com ABSTRACT Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance — especially in the presence of limited data. In this work, we propose a simple and efﬁcient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is associated with and optimized only on speciﬁc data shards. Combining these partitions into subnetworks allows us to deﬁne the “out-of-training-sample” loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being signiﬁcantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging. 1 I NTRODUCTION Due to their remarkable generalization capabilities, deep neural networks have become the de-facto models for a wide range of complex tasks. Combining large models, large-enough datasets, and sufﬁcient computing capabilities enable researchers to train powerful models through gradient descent. Regardless of the data regime, however, the choice of hyperparameters — such as neural architecture, data augmentation strategies, regularization, or which optimizer to choose — plays a crucial role in the ﬁnal model’s generalization capabilities. Hyperparameters allow encoding good inductive biases that effectively constrain the models’ hypothesis space (e.g., convolutions for vision tasks), speed up learning, or prevent overﬁtting in the case of limited data. Whereas gradient descent enables the tuning of model parameters, accessing hyperparameter gradients is more complicated. The traditional and general way to optimize hyperparameters operates as follows; 1) partition the dataset into training and validation data1, 2) pick a set of hyperparameters and optimize the model on the training data, 3) measure the performance of the model on the validation data and ﬁnally 4) use the validation metric as a way to score models or perform search over the space of hyperparameters. This approach inherently requires training multiple models and consequently requires spending resources on models that will be discarded. Furthermore, traditional tuning requires a validation set since optimizing the hyperparameters on the training set alone cannot identify the right inductive biases. A canonical example is data augmentations — they are not expected to improve training set performance, but they greatly help with generalization. In the low data regime, deﬁning a validation set that cannot be used for tuning model parameters is undesirable. Picking the right amount of validation data is a hyperparameter in itself. The conventional rule of thumb to use ∼10% of all data can result in signiﬁcant overﬁtting, as pointed out by Lorraine et al. (2019) , when one has a sufﬁciently large number of hyperparameters to tune. Furthermore, a validation set can be challenging ∗Work done while at Qualcomm AI Research. Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. and/or its subsidiaries. 1a third partition, the test or holdout set is used to estimate the ﬁnal model performance 1 arXiv:2304.14766v1  [cs.LG]  28 Apr 2023Published as a conference paper at ICLR 2023 to obtain in many use cases. An example is Federated Learning (FL) (McMahan et al., 2017), which we speciﬁcally consider in our experimental section. In FL, each extra training run (for,e.g., a speciﬁc hyperparameter setting) comes with additional, non-trivial costs. Different approaches have been proposed in order to address these challenges. Some schemes optimize hyperparameters during a single training run by making the hyperparameters part of the model (e.g., learning dropout rates with concrete dropout (Gal et al., 2017), learning architectures with DARTs (Liu et al., 2018) and learning data-augmentations with schemes as in Benton et al. (2020); van der Wilk et al. (2018)). In cases where the model does not depend on the hyperparameters directly but only indirectly through their effect on the value of the ﬁnal parameters (through optimization), schemes for differentiating through the training procedures have been proposed, such as Lorraine et al. (2019). Another way of optimizing hyperparameters without a validation set is through the canonical view on model selection (and hence hyperparameter optimization) through the Bayesian lens; the concept of optimizing the marginal likelihood. For deep neural networks, however, the marginal likelihood is difﬁcult to compute. Prior works have therefore developed various approximations for its use in deep learning models and used those to optimize hyperparameters in deep learning, such as those of data augmentation (Schw¨obel et al., 2021; Immer et al., 2022). Still, however, these come at a signiﬁcant added computational expense and do not scale to larger deep learning problems. This paper presents a novel approach to hyperparameter optimization, inspired by the marginal likelihood, that only requires a single training run and no validation set. Our method is more scalable than previous works that rely on marginal likelihood and Laplace approximations (which require computing or inverting a Hessian (Immer et al., 2021)) and is broadly applicable to any hierarchical modelling setup. 2 M ARGINAL LIKELIHOOD AND PRIOR WORK In Bayesian inference, the rules of probability dictate how any unknown, such as parameters w or hyperparameters ψ, should be determined given observed data D. Let p(w) be a prior over w and p(D|w,ψ) be a likelihood for Dwith ψbeing the hyperparameters. We are then interested in the posterior given the data p(w|D,ψ) =p(D|w,ψ)p(w)/p(D|ψ). The denominator term p(D|ψ) is known as the marginal likelihood, as it measures the probability of observing the data given ψ, irrespective of the value of w: p(D|ψ) = ∫ p(w)p(D|w,ψ)dw. Marginal likelihood has many desirable properties that make it a good criterion for model selection and hyperparameter optimization. It intuitively implements the essence of Occam’s Razor principle (MacKay, 2003, § 28). In the PAC-Bayesian literature, it has been shown that higher marginal likelihood gives tighter frequentist upper bounds on the generalization performance of a given model class (McAllester, 1998; Germain et al., 2016). It also has close links to cross-validation (see section 2.1) and can be computed from the training data alone. However, computation of the marginal likelihood in deep learning models is usually prohibitively expensive and many recent works have proposed schemes to approximate the marginal likelihood for differentiable model selection (Lyle et al., 2020; Immer et al., 2021; 2022; Schw¨obel et al., 2021). 2.1 “L EARNING SPEED ” PERSPECTIVE Lyle et al. (2020); Fong and Holmes (2020) pointed out the correspondence between “learning speed” and marginal likelihood. Namely, the marginal likelihood of the data Dconditioned on some hyperparameters ψcan be written as: log p(D|ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] (1) where (D1,..., DC) is an arbitrary partitioning of the training dataset Dinto Cshards or chunks2, and p(w|D1:k,ψ) is the posterior over parameters of a function fw : X → Y, from the input domain Xto the target domain Yafter seeing data in shards 1 through k. The right-hand side can be interpreted as a type of cross-validation in which we ﬁx an ordering over the shards and measure the “validation” performance on each shardDk using a model trained on the preceding shards D1:k−1. 2We use the terms “chunk” and “shard” interchangeably. 2Published as a conference paper at ICLR 2023 Alternatively, it can be viewed as the learning speed of a (probabilistic) model: i.e., a measure of how quickly it learns to perform well on new shards of data after only having been ﬁt to the previous shards (through exact Bayesian updating). This perspective neatly illustrates why models with higher marginal likelihood can exhibit good inductive biases, e.g., encoded through ψ, w and fw. Namely, such models can be expected to learn faster and generalize better after seeing fewer samples. For example, if the hypothesis space is constrained3to functions satisfying symmetries present in the data, we need fewer data to identify the correct function (Sokolic et al., 2017; Sannai et al., 2021). We argue that the “learning speed” aspect of marginal likelihood — i.e., measuring how well the model generalizes to new data in the training set, having been trained only on the previous data points — is the key property making marginal likelihood a useful tool for selecting hyperparameters. 2.2 T RAINING SPEED FOR HYPERPARAMETER OPTIMIZATION Computing the “learning speed”, requires samples from the posteriorp(w|D1:k,ψ). Unfortunately, in deep learning settings, such samples are impractical to obtain; thus, prior works have focused on more scalable alternatives. Lyle et al. (2020) propose to approximate the objective in Eq. 1 by looking at the training speed during standard training of a neural network by SGD. Speciﬁcally, they deﬁne the training speed as the reduction in the training loss after a single SGD parameter update, summed over all updates in the ﬁrst epoch. They argue that, during the ﬁrst epoch of training, after the neural network parameters, w, have been updated with SGD steps using data from shards D1:k, they can be approximately used in place of the sample from the posterior p(w|D1:k,ψ) in Eq. 1. They extend the analogy to training past one epoch and use the training speed estimate for model selection (Ru et al., 2021). As pointed out by the authors, however, the analogy between learning speed and training speed somewhat breaks down after 1 epoch of training. The network parameters have “seen” every datapoint in the training set after1 epoch, and hence the connection to measuring the model’s generalization capability is weakened. For the sake of scalability and alignment with deep learning practice, we also focus on simple pointwise approximations qk(w) = δ(w = ˆwk) to the posteriors p(w|D1:k,ψ). However, in contrast to prior work, we explicitly parametrize the learning procedure such that, at any given training iteration, we have access to a model that is trained only on a subset of the dataD1:k. In doing so, we can approximate the objective in Eq. 1, and thus use it to optimize the hyperparameters during the entire training run. 3 P ARTITIONED NEURAL NETWORKS Our goal is to optimize the objective LML (D,ψ) = C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (2) wrt. ψ, which is an approximation to the lower-bound presented in Eq. 1 above. In Appendix A, we show that the left-hand side is also a lower-bound on the marginal likelihood under some unobtrusive conditions. As mentioned in Section 2.2, our goal is to propose an architecture and a training scheme so that we can easily obtain models trained on only subsets of the data D1:k for all k throughout training. We propose that each {qk(w)}C k=1 optimizes a subset of the parameters of the neural network, in a manner that allows us to extract “subnetworks” from the main network that have been trained on speciﬁc chunks of data. We describe the partitioning scheme below. Partitioning the parameters Denote the concatenations of the weights of a neural networkw ∈RN. We can deﬁne a partitioning ((w1,..., wC),P) of the parameters into C partitions, such that w = Pconcat(w1,..., wC) for a permutation matrix P ∈{0,1}N×N. For ease of exposition, we drop the dependence on P, assuming that w is already arranged such that P is identity, P = IN×N. Given the partitioning (w1,..., wC) of the parameters, we then specify Csubnetworks with weights w(1) s ,..., w(C) s such that w(k) s = concat(w1,..., wk, ˆwk+1,..., ˆwC), where ˆwi are some default 3or if the learning algorithm is heavily biased towards returning hypotheses that satisfy a given invariance, e.g., through the use of a prior. 3Published as a conference paper at ICLR 2023 values not optimized during training4. More speciﬁcally, the k-th subnetwork, wk s, retains the ﬁrst kpartitions from the weight partitioning and sets the remaining parameters to ˆwk+1:C. Note that, if each wk is only updated on chunks D1:k, the subnetwork w(k) s is only comprised of weights that have been updated on D1:k. Thus, we can view the parameters of w(k) s as an approximation to qk(w). Although, given that a subset of the parameters in each w(k) s is ﬁxed, this would likely be a poor approximation to the true posterior over the weights given D1:k, it could be, intuitively, a reasonable approximation in function space5. Partitioned training Having partitioned the dataset Dinto Cchunks (D1,..., Dk), we update each partition wk by optimising the negative log-likelihood6on chunks D1:k using subnetwork w(k) s by computing the following gradients: ∇wkL ( D1:k,w(k) s ) = ∑ (x,y)∈D1:k ∇wk log p ( y ⏐⏐⏐x; w(k) s ,ψ ) . (3) We interleave stochastic gradient updates of each partition of the weights with updating the hyperpa- rameters ψusing LML in Eq. 2: ∇ψLML (D,ψ) ≈ C∑ k=2 ∑ (x,y)∈Dk ∇ψlog p ( y ⏐⏐⏐x,w(k−1) s ,ψ ) . (4) This can be seen as the sum of the out-of-sample losses for each subnetwork w(k) s . The scheme is illustrated in Figure 1. For details of how the updates are scheduled in our experiments, see Appendix I. Note that, while we could incorporate the gradient of the ﬁrst term from Eq. 1 corresponding to Eq0(w)[log p(D1|w,ψ)] in Eq. 4, we chose to leave it out. Hence, the gradient of Eq. 4 is of an estimate that can be viewed as an approximation to the conditional marginal likelihood log p(D2:C|D1,ψ). Conditional marginal likelihood has been shown to have many desirable properties for model selection and, in many cases, can be a better proxy for generalization (Lotﬁ et al., 2022). Weights: w = (w1,w2,w3) Alternate: Optimize parameters: log p ( D1 |(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) w.r.t. w1 log p ( D1:2|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) w.r.t. w2 log p ( D1:3|(w1,w2,w3)   Subnet. 3 ,ψ ) w.r.t. w3 Optimize hyper parameters ψon: log p ( D2|(w1, ˆw2, ˆw3)   Subnet. 1 ,ψ ) + logp ( D3|(w1,w2, ˆw3)   Subnet. 2 ,ψ ) Figure 1: Best viewed in colour. Illustration of the partitioning scheme for a single hidden layer perceptron with C = 3chunks. This procedure, inspired by the marginal likelihood, has several desirable properties compared to prior work. 1) Our objective is computationally efﬁcient, with a computational cost roughly corresponding to evaluating subnetworks on the training set. There is no need to compute nor invert a Hessian with 4e.g., ˆwi could be the value of the weights at initialization, or ˆwi = 0 corresponding to pruning those parameters and obtaining a proper subnetwork. 5Since a) the mapping from parameters to functions is not bijective and b) neural networks are highly overparameterised and can be heavily pruned while retaining performance (Frankle and Carbin, 2018), obtaining a good ﬁt to a subset of the training data with a subset of the model parameters should be possible. Furthermore, “scaling laws” indicate that the beneﬁt of having more parameters becomes apparent mostly for larger dataset sizes (Kaplan et al., 2020), thus it is reasonable for subnetworks ﬁt to more data to have more learnable parameters. 6Optionally with an added negative log-prior regularization term log p(w(k) s ). 4Published as a conference paper at ICLR 2023 respect to the weights, as in the Laplace approximation (Immer et al., 2021; 2022). 2) Our objective is readily amenable to optimization by stochastic gradient descent; we do not have to iterate over the entire training set to compute a single gradient update for the hyperparameters. 3) Compared to the training speed objective (Lyle et al., 2020), in our method, the training of the weights in each subnetwork progresses independently of the data in future chunks. Hence, it can be seen as more truthfully measuring the generalization capability of a model using a given set of hyperparameters. Partitioning Schemes There are several ways in which the neural network weights can be partitioned. In our experiments in Section 5, we partition the weights before beginning training by assigning a ﬁxed proportion of weights in each layer to a given partition at random. For each subnetwork, for the weight partitions corresponding to future chunks, we use the values of the weights at initialisation. For a discussion of partitioning schemes, see Appendix C. 4 R ELATED WORKS Hyperparameter optimization in deep learning Many works have tackled the challenge of op- timizing hyperparameters in deep learning. Works on implicit differentiation, such as the one by Lorraine et al. (2019), allow for optimizing training hyperparameters such as the learning rate, weight- decay, or other hyperparameters that affect the ﬁnal neural network weights only through the training routine. Other works have proposed ways to parameterize and optimize data-augmentations (Cubuk et al., 2018; Li et al., 2020), search-spaces for neural network architectures, as well as methods to optimize architectures using gradient-based optimization (Liu et al., 2018; Elsken et al., 2019). All of the above works have primarily relied on optimizing hyperparameters on a separate validation set and are compatible with the objective deﬁned in this work. Several works have also aimed to cast learning data augmentations as an invariance learning problem. They do so by parameterizing the model itself with data augmentations, and frame invariance learning as a model selection problem (van der Wilk et al., 2018; Benton et al., 2020; Schw¨obel et al., 2021; Nabarro et al., 2022; Immer et al., 2022). We compare against Benton et al. (2020) (“Augerino”) and Immer et al. (2022) (“Differentiable Laplace”) on this task in the experimental section. Hyperparameter optimization without a validation set A limited number of works consider learning hyperparameters without a validation set in a deep learning context. Benton et al. (2020) propose a simple method for learning invariances without a validation set by regularising invariance hyperparameters to those resulting in higher invariance. They show that the invariances found tend to be insensitive to the regularisation strength, determined by another hyperparameter. However, the method relies on being able to a priori deﬁne which hyperparameters lead to higher invariance through a suitable regularisation function. In more complex invariance learning settings, deﬁning the regulariser can be challenging. For example, if data-augmentation transformations were to be parameterized by a neural network (as proposed in Lorraine et al. (2019)), it is non-trivial to devise an adequate regulariser. We show that our method can be applied to such settings. Other works focus on deriving tractable approximations to the marginal likelihood for deep neural networks. Schw ¨obel et al. (2021) propose only marginalising-out the parameters in the last layer of the neural network by switching it out for a Gaussian Process. They treat the preceding layer effectively as a hyperparameter, and optimize invariance parameters using the marginal likelihood. Although they show promising results on MNIST, they found they “were unable to learn invariances for CIFAR-10” (Schw¨obel et al., 2021, §7) and highlighted the need to marginalise lower layers as well. In contrast, our objective can be seen as being inspired by marginal likelihood where arbitrary network layers can be “marginalised”, and works on datasets like CIFAR-10. Immer et al. (2022) have adapted the Laplace approximation (Immer et al., 2021) to make it tractable for learning data augmentations. In contrast to Schw¨obel et al. (2021), they approximately marginalize out all the network parameters, and performs favourably. Their approximation, however, requires approximations to a Hessian w.r.t. all network parameters; for that reason, their work reports results for architectures only up to a ResNet-14, whereas our method can easily scale to larger architectures. Hyperparameter optimization in FL Improving hyperparameter optimization is especially rele- vant to FL. Given the potential system level constraints (Wang et al., 2021), methods that optimize the hyperparameters and parameters in a single training run are preferred. On this note, Khodak et al. (2021) introduced FedEx and showed that it can successfully optimize the client optimizer 5Published as a conference paper at ICLR 2023 hyperparameters. FedEx relies on a training/validation split on the client level and uses a REIN- FORCE type of gradient (Williams, 1992) estimator, which usually exhibits high variance and needs baselines to reduce it (Mohamed et al., 2020). This is in contrast to partitioned networks, which use standard, low-variance backpropagation for the hyperparameters and no separate validation set per client. To optimize the other hyperparameters, Khodak et al. (2021) wrapped FedEx with a traditional hyperparameter optimization strategy, the successive halving algorithm. This is orthogonal to our method and could be applied to partitioned networks as well. In Zhou et al. (2021), the authors perform a hyperparameter search independently on each client with some off-the-shelf methods and then aggregate the results of the search at the server once in order to identify the best hyperparameter setting. The main drawback of this method compared to partitioned networks is that when the local client datasets are small, a client-speciﬁc validation set is not informative, and the aggregation happens only once. Finally, there is also the recent work from Seng et al. (2022) which performs hyperparameter optimization and neural architecture search in the federated setting. Similarly to prior works, it requires client-speciﬁc validation data in order to optimize the hyperparameters. 5 E XPERIMENTS 1 5 10 15 20 25 30 Num. inputs 0.7 0.8 0.9 1.0 Accuracy 0.5 0.4 0.3 0.2 0.1 0.0 Average Log-likelihoodPosthoc Diagonal Laplace Train T est 1 5 10 15 20 25 30 Num. inputs 103 102 Log Marginal Likelihood Estimate Partitioned (a) 0 400080001200016000200002400028000 Iteration 0 5 10 15 20 25Input Mask Element 0.00 0.25 0.50 0.75 1.00 Mask Probability  (b) Figure 2: (a) Demonstrating the ability of the marginal-likelihood inspired objective LML to identify the correct model on a toy input selection task. We plot the hyperparameter objective, train and test set accuracy, and train and test set log-likelihood with the partitioned networks method (left), and the post-hoc diagonal Laplace method (Immer et al., 2021) (right). (b) Mask over input features learned by partitioned networks over time. The ﬁrst 15 features are correctly identiﬁed. Input Selection To demonstrate that LML is a good objective for model selection that captures the desirable properties of the marginal likelihood, we ﬁrst deploy our method on the toy model selection task of Lyle et al. (2020): there the ﬁrst 15 features are informative, and the remaining15 are spurious y∼Bern (1 2 ) x = [ y+ ϵ1,...,y + ϵ15   Informative ,ϵ16,...,ϵ 30   Spurious ]⊺ ϵ1,...,ϵ 30 iid ∼N(0,1). We specify a ﬁxed mask over the inputs prior to training, where the ﬁrst Kinputs remain unmasked, and the remainder is masked. We expect that, given multiple models with different (ﬁxed) masks over the inputs, the proposed objective will be able to identify the correct one — i.e., the one that keeps only the informative features. We train multiple fully connected neural networks (MLPs) on a training set of 1000 examples using our method and compare the ﬁnal values of the LML objective. The results are shown in Figure 2a. LML correctly identiﬁes 15 input features as the optimum, and correlates well with test accuracy and log-likelihood. Training loss and training accuracy, on the other hand, cannot alone disambiguate whether to use 15 or more input features. Differentiable input selection We further show that we can learn the correct mask over the inputs in a differentiable manner using our method during a single training run. We parameterize a learnable mask over the inputs with a concrete Bernoulli distribution (Maddison et al., 2016) and treat the parameters of the mask distribution as a hyperparameter. We optimize them with respect to the proposed objective using our method. The evolution of the learned mask during training is shown in Figure 2b, where we see that we can correctly identify the ﬁrst 15 informative features. 6Published as a conference paper at ICLR 2023 Learning invariances through data-augmentations Following previous literature on learning soft invariances through learning data augmentations (Nabarro et al., 2022; van der Wilk et al., 2018; Benton et al., 2020; Schw ¨obel et al., 2021; Immer et al., 2022), we show that we can learn useful afﬁne image augmentations, resulting in gains in test accuracy. We specify afﬁne data augmentations as part of a probabilistic model as done by van der Wilk et al. (2018), averaging over multiple data augmentation samples during training and inference. This allows us to treat the data-augmentation distribution as a model hyperparameter rather than a training hyperparameter. For datasets, we consider MNIST, CIFAR10, TinyImagenet along with rotCIFAR10 and rotTinyImagenet, variants where the datapoints are randomly rotated at the beginning of training by angles sampled uniformly from [−π,π] (Immer et al., 2022). Experimental setup details are provided in Appendix I. For the CIFAR10 and rotCIFAR10 datasets, we consider as baselines standard training with no augmentations, Augerino (Benton et al., 2020) and Differentiable Laplace (Immer et al., 2022). Following Immer et al. (2022), we use ﬁxupResNets (Zhang et al., 2019) for the architectures. The results can be seen in Table 1. There, we observe that partitioned networks outperform all baselines in the case of CIFAR10 for both ResNet variants we consider. On RotCIFAR10, we observe that partitioned networks outperform the baseline and Augerino, but it is slightly outperformed by Differentiable Laplace, which optimizes additional prior hyperparameters. To demonstrate the scalability of partitioned networks, for the (rot)TinyImagenet experiments we consider a ResNet-50 architecture with GroupNorm(2). In Table 1 we observe that in both cases, partitioned networks learn invariances successfully and improve upon the baseline. Relative to Augerino, we observe that partitioned networks either improve (TinyImagenet) or are similar (rotTinyImagenet). Table 1: Test accuracy with learning afﬁne augmentations on (rot)CIFAR10 and (rot)TinyImagenet. Method Dataset Architecture Baseline Augerino Diff. Laplace Partitioned RotCIFAR10 ﬁxupResNet-8 54.2±0.4 75.4±0.2 79.5±0.6 79.1±0.0 CIFAR10 ﬁxupResNet-8 74.1±0.5 79.0±1.0 84.2±0.8 86.1±0.4 ﬁxupResNet-14 79.5±0.3 83.0±0.1 88.1±0.2 89.1±0.8 RotTinyImagenet ResNet-50 31.5±0.6 44.5±0.2 OOM7 43.9±0.3 TinyImagenet ResNet-50 44.2±0.5 41.1±0.2 OOM 48.6±0.0 Imbuing a model with useful invariances is particularly useful in the low-data regime, due to better data efﬁciency. To show that, we perform experiments where we artiﬁcially reduce the size of the training dataset. The results can be seen in Figure 3. We see that by learning augmentations with partitioned networks, we can drastically improve performance in the low-data regime upon a baseline that does not learn augmentations, while performing favorably against prior works in most cases. On MNIST, our method outperforms the last-layer marginal-likelihood method (last-layer ML) by Schw¨obel et al. (2021) in the large data regime but underperforms in the low-data regime. That is likely to be expected, as their work ﬁts a Gaussian Process (GP) at the last layer (Wilson et al., 2016), which is better tailored for the low-data regime and results into a more ﬂexible model (due to the GP corresponding to an additional, inﬁnite width, layer). Since the MNIST-CNN is sufﬁciently small to ﬁt multiple networks into memory, we also compare to a variant of our method where, instead of partitioning a single network, we train Cdifferent networks where network kis ﬁt on data D1:k. This serves as an upper bound on the performance of the partitioned networks. We see that by partitioning a single network, we can achieve almost equivalent accuracy. On CIFAR10, partitioned networks outperform all other works on all data sizes we considered. On RotCIFAR10, partitioned networks perform again favourably, but they are marginally outperformed by differentiable Laplace in the low-data regime. Compared to partitioned networks where we only optimize augmentations, differentiable Laplace also optimizes the precision of a Gaussian prior over the weights, which better combats overﬁtting in the low-data regime. On both the TinyImagenet and rotTinyImagenet experiments we observe that partitioned networks either outperform or are similar to the baselines on all data sizes considered. 7Out of memory error on a 32GB Nvidia V100. 7Published as a conference paper at ICLR 2023 5000 20000 60000 Dataset Size 0.98 0.99T est Accuracy Baseline Last-layer ML Augerino Diff. Laplace Partitioned (Ens.) Partitioned (a) MNIST 0.25 0.50 0.75 1 5 10 20 50 Dataset Size (x1000) 0.25 0.50 0.75  (b) (rot)CIFAR10 0.25 0.50 10 50 100 Dataset Size (x1000) 0.2 0.4  (c) (rot)TinyImagenet Figure 3: Learning afﬁne data augmentations on subsets of data. (b) uses a ﬁxupResNet-8 architecture whereas (c) a ResNet-50 architecture. (b,c) Top: normal dataset, bottom: rotated dataset. Comparisons to traditional training / validation split We further perform comparisons between partitioned networks and the more traditional training/validation split (denoted as validation set optimization) with additional ﬁnetuning to the task of learning data augmentations. This is realized as follows; we partition 20kCIFAR10 examples into training and validation data of speciﬁc proportions. We then either train a partitioned network (along with the hyperparameters on LML) on these two chunks of data or train a standard network on the training set while using the validation set loss to obtain gradients for the data augmentation hyperparameters. For the validation set optimization baseline, once the hyperparameters are optimized, the resulting network is ﬁnetuned on the whole dataset for 20 epochs. The results for varying chunk proportions are provided in Table 2. Table 2: Learning afﬁne augmentations with ﬁxupResNet-14 on subset of CIFAR-10 (20kexamples). NaN denotes that a run crashed. Chunk Proportions Method [0.3,0.7] [0 .5,0.5] [0 .7,0.3] [0 .8,0.2] [0 .9,0.1] Partitioned 82.9%±0.3 83.0%±0.01 83.7%±0.2 84.0%±0.6 84.6%±0.05 Validation set optim. NaN 78.9%±0.04 81.5%±0.2 82.6%±0.1 83.4%±0.1 +Finetune NaN 81.3%±0.09 82.5%±0.2 83.5%±0.1 83.8%±0.3 Table 3: Learning a feature extractor (ﬁrst 2 out of 3 stages of a Wide ResNet-20) as a hyperparameter on CIFAR10. Method Chunk Proportions Test accuracy Validation set optim. [0.9,0.1] 59 .6%±0.6 Partitioned [0.1,0.8,0.1] 87.3%±0.8 We can see that partitioned net- works (that do not employ ad- ditional ﬁnetuning) outperform validation set optimization with ﬁnetuning in all settings we tried. The gap does get smaller when we move to the more tra- ditional 90/10 splits for train- ing/validation: a 10% proportion for validation data is enough to optimize a handful of hyper- parameters (just 6 scalars). To corroborate this claim, we set up an additional experiment; we use a Wide ResNet-20 on the full CIFAR10 dataset, where the ﬁrst two out of the three stages (13 convolution layers) are considered as hyperparameters. The results for this setting can be seen in Table 3. We see that 10% validation data are not enough, and the validation set optimization baseline performs poorly. This is in contrast to partitioned networks, where with three chunks, we can learn all of these hyperparameters successfully. Note that, compared to Augerino, applying partitioned networks to this setting is straightforward. To apply Augerino, one would have to come up with a metric that can be used to regularize the feature extractor towards “higher invariance”. Partitioned networks for federated learning We consider federated learning (FL) (McMahan et al., 2017), a setting where data is distributed across many clients. In this setting, there are system properties that make hyperparameter optimization especially challenging (Wang et al., 2021). More speciﬁcally, obtaining a validation set and performing multiple training runs with different 8Published as a conference paper at ICLR 2023 hyperparameter settings might not be possible due to the additional communication and computation costs, and transient client availability (clients join and leave the training process at any time). Optimizing hyperparameters together with the model parameters in a single run is therefore especially beneﬁcial (Wang et al., 2021), and partitioned networks are a good ﬁt for FL. We extend our centralized experimental setup to FL by splitting all N clients into Cnon-overlapping chunks, such that each chunk is understood as the union of all clients’ data shards that belong to that chunk. During federated training, a client belonging to chunk ksequentially optimizes partitions wk:C through sub-networks w(k:C) s and computes a gradient wrt. the hyperparameters ψ. Note that partitions w1:k remain unchanged and do not need to be communicated back to the server. This reduction in upload costs is a welcome property for FL, where upload costs can bottleneck system design. The server receives the (hyper-) parameter updates, averages them, and applies the result as a “gradient” to the server-side model in the traditional federated manner (Reddi et al., 2020). For partitioned networks, the hyperparameters that we optimize are the data augmentation parameters and, since we also include dropout in these architectures, the dropout rates (with the concrete relaxation from Maddison et al. (2016)). As a baseline, we consider the standard federated training without learning hyperparameters (denoted as FedAvg) as well as learning the augmentation parameters with Augerino Benton et al. (2020). Please see Appendix J for a detailed explanation of our FL setup. Table 4 summarizes our results using different sub-sets and variations of MNIST and CIFAR10, where we also included rotMNIST Larochelle et al. (2007) as another dataset. We can see that partitioned networks allow training models that generalize better than both FedAvg and FedAvg with Augerino, at reduced communication costs. Especially when the true data-generating process and underlying source of non-i.i.d.-ness are explicitly accounted for — here in the form of rotation — the beneﬁts of learning the augmentations with partitioned networks become apparent. For example, we observe that on the rotated datasets, partitioned networks learn to correctly increase the rotation angle. Table 4: Validation accuracy averaged over the last10 evaluations, each 10 rounds apart; standard- error is computed across 4 random seeds. All datasets are adapted to the federated setting and are synthetically split to be non-i.i.d. sampled as described in Appendix J.2. Dataset & size ↑MNIST ↑RotMNIST ↓Upload Method 1.25k 5k 50k 1.25k 5k 50k [%] FedAvg 95.4%±0.1 97.4%±0.1 99.0%±0.1 80.5%±0.0 90.4%±0.5 96.8%±0.1 100 FedAvg + Augerino 94.2%±0.5 96.4%±0.1 99.1%±0.0 79.5%±0.3 89.0%±2.0 95.3%±0.2 100 FedAvg + Partitioned97.0%±0.1 98.3%±0.0 99.2%±0.1 85.7%±0.9 93.5%±0.6 97.8%±0.1 77 ↑CIFAR10 ↑RotCIFAR10 ↓Upload 1.25k 5k 45k 1.25k 5k 45k [%] FedAvg 50.2%±0.4 64.5%±0.3 79.2%±0.7 35.6%±0.3 45.2%±0.1 53.9%±1.1 100 FedAvg + Augerino 49.9%±0.8 65.0%±0.2 79.9%±0.4 36.1%±0.2 45.0%±0.2 56.4%±0.7 100 FedAvg + Partitioned50.8%±1.0 64.8%±0.4 81.5%±0.5 37.1%±0.2 45.3%±0.3 60.6%±0.2 91 6 D ISCUSSION We propose partitioned networks as a new method for hyperparameter optimization inspired by the marginal likelihood objective. It provides a general and scalable solution to ﬁnding hyperparameters in a single training run without requiring access to a validation set while introducing less additional overhead to the training task than existing approaches. We showed that partitioned networks are applicable on a wide range of tasks; they can identify the correct model on illustrative toy examples, they can learn data augmentations in a way that improves data efﬁciency, they can optimize general feature extractors as hyperparameters and they can also optimize dropout rates. In the federated setting, partitioned networks allow us to overcome practical challenges, reduce the communication overhead and obtain better models. The notion of partitioned networks we propose in this work is novel to the literature and an orthogonal approach to many existing hyperparameter tuning algorithms. Like any other method, partitioned networks come with their own limitations, e.g., needing a partitioning strategy. We expand upon them in appendix H. We hope to see our method successfully reducing the need to perform hyperparameter search through repeated training and thereby contribute to the community’s effort to reduce its carbon footprint. 9Published as a conference paper at ICLR 2023 REFERENCES Gregory Benton, Marc Finzi, and Andrew G Wilson. Augerino, github, com- mit=fd542eb90ac6b1c0959156c1f6ad2ba8719d8572. https://github.com/g-benton/ learning-invariances/. (on page 18) Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural networks from training data. Advances in neural information processing systems, 33:17605–17616, 2020. (on page 2, 5, 7, 9, 16, 18, 20, 24, 25) Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. (on page 5) Kamal Dys. Cifar10 resnet: 90+% accuracy;less than 5 min. https://www.kaggle.com/code/ kmldas/cifar10-resnet-90-accuracy-less-than-5-min . Accessed: 2022-09- 17. (on page 26) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The Journal of Machine Learning Research, 20(1):1997–2017, 2019. (on page 5) Edwin Fong and Chris C Holmes. On the marginal likelihood and cross-validation. Biometrika, 107 (2):489–496, 2020. (on page 2) Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. (on page 4) Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout.Advances in neural information processing systems, 30, 2017. (on page 2) Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien. Pac-bayesian theory meets bayesian inference. Advances in Neural Information Processing Systems, 29, 2016. (on page 2) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015. (on page 23) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. (on page 23) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. (on page 23) Alexander Immer and Tycho F. A. van der Ouderaa. Learning invariances with laplace ap- proximations (lila), github, commit=c0c4a09a109ed2f55e887def7d854b8a3a2330ef. https: //github.com/tychovdo/lila. (on page 17) Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R¨atsch, and Khan Mohammad Emtiyaz. Scalable marginal likelihood estimation for model selection in deep learning. In International Conference on Machine Learning, pages 4563–4573. PMLR, 2021. (on page 2, 5, 6, 24) Alexander Immer, Tycho F. A. van der Ouderaa, Gunnar R¨atsch, Vincent Fortuin, and Mark van der Wilk. Invariance learning in deep neural networks with differentiable laplace approximations, 2022. URL https://arxiv.org/abs/2202.10638. (on page 2, 5, 7, 15, 16, 17, 18, 22, 23, 24, 25) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015. (on page 23) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (on page 4) 10Published as a conference paper at ICLR 2023 Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina F Balcan, Virginia Smith, and Ameet Talwalkar. Federated hyperparameter tuning: Challenges, baselines, and connections to weight- sharing. Advances in Neural Information Processing Systems, 34:19184–19197, 2021. (on page 5, 6) Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480, 2007. (on page 9) Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. (on page 24) Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M Robertson, and Yongxin Yang. Dada: Differentiable automatic data augmentation. arXiv preprint arXiv:2003.03780, 2020. (on page 5) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018. (on page 2, 5) Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. CoRR, abs/1911.02590, 2019. URL http://arxiv.org/abs/ 1911.02590. (on page 1, 2, 5) Sanae Lotﬁ, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew Gordon Wilson. Bayesian model selection, the marginal likelihood, and generalization. In Kamalika Chaud- huri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of Pro- ceedings of Machine Learning Research , pages 14223–14247. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/lotfi22a.html. (on page 4) Clare Lyle, Lisa Schut, Robin Ru, Yarin Gal, and Mark van der Wilk. A bayesian perspective on training speed and model selection. Advances in Neural Information Processing Systems , 33: 10396–10408, 2020. (on page 2, 3, 5, 6) David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003. (on page 2) Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016. (on page 6, 9, 26, 27) David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pages 230–234, 1998. (on page 2) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial intelli- gence and statistics, pages 1273–1282. PMLR, 2017. (on page 2, 8) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. J. Mach. Learn. Res., 21(132):1–62, 2020. (on page 6) Seth Nabarro, Stoil Ganev, Adri`a Garriga-Alonso, Vincent Fortuin, Mark van der Wilk, and Laurence Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In Uncertainty in Artiﬁcial Intelligence, pages 1434–1444. PMLR, 2022. (on page 5, 7) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an- imperative-style-high-performance-deep-learning-library .pdf. (on page 22) 11Published as a conference paper at ICLR 2023 Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone ˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020. (on page 9, 26, 27) Robin Ru, Clare Lyle, Lisa Schut, Miroslav Fil, Mark van der Wilk, and Yarin Gal. Speedy performance estimation for neural architecture search. Advances in Neural Information Processing Systems, 34:4079–4092, 2021. (on page 3) Akiyoshi Sannai, Masaaki Imaizumi, and Makoto Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In Uncertainty in Artiﬁcial Intelligence, pages 771–780. PMLR, 2021. (on page 3) Pola Schw¨obel, Martin Jørgensen, Sebastian W. Ober, and Mark van der Wilk. Last layer marginal likelihood for invariance learning, 2021. URL https://arxiv.org/abs/2106.07512. (on page 2, 5, 7, 15, 16, 23, 24, 26, 27) Jonas Seng, Pooja Prasad, Devendra Singh Dhami, and Kristian Kersting. Hanf: Hyperparameter and neural architecture search in federated learning. arXiv preprint arXiv:2206.12342, 2022. (on page 6) Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization error of invariant classiﬁers. In Artiﬁcial Intelligence and Statistics, pages 1094–1103. PMLR, 2017. (on page 3) Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using the marginal likelihood. Advances in Neural Information Processing Systems, 31, 2018. (on page 2, 5, 7, 16) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equiv- ariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210–218. Springer, 2018. (on page 15) Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021. (on page 5, 8, 9) Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. (on page 6) Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning. In Artiﬁcial intelligence and statistics, pages 370–378. PMLR, 2016. (on page 7) Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018. (on page 23, 26) Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. (on page 23) Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. (on page 7, 18, 23) Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, and Heiko Ludwig. Flora: Single-shot hyper-parameter optimization for federated learning. arXiv preprint arXiv:2112.08524, 2021. (on page 6) 12Published as a conference paper at ICLR 2023 A LML IS A LOWER -BOUND TO THE MARGINAL LIKELIHOOD In this section, we show that the objective in equation 2 is a lower-bound on the marginal likelihood, under a mild assumption on each approximate posterior qk(w). The aim is to approximate: log p(D|ψ) = C∑ k=1 log p(Dk|D1:k−1,ψ) (5) Our partitioned approximation is given by: C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (6) We can get the equation for the gap between quantities in 5 and 6: gap = C∑ k=1 log p(Dk|D1:k−1,ψ) − C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] (7) = C∑ k=1 Eqk−1(w) [log p(Dk|D1:k−1,ψ) −log p(Dk|w,ψ)] (8) = C∑ k=1 Eqk−1(w) [ log p(Dk|D1:k−1,ψ) p(Dk|w,ψ) ] (9) = C∑ k=1 Eqk−1(w)  log p(w,Dk|D1:k−1)    p(w|D1:k,ψ)p(Dk|D1:k−1,ψ)p(w|D1:k−1,ψ) p(w|D1:k,ψ)p(Dk|w,ψ)p(w|D1:k−1,ψ)   p(w,Dk|D1:k−1)   (10) = C∑ k=1 Eqk−1(w) [ log p(w|D1:k−1,ψ) p(w|D1:k,ψ) ] (11) = C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk−1(w)∥p(w|D1:k−1,ψ)] (12) We now make two assumptions • DKL [qk−1(w)∥p(w|D1:k,ψ)] ≥DKL [qk(w)∥p(w|D1:k,ψ)]. This is motivated from the fact that qk(w) is trained on all data chunks D1:k so it is expected to be a better approxima- tion to the posterior p(w|D1:k), compared to qk−1(w) which is only trained on D1:k−1. • DKL [qC−1(w)∥p(w|D1:C,ψ)] ≥DKL [q0(w)∥p(w)]. Since we are free to choose the approximate posterior before seeing any data — q0(w)—, we can set it to be equal to the prior p(w) which, together with the positivity of the KL divergence, trivially satisﬁes this assumption. Therefore, by rearranging Eq. 12 and using our two assumptions we have that the gap is positive gap =−DKL [q0(w)∥p(w)] +DKL [qC−1(w)∥p(w|D1:C,ψ)] + C∑ k=1 DKL [qk−1(w)∥p(w|D1:k,ψ)] −DKL [qk(w)∥p(w|D1:k,ψ)] ≥0, (13) and our approximation is a lower bound to the marginal likelihood, i.e., log p(D|ψ) ≥ C∑ k=1 Eqk−1(w) [log p(Dk|w,ψ)] . (14) 13Published as a conference paper at ICLR 2023 B P ARTITIONED NETWORKS AS A SPECIFIC APPROXIMATION TO THE MARGINAL LIKELIHOOD In this section of the appendix, we show that the partitioned neural networks we presented in the paper are a particular instance of the approximation to the marginal likelihood shown in equation 2. Consider a dataset Dcomprised of C shards, i.e. D= (D1,..., DC), along with a model, e.g., a neural network, with parameters w ∈RDw, a prior p(w) = ∏Dw j=1 N(wj|0,λ) and a likelihood p(D|w,ψ) with hyperparameters ψ. Assuming a sequence over the dataset chunks, we can write out the true marginal likelihood as log p(D|ψ) = ∑ k log p(Dk|D1:k−1,ψ) = ∑ k log Ep(w|D1:k−1,ψ) [p(Dk|w,ψ)] (15) ≥ ∑ k Ep(w|D1:k−1,ψ) [log p(Dk|w,ψ)] . (16) Since the true posteriors p(w|D1:j,ψ) for j ∈{1,...,C }are intractable, we can use variational inference to approximate them with qφj(w) for j ∈{1,...,C }, with φj being the to-be-optimized parameters of the j’th variational approximation. Based on the result from Appendix A, whenqφj(w) are optimized to match the respective posteriors p(w|D1:j,ψ), we can use them to approximate the marginal likelihood as log p(D|ψ) ≥ ∑ k Eqφk−1 (w) [log p(Dk|w,ψ)] . (17) Partitioned networks correspond to a speciﬁc choice for the sequence of approximating distribution families qφk(w). Speciﬁcally, we partition the parameter space w into Cchunks, i.e., wk ∈RDwk, such that ∑ kDwk = Dw, and we associate each parameter chunk wk with a data shard Dk. Let rφk(wk) be base variational approximations over wk with parameters φk. Each approximate distribution qφk(w) is then deﬁned in terms of these base approximations, i.e., qφk(w) =   k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) (18) where r0(·) is some base distribution with no free parameters. In accordance with the assumptions in appendix A, we can then ﬁt each qφk(w) by minimising the KL-divergence to p(w|D1:k,ψ) – the posterior after seeing kchunks: DKL [qφk(w)∥p(w|D1:k,ψ)] =−Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] + logp(D1:k|ψ) (19) (20) Finding the optimum with respect to φk: arg min φk DKL [qφk(w)∥p(w|D1:k,ψ)] = (21) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [qφk(w)∥p(w)] (22) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] + DKL     k−1∏ j=1 rφj(wj)  rφk(wk) ( K∏ m=k+1 r0(wm) ) ∥ K∏ i p(wi)   (23) = arg min φk −Eqφk(w)[log p(D1:k|w,ψ)] +DKL [rφk(wk)∥p(wk)] . (24) We can now obtain partitioned networks by assuming that rφk(wk) = N(wk|φk,νI) for k ∈ {1,...,C }, r0(w) = N(w|ˆw,νI), with ˆw being the parameters at initialization (i.e., before we 14Published as a conference paper at ICLR 2023 update them on data) and taking ν →0, i.e., in machine-precision, the weights are deterministic. As noted in Section I.1, we scale the weight-decay regularizer forφk (whenever used) differently for each partition k, such that it can be interpreted as regularization towards a prior. In the experiments where we do not regularize φk according to p(wk) when we optimize them, this implicitly corresponds to λ→∞ (i.e. the limiting behaviour when the variance of p(w) goes to inﬁnity), which makes the contribution of the regularizer negligible. C P ARTITIONING SCHEMES There are several ways in which we could aim to partition the weights of a neural network. Throughout the experimental section 5, we partition the weights by assigning a ﬁxed proportion of weights in each layer to a given partition at random. We call this approach random weight partitioning. We also experimented with other partitioning schemes. For example, we tried assigning a ﬁxed proportion of a layer’s outputs (e.g., channels in a convolution layer) to each partition. All weights in a given layer that a speciﬁc output depends on would then be assigned to that partition. We call this approach node partitioning. Both approaches are illustrated in Figure 4. One beneﬁt of the node partitioning scheme is that it makes it possible to update multiple partitions with a single batch; This is because we can make a forward pass at each linear or convolutional layer with the full network parameters w, and, instead, mask the appropriate inputs and outputs to the layer to retrieve an equivalent computation to that with w(k) s . The gradients also need to be masked on the backward pass adequately. No such simpliﬁcation is possible with the random weight partitioning scheme; if we were to compute a backward pass for a single batch of examples using different subnetworks for each example, the memory overhead would grow linearly with the number of subnetworks used. In initial experiments, we found both random weight partitioning and node partitioning performed similarly. In the experimental section 5, we focused on the former, as it’s easier to reason about with relation to e.g., dropout. Throughout this work, partitioning happens prior to initiating training, and remains ﬁxed throughout. It might also be possible to partition the network parameters dynamically during training, which we leave for future work.   w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (a) Random weight partitioned In node assignment    Out node assignment      w11 w12 w13 w14 w15 w21 w22 w23 w24 w25 w31 w32 w33 w34 w35 w41 w42 w43 w44 w45 w51 w52 w53 w54 w55 w61 w62 w63 w64 w65   (b) Node partitioned Figure 4: Figures showing how the weights within a single weight matrix W ∈R6×5 for a linear layer would be partitioned. D S CALABILITY In the paper, we claim that our method is scalable compared to Schw¨obel et al. (2021) and Immer et al. (2022). What constraints the scalability of the mentioned prior works, however, is different. For the Last Layer Marginal Likelihood, although the approach works on small datasets such as PCAM (Veeling et al., 2018) and MNIST, the authors report that they were unable to learn invariances 15Published as a conference paper at ICLR 2023 on larger datasets such as CIFAR10. In (Schw¨obel et al., 2021, section 7), they explore the issue of scalability in more detail, and showcase that last layer marginal likelihood is insufﬁcient. Differentiable Laplace performs well, even on more complex datasets, such as CIFAR10. Their scalability, however, is limited by the computational and memory complexity of their method, which we go into in more detail in the section below. D.1 C OMPLEXITY ANALYSIS First, we consider the scalability of our algorithm in terms of computational and memory complexity. In particular, we show that our method scales much more favourably compared to Differentiable Laplace (Immer et al., 2022). We present our analysis for a feed-forward model of depth L, with layer widths D8. In order to directly compare to Immer et al. (2022) and Benton et al. (2020), we consider the complexities in the invariance learning setup (Benton et al., 2020; van der Wilk et al., 2018) withSaugmentation samples. In other experiments, hyperparameter optimization setups, S can be taken to be 1. The notation is summarized in Table 5. N Number of datapoints in dataset D NB Batch size S Number of augmentation samples9 C Output size (number of classes) D Feedforward network layer widths L Feedforward network depth P Number of parameters (s.t. O(P) =O(LD2 + DC)) Table 5: Notation for complexity analysis. We consider the computational and memory costs of 1) obtaining a gradient with respect to the parameters 2) obtaining a gradient with respect to the hyperparameters, and 3) computing the value of the model/hyperparameter selection objective for each method. All analysis assumes computation on a Monte-Carlo estimate of the objective on a single batch of data. In Tables 6 and 7, we assume that C <D, and hence, for the clarity of comparison, sometimes fold a factor depending Cinto a factor depending on Dif it’s clearly smaller. This hiding of the factors was only done for Differentiable Laplace, which is the worst scaling method. D.1.1 C OMPUTATIONAL COMPLEXITY Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBPS) O(NBPS) O(NBPS) Augerino O(NBPS) O(NBPS) O(NBPS) Diff. Laplace O(NBPS) O(NBPS+NCP +NCDLS + LD3) O(NPS + NCP +NCDLS + LD3) Table 6: Computational Complexities. The two terms highlighted for Augerino can be computed in a single backward pass. For Differentiable Laplace, the terms in blue can be amortised over multiple hyperparameter backward passes. That is why, in their method, they propose updating the hyperparameters once every epoch on (possibly) multiple batches of data, rather than once on every batch as is done with Partitioned Networks and Augerino. 8This is for the ease of comparison. Same upper bound complexities will hold for a network of variable sizes Dℓ for ℓ∈[L], where D= maxℓ Dℓ 9Only relevant for invariance learning. 16Published as a conference paper at ICLR 2023 D.1.2 M EMORY COMPLEXITY The memory complexities for Partitioned Networks, Augerino, and Differentiable Laplace are shown in Table 7. Crucially, the memory required to update the hyperparameters for Differentiable Laplace scales as O(NBSLD2 + P), with a term depending on the square of the network widths. This can become prohibitively expensive for larger models, and is likely the reason why their paper only considers experiments on architectures with widths up to a maximum of 256. Param. Backward Hyperparam. Backward Hyperparam. Objective Partitioned O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Augerino O(NBSLD+ P) O(NBSLD+ P) O(NBSD+ P) Diff. Laplace O(NBSLD+ P) O(NBSLD2 + P) O(NBSLD2 + P) Table 7: Memory Complexities. Differences are highlighted in red. D.2 P RACTICAL SCALABILITY A complexity analysis in big- Onotation as provided by us in the previous sections allows to understand scalability in the limit, but constant terms that manifest in practice are still of interest. In this section we aim present real timing measurements for our method in comparison to Augerino and Differential Laplace, and elaborate on what overhead might be expected with respect to standard neural network training. The empirical timings measurements on an NVIDIA RTX 3080-10GB GPU are shown in Table 8. We used a batch-size of 250, 200 for the MNIST and CIFAR10 experiments respectively, and 20 augmentation samples, just like in our main experiments in Table 1 and Figure 3. As can be seen, the overhead from using a partitioned network is fairly negligible compared to a standard forward and backward pass. The one difference compared to Augerino is, however, the fact that a separate forward-backward pass needs to be made to update the hyperparameters and regular parameters. This necessity is something that can be side-stepped with alternative partitioning schemes, as preliminarily mentioned in appendix C, and is an interesting direction for future research. MNIST CIFAR10 Method CNN ﬁxupResNet-8 ﬁxupResNet-14 Augerino ×1 ×1 ×1 Diff. Laplace† Param. ×1 ×1 ×1 Hyperparam. ×2015.6 ×18.2 - Partitioned Param. ×1.08 ×1.17 ×1.21 Hyperparam. ×1.08 ×1.08 ×1.09 Table 8: Relative empirical time increase with respect to a regular parameter update during standard training. †The timing multipliers with respect to the baseline for ﬁxupResNet-8 are taken from the timings reported in (Immer et al., 2022, Appendix D.4). On the ResNet-14, we get an out-of- memory error during the hyperparam. update step with Differentiable Laplace on the NVIDIA RTX 3080-10GB GPU when running with the ofﬁcial codebase (Immer and van der Ouderaa). Memory Overhead Our proposed method’s memory consumption scales in the same way as Augerino or vanilla neural network training. There is a minor constant memory overhead due to having to store the assignment of weights to partitions. In general, only log Cbits per parameter are necessary to store the partition assignments, whereCis the number of chunks. In our implementation, we only consider C <28, and hence store the assignments in byte tensors. This means that the partitioned models require extra 25% memory for storing the parameters (when using 32bit ﬂoats to represent the parameters). 17Published as a conference paper at ICLR 2023 If the “default” weight values (i.e. those denoted ˆwi in Figure 1) are non-zero, there is an additional overhead to storing those as well, which doubles the memory required to store the parameters. We observed there was no difference in performance when setting default weight values to 0 in architectures in which normalisation layers are used (i.e. most modern architectures). As such, we would in general recommend to set the default weight values to 0. However, we found setting default values to the initialised values to be necessary for stability of training deep normalisation-free architectures such as the ﬁxup architectures (Zhang et al., 2019) we used to compare with Differentiable Laplace. As their method is not compatible with BatchNorm, we used these architectures in our experiments, and hence used non-zero default values. Lastly, if the default weight values are set to the (random) initialisation values, it is possible to write a cleverer implementation in which only the random seeds are stored in memory, and the default values are re-generated every time they are need in a forward and a backward pass. This would make the memory overhead from storing the default values negligible. E N OTE ON AUGERINO In replicating Augerino (Benton et al., 2020) within our code-base and experimenting with the implementation, we discovered a pathological behaviour that is partly mirrored by the authors of Immer et al. (2022). In particular, note that the loss function (Benton et al., 2020, Equation (5)) proposed by the authors is problematic in the sense that for any regularization strength λ> 0, the optimal loss value is negative inﬁnity since the regularization term (negative L2-norm) is unbounded. In our experiments we observe that for a sufﬁciently-large value of λand after a sufﬁcient number of iterations, this behaviour indeed appears and training diverges. In practice, using Augerino therefore necessitates either careful tuning of λ, clipping the regularisation term (a method that introduces yet another hyperparameter), or other techniques such as early stopping. In the open-source repository for the submission (Benton et al.), it can be seen that on many experiments the authors use a ”safe” variant of the objective, in which they clip the regulariser (without pass-through of the gradient) once the l∞-norm of any of the hyperparameters becomes larger than an arbitrary threshold. Without using this adjustment, we found that the Augerino experiments on MNIST crashed every time with hyperparameters diverging to inﬁnity. F S ENSITIVITY TO PARTITIONING F.1 S ENSITIVITY IN TERMS OF FINAL PERFORMANCE (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 5: Learning afﬁne augmentations on MNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Partitioned networks allow for learning hyperparameters in a single training run, however, they introduce an additional hyperparameter in doing so: the partitioning scheme. The practitioner needs to choose the number of chunks C, the relative proportions of data in each chunk, and the relative proportions of parameters assigned to each of the Cpartitions wk. We investigate the sensitivity to the partitioning scheme here. We show that our results are fairly robust to partitioning through a grid-search over parameter partitions and chunk proportions on the afﬁne augmentation learning task on MNIST with the CNN architecture we use throughout this work. 18Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 6: Learning afﬁne augmentations on RotMNIST with a CNN ﬁt on all data. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. Figure 5 and Figure 6 show the test accuracy for a choice of chunk and parameter proportions across two, three and four chunks. The proportions are to be read as un-normalized distributions; for example, chunk proportions set to [1,8] denotes that there are 8×as many datapoints assigned to the second compared to the ﬁrst. Each conﬁguration was run with 2 random seeds, and we report the mean across those runs in the ﬁgure. The same architecture used was the same as for the main MNIST experiments in section 5 (see Appendix I.4 for details). We observe that for various partition/dataset-chunking conﬁgurations, all models achieve fairly similar ﬁnal test accuracy. There is a trend for models with a lot of parameters assigned to later chunks, but with few datapoints assigned to later chunks, to perform worse. While these results show a high level of robustness against the choice of additional hyperparameters introduced by our method, these results do show an opportunity or necessity for choosing the right partitioning scheme in order to achieve optimal performance. F.2 S ENSITIVITY IN TERMS OF HYPERPARAMETERS FOUND To compare how the different partitioning schemes qualitatively impact the hyperparameters that the method identiﬁes, we also retrain vanilla models from scratch using the hyperparameter values found using partitioned networks. Namely, we take the ﬁnal value of the hyperparameters learned with partitioned networks with a given partitioning scheme, and plot the ﬁnal test set accuracy of a vanilla neural network model trained from scratch with those hyperparameters. The results are shown in Figures 7 and 8. (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.990 0.992 0.994 0.996 T est Accuracy Figure 7: Standard neural network trained onMNIST with a CNN ﬁt on all data, with hyperparameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 5. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. G H OW GOOD ARE THE HYPERPARAMETERS FOUND ? Here we show that the hyperparameters found by partitioned networks are also a good set of hyperparameters for vanilla neural networks retrained from scratch. This section expands on the 19Published as a conference paper at ICLR 2023 (a) 2 chunks [8,1] [4,1] [1,1] [1,4] [1,8] Param. Proportions [8,1] [4,1] [1,1] [1,4] [1,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (b) 3 chunks [8,4,1][3,2,1][1,1,1][1,2,3][1,4,8] Param. Proportions [8,4,1] [3,2,1] [1,1,1] [1,2,3] [1,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy  (c) 4 chunks [8,4,2,1][4,3,2,1][1,1,1,1][1,2,3,4][1,2,4,8] Param. Proportions [8,4,2,1] [4,3,2,1] [1,1,1,1] [1,2,3,4] [1,2,4,8]Chunk Proportions 0.987 0.988 0.989 0.990 T est Accuracy Figure 8: Standard neural network trained on RotMNIST with a CNN ﬁt on all data, with hyper- parameters found using partitioned networks with chunk and parameter proportions corresponding to those in Figure 6. x- and y−ticks denote the ratios of parameters/datapoints assigned to each partition/chunk respectively. experiment in section F.2. To validate this claim, we conducted a fairly extensive hyperparameter search on the afﬁne augmentation learning task on RotMNIST; we trained 200 models by ﬁrst sampling a set of afﬁne augmentation parameters uniformly at random from a predeﬁned range 10, and then training a neural network model (that averages across augmentation samples at train and test time, as described in Benton et al. (2020)) with standard neural training with those hyperparameters ﬁxed throughout. In Figure 9, we plot the ﬁnal test-set performance of all the models trained with those hyperparameters sampled from a ﬁxed range. Alongside, we show the hyperparameters and test-set performance of the partitioned networks as they progress throughout training. The partitioned networks consistently achieve ﬁnal test-set performance as good as that of the best hyperparameter conﬁgurations iden- tiﬁed through extensive random sampling of the space. We also show the test-set performance of neural network models, trained through standard training, with hyperparameters ﬁxed to the ﬁnal hyperparameter values identiﬁed by the partitioned networks. The hyperparameters identiﬁed by partitioned networks appear to also be good for regular neural networks; the standard neural networks with hyperparameters identiﬁed through partitioned training also outperform the extensive random sampling of the hyperparameter space. Furthermore, Figure 9 shows that partitioned networks do learn full rotation invariance on the RotMNIST task, i.e. when full rotation invariance is present in the data generating distribution. 0.0 0.2 0.4 Translation X 0.96 0.97 0.98 0.99T est Accuracy 0.0 0.2 0.4 Translation Y 0 /2 Rotation Random Sampling Partitioned Runs Partitioned Runs Final Partitioned Runs Final - Retrained 0.0 0.2 0.4 Scale X 0.0 0.2 0.4 0.6 Scale Y 0.0 0.2 0.4 Shear Figure 9: The test-set performance plotted alongside (1D projections of) afﬁne augmentation hyper- parameters on the RotMNIST task with MNIST-CNN. Final test-set accuracies are shown for the hyperparameters sampled randomly for a neural network model trained through standard training with those hyperparameters ﬁxed (+). For multiple partitioned networks runs, the plot shows the progres- sion of the identiﬁed hyperparameters and the test-set performance through the training run ( ), as well as the ﬁnal hyperparameters and test-set performance ( ). Lastly, the plot also shows the ﬁnal test-set accuracies of models trained through standard training on the ﬁnal hyperparameters identiﬁed through partitioned training ( ). 10The ranges were: Uniform(0,π) for the maximum rotation, and Uniform(0,1 2 ) for all the remaining afﬁne augmentation parameters (maximum shear, maximum x−and y−translation, and maximum x−and y−scale). 20Published as a conference paper at ICLR 2023 H L IMITATIONS As mentioned in the main text, our method improves upon existing work, but also comes with its own limitations. Complexity Inherent to our method — as presented in e.g. Figure 1 — is the necessity for an additional forward-backward pass to update the hyperparameters. Consequently, hyperparameter optimization has additional costs which, however, are signiﬁcantly less than the computational costs of existing work, as we discuss in more detail in Appendix D.1 and the experimental section. Furthermore, empirically, partitioned networks usually require more training iterations to converge. Performance Assuming the optimal hyper-parameters are given, training the full, non-partitioned networks based on those optimal values can be expected to yield better performance compared to the ﬁnal model found by partitioned training. Partitioning the network inherently constrains the network capacity, causing some loss of performance. Opportunities for alleviating this performance loss while still enjoying single-run hyperparameter optimization through partitioned training will be left to future work. These include for example adjusting training rounds or increasing network capacity in the ﬁrst place. Partitioning While partitioned networks allows for automatic optimization of, intuitively, hard to tune hyperparameters, such as augmentation parameters, they come with the additional limitation of requiring to partition both the data and the model. This introduces an additional hyperparameter, namely, the partitioning strategy. While our default strategy of assigning more parameters and data to the ﬁrst chunk works reasonably well on all of the experiments we consider, if one targets obtaining the best possible performance on a given task, the partitioning strategy might need additional tuning. We provide some empirical results about the sensitivity to partitioning in appendix F.1 I E XPERIMENTAL DETAILS I.1 P ARTITIONED TRAINING Partitioned parameter update scheduling The gradient computation of Equation 3, as described in the main text, requires that the data-points for updating a given subnetwork w(k) s come from the appropriate dataset chunks (x,y) ∈D1:k for a chunk k. Depending on the partitioning scheme (Appendix C), evaluating different subnetworks for different chunks can or cannot be done in a single mini-batch. More speciﬁcally, the random weight-partitioning we chose for our experiments requires a separate mini-batch per subnetwork (in order to keep the memory cost the same as for standard neural network training). An immediate question arising from a chunked dataset and several partitions is to deﬁne the order and frequency of updates across subnetworks. In our experiments we deﬁne (non-uniform) splits of the training dataset Dacross the Cchunks, which requires a tailored approach to sampling the data. More speciﬁcally, for a given (normalized) ratio of chunk-sizes [u1,...,u C], each iteration of partitioned training proceeds as follows: 1. Sample a partition index k∼Cat(u1,...,u C) 2. Sample a mini-batch ˜Dof examples uniformly from D1:k. 3. Evaluate log p( ˜D|w(k) s ,ψ) using subnetwork w(k) s and 4. compute the (stochastic) gradient wrt. partition parameters wk (Eq. 3). 5. Update partition parameters wk using an optimizer, such as SGD or Adam. This sampling scheme results in a data-point (x,y) ∈Dk from earlier chunks to be sampled more often. Concretely, the probability that an example in chunk kwill be sampled is ∝∑ i≤kui. This is done so that each partition wk is updated with equal probability on each of the examples in D1:k As a result, we use with replacement sampling for the partitioned network training throughout the experimental section. 21Published as a conference paper at ICLR 2023 Gradient optimization of partitioned parameters A consequence of per-partition updates with the random weight partitioning scheme (appendix C) is that, for a chosen partition wk to update, all other partitions do not receive a gradient update. In other words, the gradient at each iteration is sparse. Consequently, many off-the-shelve momentum-based optimizers will not account correctly. Speciﬁcally, we implement modiﬁcations to the PyTorch Paszke et al. (2019) provided optimizers that allow us to track per-partition momenta, number of steps, etc. Note that this creates a disconnect between the number of iterations across all partitions and the number of iterations per-partition. Doing so, however aligns the computational cost of training the partitioned network parameters with the cost of training regular neural network parameters. Regardless, we do not alter the way learning-rate schedulers behave in our experiments and anneal learning-rates according to the total number of iterations. Similarly, we report the total number of iterations when comparing against baselines that update all network-parameters per iteration. While a simple gradient-accumulation scheme across mini-batches would result in a single gradient across all partitions, this approach inherently clashes with non-uniform partitioning [u1,...,u C]. Instead, we chose to sequentially apply gradients computed on a single partition, as described in the previous paragraphs. A further advantage of this approach is that learning progress made by updating partition wk immediately inﬂuences (and can improve) the prediction of subnetworks w(k) s ,w(k+1) s ,..., w(C) s . Gradient optimization of hyperparameters Our partitioned network scheme makes it easy to compute stochastic gradients of the hyperparameter objective LML in Eq. 4 using batch gradient descent optimization methods. After every update to a randomly sampled network partition (see previous paragraph), we update hyperparamters ψas follows: • sample a dataset chunk index k ∼Cat(u2 Z ,..., uC Z ). Ratios are re-normalized to exclude D1. • sample a mini-batch ˜Dof examples uniformly from Dk (Note the choice of Dk instead of D1:k). • Evaluate log p( ˜D|w(k−1) s ,ψ) using subnetwork w(k−1) s and • compute the (stochastic) gradient wrt. hyperparameters ψ(Eq. 4). • Update partition parameters ψusing an optimizer, such as SGD or Adam. The above sampling procedure yields an unbiased estimate of gradients in eq. 4. The fact that we optimize hyperparameters with gradients based on data from a single chunk at a time is again a consequence of the random weight-partitioning scheme for the partitioned networks. It is possible to compute gradients wrt. ψfor mini-batches with examples from multiple chunks at a time. With the random weight partitioning scheme, this would result in an increased memory overhead. Lastly, we could also accumulate gradients from different chunks, similarly to Immer et al. (2022), and this would likely result in a lower-variance estimate per update . It is also possible to reduce the computational overhead of evaluating two mini-batches per iteration (one for updates to wk, one for ψ) as we do in our experiments by interleaving hyperparameter updates at less frequent intervals. We leave an exploration of these design choices to future work. Throughout all experiments, except those in the federated settings (see section J), we use the same batch-size for the hyperparameter udpates as for the regular parameter updates. Weight-decay For partitioned networks, whenever using weight-decay, we scale the weight decay for earlier partitions with the reciprocal of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. This makes the training compatible with the variational interpretation in Appendix B. I.2 P ARTITIONED AFFINE TRANSFORMATIONS In Appendix C we described how we realize partitioned versions of fully-connected and convolutional layers. Design choices for other parameterized network layers used in our experiments are described below. 22Published as a conference paper at ICLR 2023 Normalization layers It is common-place in most architectures to follow a normalization layer (such as BatchNorm (Ioffe and Szegedy, 2015), GroupNorm (Wu and He, 2018)) with an element- wise or channel-wise, afﬁne transformation. Namely, such a transformation multiplies its input h by a scale vector s and adds a bias vector b: o = h ∗s + b. For random weight-partitioned networks, we parameterize such afﬁne transformations by deﬁning separate vectors {s1,..., sC} and {b1,..., bC}for each partition; the actual scale and bias used in a given subnetwork w(k) s are s(k) s = ∏ i∈{1,...,k}si and b(k) s = ∑ i∈{1,...,k}bi respectively. This ensures that the ﬁnal afﬁne transformation for each subnetwork w(k) s depends on the parameters in the previous partitions [1,...,k −1]. Doing so increases the parameter count for the partitioned networks in architectures that use those normalization layers by a negligible amount. Scale and bias in FixUp networks The FixUp paper (Zhang et al., 2019) introduces extra scales and biases into the ResNet architecture that transform the entire output of the layers they follow. We turn these into “partitioned” parameters using the same scheme as that for scales and biases of afﬁne transformations following normalization layers. For partitioned networks, through-out the paper, we match the proportion of parameters assigned to each partition kin each layer to the proportion of data examples in the corresponding chunk Dk. I.3 A RCHITECTURE CHOICES Input selection experiments We use a fully-connected feed-forward neural network with2 hidden layers of size [256,256], and with GeLU (Hendrycks and Gimpel, 2016) activation functions. We initialise the weights using the Kaiming uniform scheme (He et al., 2015). For partitioned networks, we use the random-weight partitioning scheme. Fixup Resnet For all experiments using FixUp ResNets we follow Immer et al. (2022); Zhang et al. (2019), and use a 3-stage ResNet with channel-sizes (16,32,64) per stage, with identity skip- connections for the residual blocks as described in He et al. (2016). The residual stages are followed by average pooling and a ﬁnal linear layer with biases. We use 2D average pooling in the residual branches of the downsampling blocks.We initialize all the parameters as described in Zhang et al. (2019). Wide ResNet For all experiments using a Wide-ResNet-N-D (Zagoruyko and Komodakis, 2016), with N being the depth and D the width multiplier, we use a 3 stage ResNet with channel-sizes (16D,32D,64D). We use identity skip-connections for the residual blocks, as described in He et al. (2016), also sometimes known as ResNetV2. ResNet-50 We use the ”V2” version of Wide ResNet as described in (Zagoruyko and Komodakis, 2016) and replace BatchNormalization with GroupNormalization using 2 groups. We use the ’standard’ with withD= 1and three stages of 8 layers for a 50-layer deep ResNet. We use ReLU activations for all ResNet experiments throughout. MNIST CNN For the MNIST experiments, we use the same architecture as Schw¨obel et al. (2021) illustrated in the replicated Table 9. Table 9: CNN architecture for MNIST experiments Layer Speciﬁcation 2D convolution channels=20, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 2D convolution channels=50, kernel size=(5,5), padding=2, activation=ReLU Max pooling pool size=(2,2), stride=2 Fully connected units=500, activation=ReLU Fully connected units=50, activation=ReLU Fully connected units=10, activation=Softmax 23Published as a conference paper at ICLR 2023 I.4 T RAINING DETAILS Learning afﬁne augmentations For the parametrization of the learnable afﬁne augmentation strategies, we follow prior works for a fair comparison. More speciﬁcally, for our MNIST based setup we follow the parametrization proposed in Schw¨obel et al. (2021) whereas for our CIFAR10 based setup we use the generator parametrization from Immer et al. (2022). Input selection experiments For the model selection (non-differentiable) input selection exper- iments, we train all variants with Adam with a learning rate of 0.001 and a batch-size of 256 for 10000 iterations. For both Laplace and partitioned networks, we do early stopping based on the marginal likelihood objective (LML for partitioned networks). We use weight-decay 0.0003 in both cases. For the post-hoc Laplace method, we use the diagonal Hessian approximation, following the recommendation in (Immer et al., 2021). For partitioned networks, we divide the data and parameters into 8 chunks of uniform sizes. We plot results averaged across 3 runs. Mask learning for input selection experiment We use the same optimizer settings as for the input selection experiment. We train for 30000 iterations, and optimize hyperparameters with Adam with a learning rate of 0.001. We divide the data and parameters into 4 uniform chunks. MNIST experiments We follow Schw¨obel et al. (2021), and optimize all methods with Adam with a learning rate of 0.001, no weight decay, and a batch-size of 200. For the partitioned net- works and Augerino results, we use 20 augmentation samples. We use an Adam optimizer for the hyperparameters with a learning rate of 0.001 (and default beta parameters). For Augerino on MNIST, we use the “safe” variant, as otherwise the hyperparameters and the loss diverge on every training run. We elaborate on this phenomenon in Appendix E. Otherwise, we follow the recommended settings from (Benton et al., 2020) and Immer et al. (2022), namely, a regularization strength of 0.01, and a learning rate for the hyperparameters of 0.05. For both MNIST and CIFAR experiments, we found it beneﬁcial to allocate more data to either the earlier, or the later, chunks. Hence, we use 3 chunks with [80%,10%,10%] split of examples for all MNIST and CIFAR experiments. CIFAR variations experiments We again follow Immer et al. (2022), and optimize all ResNet models with SGD with a learning rate of 0.1 decayed by a factor of 100×using Cosine An- nealing, and momentum of 0.9 (as is standard for ResNet models). We use a batch-size of 250. We again use Adam for hyperparameter optimization with a learning rate of 0.001 (and default beta parameters). We train our method for [2400,8000,12000,20000,40000] iterations on subsets [1000,5000,10000,20000,50000] respectively for CIFAR-10, just as in (Immer et al., 2022). For all methods, we used a weight-decay of 1e−4. For partitioned networks, we increase the weight decay for earlier partitions with the square root of the number of examples in chunks used to optimize them, following the diagonal Gaussian prior interpretation of weight-decay. We use3 chunks with [80%,10%,10%] split of examples. For RotCIFAR-10 results, we noticed our method hasn’t fully converged (based on training loss) in this number of iterations, and so we doubled the number of training iterations for the RotMNIST results. This slower convergence can be explained by the fact that, with our method, we only update a fraction of the network parameters at every iteration. TinyImagenet experiments Our experiments with TinyImagenet (Le and Yang, 2015) closely follow the setting for the CIFAR-10 experiments described above. Images are of size64x64 pixels, to be classiﬁed into one of 200 classes. The training-set consists of 100000 images and we compare our method against baselines on subset of [10000,50000,100000] datapoints. For the standard version of TinyImagenet, we train for [80000,80000,40000] steps respectively and for the rotated version of TinyImagenet we train for 120000 steps for all subset sizes. We tuned no other hyper-parameters compared to the CIFAR-10 setup and report our method’s result for a partitioning with[80%,20%] across 2 chunks after ﬁnding it to perform slightly better than a [80%,10%,10%] split across 3 chunks in a preliminary comparison. 24Published as a conference paper at ICLR 2023 Fine-tuning experiments For the ﬁne-tuning experiments in table 2, we trained a FixUp ResNet-14 on a subset of 20000 CIFAR10 examples, while optimizing afﬁne augmentations (following afﬁne augmentations parameterization in (Benton et al., 2020)). We used the same optimizer settings as for all other CIFAR experiments, and trained for 80000 iterations, decaying the learning rate with Cosine Annealing for the ﬁrst 60000 iterations. For ﬁne-tuning of validation-set optimization models, we used SGD with same settings, overriding only the learning rate to 0.01. We tried a learning rate of 0.01 and 0.001, and selected the one that was most favourable for the baseline based on the test accuracy. We also tried training on the full CIFAR-10 dataset, but found that all methods ended up within a standard error of each other when more than 70% of the data was assigned to the ﬁrst chunk (or training set, in the case of validation set optimization). This indicates that CIFAR-10 is sufﬁciently larger that, when combined with afﬁne augmentation learning and the relatively small ResNet-14 architecture used, using the extra data in the 2nd partition (or the validation set) results in negligible gains. I.5 D ATASETS Input selection synthetic dataset For the input selection dataset, we sample 3000 datapoints for the training set as described in section 5, and we use a fresh sample of 1000 datapoints for the test set. RotMNIST Sometimes in the literature, RotMNIST referes to a speciﬁc subset of 12000 MNIST examples, whereas in other works, the full dataset with 60000 examples is used. In this work, following (Benton et al., 2020; Immer et al., 2022) we use the latter. J F EDERATED PARTITIONED TRAINING In this section, we explain how partitioned networks can be applied to the federated setting, as well as the experimental details. J.1 P ARTITIONED NETWORKS IN FL In order to apply partitioned networks to the federated setting, we randomly choose a partition for each client such that the marginal distribution of partitions follows a pre-determined ratio. A given chunk Dk therefore corresponds to the union of several clients’ datasets. Analogous to how “partitioned training” is discussed in the main text and Appendix I, we desire each partition wk to be updated on chunks D1:k. Equation 3 in the main text explains which data chunks are used to compute gradients wrt. parameter partition wk. An analogous perspective to this objective is visualized by the exemplary algorithm in Figure 1 and asks which partitions are inﬂuenced (i,e., updated) by data from chunk Dk: A data chunk Dk is used to compute gradients wrt. partitions wk:C through subnetworks w(k) s to w(C) s respectively. Consequently, a client whose dataset is assigned to chunkDk can compute gradients for all partitions wk:C. Updating network partitions Due to the weight-partitioned construction of the partitioned neural networks, it is not possible to compute gradients with respect to all partitions in a single batched forward-pass through the network. Additionally, a change to the partition parameters wk directly inﬂuences subnetworks w(k+1) s to w(C) s . In order to avoid the choice of ordering indices kto Cfor the client’s local update computation, we update each partition independently while keeping all other partitions initialised to the server-provided values that the client received in that round t: Denote Di,k as the dataset of client iwhere we keep index kto emphasize the client’s assignment to chunkk. Further denote wt+1 j,i as the partition wt j after having been updated by client ion dataset Di,k. wt+1 j,i = arg max wj log p ( Di,k|(wt 1,..., wt j, ˆwt j+1,..., ˆwt j+C),ψ ) ∀j ∈[k,C], (25) where the details of optimization are explained in the following section. We leave an explo- ration for different sequential updating schemes to future work. The ﬁnal update communi- cated by a client to the server consists of the concatenation of all updated parameter partitions 25Published as a conference paper at ICLR 2023 wt+1 .,i = concat(wt+1 k,i ,..., wt+1 C,i ). Note that partitions (wt 1,..., wt k−1) have not been modiﬁed and need not be communicated to the server. The resulting communication reductions make partitioned networks especially attractive to FL as data upload from client to server poses a signiﬁcant bottleneck. In practice, we expect the beneﬁts of these communication reductions to outweigh the additional computation burden of sequentially computing gradients wrt., to multiple partitions. The server receives wt+1 .,i from all clients that participates in round t, computes the delta’s with the global model and proceeds to average them to compute the server-side gradient in the typical federated learning fashion (Reddi et al., 2020). Updating hyperparameters The computation of gradients on a clientiwrt. ψis a straight-forward extension of equation 4 and the exemplary algorithm of Figure 1: ∇ψLML (Di,k,ψ) ≈∇ψlog p ( Di,k|w(t+1),(k−1) s,i ,ψ ) , (26) where Di,k corresponds to client i’s local dataset which is assigned to chunk k and w(t+1),(k−1) s corresponds to the (k−1)’th subnetwork after incorporating all updated partitionsw(t+1),(k−1) s,i = concat(wt 1,..., wt k−1,wt+1 k,i ,..., wt+1 C,i ). Note that we compute a full-batch update to ψin MNIST experiments and use a batch-size equal to the batch-size for the partitioned parameter updates for CIFAR10. Upon receiving these gradients from all clients in this round, the server averages them to form a server-side gradient. Conceptually, this approach to updating ψcorresponds to federated SGD. J.2 F EDERATED SETUP Non-i.i.d. partitioning For our federated experiments, we split the 50kMNIST and 45kCIFAR10 training data-points across 100 clients in a non-i.i.d. way to create the typical challenge to federated learning experiments. In order to simulate label-skew, we follow the recipe proposed in Reddi et al. (2020) with α= 1.0 for CIFAR10 and α= 0.1 for MNIST. Note that with α= 0.1, most clients have data corresponding to only a single digit. For our experiments on rotated versions of CIFAR10 and MNIST, we sample a degree of rotation per data-point and keep it ﬁxed during training. In order to create a non-i.i.d partitioning across the clients, we bin data-points according to their degree of rotation into 10 bins and sample using the same technique as for label-skew with α = 0.1 for both datasets. Learning curves are computed using the 10k MNIST and 5k CIFAR10 validation data-points respectively. For the rotated dataset experiments, we rotate the validation set in the same manner as the training set. Architectures and experimental setup We use the convolutional network provided at Schw¨obel et al. (2021) for MNIST and the ResNet-9 (Dys) model for CIFAR10 but with group normaliza- tion (Wu and He, 2018) instead of batch normalization. We include (learnable) dropout using the continuous relaxation proposed at Maddison et al. (2016) between layers for both architectures. We select 3 chunks for MNIST with a [0.7,0.2,0.1] ratio for both, client-assignments and parameter- partition sizes. For CIFAR10, we found a [0.9,0.1] split across 2 sub-networks to be beneﬁcial. In addition to dropout logits, ψencompasses parameters for afﬁne transformations, i.e., shear, trans- lation, scale and rotation. We report results after 2kand 5krounds, respectively, and the expected communication costs as a percentage of the non-partitioned baseline. Shared setting In order to elaborate on the details to reproduce our results, we ﬁrst focus on the settings that apply across all federated experiments. We randomly sample the corresponding subset of 1.25k, 5kdata-points from the full training set and keep that selection ﬁxed across experiments (i,e., baselines and partitioned networks) as well as seeds. The subsequent partitioning across clients as detailed in the previous paragraph is equally kept ﬁxed across experiments and seeds. Each client computes updates for one epoch of its local dataset, which, for the low data regimes of 1.25k data-points globally, results in single update per client using the entire local dataset. We averaged over 10 augmentation samples for the forward pass in both training and inference. MNIST & RotMNIST For 5k data-points and correspondingly 50 data-points on average per client, most clients perform a single update step. A small selection of clients with more than 64 data- 26Published as a conference paper at ICLR 2023 points performs two updates per round. For the experiments using the full dataset and a mini-batch size of 64, each client performs multiple updates per round. After initial exploration on the baseline FedAvg task, we select a local learning-rate of 5e−2 and apply standard SGD. The server performs Adam Reddi et al. (2020) with a learning rate of 1e−3 for the model parameters. We keep the other parameters of Adam at their standard PyTorch values. We ﬁnd this setting to generalize to the partitioned network experiments but found a higher learning rate of3e−3 for the hyper-parameters to be helpful. We chose the convolutional network from Schw¨obel et al. (2021) with (learned) dropout added between layers. The model’s dropout layers are initialized to drop10% of hidden activations. For the baseline model we keep the dropout-rate ﬁxed and found 10% to be more stable than 30%. CIFAR10 & RotCIFAR10 We ﬁx a mini-batch size of 32, leading to multiple updates per client per round in both, the full dataset regime as well as the5kdata-points setting. Similarly to the MNIST setting, we performed an initial exploration of hyperparameters on the baseline FedAvg task and use the same ones on partitioned networks. We used dropout on the middle layer of each block which was initialized to 0.1 for both the baseline and partitioned networks and whereas partitioned networks optimized it with LML and the concrete relaxation from Maddison et al. (2016), the baseline kept it ﬁxed. For the server side optimizer we used Adam with the default betas and a learning rate of 1e−2, whereas for the hyperparameters we used Adam with the default betas and a learning rate of 1e−3. In both cases we used an ϵ= 1e−7. For the local optimizer we used SGD with a learning rate of 10−0.5 and no momentum. J.3 MNIST LEARNING CURVES In Figure 10 we show learning curves for the three considered dataset sizes on the standard MNIST task. Each learning curve is created by computing a moving average across 10 evaluations, each of which is performed every 10 communication rounds, for each seed. We then compute the average and standard-error across sees and plot those values on the y-axis. On the x-axis we denote the total communication costs (up- and download) to showcase the partitioned networks reduction in communication overhead. We see that especially for the low dataset regime, training has not converged yet and we expect performance to improve for an increased number of iterations. Figure 10: Learning curves for MNIST experiments on 1.25k, 5kand 50kdata-points respectively. 27",
      "meta_data": {
        "arxiv_id": "2304.14766v1",
        "authors": [
          "Bruno Mlodozeniec",
          "Matthias Reisser",
          "Christos Louizos"
        ],
        "published_date": "2023-04-28T11:24:41Z",
        "pdf_url": "https://arxiv.org/pdf/2304.14766v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces 'Partitioned Neural Networks,' a novel and efficient method for hyperparameter optimization (HPO) that is inspired by the marginal likelihood, yet requires only a single training run and no validation data. The core contribution is enabling HPO by partitioning the training data and a neural network model into K data shards and parameter partitions, respectively. Each partition is optimized only on specific data shards, and the 'out-of-training-sample' loss on unseen data shards serves as the objective for hyperparameter optimization. This approach is demonstrated to be computationally cheaper and more scalable than prior marginal likelihood-based methods and is particularly beneficial for scenarios like federated learning where traditional HPO is challenging.",
        "methodology": "The method optimizes a lower-bound approximation to the marginal likelihood, denoted as LML, which leverages a 'learning speed' perspective. Neural network weights are partitioned into C chunks (w1, ..., wC). Subnetworks w(k)s are formed by concatenating w1, ..., wk and setting subsequent parameters to default values (e.g., initialization). Model parameters wk are updated by optimizing the negative log-likelihood on data from shards D1:k using subnetwork w(k)s. Hyperparameters (ψ) are optimized with respect to the LML objective (Equation 4), which is computed as the sum of out-of-sample losses on current data shard Dk using subnetworks w(k-1)s trained on preceding shards D1:k-1. The authors use random weight partitioning for experiments, where a fixed proportion of weights in each layer is randomly assigned to partitions. Optimization is performed using stochastic gradient descent (SGD) for both model parameters and hyperparameters, with specialized scheduling for per-partition updates and scaled weight-decay for earlier partitions.",
        "experimental_setup": "The method was evaluated on several tasks: a toy input selection problem (both fixed mask and differentiable mask learning), learning invariances through affine data augmentations, optimizing a feature extractor, and hyperparameter optimization in a federated learning (FL) setting. Datasets included synthetic data for input selection, MNIST, CIFAR10, TinyImagenet, and their rotated variants (rotCIFAR10, rotTinyImagenet, rotMNIST) for invariance learning. FL experiments used non-i.i.d. splits of MNIST and CIFAR10 across 100 clients. Architectures varied from MLPs for toy tasks to CNNs, fixupResNets (8, 14 layers), ResNet-50 with GroupNorm, and Wide ResNet-20. Baselines included Post-hoc Diagonal Laplace, standard training, Augerino, Differentiable Laplace, Last-layer marginal likelihood, traditional validation set optimization with fine-tuning, and FedAvg with/without Augerino. Validation was primarily based on test accuracy, log-likelihood, and analysis of learned mask probabilities. Experiments used 2, 3, or 4 chunks, often with non-uniform data/parameter distributions.",
        "limitations": "The method introduces an additional computational overhead due to the necessity of an extra forward-backward pass for hyperparameter updates, although this overhead is significantly less than existing marginal likelihood approximation methods. Empirically, partitioned networks may require more training iterations to converge. The partitioning of the network inherently constrains its capacity, potentially leading to some loss of performance compared to a full, non-partitioned network trained with perfectly tuned hyperparameters. Furthermore, the partitioning strategy itself (number of chunks, relative data/parameter proportions) becomes an additional hyperparameter that may require tuning for optimal performance. The authors also noted pathological behavior in one of the baseline methods (Augerino), requiring careful tuning or clipping to prevent divergence.",
        "future_research_directions": "Future research could explore dynamic partitioning of network parameters during training, rather than fixing them upfront. Investigating alternative partitioning schemes that allow updating multiple partitions with a single batch could potentially reduce computational overhead. Opportunities exist to alleviate the observed performance loss by adjusting training rounds or increasing network capacity. Optimizing the scheduling of hyperparameter updates (e.g., gradient accumulation, less frequent updates) is another area for exploration to manage computational overhead and variance. Finally, in the federated learning context, exploring different sequential updating schemes for partitions on client devices is a promising direction."
      }
    },
    {
      "title": "Learning to Mutate with Hypergradient Guided Population"
    },
    {
      "title": "Implicit differentiation of Lasso-type models for hyperparameter optimization",
      "abstract": "Setting regularization parameters for Lasso-type estimators is notoriously\ndifficult, though crucial in practice. The most popular hyperparameter\noptimization approach is grid-search using held-out validation data.\nGrid-search however requires to choose a predefined grid for each parameter,\nwhich scales exponentially in the number of parameters. Another approach is to\ncast hyperparameter optimization as a bi-level optimization problem, one can\nsolve by gradient descent. The key challenge for these methods is the\nestimation of the gradient with respect to the hyperparameters. Computing this\ngradient via forward or backward automatic differentiation is possible yet\nusually suffers from high memory consumption. Alternatively implicit\ndifferentiation typically involves solving a linear system which can be\nprohibitive and numerically unstable in high dimension. In addition, implicit\ndifferentiation usually assumes smooth loss functions, which is not the case\nfor Lasso-type problems. This work introduces an efficient implicit\ndifferentiation algorithm, without matrix inversion, tailored for Lasso-type\nproblems. Our approach scales to high-dimensional data by leveraging the\nsparsity of the solutions. Experiments demonstrate that the proposed method\noutperforms a large number of standard methods to optimize the error on\nheld-out data, or the Stein Unbiased Risk Estimator (SURE).",
      "full_text": "Implicit differentiation of Lasso-type models for hyperparameter optimization Quentin Bertrand* 1 Quentin Klopfenstein* 2 Mathieu Blondel3 Samuel Vaiter4 Alexandre Gramfort1 Joseph Salmon5 Abstract Setting regularization parameters for Lasso-type estimators is notoriously difﬁcult, though cru- cial in practice. The most popular hyperparam- eter optimization approach is grid-search using held-out validation data. Grid-search however re- quires to choose a predeﬁned grid for each pa- rameter, which scales exponentially in the num- ber of parameters. Another approach is to cast hyperparameter optimization as a bi-level opti- mization problem, one can solve by gradient de- scent. The key challenge for these methods is the estimation of the gradient w.r.t.the hyperpa- rameters. Computing this gradient via forward or backward automatic differentiation is possible yet usually suffers from high memory consump- tion. Alternatively implicit differentiation typi- cally involves solving a linear system which can be prohibitive and numerically unstable in high dimension. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case for Lasso-type problems. This work introduces an efﬁcient implicit differentia- tion algorithm, without matrix inversion, tailored for Lasso-type problems. Our approach scales to high-dimensional data by leveraging the sparsity of the solutions. Experiments demonstrate that the proposed method outperforms a large num- ber of standard methods to optimize the error on held-out data, or the Stein Unbiased Risk Esti- mator (SURE). *Equal contribution 1Université Paris-Saclay, Inria, CEA, Palaiseau, France 2Institut Mathématique de Bourgogne, Univer- sité de Bourgogne, Dijon, France 3Google Research, Brain team, Paris, France 4CNRS and Institut Mathématique de Bourgogne, Université de Bourgogne, Dijon, France 5IMAG, Université de Montpellier, CNRS, Montpellier, France. Correspondence to: Quentin Bertrand <quentin.bertrand@inria.fr>, Quentin Klopfen- stein <quentin.klopfenstein@u-bourgogne.fr>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). 1. Introduction In many statistical applications, the number of parame- ters p is much larger than the number of observations n. In such scenarios, a popular approach to tackle linear re- gression problems is to consider convex ℓ1-type penalties, used in Lasso (Tibshirani, 1996), Group-Lasso (Yuan and Lin, 2006), Elastic-Net (Zou and Hastie, 2005) or adap- tive Lasso (Zou, 2006). These Lasso-type estimators rely on regularization hyperparameters, trading data ﬁdelity against sparsity. Unfortunately, setting these hyperparame- ters is hard in practice: estimators based on ℓ1-type penal- ties are indeed more sensitive to the choice of hyperparam- eters than ℓ2 regularized estimators. To control for overﬁtting, it is customary to use different datasets for model training ( i.e., computing the regression coefﬁcients) and hyperparameter selection ( i.e., choosing the best regularization parameters). A metric, e.g., hold- out loss , is optimized on a validation dataset (Stone and Ramer, 1965). Alternatively one can rely on a statistical criteria that penalizes complex models such as AIC/BIC (Liu et al., 2011) or SURE (Stein Unbiased Risk Estima- tor, Stein 1981). In all cases, hyperparameters are tuned to optimize a chosen metric. The canonical hyperparameter optimization method is grid-search. It consists in ﬁtting and selecting the best model over a predeﬁned grid of parameter values. The complexity of grid-search is exponential with the number of hyperparameters, making it only competitive when the number of hyperparameters is small. Other hyperparameter selection strategies include random search (Bergstra and Bengio, 2012) and Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) that aims to learn an approxima- tion of the metric over the parameter space and rely on an exploration policy to ﬁnd the optimum. Another line of work for hyperparameter optimization (HO) relies on gradient descent in the hyperparameter space. This strategy has been widely explored for smooth objective functions (Larsen et al., 1996; Bengio, 2000; Larsen et al., 2012). The main challenge for this class of methods is estimating the gradient w.r.t.the hyperparame- ters. Gradient estimation techniques are mostly divided in two categories. Implicit differentiation requires the exact arXiv:2002.08943v3  [stat.ML]  3 Sep 2020Implicit differentiation of Lasso-type models for hyperparameter optimization solution of the optimization problem and involves the res- olution of a linear system (Bengio, 2000). This can be ex- pensive to compute and lead to numerical instabilities, es- pecially when the system is ill-conditioned (Lorraine et al., 2019). Alternatively, iterative differentiation computes the gradient using the iterates of an optimization algorithm. Backward iterative differentiation (Domke, 2012) is com- putationally efﬁcient when the number of hyperparameters is large. However it is memory consuming since it requires storing all intermediate iterates. In contrast, forward itera- tive differentiation (Deledalle et al., 2014; Franceschi et al., 2017) does not require storing the iterates but can be com- putationally expensive with a large number of hyperparam- eters; see Baydin et al. (2018) for a survey. This article proposes to investigate the use of these meth- ods to set the regularization hyperparameters in an auto- matic fashion for Lasso-type problems. To cover the cases of both low and high number of hyperparameters, two esti- mators are investigated, namely the Lasso and the weighted Lasso which have respectively one or as many parameters as features. Our contributions are as follows: • We show that forward iterative differentiation of block coordinate descent (BCD), a state-of-the-art solver for Lasso-type problems, converges towards the true gra- dient. Crucially, we show that this scheme converges linearly once the support is identiﬁed and that its limit does not depend of the initial starting point. • These results lead to the proposed algorithm (Algo- rithm 2) where the computation of the Jacobian is de- coupled from the computation of the regression co- efﬁcients. The later can be done with state-of-the-art convex solvers, and interestingly, it does not require solving a linear system, potentially ill-conditioned. • We show through an extensive benchmark on simu- lated and real high dimensional data that the proposed method outperforms state-of-the-art HO methods. Our work is somewhat similar to Gregor and LeCun (2010); Xin et al. (2016); Borgerding et al. (2017); Liu et al. (2018); Wu et al. (2019), where the solution is differenti- ated w.r.t. optimization parameters instead of the regular- ization parameter. However the goal is very different as they want to accelerate the optimization algorithm whereas we provide an efﬁcient algorithm to compute the gradient. Notation The design matrix is X ∈Rn×p (corresponding to nsamples and pfeatures) and the observation vector is y ∈Rn. The regularization parameter, possibly multivari- ate, is denoted by λ = (λ1,...,λ r)⊤ ∈Rr. We denote ˆβ(λ) ∈Rp the regression coefﬁcients associated to λ. We denote ˆJ(λ) ≜ (∇λˆβ(λ) 1 ,..., ∇λˆβ(λ) p )⊤∈Rp×r the weak Jacobian (Evans and Gariepy, 1992) of ˆβ(λ) w.r.t.λ. For a function ψ : Rp ×Rr →R with weak derivatives of order two, we denote by∇βψ(β,λ) ∈Rp(resp. ∇λ(β,λ) ∈Rr) its weak gradient w.r.t.the ﬁrst parameter (resp. the second parameter). The weak Hessian ∇2ψ(β,λ) is a matrix in R(p+r)×(p+r) which has a block structure ∇2ψ(β,λ) = (∇2 βψ(β,λ) ∇2 β,λψ(β,λ) ∇2 λ,βψ(β,λ) ∇2 λψ(β,λ) ) . The support of ˆβ(λ) (the indices of non-zero coefﬁcients) is denoted by ˆS(λ), and ˆs(λ) represents its cardinality (i.e., the number of non-zero coefﬁcients). The sign vec- tor sign ˆβ(λ) ∈Rp is the vector of component-wise signs (with the convention thatsign(0) = 0) of ˆβ(λ). Note that to ease the reading, we drop λin the notation when it is clear from the context and use ˆβ, ˆJ, ˆS and ˆs. The Mahalanobis distance of a vector x ∈Rp and a matrix A ≻0 is noted ∥x∥A ≜ √ x⊤A−1x. 2. Background 2.1. Problem setting To favor sparse coefﬁcients, we consider Lasso-type es- timators based on non-smooth regularization functions. Such problems consist in ﬁnding: ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (1) The Lasso (Tibshirani, 1996) is recovered, with the number of hyperparameters set to r= 1: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 , (2) while the weighted Lasso (wLasso, Zou 2006, introduced to reduce the bias of the Lasso) has r= phyperparameters and reads: ψ(β,λ) = 1 2n∥y−Xβ∥2 2 + p∑ j=1 eλj|βj|. (3) Note that we adopt the hyperparameter parametrization of Pedregosa (2016), i.e., we write the regularization parame- ter as eλ. This avoids working with a positivity constraint in the optimization process and ﬁxes scaling issues in the line search. It is also coherent with the usual choice of a geometric grid for grid-search (Friedman et al., 2010). Remark 1. Other formulations could be investigated like Elastic-Net or non-convex formulation, e.g., MCP (Zhang, 2010). Our theory does not cover non-convex cases, though we illustrate that it behaves properly numerically. Handling such non-convex cases is left as a question for future work. The HO problem can be expressed as a nested bi-level op- timization problem. For a given differentiable criterion C: Rp ↦→R (e.g., hold-out loss or SURE), it reads:Implicit differentiation of Lasso-type models for hyperparameter optimization arg min λ∈Rr { L(λ) ≜ C ( ˆβ(λ) )} s.t. ˆβ(λ) ∈arg min β∈Rp ψ(β,λ) . (4) Note that SURE itself is not necessarily weakly differen- tiable w.r.t. ˆβ(λ). However a weakly differentiable approx- imation can be constructed (Ramani et al., 2008; Deledalle et al., 2014). Under the hypothesis that Problem (1) has a unique solution for every λ∈Rr, the function λ↦→ˆβ(λ) is weakly differentiable (Vaiter et al., 2013). Using the chain rule, the gradient of Lw.r.t.λthen writes: ∇λL(λ) = ˆJ⊤ (λ)∇C ( ˆβ(λ) ) . (5) Computing the weak Jacobian ˆJ(λ) of the inner problem is the main challenge, as once the hypergradient ∇λL(λ) has been computed, one can use usual gradient descent, λ(t+1) = λ(t) −ρ∇λL(λ(t)), for a step size ρ > 0. Note however that Lis usually non-convex and conver- gence towards a global minimum is not guaranteed. In this work, we propose an efﬁcient algorithm to compute ˆJ(λ) for Lasso-type problems, relying on improved forward dif- ferentiation. 2.2. Implicit differentiation (smooth case) Implicit differentiation, which can be traced back to Larsen et al. (1996), is based on the knowledge of ˆβ and requires solving a p×plinear system (Bengio, 2000, Sec. 4). Since then, it has been extensively applied in various contexts. Chapelle et al. (2002); Seeger (2008) used implicit differ- entiation to select hyperparameters of kernel-based mod- els. Kunisch and Pock (2013) applied it to image restora- tion. Pedregosa (2016) showed that each inner optimiza- tion problem could be solved only approximately, leverag- ing noisy gradients. Related to our work, Foo et al. (2008) applied implicit differentiation on a “weighted” Ridge-type estimator (i.e., a Ridge penalty with one λj per feature). Yet, all the aforementioned methods have a common draw- back : they are limited to the smooth setting, since they rely on optimality conditions for smooth optimization. They proceed as follows: if β ↦→ψ(β,λ) is a smooth convex function (for any ﬁxed λ) in Problem (1), then for all λ, the solution ˆβ(λ) satisﬁes the following ﬁxed point equation: ∇βψ ( ˆβ(λ),λ ) = 0 . (6) Then, this equation can be differentiated w.r.t.λ: ∇2 β,λψ( ˆβ(λ),λ) + ˆJ⊤ (λ)∇2 βψ( ˆβ(λ),λ) = 0. (7) Assuming that ∇2 βψ( ˆβ(λ),λ) is invertible this leads to a closed form solution for the weak Jacobian ˆJ(λ): ˆJ⊤ (λ) = −∇2 β,λψ ( ˆβ(λ),λ )( ∇2 βψ(β(λ),λ) )    p×p −1 , (8) which in practice is computed by solving a linear system. Unfortunately this approach cannot be generalized for non- smooth problems since Equation (6) no longer holds. 2.3. Implicit differentiation (non-smooth case) Related to our work Mairal et al. (2012) used implicit dif- ferentiation with respect to the dictionary ( X ∈Rn×p) on Elastic-Net models to perform dictionary learning. Regard- ing Lasso problems, the literature is quite scarce, see (Dos- sal et al., 2013; Zou et al., 2007) and (Vaiter et al., 2013; Tibshirani and Taylor, 2011) for a more generic setting encompassing weighted Lasso. General methods for gra- dient estimation of non-smooth optimization schemes ex- ist (Vaiter et al., 2017) but are not practical since they de- pend on a possibly ill-posed linear system to invert. Amos and Kolter (2017) have applied implicit differentiation on estimators based on quadratic objective function with lin- ear constraints, whereas Niculae and Blondel (2017) have used implicit differentiation on a smooth objective func- tion with simplex constraints. However none of these ap- proaches leverages the sparsity of Lasso-type estimators. 3. Hypergradients for Lasso-type problems To tackle hyperparameter optimization of non-smooth Lasso-type problems, we propose in this section an efﬁcient algorithm for hypergradient estimation. Our algorithm re- lies on implicit differentiation, thus enjoying low-memory cost, yet does not require to naively solve a (potentially ill-conditioned) linear system of equations. In the sequel, we assume access to a (weighted) Lasso solver, such as ISTA (Daubechies et al., 2004) or Block Coordinate De- scent (BCD, Tseng and Yun 2009, see also Algorithm 5). 3.1. Implicit differentiation Our starting point is the key observation that Lasso-type solvers induce a ﬁxed point iteration that we can leverage to compute a Jacobian. Indeed, proximal BCD algorithms (Tseng and Yun, 2009), consist in a local gradient step com- posed with a soft-thresholding step (ST),e.g., for the Lasso, for j ∈1,...,p : βj ←ST ( βj −X⊤ :,j(Xβ −y) ∥X:,j∥2 , neλ ∥X:,j∥2 ) (9) where ST(t,τ) = sign(t)·(|t|−τ)+ for any t∈R and τ ≥ 0 (extended for vectors component-wise). The solution ofImplicit differentiation of Lasso-type models for hyperparameter optimization the optimization problem satisﬁes, for anyα> 0, the ﬁxed- point equation (Combettes and Wajs, 2005, Prop. 3.1), for j ∈1,...,p : ˆβ(λ) j = ST ( ˆβ(λ) j −1 αX⊤ j,:(Xˆβ(λ) −y),neλ α ) . (10) The former can be differentiated w.r.t. λ, see Lemma A.1 in Appendix, leading to a closed form solution for the Ja- cobian J(λ) of the Lasso and the weighted Lasso. Proposition 1(Adapting Vaiter et al. 2013, Thm. 1) . Let ˆSbe the support of the vector ˆβ(λ). Suppose that X⊤ ˆSXˆS ≻0 , then a weak Jacobian ˆJ = ˆJ(λ) of the Lasso writes: ˆJˆS = −neλ( X⊤ ˆSXˆS )−1 sign ˆβˆS, (11) ˆJˆSc = 0 , (12) and for the weighted Lasso: ˆJˆS,ˆS = − ( X⊤ ˆSXˆS )−1 diag ( neλˆS ⊙sign ˆβˆS ) (13) ˆJj1,j2 = 0 if j1 /∈ˆSor if j2 /∈ˆS . (14) The proof of Proposition 1 can be found in Appendix A.1. Note that the positivity condition in Proposition 1 is satis- ﬁed if the (weighted) Lasso has a unique solution. More- over, even for multiple solutions cases, there exists at least one satisfying the positivity condition (Vaiter et al., 2013). Proposition 1 shows that the Jacobian of the weighted Lasso ˆJ(λ) ∈ Rp×p is row and column sparse. This is key for algorithmic efﬁciency. Indeed, a priori, one has to store a possibly dense p×p matrix, which is prohibitive when pis large. Proposition 1 leads to a simple algorithm (see Algorithm 1) to compute the Jacobian in a cheap way, as it only requires storing and inverting an ˆs×ˆs matrix. Even if the linear system to solve is of size ˆs×ˆs, instead of p×pfor smooth objective function, the system to invert can be ill-conditioned, especially when a large support size ˆsis encountered. This leads to numerical instabilities and slows down the resolution (see an illustration in Figure 2). Forward (Algorithm 3 in Appendix) and backward (Algo- rithm 4 in Appendix) iterative differentiation, which do not require solving linear systems, can overcome these issues. 3.2. Link with iterative differentiation Iterative differentiation in the ﬁeld of hyperparameter set- ting can be traced back to Domke (2012) who derived a backward differentiation algorithm for gradient descent, heavy ball and L-BFGS algorithms applied to smooth loss functions. Agrawal et al. (2019) generalized it to a spe- ciﬁc subset of convex programs. Maclaurin et al. (2015) derived a backward differentiation for stochastic gradient Algorithm 1IMPLICIT DIFFERENTIATION input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. and Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. ˆJ = 0p ˆJˆS = −neλ(X⊤ ˆSXˆS)−1 sign ˆβˆS if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. ˆJ= 0p×p ˆJˆS,ˆS = −(X⊤ ˆSXˆS)−1 diag(neλˆS ⊙sign ˆβˆS) return ˆβ, ˆJ descent. On the other hand Deledalle et al. (2014) used forward differentiation of (accelerated) proximal gradient descent for hyperparameter optimization with non-smooth penalties. Franceschi et al. (2017) proposed a benchmark of forward mode versus backward mode, varying the num- ber of hyperparameters to learn. Frecon et al. (2018) cast the problem of inferring the groups in a group-Lasso model as a bi-level optimization problem and solved it using back- ward differentiation. Forward differentiation consists in differentiating each step of the algorithm (w.r.t.λin our case). For the Lasso solved with BCD it amounts differentiating Equation (9), and leads to the following recursive equation for the Jacobian, for j ∈1,...p , with zj = βj −X⊤ :,j(Xβ −y)/∥X:,j∥2: Jj ←∂1 ST ( zj, neλ ∥X:,j∥2 )( Jj − 1 ∥X:,j∥2 X⊤ :,jXJ ) + ∂2 ST ( zj, neλ ∥X:,j∥2 ) neλ ∥X:,j∥2 , (15) see Algorithm 3 (in Appendix) for full details. Our proposed algorithm uses the fact that after a ﬁ- nite number of epochs ∂1 ST(zj,neλ/∥X:,j∥2) and ∂2 ST(zj,neλ/∥X:,j∥2) are constant (they no no longer depends on the current β). Indeed, the sign of ˆβ is iden- tiﬁed after a ﬁnite number of iterations thus the partial derivatives are constant. It is then possible to decouple the computation of the Jacobian by only solving Problem (1) in a ﬁrst step and then apply the forward differentiation recur- sion steps, see Algorithm 2. This can be seen as the forward counterpart in a non-smooth case of the recent paper Lor- raine et al. (2019). An additional beneﬁt of such updates is that they can be restricted to the (current) support, which leads to faster Jacobian computation. We now show that the Jacobian computed using forward differentiation and our method, Algorithm 2, converges to- ward the true Jacobian.Implicit differentiation of Lasso-type models for hyperparameter optimization Proposition 2. Assuming the Lasso solution (Prob- lem (2)) (or weighted Lasso Problem (3)) is unique, then Algorithms 2 and 3 converge toward the Jaco- bian ˆJ deﬁned in Proposition 1. Algorithm 3 com- putes the Jacobian along with the regression coefﬁ- cients, once the support has been identiﬁed, the Jaco- bian converges linearly. Algorithm 2 computes ﬁrst the coefﬁcients ˆβ and then the Jacobian ˆJ, provided that the support has been identiﬁed in the ﬁrst step, the convergence is linear in the second, with the same rate as Algorithm 3: ∥J(k+1) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤Ck∥J(k) ˆS −ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 where C = ∥A(jˆs) ...A (j1)∥2 <1, j1,...,j ˆs are the indices of the support of ˆβin increasing order and A(js) = Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 :,js ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 js,: ∥X:,js∥ ∈Rˆs×ˆs. Proof of Proposition 2 can be found in Appendix A.2 and A.3. Remark 3. Uniqueness. As proved in Tibshirani (2013, Lem. 3 and 4) the set of (pathological) lambdas where the Lasso solution is not unique is typically empty. More- over if the Lasso solution is not unique, there could be a non-continuous solution path λ↦→ˆβ(λ), leaving only non- gradient based methods available. Even if Proposition 2 does not provide theoretical guarantees in such a patholog- ical setting, one can still apply Algorithms 2 and 3, see Appendix E.1 for experiments in this settings. Remark 4. Rate for the backward differentiation. The backward and forward differentiation compute the same quantity: ∇λL(λ), but the backward differentiation di- rectly computes the product given in Equation (5) leading to the gradient ofL(λ). Proposition 2 provides rates for the convergence of the Jacobian Jwhich leads to rates for the gradient i.e., for the backward algorithm as well. As an illustration, Figure 1 shows the times of computa- tion of a single gradient ∇λL(λ) and the distance to “op- timum” of this gradient as a function of the number of it- erations in the inner optimization problem for the forward iterative differentiation (Algorithm 3), the backward iter- ative differentiation (Algorithm 4), and the proposed algo- rithm (Algorithm 2). The backward iterative differentiation is several order of magnitude slower than the forward and our implicit forward method. Moreover, once the support has been identiﬁed (after 20 iterations) the proposed im- plicit forward method converges faster than other methods. Note also that in Propositions 1 and 2 the Jacobian for the Imp. F. Iterdiﬀ. (ours)F. Iterdiﬀ.B. Iterdiﬀ. 20 40 60 Number of iterations 10−1 100 101 Times (s) 20 40 60 Number of iterations 10−7 10−5 Objective minus optimum Figure 1.Time to compute a single gradient(Synthetic data, Lasso, n,p = 1000,2000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gra- dient ∇λL(λ)(right) for the Lasso estimator. The “optimum” is here the gradient given by implicit differentiation (Algorithm 1). Lasso only depends on the support (i.e., the indices of the non-zero coefﬁcients) of the regression coefﬁcients ˆβ(λ). In other words, once the support of ˆβ(λ) is correctly identi- ﬁed, even if the value of the non-zeros coefﬁcients are not correctly estimated, the Jacobian is exact, see Sun et al. (2019) for support identiﬁcation guarantees. 4. Experiments Our Python code is released as an open source package: https://github.com/QB3/sparse-ho. All the experiments are written in Python using Numba (Lam et al., 2015) for the critical parts such as the BCD loop. We com- pare our gradient computation technique against other com- petitors (see the competitors section) on the HO problem (Problem (4)). Solving the inner optimization problem.Note that our proposed method, implicit forward differentiation, has the appealing property that it can be used with any solver. For instance for the Lasso one can combine the proposed al- gorithm with state of the art solver such as Massias et al. (2018) which would be tedious to combine with iterative differentiation methods. However for the comparison to be fair, for all methods we have used the same vanilla BCD algorithm (recalled in Algorithm 5). We stop the Lasso- types solver when f(β(k+1))−f(β(k)) f(0) <ϵtol ,where f is the cost function of the Lasso or wLasso and ϵtol a given toler- ance. The tolerance is ﬁxed at ϵtol = 10−5 for all methods throughout the different benchmarks. Line search. For each hypergradient-based method, the gradient step is combined with a line-search strategy fol- lowing the work of Pedregosa (2016)1. Initialization. Since the function to optimize Lis not con- 1see https://github.com/fabianp/hoag for detailsImplicit differentiation of Lasso-type models for hyperparameter optimization Table 1.Summary of cost in time and space for each method Mode Computed Space Time Space Time quantity (Lasso) (Lasso) (wLasso) (wLasso) F. Iterdiff. J O(p) O(2npniter) O(p2) O(np2niter) B. Iterdiff. J⊤v O(2pniter) O(npniter + np2niter) O(p2niter) O(npniter + np2niter) Implicit J⊤v O(p) O(npniter + ˆs3) O(p+ ˆs2) O(npniter + ˆs3) Imp. F. Iterdiff. J O(p) O(npniter + nˆsniter_jac) O(p+ ˆs2) O(npniter + nˆs2nit_jac) Algorithm 2IMP. F. I TERDIFF . (proposed) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter,niter_jac ∈N init : J= 0 // sequentially compute coef. & Jacobian if Lasso then Get ˆβ = Lasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS // trick for cheap updates if wLasso then Get ˆβ = wLasso(X,y,λ,n iter) and its support ˆS. dr= −X:,ˆSJˆS,ˆS for k= 0,...,n iter_jac −1 do for j ∈ˆSdo if Lasso then Jold = Jj // trick for cheap update // diff. Equation (9) w.r.t. λ Jj += X⊤ :,jdr ∥X:,j∥2 − neλ ∥X:,j∥2 sign ˆβj // O(n) dr−= X:,j(Jj,: −Jold) // O(n) if wLasso then Jold = Jj,: // trick for cheap update // diff. Equation (9) w.r.t. λ Jj,ˆS += 1 ∥X:,j∥2 X⊤ :,jdr // O(n×ˆs) Jj,j −= neλj ∥X:,j∥2 sign ˆβj // O(1) dr−= X:,j ⊗(Jj,: −Jold) // O(n×ˆs) return ˆβ,J vex, initialization plays a crucial role in the ﬁnal solution as well as the convergence of the algorithm. For instance, initializing λ = λinit in a ﬂat zone of L(λ) could lead to slow convergence. In the numerical experiments, the Lasso is initialized with λinit = λmax −log(10), where λmax is the smallest λsuch that 0 is a solution of Problem (2). Competitors. In this section we compare the empirical performance of implicit forward differentiation algorithm to different competitors. Competitors are divided in two categories. Firstly, the ones relying on hyperparameter gra- dient: • Imp. F. Iterdiff. : implicit forward differentiation (proposed) described in Algorithm 2. • Implicit: implicit differentiation, which requires solv- ing a ˆs×ˆslinear system as described in Algorithm 1. • F. Iterdiff.: forward differentiation (Deledalle et al., 2014; Franceschi et al., 2017) which jointly computes the regression coefﬁcients ˆβas well as the Jacobian ˆJ as shown in Algorithm 3. Secondly, the ones not based on hyperparameter gradient: • Grid-search: as recommended by Friedman et al. (2010), we use 100 values on a uniformly-spaced grid from λmax to λmax −4 log(10). • Random-search: we sample uniformly at random 100 values taken on the same interval as for the Grid-search [λmax −4 log(10);λmax], as suggested by Bergstra et al. (2013). • Lattice Hyp.: lattice hypercube sampling (Bousquet et al., 2017), combines the idea of grid-search and random-search. We used the sampling scheme of Bouhlel et al. (2019) and their code 2 to sample the points to evaluate the function on. • Bayesian: sequential model based optimization (SMBO) using a Gaussian process to model the objec- tive function. We used the implementation of Bergstra et al. (2013).3 The constraints space for the hyperpa- rameter search was set in[λmax −4 log(10);λmax], and the expected improvement (EI) was used as aquisition function. The cost and the quantity computed by each algorithm can be found in Table 1. The backward differentiation (Domke, 2012) is not included in the benchmark in Figure 2 since it was several orders of magnitude slower than the other techniques (see Figure 1). This is due to the high cost of the BCD algorithm in backward mode, see Table 1. 4.1. Application to held-out loss When using the held-out loss, each dataset(X,y) is split in 3 equal parts: the training set (Xtrain,ytrain), the validation set (Xval,yval) and the test set (Xtest,ytest). 2https://github.com/SMTorg/smt 3https://github.com/hyperopt/hyperoptImplicit differentiation of Lasso-type models for hyperparameter optimization (Lasso, held-out criterion). For the Lasso and the held-out loss, the bilevel optimization Problem (4) reads: arg min λ∈R ∥yval −Xval ˆβ(λ)∥2 (16) s.t. ˆβ(λ) ∈arg min β∈Rp 1 2n∥ytrain −Xtrainβ∥2 2 + eλ∥β∥1 . Figure 2 (top) shows on 3 datasets (see Appendix D for dataset details) the distance to the “optimum” of ∥yval − Xval ˆβ(λ)∥2 as a function of time. Here the goal is to ﬁnd λ solution of Problem (16). The “optimum” is chosen as the minimum of ∥yval −Xval ˆβ(λ)∥2 among all the meth- ods. Figure 2 (bottom) shows the loss ∥ytest −Xtest ˆβ(λ)∥2 on the test set (independent from the training set and the validation set). This illustrates how well the estimator gen- eralizes. Firstly, it can be seen that on all datasets the pro- posed implicit forward differentiation outperforms forward differentiation which illustrates Proposition 2 and corrobo- rates the cost of each algorithm in Table 1. Secondly, it can be seen that on the 20news dataset (Figure 2, top) the im- plicit differentiation (Algorithm 1) convergence is slower than implicit forward differentiation, forward differentia- tion, and even slower than the grid-search. In this case, this is due to the very slow convergence of the conjugate gra- dient algorithm (Nocedal and Wright, 2006) when solving the ill-conditioned linear system in Algorithm 1. (MCP , held-out criterion). We also applied our algorithm on an estimator based on a non-convex penalty: the MCP (Zhang, 2010) with 2 hyperparameters. Since the penalty is non-convex the estimator may not be continuous w.r.t.hy- perparameters and the theory developed above does not hold. However experimentally implicit forward differen- tiation outperforms forward differentiation for the HO, see Appendix C for full details. 4.2. Application to another criterion: SURE Evaluating models on held-out data makes sense if the de- sign is formed from random samples as it is often consid- ered in supervised learning. However, this assumption does not hold for certain kinds of applications in signal or image processing. For these applications, the held-out loss cannot be used as the criterion for optimizing the hyperparame- ters of a given model. In this case, one may use a proxy of the prediction risk, like the Stein Unbiased Risk Estimation (SURE, Stein (1981)). The SURE is an unbiased estimator of the prediction risk under weak differentiable conditions. The drawback of this criterion is that it requires the knowl- edge of the variance of the noise. The SURE is deﬁned as follows: SURE(λ) =∥y−Xˆβ(λ)∥2−nσ2+2σ2dof( ˆβ(λ)) , where the degrees of freedom (dof Efron 1986) is deﬁned as dof( ˆβ(λ)) =∑n i=1 cov(yi,(Xˆβ(λ))i)/σ2 .The dof can be seen a measure of the complexity of the model, for in- stance for the Lasso dof ( ˆβ(λ)) = ˆs, see Zou et al. (2007). The SURE can thus be seen as a criterion trading data- ﬁdelity against model complexity. However, the dof is not differentiable (not even continuous in the Lasso case), yet it is possible to construct a weakly differentiable ap- proximation of it based on Finite Differences Monte-Carlo (see Deledalle et al. 2014 for full details), with ϵ >0 and δ∼N(0,Idn): dofFDMC(y,λ,δ,ϵ ) =1 ϵ⟨Xˆβ(λ)(y+ ϵδ) −Xˆβ(λ)(y),δ⟩. We use this smooth approximation in the bi-level optimiza- tion problem to ﬁnd the best hyperparameter. The bi-level optimization problem then reads: arg min λ∈R ∥y−Xˆβ(λ)∥2 + 2σ2dofFDMC(y,λ,δ,ϵ ) (17) s.t. ˆβ(λ)(y) ∈arg min β∈Rp 1 2n∥y−Xβ∥2 2 + eλ∥β∥1 ˆβ(λ)(y+ ϵδ) ∈arg min β∈Rp 1 2n∥y+ ϵδ−Xβ∥2 2 + eλ∥β∥1 Note that solving this problem requires the computation of two (instead of one for the held-out loss) Jacobians w.r.t.λ of the solution ˆβ(λ) at the points yand y+ ϵδ. (Lasso, SURE criterion). To investigate the estimation per- formance of the implicit forward differentiation in com- parison to the competitors described above, we used as metric the (normalized) Mean Squared Error (MSE) de- ﬁned as MSE ≜ ∥ˆβ−β∗∥2/∥β∗∥2. The entries of the design matrix X ∈Rn×p are i.i.d. random Gaussian vari- ables N(0,1). The number of rows is ﬁxed to n = 100. Then, we generated β∗with 5 non-zero coefﬁcients equals to 1. The vector y was computed by adding to Xβ∗addi- tive Gaussian noise controlled by the Signal-to-Noise Ra- tio: SNR ≜ ∥Xβ∗∥/∥y−Xβ∗∥(here SNR = 3). Fol- lowing Deledalle et al. (2014), we set ϵ = 2σ/n0.3. We varied the number of featurespbetween 200 and 10,000 on a linear grid of size 10. For a ﬁxed number of features, we performed 50 repetitions and each point of the curves rep- resents the mean of these repetitions. Comparing efﬁciency in time between methods is difﬁcult since they are not di- rectly comparable. Indeed, grid-search and random-search discretize the HO space whereas others methods work in the continuous space which is already an advantage. How- ever, to be able to compare the hypergradient methods and possibly compare them to the others, we computed the to- tal amount of time for a method to return its optimal value of λ. In order to have a fair comparison, we compared 50 evaluations of the line-search for each hypergradient meth- ods, 50 evaluations of the Bayesian methods and ﬁnally 50 evaluations on ﬁxed or random grid. We are aware that the cost of each of these evaluations is not the same but it al- lows to see that our method stays competitive in time with optimizing one parameter. Moreover we will also see that our method scales better with a large number of hyperpa- rameters to optimize.Implicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search Lattice Hyp. 0.0 0.5 1.0 10−5 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p = 19, 959) 0 5 10 15 10−3 10−2 10−1 100 101 102 20news (p = 130, 107) 0 100 200 300 10−4 10−3 10−2 10−1 100 101 ﬁnance (p = 1, 668, 737) 0.0 0.5 1.0 Time (s) 10−1 100 Loss on test set 0 5 10 15 Time (s) 101 102 0 100 200 300 Time (s) 10−1 100 101 Figure 2.Computation time for the HO of the Lasso on real data.Distance to “optimum” (top) and performance (bottom) on the test set for the Lasso for 3 different datasets: rcv1, 20news and ﬁnance. Imp. F. Iterdiﬀ. (ours) Implicit F. Iterdiﬀ. Grid-search Bayesian Random-search 200 2500 5000 7500 10000 Number of features (p) 0.000 0.001 0.002 0.003 0.004 relative MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 Time (s) Figure 3.Lasso: estimation performance. Estimation relative Mean Squared Error (left) and running time (right) as a function of the number of features for the Lasso model. Figure 3 shows the inﬂuence of the number of features on the relative MSE (ie. MSE of a method minus the MSE of our implicit forward method) and the computation time. First, MSE of all gradient based methods is lower than the other methods which means that ˆβ(λ) leads to a better es- timation when λ is chosen via the gradient based meth- ods. This illustrates that continuous optimization for hy- perparameter selection leads to better estimation perfor- mance than discrete or Bayesian optimization. Yet, the running time of our proposed method is the lowest of all hypergradient-based strategies and competes with the grid- search and the random-search. (Weighted Lasso vs Lasso, SURE criterion). As our method leverages the sparsity of the solution, it can be used for HO with a large number of hyperparameters, contrary to classi- cal forward differentiation. The weighted Lasso (wLasso, Zou 2006) has p hyperparameters and was introduced to reduce the bias of the Lasso. However setting the phyper- parameters is impossible with grid-search. Figure 4 shows the estimation MSE and the running time of the different methods to obtain the hyperparameter val- ues as a function of the number of features used to simu- late the data. The simulation setting is here the same as for the Lasso problems investigated in Figure 3 ( n = 100, SNR = 3). We compared the classical Lasso estimator and the weighted Lasso estimator where the regularization hy- perparameter was chosen using implicit forward differenti- ation and the forward iterative differentiation as described in Algorithm 3. Problem (4) is not convex for the weighted Lasso and a descent algorithm like ours can be trapped in local minima, crucially depending on the starting point λinit. To alleviate this problem, we introduced a regular- ized version of Problem (4): arg min λ∈R C ( ˆβ(λ) ) + γ p∑ j λ2 j s.t. ˆβ(λ) ∈arg min β∈Rp ≜ ψ(β,λ) . (18) The solution obtained by solving Equation (18) is then used as the initialization λ(0) for our algorithm. In this experiment the regularization term is constant γ =Implicit differentiation of Lasso-type models for hyperparameter optimization Lasso F. Iterdiﬀ. Lasso Implicit Lasso Backward Lasso Imp. F. Iterdiﬀ. (ours) wLasso F. Iterdiﬀ. wLasso Implicit wLasso Backward wLasso Imp. F. Iterdiﬀ. (ours) 200 2500 5000 7500 10000 Number of features (p) 0.00 0.05 0.10 0.15 MSE 200 2500 5000 7500 10000 Number of features (p) 10−1 100 101 102 103 Time (s) Figure 4.Lasso vs wLasso.Estimation Mean Squared Error (left) and running (right) of competitors as a function of the number of features for the weighted Lasso and Lasso models. C(β(λmax))/10. We see in Figure 4 that the weighted Lasso gives a lower MSE than the Lasso and allows for a better recovery of β∗. This experiment shows that the amount of time needed to obtain the vector of hyperparameters of the weighted Lasso via our algorithm is in the same range as for obtaining the unique hyperparameter of the Lasso prob- lem. It also shows that our proposed method is much faster than the naive way of computing the Jacobian using for- ward or backward iterative differentiation. The implicit dif- ferentiation method stays competitive for the wLasso due to the small support of the solution and hence a small ma- trix to inverse. A maximum running time threshold was used for this experiment checking the running time at each line-search iteration, explaining why the forward differen- tiation and backward differentiation of the wLasso does not explode in time on Figure 4. Conclusion In this work we studied the performance of several methods to select hyperparameters of Lasso-type estimators show- ing results for the Lasso and the weighted Lasso, which have respectively one or phyperparameters. We exploited the sparsity of the solutions and the speciﬁc structure of the iterates of forward differentiation, leading to our im- plicit forward differentiation algorithm that computes efﬁ- ciently the full Jacobian of these estimatorsw.r.t.the hyper- parameters. This allowed us to select them through a stan- dard gradient descent and have an approach that scales to a high number of hyperparameters. Importantly, contrary to a classical implicit differentiation approach, the proposed algorithm does not require solving a linear system. Fi- nally, thanks to its two steps nature, it is possible to lever- age in the ﬁrst step the availability of state-of-the-art Lasso solvers that make use of techniques such as active sets or screening rules. Such algorithms, that involve calls to in- ner solvers run on subsets of features, are discontinuous w.r.t.hyperparameters which would signiﬁcantly challenge a single step approach based on automatic differentiation. Acknowledgments This work was funded by ERC Start- ing Grant SLAB ERC-StG-676943 and ANR GraVa ANR- 18-CE40-0005.Implicit differentiation of Lasso-type models for hyperparameter optimization References A. Agrawal, B. Amos, S. Barratt, S. Boyd, S. Diamond, and J. Z. Kolter. Differentiable convex optimization layers. In Advances in neural information processing systems , pages 9558–9570, 2019. B. Amos and J. Z. Kolter. Optnet: Differentiable optimiza- tion as a layer in neural networks. In ICML, volume 70, pages 136–145, 2017. A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18(153):1–43, 2018. Y . Bengio. Gradient-based optimization of hyperparame- ters. Neural computation, 12(8):1889–1900, 2000. J. Bergstra and Y . Bengio. Random search for hyper- parameter optimization. J. Mach. Learn. Res., 2012. J. Bergstra, D. Yamins, and D. D. Cox. Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science conference, pages 13–20, 2013. M. Borgerding, P. Schniter, and S. Rangan. Amp-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing , 65(16):4293–4308, 2017. M. A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Mor- lier, and J. R. R. A. Martins. A python surrogate model- ing framework with derivatives. Advances in Engineer- ing Software, page 102662, 2019. ISSN 0965-9978. doi: https://doi.org/10.1016/j.advengsoft.2019.03.005. O. Bousquet, S. Gelly, K. Kurach, O. Teytaud, and D. Vin- cent. Critical hyper-parameters: No random, no cry. arXiv preprint arXiv:1706.03200, 2017. P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. , 5(1):232, 2011. E. Brochu, V . M. Cora, and N. De Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical re- inforcement learning. 2010. O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector ma- chines. Machine learning, 46(1-3):131–159, 2002. P. L. Combettes and V . R. Wajs. Signal recovery by proxi- mal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Comm. Pure Appl. Math., 57(11): 1413–1457, 2004. C.-A. Deledalle, S. Vaiter, J. Fadili, and G. Peyré. Stein Unbiased GrAdient estimator of the Risk (SUGAR) for multiple parameter selection. SIAM J. Imaging Sci. , 7 (4):2448–2487, 2014. J. Domke. Generic methods for optimization-based model- ing. In AISTATS, volume 22, pages 318–326, 2012. C. Dossal, M. Kachour, M.J. Fadili, G. Peyré, and C. Ches- neau. The degrees of freedom of the lasso for general design matrix. Statistica Sinica, 23(2):809–828, 2013. B. Efron. How biased is the apparent error rate of a pre- diction rule? J. Amer. Statist. Assoc., 81(394):461–470, 1986. L. C. Evans and R. F. Gariepy. Measure theory and ﬁne properties of functions. CRC Press, 1992. C. S. Foo, C. B. Do, and A. Y . Ng. Efﬁcient multiple hyper- parameter learning for log-linear models. InAdvances in neural information processing systems, pages 377–384, 2008. L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. For- ward and reverse gradient-based hyperparameter opti- mization. In ICML, pages 1165–1173, 2017. J. Frecon, S. Salzo, and M. Pontil. Bilevel learning of the group lasso structure. InAdvances in Neural Information Processing Systems, pages 8301–8311, 2018. J. Friedman, T. J. Hastie, and R. Tibshirani. Regulariza- tion paths for generalized linear models via coordinate descent. J. Stat. Softw., 33(1):1–22, 2010. K. Gregor and Y . LeCun. Learning fast approximations of sparse coding. In ICML, pages 399–406, 2010. E. Hale, W. Yin, and Y . Zhang. Fixed-point continuation for ℓ1-minimization: Methodology and convergence. SIAM J. Optim., 19(3):1107–1130, 2008. K. Kunisch and T. Pock. A bilevel optimization approach for parameter learning in variational models. SIAM J. Imaging Sci., 6(2):938–983, 2013. S. K. Lam, A. Pitrou, and S. Seibert. Numba: A LLVM- based Python JIT Compiler. In Proceedings of the Sec- ond Workshop on the LLVM Compiler Infrastructure in HPC, pages 1–6. ACM, 2015.Implicit differentiation of Lasso-type models for hyperparameter optimization J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural networks: the optimal use of a validation set. In Neural Networks for Signal Process- ing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop, 1996. J. Larsen, C. Svarer, L. N. Andersen, and L. K. Hansen. Adaptive regularization in neural network modeling. In Neural Networks: Tricks of the Trade - Second Edition , pages 111–130. Springer, 2012. J. Liu, X. Chen, Z. Wang, and W. Yin. Alista: Analytic weights are as good as learned weights in lista. In Inter- national Conference on Learning Representations, 2018. W. Liu, Y . Yang, et al. Parametric or nonparametric? a parametricness index for model selection. Ann. Statist., 39(4):2074–2102, 2011. J. Lorraine, P. Vicol, and D. Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. arXiv preprint arXiv:1911.02590, 2019. D. Maclaurin, D. Duvenaud, and Ryan Adams. Gradient- based hyperparameter optimization through reversible learning. In ICML, volume 37, pages 2113–2122, 2015. J. Mairal, F. Bach, and J. Ponce. Task-driven dictionary learning. IEEE Trans. Pattern Anal. Mach. Intell., 34(4): 791–804, 2012. M. Massias, A. Gramfort, and J. Salmon. Celer: a Fast Solver for the Lasso with Dual Extrapolation. In ICML, volume 80, pages 3315–3324, 2018. M. Massias, S. Vaiter, A. Gramfort, and J. Salmon. Dual extrapolation for sparse generalized linear models.arXiv preprint arXiv:1907.05830, 2019. V . Niculae and M. Blondel. A regularized framework for sparse and structured neural attention. In Advances in neural information processing systems , pages 3338– 3348, 2017. J. Nocedal and S. J. Wright. Numerical optimization . Springer Series in Operations Research and Financial Engineering. Springer, New York, second edition, 2006. F. Pedregosa. Hyperparameter optimization with approxi- mate gradient. In ICML, 2016. S. Ramani, T. Blu, and M. Unser. Monte-Carlo SURE: a black-box optimization of regularization parameters for general denoising algorithms. IEEE Trans. Image Pro- cess., 17(9):1540–1554, 2008. M. W. Seeger. Cross-validation optimization for large scale structured classiﬁcation kernel methods. J. Mach. Learn. Res., 9:1147–1178, 2008. J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems , 2012. E. Soubies, L. Blanc-Féraud, and G. Aubert. A uniﬁed view of exact continuous penalties for ℓ2-ℓ0 minimiza- tion. SIAM J. Optim., 27(3):2034–2060, 2017. C. M. Stein. Estimation of the mean of a multivariate nor- mal distribution. Ann. Statist., 9(6):1135–1151, 1981. L. R. A. Stone and J.C. Ramer. Estimating W AIS IQ from Shipley Scale scores: Another cross-validation. Journal of clinical psychology, 21(3):297–297, 1965. Y . Sun, H. Jeong, J. Nutini, and M. Schmidt. Are we there yet? manifold identiﬁcation of gradient-related proxi- mal methods. In AISTATS, volume 89, pages 1110–1119, 2019. R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B Stat. Methodol., 58(1):267– 288, 1996. R. J. Tibshirani. The lasso problem and uniqueness. Elec- tron. J. Stat., 7:1456–1490, 2013. R. J. Tibshirani and J. Taylor. The solution path of the generalized lasso. Ann. Statist., 39(3):1335–1371, 2011. P. Tseng and S. Yun. Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. J. Optim. Theory Appl., 140(3):513, 2009. S. Vaiter, C.-A. Deledalle, G. Peyré, C. Dossal, and J. Fadili. Local behavior of sparse analysis regulariza- tion: Applications to risk estimation. Appl. Comput. Harmon. Anal., 35(3):433–451, 2013. S. Vaiter, C.-A. Deledalle, G. Peyré, J. M. Fadili, and C. Dossal. The degrees of freedom of partly smooth reg- ularizers. Ann. Inst. Stat. Math., 69(4):791–832, 2017. K. Wu, Y . Guo, Z. Li, and C. Zhang. Sparse coding with gated learned ista. In International Conference on Learning Representations, 2019. B. Xin, Y . Wang, W. Gao, D. Wipf, and B. Wang. Maximal sparsity with deep networks? In Advances in Neural In- formation Processing Systems, pages 4340–4348, 2016. M. Yuan and Y . Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006. C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):894–942, 2010.Implicit differentiation of Lasso-type models for hyperparameter optimization H. Zou. The adaptive lasso and its oracle properties. J. Amer. Statist. Assoc., 101(476):1418–1429, 2006. H. Zou and T. J. Hastie. Regularization and variable se- lection via the elastic net. J. R. Stat. Soc. Ser. B Stat. Methodol., 67(2):301–320, 2005. H. Zou, T. J. Hastie, and R. Tibshirani. On the “degrees of freedom” of the lasso. Ann. Statist., 35(5):2173–2192, 2007.Implicit differentiation of Lasso-type models for hyperparameter optimization A. Proofs A.1. Proof of Proposition 1 We start by a lemma on the weak derivative of the soft-thresholding. Lemma A.1. The soft-thresholding ST :R×R+ ↦→R deﬁned by ST(t,τ) = sign(t) ·(|t|−τ)+ is weakly differentiable with weak derivatives ∂1 ST(t,τ) =1{|t|>τ} , (19) and ∂2 ST(t,τ) =−sign(t) ·1{|t|>τ} , (20) where 1{|t|>τ}= { 1, if |t|>τ, 0, otherwise. (21) Proof. See (Deledalle et al., 2014, Proposition 1) Proof. (Proposition 1, Lasso ISTA) The soft-thresholding is differentiable almost everywhere (a.e.), thus Equation (10) can be differentiated a.e. thanks to the previous lemma, and for any α> 0 ˆJ=   1{|ˆβ1|>0} ... 1{|ˆβp|>0}  ⊙ ( Idp−1 αX⊤X ) ˆJ− neλ α   sign( ˆβ1)1{|ˆβ1|>0} ... sign( ˆβp)1{|ˆβp|>0}   . Inspecting coordinates inside and outside the support of ˆβleads to: { ˆJˆSc = 0 ˆJˆS = ˆJˆS −1 αX⊤ :,ˆSX:,ˆS ˆJˆS −neλ α sign ˆβˆS . (22) Rearranging the term of Equation (22) it yields: X⊤ :,ˆSX:,ˆS ˆJˆS = −neλsign ˆβˆS (23) ˆJˆS = −neλ ( X⊤ :,ˆSX:,ˆS )−1 sign ˆβˆS . (24) (Proposition 1, Lasso BCD) The ﬁxed point equations for the BCD case is ˆβj = ST ( ˆβj − 1 ∥X:j∥2 2 X⊤ :j(Xˆβj −y), neλ ∥X:j∥2 2 ) . (25) As before we can differentiate this ﬁxed point equation Equation (25) ˆJj = 1{|ˆβj|>τ}· ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) − neλ ∥X:j∥2 2 sign (ˆβj)1{|ˆβj|>τ} , (26) leading to the same result.Implicit differentiation of Lasso-type models for hyperparameter optimization A.2. Proof of Proposition 2 in the ISTA case Proof. (Lasso case, ISTA) In Algorithm 3, β(k) follows ISTA steps, thus (β(k))l∈N converges toward the solution of the Lasso ˆβ. Let ˆS be the support of the Lasso estimator ˆβ, and ν( ˆS) > 0 the smallest eigenvalue of X⊤ :,ˆSX:,ˆS. Under uniqueness assumption proximal gradient descent ( a.k.a. ISTA) achieves sign identiﬁcation (Hale et al., 2008), i.e., there exists k0 ∈N such that for all k≥k0 −1: sign β(k+1) = signˆβ . (27) Recalling the update of the Jacobian Jfor the Lasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 sign β(k+1) , it is clear that J(k) is sparse with the sparsity pattern β(k) for all k≥k0. Thus we have that for all k≥k0: J(k+1) ˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSXJ(k) − neλ ∥X∥2 2 sign ˆβˆS = J(k) ˆS − 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆSJ(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) J(k) ˆS − neλ ∥X∥2 2 sign ˆβˆS. (28) One can remark that ˆJdeﬁned in Equation (11), satisﬁes the following: ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS ) ˆJˆS − neλ ∥X∥2 2 sign ˆβˆS . (29) Combining Equations (28) and (29) and denoting ν( ˆS) >0 the smallest eigenvalue of X⊤ ˆSXˆS, we have for all k≥k0: J(k+1) ˆS − ˆJˆS = ( IdˆS− 1 ∥X∥2 2 X⊤ :,ˆSX:,ˆS )( J(k) ˆS − ˆJˆS ) ∥J(k+1) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 ) ∥J(k) ˆS − ˆJˆS∥2 ∥J(k) ˆS − ˆJˆS∥2 ≤ ( 1 − ν( ˆS) ∥X∥2 2 )k−k0 ∥J(k0) ˆS − ˆJˆS∥2 . Thus the sequence of Jacobian ( J(k)) k∈N converges linearly to ˆJonce the support is identiﬁed. Proof. (wLasso case, ISTA) Recalling the update of the Jacobian J ∈Rp×p for the wLasso solved with ISTA is the following: J(k+1) = ⏐⏐⏐sign β(k+1) ⏐⏐⏐⊙ ( Id − 1 ∥X∥2 2 X⊤X ) J(k) − neλ ∥X∥2 2 diag ( sign β(k+1) ) , (30) The proof follows exactly the same steps as the ISTA Lasso case to show convergence in spectral norm of the sequence (J(k))k∈N toward ˆJ.Implicit differentiation of Lasso-type models for hyperparameter optimization A.3. Proof of Proposition 2 in the BCD case The goal of the proof is to show that iterations of the Jacobian sequence (J(k))k∈N generated by the Block Coordinate Descent algorithm (Algorithm 3) converges toward the true Jacobian ˆJ. The main difﬁculty of the proof is to show that the Jacobian sequence follows a Vector AutoRegressive (V AR, see Massias et al. (2019, Thm. 10) for more detail),i.e., the main difﬁculty is to show that there exists k0 such that for all k≥k0: J(k+1) = AJ(k) + B , (31) with A∈Rp×p a contracting operator and B ∈Rp. We follow exactly the proof of Massias et al. (2019, Thm. 10). Proof. (Lasso, BCD, forward differentiation (Algorithm 3)) Let j1,...,j S be the indices of the support of ˆβ, in increasing order. As the sign is identiﬁed, coefﬁcients outside the support are 0 and remain 0. We decompose the k-th epoch of coordinate descent into individual coordinate updates: Let ˜β(0) ∈Rp denote the initialization (i.e., the beginning of the epoch, ), ˜β(1) = β(k) the iterate after coordinate j1 has been updated, etc., up to ˜β(S) after coordinate jS has been updated, i.e., at the end of the epoch ( ˜β(S) = β(k+1)). Let s ∈S, then ˜β(s) and ˜β(s−1) are equal everywhere, except at coordinate js: ˜J(s) js = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjs after sign identiﬁcation we have: = ˜J(s−1) js − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ˜J(s) ˆS = ( Idˆs− 1 ∥X:,js∥2 ejse⊤ jsX⊤ :,ˆSX:,ˆS )    As ˜J(s−1) ˆS − 1 ∥X:,js∥2 sign ˆβjs ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s) ˆS =  Idˆs− ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥      A(s) ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(s−1) ˆS − ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥2 sign ˆβjs    b(s) We thus have: ( X⊤ :,ˆSX:,ˆS )1/2 ˜J(ˆs) ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs ( X⊤ :,ˆSX:,ˆS )1/2 J(1) ˆS + AS...A 2b1 + ··· + ASbS−1 + bS   b∈Rˆs . After sign identiﬁcation and a full update of coordinate descent we thus have: ( X⊤ :,ˆSX:,ˆS )1/2 J(t+1) ˆS = A ( X⊤ :,ˆSX:,ˆS )1/2 J(t) ˆS + b . (32) Lemma A.2. ∥As∥2 ≤1 , Moreover if A(s)x = ∥x∥then x∈vect   ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs   ⊤ (33) Proof. ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejse⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥Implicit differentiation of Lasso-type models for hyperparameter optimization is a symmetric rank 1 matrix, its non-zero eigenvalue is e⊤ js ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs = e⊤ js X⊤ :,ˆSX:,ˆS ∥X:,js∥2 ejs = 1 . An eigenvector associated to this non-zeros eigenvalue is ( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs . Asis symmetric and real, is diagonalisable in an orthogonal basis, it has eigenvalue1 with multiplicity ˆs−1 and eigenvalue 0 with multiplicity 1. Moreover if ∥Ax∥= ∥x∥, then x∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ . Lemma A.3. ∥A∥2 <1 . Proof. A= A(ˆs) ...A (1) We have ∥A∥≤∥ A(ˆs)∥   ≤1 ... ∥A(1)∥   ≤1 ≤1 . Let x∈Rˆs such that ∥Ax∥= ∥x∥, we thus have for all s∈1,..., ˆs, A(s)x = ∥x∥. Using Lemma A.3 we have that for all s∈1,..., ˆsx ∈vect (( X⊤ :,ˆSX:,ˆS )1/2 ∥X:,js∥ ejs )⊤ , i.e., x∈vect (( X⊤ :,ˆSX:,ˆS )1/2)⊤ = {0}because X⊤ :,ˆSX:,ˆS ≻0 Using Equation (32) we have: ∥J(t+1) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 ≤∥A∥2∥J(t) ˆS − ˆJ∥(X⊤ :,ˆSX:,ˆS)−1 , (34) with ∥A∥2 < 1, which leads to the desire result. Since the recursion of the Jacobian sequences of Algorithm 2 and Algorithm 2 are the same once the support is identiﬁed, the proof of convergence of Algorithm 2 is the same (provided that support identiﬁcation has been achieved). Proof. (wLasso case, BCD) As for the Lasso case: ˜J(s) js,: = ˜J(s−1) js,: − 1 ∥X:,js∥2 X⊤ :,jsX ˜J(s−1) − 1 ∥Xjs∥2 sign βjsejse⊤ js after sign identiﬁcation we have: ˜J(s) js,ˆS = ˜J(s−1) js,ˆS − 1 ∥X:,js∥2 X⊤ :,jsX:,ˆS ˜J(s−1) ˆS,ˆS − 1 ∥X:,js∥2 sign ˆβjsejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s) ˆS,ˆS = ( Idn− (X⊤ :,ˆSX:,ˆS)1/2ejse⊤ js(X⊤ :,ˆSX:,ˆS)1/2 ∥X:,js∥2 )    A(s) (X⊤ :,ˆSX:,ˆS)1/2 ˜J(s−1) ˆS,ˆS −sign ˆβjs ∥X:,js∥2 (X⊤ :,ˆSX:,ˆS)1/2    B(s) ejse⊤ js (X⊤ :,ˆSX:,ˆS)1/2 ˜J(ˆs) ˆS,ˆS = A(ˆs) ...A (1)    A∈Rˆs×ˆs (X⊤ :,ˆSX:,ˆS)1/2 ˜J(0) ˆS,ˆS + A(ˆs) ...A (2)B(1)ej1 e⊤ j1 + ··· + B(ˆs)ejˆse⊤ jˆs    D∈Rˆs×ˆs . (35) As in the Lasso case, Equation (35) leads to linear convergence once the support is identiﬁed for Algorithms 2 and 3.Implicit differentiation of Lasso-type models for hyperparameter optimization B. Block coordinate descent algorithms Algorithm 3 presents the forward iteration scheme which computes iteratively the solution of the Lasso or wLasso jointly with the Jacobian computation. This is the naive way of computing the Jacobian without taking advantage of its sparsity. Eventually, it requires to differentiate every lines of code w.r.t. to λ and take advantage of the BCD updates for cheap updates on the Jacobian as well. Algorithm 3FORWARD ITERDIFF (Deledalle et al., 2014; Franceschi et al., 2017) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // jointly compute coef. & Jacobian β = 0 // potentially warm started J= 0 // potentially warm started r= y−Xβ dr= −XJ for k= 0,...,n iter −1 do for j = 0,...,p −1 do // update the regression coefficients βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // update the Jacobian if Lasso then Jold = Jj Jj = |sign βj| ( Jj + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ Jj −= neλ ∥X:,j∥2 sign βj // diff. w.r.t. λ drj −= X:,j(Jj −Jold) if wLasso then Jold = Jj,: Jj,: = |sign βj| ( Jj,: + 1 ∥X:,j∥2 X⊤ :,jdr ) // diff. w.r.t. λ1,...,λ p Jj,j −= neλj ∥X:,j∥2 sign βj // diff. w.r.t. λ1,...,λ p dr−= X:,j(Jj −Jold) return βniter ,Jniter (λ) Algorithm 4 describes the backward iterative differentiation algorithm used for benchmark. Backward differentiation requires the storage of every updates on β. As Figure 1 shows, this algorithm is not efﬁcient for our case because the function to differentiate f : R →Rp ( f : Rp →Rp, for the wLasso) has a higher dimension output space than the input space. The storage is also an issue mainly for the wLasso case which makes this algorithm difﬁcult to use in practice in our context. Algorithm 5 presents the classical BCD iterative scheme for solving the Lasso problem using the composition of a gradient step with the soft-thresholding operator.Implicit differentiation of Lasso-type models for hyperparameter optimization Algorithm 4BACKWARD ITERDIFF (Domke, 2012) input : X ∈Rn×p,y ∈Rn,λ ∈R,niter ∈N // backward computation of ˆβ and ˆJ⊤ (λ)α β = 0 // potentially warm started // compute the regression coefficients and store the iterates for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) // Init. backward differentiation g= 0 // g stores ˆJ⊤ λ α // compute the Jacobian for k= niter down to 1 do for j = 0,...,p −1 do if Lasso then g−= neλ ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX // O(np) if wLasso then gj −= neλj ∥X:,j∥2 αjsign β(k) j αj ∗= |sign β(k) j | α−= 1 ∥X:,j∥2 αjX⊤ :,jX return βniter ,g(1) Algorithm 5BCD FOR THE LASSO (Friedman et al., 2010) input : X ∈Rn×p,y ∈Rn,λ ∈R,β(0) ∈Rp,niter ∈N β = β(0) // warm start for k= 0,...,n iter −1 do for j = 0,...,p −1 do βold = βj zj = βj + 1 ∥X:,j∥2 X⊤ :,jr // gradient step βj = ST(zj,neλ/∥X:,j∥2) // proximal step r−= X:,j(βj −βold) return βniterImplicit differentiation of Lasso-type models for hyperparameter optimization C. Derivations for MCP Let us remind the deﬁnition of the Minimax Concave Penalty (MCP) estimator introduced by Zhang (2010), also analyzed under the name CELE0 by Soubies et al. (2017). First of all, for any t∈R: pMCP λ,γ (t) = { λ|t|− t2 2γ, if |t|≤ γλ 1 2 γλ2, if |t|>γλ . (36) The proximity operator of pλ,γ for parameters λ >0 and γ >1 is deﬁned as follow (see Breheny and Huang 2011, Sec. 2.1): proxMCP λ,γ (t) = {ST(t,λ) 1−1 γ if |t|≤ γλ t if |t|>γλ . (37) For ourselves we choose as for the Lasso an exponential parametrization of the coefﬁcients, for λ∈R and γ >0: ˆβ(λ,γ)(y) ≜ arg min β∈Rp 1 2n∥y−Xβ∥2 2 + p∑ j=1 pMCP eλ,eγ (|βj|) . (38) Update rule for Coordinate Descent Below, we provide equation to update the coefﬁcient in the coordinate descent algorithm of the MCP: βj ←arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + p∑ j′̸=j pMCP eλ,eγ(βj′) +pMCP eλ,eγ(βj) = arg min βj∈R 1 2n∥y−βjX:,j − ∑ j′̸=j βj′X:,j′∥2 2 + pMCP eλ,eγ(βj) = arg min βj∈R ∥X:,j∥2 2   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2n  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + 1 ∥X:,j∥2 2 pMCP eλ,eγ(βj)   = arg min βj∈R   1 2Lj  βj − 1 ∥X:,j∥2 2 ⟨ y− ∑ j′̸=j βj′X:,j′,X:,j ⟩  2 + pMCP eλ,eγ(βj)  ,with Lj ≜ n ∥X:,j∥2 2 = proxMCP eλ/Lj,eγLj ( βj − 1X2 :,j X⊤ :,j(Xβ −y),λ ) . (39) One can write the following ﬁxed point equation satisﬁed by the estimator ˆβ, with Lj = ∥X:,j∥2 /n: ˆβj = proxMCP eλ/Lj,eγLj   ⟨ y− ∑ k̸=j ˆβkX:,k, X:,j ∥X:,j∥2 ⟩  = proxMCP eλ/Lj,eγLj ( ˆβj − 1 ∥X:,j∥2 X⊤ :,j ( Xˆβ−y )) . (40) Since the MCP penalty is non-convex, the estimator may not be continuous w.r.t. hyperparameters and gradient based hyperparameter optimization may not be theoretically justiﬁed. However we can differentiate the ﬁxed point equationImplicit differentiation of Lasso-type models for hyperparameter optimization Imp. F. iterdiﬀ. (ours) F. iterdiﬀ. Grid-search 0 2 4 10−4 10−3 10−2 10−1 100 Objective minus optimum rcv1 (p=19,959) 0 10 20 30 10−2 10−1 100 101 102  20news (p=130,107) 0 2 4 Time (s) 10−1 100 Loss on test set 0 10 20 30 Time (s) 101 102 Figure 5.Computation time for the HO of the MCP on real dataDistance to “optimum” (top) and performance (bottom) on the test set for the MCP. Equation (40) almost everywhere: ˆJj = ( ˆJj − 1 ∥X:j∥2 2 X⊤ :jX ˆJ ) · ∂proxMCP eλ/Lj,eγLj ∂t ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eλ Lj ∂proxMCP eλ/Lj,eγLj ∂λ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) + eγLj ∂proxMCP eλ/Lj,eγLj ∂γ ( ˆβj − 1X2 :,j X⊤ :,j(Xβ −y) ) . (41) where ∂proxMCP λ,γ ∂t (t) = { |sign t| 1−1 γ , if |t|≤ λγ 1, otherwise , (42) ∂proxMCP λ,γ ∂λ (t) =    0, if |t|≤ λ −sign t 1−1 γ , if λ≤|t|≤ λγ 0, if |t|>λγ , (43) ∂proxMCP λ,γ ∂γ (t) = { −ST(t,λ) (γ−1)2 if |t|≤ λγ 0 if |t|>λγ . (44) Contrary to other methods, HO based algorithms do not scale exponentially in the number of hyperparameters. Here we propose experiments on the held-out loss with the MCP estimator (Zhang, 2010), which has 2 hyperparameters λand γ. Our algorithm can generalize to such non-smooth proximity-based estimator. Comments on Figure 5 (MCP , held-out criterion). Figure 5 (top) shows the convergence of the optimum on 2 datasets (rcv1 and 20news) for the MCP estimator. As before implicit forward differentiation outperforms forward differentiation illustrating Proposition 2 and Table 1.Implicit differentiation of Lasso-type models for hyperparameter optimization D. Datasets and implementation details The code used to produce all the ﬁgures as well as the implementation details can be found in the supplementary material in the forward_implicit/expesfolder. In particular in all experiments, for our algorithm, implicit forward differentiation, the size of the loop computing the Jacobian is ﬁxed: n_iter_jac = 100. Reminding that the goal is to compute the gradient: ˆJ⊤ (λ)∇C ( ˆβ(λ) ) , (45) we break the loop if ∥(J(k+1) −J(k))∇C( ˆβ(λ))∥≤∥∇C ( ˆβ(λ))∥×ϵjac , (46) with ϵjac = 10−3. All methods beneﬁt from warm start. D.1. Details on Figure 1 Figure 1 is done using synthetic data. As described in Section 4.2, X ∈ Rn×p is a Toeplitz correlated ma- trix, with correlation coefﬁcient ρ = 0 .9, (n,p) = (1000 ,2000). β ∈ Rp is chosen with 5 non-zero coefﬁ- cients chosen at random. Then y ∈ Rn is chosen to be equal to Xβ contaminated by some i.i.d. random Gaus- sian noise, we chose SNR = 3. For Figure 1 all the implementation details can be found in the joint code in the forward_implicit/examples/plot_time_to_compute_single_gradient.py ﬁle. Figure 1 shows the time of compu- tation of one gradient and the distance to ”optimum”. For this ﬁgure we evaluated the gradient in λ= λmax −ln(10). The ”optimum” is the gradient obtained using the implicit differentiation method. D.2. Details on Figure 2 Let us ﬁrst begin by a description of all the datasets and where they can be downloaded. rcv1. The rcv1 dataset can be downloaded here: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multilabel.html#rcv1v2%20(topics;%20subsets). The dataset contains n = 20,242 sam- ples and p= 19,959 features. 20news. The 20news dataset can be downloaded here https://www.csie.ntu.edu.tw/~cjlin/ libsvmtools/datasets/multiclass.html#news20. The dataset contains n = 11 ,314 samples and p= 130,107 features. ﬁnance. The ﬁnance (E2006-log1p on libsvm) dataset can be downloaded here: https://www.csie.ntu.edu. tw/~cjlin/libsvmtools/datasets/regression.html#E2006-log1p. The dataset contains n= 16,087 samples and p= 1,668,737 features. All the implementation details can be found in the code: forward_implicit/expes/main_lasso_pred.py. D.3. Details on Figure 3 Figure 3 was performed using simulated data. The matrix X ∈Rn×p was obtained by simulated n×pi.i.d. Gaussian variables N(0,1). The number of rows was ﬁxed atn= 100and we changed the number of columnspfrom 200 to 10,000 on a linear grid of size 10. Then , we generated β∗with 5 coefﬁcients equal to 1 and the rest equals to 0. The vector y is equal to Xβ∗contaminated by some i.i.d. random Gaussian noise controlled by a SNR value of 3. We performed 50 repetitions for each value of pand computed the average MSE on these repetitions. The initial value for the line-search algorithm was set at λmax + ln(0.7) and the number of iterations for the Jacobian at 500 for the whole experiment. All the implementation details can be found in the code : forward_implicit/expes/main_lasso_est.py. D.4. Details on Figure 4 Figure 4 was performed using the same simulating process as described above only this time we performed only 25 repeti- tions for each value of p. We had to deal with the fact that Problem (4) is not convex for the weighted Lasso which means that our line-search algorithm could get stuck in local minima. In order to alleviate this problem, we introduced Equa- tion (18) to obtain an initial point for the line-search algorithm. We chose the regularization term to be constant and equalsImplicit differentiation of Lasso-type models for hyperparameter optimization to C(β(λmax))/10. We used a time treshold of 500 seconds which was hit only by the forward differentiation algorithm for the wLasso. The details about this experiment can be found in the code : forward_implicit/expes/main_wLasso.py.Implicit differentiation of Lasso-type models for hyperparameter optimization E. Supplementary experiments E.1. Experiments with a non-unique solution to the inner problem We recall here that the bi-level optimization Problem (4) is solved using gradient descent. We recall also that gradient descent may not converge toward a global minima since the optimized function λ↦→L(λ) may not be convex. It may be even worse: if the inner optimization problem has not a unique solution, the function λ ↦→L(λ) may not be continuous. However our algorithm can still be applied to compute the hypergradient. Figure 6 shows the time to compute a single (hyper)gradient when the solution to the inner problem is not unique. As proved for instance in Tibshirani (2013, Lemma 3 and 4), the set of parameters where the Lasso solution is not unique is typically ∅or a set whose Lebesgue measure is zero. Moreover, there exist settings such that the solution path (as a multivalued mapping) could be non-continuous, which leaves only non-gradient based methods available. Thus, we decided to not investigate the theory in such pathological settings. The authors are not aware of a classical dataset where non-uniqueness arises. Nevertheless, in the case where there existsλsuch that the solution set is not reduced to a singleton, our proposed algorithm can still be applied to any solution without theoretical guarantees. Experimental setting for non-uniqueness.For completeness, we run our methods on the following toy example Tibshi- rani (2013): we consider a design Xsuch that n= 100, p= 10000and X1,X2,X3 are generated iid following a standard normal distribution, X4 = (X2 +X3)/2 and X5,...,X p are generated i.i.d. following a standard normal distribution, then orthogonalized X5,...,X p w.r.t.Span(X1,...,X 4). We let y= −X1 +X2 +X3. We let the reader observe that when λ = 1/n, the solution set is not reduced to a singleton. In this case, similar conclusions are observed in Figure 6 as for other datasets: Imp. F. Iterdiff (ours) still outperforms its competitors. Imp. F. Iterdiﬀ. (ours) F. Iterdiﬀ. B. Iterdiﬀ. 6 8 10 Number of iterations 100 102 Times (s) 6 8 10 Number of iterations 10−11 10−8 10−5 Objective minus optimum Figure 6.Time to compute a single gradient with non-unique solution(Synthetic data, Lasso, n,p = 1000,10000). Inﬂuence on the number of iterations of BCD (in the inner optimization problem of Problem (4)) on the computation time (left) and the distance to “optimum” of the gradient∇λL(λ)(right) for the Lasso estimator.",
      "meta_data": {
        "arxiv_id": "2002.08943v3",
        "authors": [
          "Quentin Bertrand",
          "Quentin Klopfenstein",
          "Mathieu Blondel",
          "Samuel Vaiter",
          "Alexandre Gramfort",
          "Joseph Salmon"
        ],
        "published_date": "2020-02-20T18:43:42Z",
        "pdf_url": "https://arxiv.org/pdf/2002.08943v3.pdf",
        "github_url": "https://github.com/QB3/sparse-ho"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces an efficient implicit differentiation algorithm for hyperparameter optimization (HO) of Lasso-type models, specifically tailored for non-smooth objectives and high-dimensional sparse data. The key contribution is an implicit differentiation method that avoids direct matrix inversion, which is common in traditional implicit differentiation but can be numerically unstable and prohibitive for non-smooth problems. It leverages the sparsity of Lasso solutions and decouples the computation of regression coefficients from the Jacobian. The method is shown to converge linearly to the true gradient once the solution's support is identified, and it outperforms various state-of-the-art HO methods (including other gradient-based and non-gradient-based approaches) on both simulated and real-world high-dimensional datasets for held-out loss and Stein Unbiased Risk Estimator (SURE) criteria.",
        "methodology": "The methodology frames hyperparameter optimization as a nested bi-level problem, optimizing a differentiable criterion (e.g., held-out loss or SURE) with respect to hyperparameters, where the inner problem solves for Lasso or weighted Lasso regression coefficients. For non-smooth Lasso-type problems, traditional implicit differentiation methods based on optimality conditions for smooth functions are not applicable. The proposed 'Implicit Forward Iterative Differentiation' (Algorithm 2) overcomes this by exploiting the fixed-point iteration property of proximal Block Coordinate Descent (BCD) algorithms. It first solves the inner Lasso problem to identify the support of the solution (non-zero coefficients), and then applies forward differentiation recursion steps to compute the Jacobian. This two-step process avoids solving potentially ill-conditioned linear systems and exploits the sparsity of the Jacobian. The regularization parameter is parameterized as e^λ to handle positivity constraints and scaling. For the SURE criterion, a weakly differentiable approximation using Finite Differences Monte-Carlo is employed.",
        "experimental_setup": "The method's performance was evaluated against several competitors, including other gradient-based methods (Implicit Differentiation, Forward Iterative Differentiation) and non-gradient-based methods (Grid-search, Random-search, Lattice Hypercube Sampling, Bayesian optimization). All inner optimization problems were solved using a vanilla BCD algorithm with a stopping tolerance of 10^-5. Gradient-based methods employed a line-search strategy, and initialization for Lasso was set to λ_max - log(10). For the weighted Lasso, a regularized HO problem was solved to obtain a robust initialization. Experiments were conducted on: 1. Held-out loss on real-world datasets: rcv1 (n=20k, p=20k), 20news (n=11k, p=130k), and finance (n=16k, p=1.6M). 2. SURE criterion on simulated data: n=100, p ranging from 200 to 10,000, with varying SNR. Metrics included distance to optimum for the validation loss, test set loss, and relative Mean Squared Error (for estimation tasks). The method was also applied to the non-convex MCP estimator with two hyperparameters on rcv1 and 20news. The code is open-source (https://github.com/QB3/sparse-ho) and uses Numba for performance.",
        "limitations": "The theoretical guarantees (Propositions 1 and 2 regarding Jacobian convergence) assume uniqueness of the Lasso solution and do not explicitly cover non-convex penalty functions like MCP, although the method demonstrated proper numerical behavior on MCP. The HO problem itself can be non-convex for weighted Lasso or MCP, meaning gradient descent may converge to local minima, which was mitigated by a regularized initialization strategy in experiments. The SURE criterion requires prior knowledge of the noise variance. Comparing the computational efficiency of gradient-based and non-gradient-based methods is inherently challenging due to their different search space approaches (continuous vs. discrete). While the method itself is robust, other single-step automatic differentiation approaches might struggle with state-of-the-art Lasso solvers that employ discontinuous techniques like active sets or screening rules.",
        "future_research_directions": "Future research could focus on extending the theoretical guarantees to non-convex Lasso-type formulations (e.g., MCP, Elastic-Net) which are currently only supported numerically. Further investigation into the behavior and theoretical guarantees in pathological settings where Lasso solutions are not unique, though rare, could also be a direction. Exploring the integration of more advanced, specialized inner solvers (beyond vanilla BCD) and their implications for the Jacobian computation, especially considering the potential for discontinuities, remains an open area for optimization.",
        "experimental_code": "import numpy as np\nfrom scipy.sparse import issparse\n\n\nclass Forward():\n    \"\"\"Algorithm to compute the hypergradient using forward differentiation of\n    proximal coordinate descent.\n\n    The algorithm jointly and iteratively computes the regression coefficients\n    and the Jacobian using forward differentiation of proximal\n    coordinate descent.\n\n    Parameters\n    ----------\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(self, use_stop_crit=True, verbose=False):\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient, with forward differentiation of\n        proximal coordinate descent.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n        # jointly compute the regression coefficients beta and the Jacobian\n        mask, dense, jac = compute_beta(\n            X, y, log_alpha, model, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start, max_iter=max_iter, tol=tol,\n            compute_jac=True, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        if jac is not None:\n            jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n            if full_jac_v:\n                jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n        else:\n            jac_v = None\n\n        return mask, dense, jac_v, jac\n\n\ndef compute_beta(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        max_iter=1000, tol=1e-3, compute_jac=True, return_all=False,\n        save_iterates=False, verbose=False, use_stop_crit=True, gap_freq=10):\n    \"\"\"\n    Parameters\n    --------------\n    X: array-like, shape (n_samples, n_features)\n        Design matrix.\n    y: ndarray, shape (n_samples,)\n        Observation vector.\n    log_alpha: float or np.array, shape (n_features,)\n        Logarithm of hyperparameter.\n    beta0: ndarray, shape (n_features,)\n        initial value of the regression coefficients\n        beta for warm start\n    dbeta0: ndarray, shape (n_features,)\n        initial value of the jacobian dbeta for warm start\n    max_iter: int\n        number of iterations of the algorithm\n    tol: float\n        The tolerance for the optimization: if the updates are\n        smaller than ``tol``, the optimization code checks the\n        primal decrease for optimality and continues until it\n        is smaller than ``tol``\n    compute_jac: bool\n        to compute or not the Jacobian along with the regression\n        coefficients\n    model:  instance of ``sparse_ho.base.BaseModel``\n        A model that follows the sparse_ho API.\n    return_all: bool\n        to store the iterates or not in order to compute the Jacobian in a\n        backward way\n    use_stop_crit: bool\n        use a stopping criterion or do all the iterations\n    gap_freq : int\n        After how many passes on the data the dual gap should be computed\n        to stop the iterations.\n\n    Returns\n    -------\n    mask : ndarray, shape (n_features,)\n        The mask of non-zero coefficients in beta.\n    dense : ndarray, shape (n_nonzeros,)\n        The beta coefficients on the support\n    jac : ndarray, shape (n_nonzeros,) or (n_nonzeros, q)\n        The jacobian restricted to the support. If there are more than\n        one hyperparameter then it has two dimensions.\n    \"\"\"\n    n_samples, n_features = X.shape\n    is_sparse = issparse(X)\n    if not is_sparse and not np.isfortran(X):\n        X = np.asfortranarray(X)\n    L = model.get_L(X)\n\n    ############################################\n    alpha = np.exp(log_alpha)\n\n    if hasattr(model, 'estimator') and model.estimator is not None:\n        return model._use_estimator(X, y, alpha, tol)\n\n    try:\n        alpha.shape[0]\n        alphas = alpha.copy()\n    except Exception:\n        alphas = np.ones(n_features) * alpha\n    ############################################\n    # warm start for beta\n    beta, dual_var = model._init_beta_dual_var(X, y, mask0, dense0)\n    ############################################\n    # warm start for dbeta\n    dbeta, ddual_var = model._init_dbeta_ddual_var(\n        X, y, mask0=mask0, dense0=dense0, jac0=jac0, compute_jac=compute_jac)\n\n    # store the values of the objective\n    pobj0 = model._get_pobj0(dual_var, np.zeros(X.shape[1]), alphas, y)\n    pobj = []\n\n    ############################################\n    # store the iterates if needed\n    if return_all:\n        list_beta = []\n    if save_iterates:\n        list_beta = []\n        list_jac = []\n\n    for i in range(max_iter):\n        if verbose:\n            print(\"%i -st iteration over %i\" % (i, max_iter))\n        if is_sparse:\n            model._update_beta_jac_bcd_sparse(\n                X.data, X.indptr, X.indices, y, n_samples, n_features, beta,\n                dbeta, dual_var, ddual_var, alphas, L,\n                compute_jac=compute_jac)\n        else:\n            model._update_beta_jac_bcd(\n                X, y, beta, dbeta, dual_var, ddual_var, alphas,\n                L, compute_jac=compute_jac)\n\n        pobj.append(model._get_pobj(dual_var, X, beta, alphas, y))\n\n        if i > 1:\n            if verbose:\n                print(\"relative decrease = \", (pobj[-2] - pobj[-1]) / pobj0)\n\n        if use_stop_crit and i % gap_freq == 0 and i > 0:\n            if hasattr(model, \"_get_dobj\"):\n                dobj = model._get_dobj(dual_var, X, beta, alpha, y)\n                dual_gap = pobj[-1] - dobj\n                if verbose:\n                    print(\"dual gap %.2e\" % dual_gap)\n                if verbose:\n                    print(\"gap %.2e\" % dual_gap)\n                if dual_gap < pobj0 * tol:\n                    break\n            else:\n                if (pobj[-2] - pobj[-1] <= pobj0 * tol):\n                    break\n        if return_all:\n            list_beta.append(beta.copy())\n        if save_iterates:\n            list_beta.append(beta.copy())\n            list_jac.append(dbeta.copy())\n    else:\n        if verbose:\n            print('did not converge !')\n\n    mask = beta != 0\n    dense = beta[mask]\n    jac = model._get_jac(dbeta, mask)\n    if hasattr(model, 'dual'):\n        model.dual_var = dual_var\n        if compute_jac:\n            model.ddual_var = ddual_var\n    if save_iterates:\n        return np.array(list_beta), np.array(list_jac)\n    if return_all:\n        return mask, dense, list_beta\n    else:\n        if compute_jac:\n            return mask, dense, jac\n        else:\n            return mask, dense, None\nimport numpy as np\nfrom scipy.sparse import issparse\nfrom sparse_ho.algo.forward import compute_beta\n\n\nclass ImplicitForward():\n    \"\"\"Algorithm to compute the hypergradient using implicit forward\n    differentiation.\n\n    First the algorithm computes the regression coefficients.\n    Then the iterations of the forward differentiation are applied to compute\n    the Jacobian.\n\n    Parameters\n    ----------\n    tol_jac: float\n        Tolerance for the Jacobian computation.\n    max_iter: int\n        Maximum number of iterations for the inner solver.\n    n_iter_jac: int\n        Maximum number of iterations for the Jacobian computation.\n    use_stop_crit: bool, optional (default=True)\n        Use stopping criterion in hypergradient computation. If False,\n        run to maximum number of iterations.\n    verbose: bool, optional (default=False)\n        Verbosity of the algorithm.\n    \"\"\"\n\n    def __init__(\n            self, tol_jac=1e-3, max_iter=100, n_iter_jac=100,\n            use_stop_crit=True, verbose=False):\n        self.max_iter = max_iter\n        self.tol_jac = tol_jac\n        self.n_iter_jac = n_iter_jac\n        self.use_stop_crit = use_stop_crit\n        self.verbose = verbose\n\n    def get_beta_jac(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        \"\"\"Compute beta and hypergradient using implicit forward\n        differentiation.\n\n        Parameters\n        ----------\n        X: array-like, shape (n_samples, n_features)\n            Design matrix.\n        y: ndarray, shape (n_samples,)\n            Observation vector.\n        log_alpha: float or np.array, shape (n_features,)\n            Logarithm of hyperparameter.\n        model:  instance of ``sparse_ho.base.BaseModel``\n            A model that follows the sparse_ho API.\n        get_grad_outer: callable\n            Function which returns the gradient of the outer criterion.\n        mask0: ndarray, shape (n_features,)\n            Boolean of active feature of the previous regression coefficients\n            beta for warm start.\n        dense0: ndarray, shape (mask.sum(),)\n            Initial value of the previous regression coefficients\n            beta for warm start.\n        quantity_to_warm_start: ndarray\n            Previous Jacobian of the inner optimization problem.\n        max_iter: int\n            Maximum number of iteration for the inner solver.\n        tol: float\n            The tolerance for the inner optimization problem.\n        full_jac_v: bool\n            TODO\n        \"\"\"\n\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=tol, tol=tol, niter_jac=self.n_iter_jac, model=model,\n            max_iter=self.max_iter, verbose=self.verbose)\n        return mask, dense, jac\n\n    def compute_beta_grad(\n            self, X, y, log_alpha, model, get_grad_outer, mask0=None,\n            dense0=None, quantity_to_warm_start=None, max_iter=1000, tol=1e-3,\n            full_jac_v=False):\n        mask, dense, jac = get_bet_jac_implicit_forward(\n            X, y, log_alpha, mask0=mask0, dense0=dense0,\n            jac0=quantity_to_warm_start,\n            tol_jac=self.tol_jac, tol=tol, niter_jac=self.n_iter_jac,\n            model=model, max_iter=self.max_iter, verbose=self.verbose,\n            use_stop_crit=self.use_stop_crit)\n        jac_v = model.get_jac_v(X, y, mask, dense, jac, get_grad_outer)\n        if full_jac_v:\n            jac_v = model.get_full_jac_v(mask, jac_v, X.shape[1])\n\n        return mask, dense, jac_v, jac\n\n\ndef get_bet_jac_implicit_forward(\n        X, y, log_alpha, model, mask0=None, dense0=None, jac0=None,\n        tol=1e-3, max_iter=1000, niter_jac=1000, tol_jac=1e-6, verbose=False,\n        use_stop_crit=True):\n\n    mask, dense, _ = compute_beta(\n        X, y, log_alpha, mask0=mask0, dense0=dense0, jac0=jac0, tol=tol,\n        max_iter=max_iter, compute_jac=False, model=model, verbose=verbose,\n        use_stop_crit=use_stop_crit)\n    dbeta0_new = model._init_dbeta0(mask, mask0, jac0)\n    reduce_alpha = model._reduce_alpha(np.exp(log_alpha), mask)\n\n    _, dual_var = model._init_beta_dual_var(X, y, mask, dense)\n    jac = get_only_jac(\n        model.reduce_X(X, mask), model.reduce_y(y, mask), dual_var,\n        reduce_alpha, model.sign(dense, log_alpha), dbeta=dbeta0_new,\n        niter_jac=niter_jac, tol_jac=tol_jac, model=model, mask=mask,\n        dense=dense, verbose=verbose, use_stop_crit=use_stop_crit)\n\n    return mask, dense, jac\n\n\ndef get_only_jac(\n        Xs, y, dual_var, alpha, sign_beta, dbeta=None, niter_jac=100,\n        tol_jac=1e-4, model=\"lasso\", mask=None, dense=None, verbose=False,\n        use_stop_crit=True):\n    n_samples, n_features = Xs.shape\n\n    L = model.get_L(Xs)\n\n    residual_norm = []\n\n    if hasattr(model, 'dual'):\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n        dbeta = model.dbeta\n    else:\n        if dbeta is None:\n            dbeta = model._init_dbeta(n_features)\n        ddual_var = model._init_ddual_var(dbeta, Xs, y, sign_beta, alpha)\n\n    for i in range(niter_jac):\n        if verbose:\n            print(\"%i -st iterations over %i\" % (i, niter_jac))\n        if issparse(Xs):\n            model._update_only_jac_sparse(\n                Xs.data, Xs.indptr, Xs.indices, y, n_samples,\n                n_features, dbeta, dual_var, ddual_var, L, alpha, sign_beta)\n        else:\n            model._update_only_jac(\n                Xs, y, dual_var, dbeta, ddual_var, L, alpha, sign_beta)\n        residual_norm.append(\n            model.get_jac_residual_norm(\n                Xs, y, n_samples, sign_beta, dbeta, dual_var,\n                ddual_var, alpha))\n        if use_stop_crit and i > 1:\n            # relative stopping criterion for the computation of the jacobian\n            # and absolute stopping criterion to handle warm start\n            rel_tol = np.abs(residual_norm[-2] - residual_norm[-1])\n            if (rel_tol < np.abs(residual_norm[-1]) * tol_jac\n                    or residual_norm[-1] < 1e-10):\n                break\n    # HACK we only need this for one test, do not rely on it\n    get_only_jac.n_iter = i\n\n    return dbeta\n",
        "experimental_info": "The 'Implicit Forward Iterative Differentiation' (ImplicitForward) algorithm is extensively tested across various machine learning models and criteria. The regularization parameter (lambda) is consistently parameterized as `e^λ` to handle positivity constraints and scaling, often optimized using geometric spacing of alpha values or starting from `alpha_max / C`.\n\n**Models and Criteria:**\n-   **Lasso:** Used with HeldOutMSE, CrossVal (HeldOutMSE), and FiniteDiffMonteCarloSure.\n-   **ElasticNet:** Used with HeldOutMSE and CrossVal (HeldOutMSE).\n-   **WeightedLasso:** Used with HeldOutMSE and CrossVal (HeldOutMSE).\n-   **SparseLogreg:** Used with HeldOutLogistic and LogisticMulticlass.\n\n**Hyperparameter Optimization Settings for ImplicitForward:**\n-   **Tolerance for Jacobian computation (`tol_jac`):** Values range from `1e-3` (e.g., `examples/plot_held_out_enet.py`, `expes/expe_elastic/main.py`) to `1e-8` (e.g., `examples/plot_sparse_log_reg.py`) and even `1e-32` (e.g., `expes/hypergradient/main_hypergradient.py`) for high precision.\n-   **Maximum iterations for Jacobian computation (`n_iter_jac`):** Common values include `100` (e.g., `examples/plot_held_out_enet.py`, `expes/expe_elastic/main.py`), `1000` (e.g., `examples/plot_compare_optimizers.py`, `examples/plot_sparse_log_reg.py`), and up to `5000` or `100000` for more intensive evaluations.\n-   **Maximum iterations for inner solver (`max_iter`):** Typically set to `50` or `100` for quick runs, but often extended to `1000`, `10000`, or `50000` for full convergence. Some experiments use a `max_iter` equal to `n_iter_jac`.\n-   **Tolerance for inner solver (`tol`):** Ranges from `1e-3` to `1e-8`, with some tests using extremely tight tolerances like `1e-16` or `1e-32` for convergence analysis.\n-   **Stopping Criterion (`use_stop_crit`):** Defaults to `True`, but can be explicitly set to `False` in experiments focused on analyzing convergence behavior over a fixed number of iterations.\n\n**SURE Criterion Specifics:**\n-   When `FiniteDiffMonteCarloSure` is used (e.g., `examples/plot_meg_lasso_vs_wlasso.py`), the `sigma` parameter (noise level) is provided. The `finite_difference_step` is either given explicitly or computed using a heuristic `2.0 * sigma / (X.shape[0]) ** 0.3`.\n\n**Data Splitting:**\n-   Commonly, data is split into training and validation sets (e.g., `n_samples // 2` for each), or through K-Fold Cross-Validation (`KFold(n_splits=5, shuffle=True, random_state=42)`).\n\n**Outer Optimization:**\n-   The extracted examples primarily use `GradientDescent` or `LineSearch` as outer optimizers, with `Adam` also present in one example."
      }
    },
    {
      "title": "Bayesian Optimization for Iterative Learning",
      "abstract": "The performance of deep (reinforcement) learning systems crucially depends on\nthe choice of hyperparameters. Their tuning is notoriously expensive, typically\nrequiring an iterative training process to run for numerous steps to\nconvergence. Traditional tuning algorithms only consider the final performance\nof hyperparameters acquired after many expensive iterations and ignore\nintermediate information from earlier training steps. In this paper, we present\na Bayesian optimization (BO) approach which exploits the iterative structure of\nlearning algorithms for efficient hyperparameter tuning. We propose to learn an\nevaluation function compressing learning progress at any stage of the training\nprocess into a single numeric score according to both training success and\nstability. Our BO framework is then balancing the benefit of assessing a\nhyperparameter setting over additional training steps against their computation\ncost. We further increase model efficiency by selectively including scores from\ndifferent training steps for any evaluated hyperparameter set. We demonstrate\nthe efficiency of our algorithm by tuning hyperparameters for the training of\ndeep reinforcement learning agents and convolutional neural networks. Our\nalgorithm outperforms all existing baselines in identifying optimal\nhyperparameters in minimal time.",
      "full_text": "Bayesian Optimization for Iterative Learning Vu Nguyen ∗ University of Oxford vu@robots.ox.ac.uk Sebastian Schulze ∗ University of Oxford sebastian.schulze@eng.ox.ac.uk Michael A. Osborne University of Oxford mosb@robots.ox.ac.uk Abstract The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the ﬁnal performance of hyperparam- eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress- ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train- ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper- parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa- rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1 Introduction Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition [20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable. Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate. In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35] ∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1909.09593v5  [cs.LG]  16 Jan 2021deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps. In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost- ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks. In particular, our algorithm outperforms existing baselines in ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are: • an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance; • an approach to learn the compression curve from the data and a data augmentation tech- nique for increased sample-efﬁciency; • demonstration on tuning DRL and convolutional neural networks. 2 Related Work in Iteration-Efﬁcient Bayesian Optimization The ﬁrst algorithm category employs stopping criteria to terminate some training runs early and allo- cate resources towards more promising settings. These criteria typically involve projecting towards a ﬁnal score from early training stages. Freeze-thaw BO [42] models the training loss over time us- ing a GP regressor under the assumption that the training loss roughly follows an exponential decay. Based on this projection, training resources are allocated to the most promising settings. Hyperband [8, 23] dynamically allocates computational resources (e.g. training epochs or dataset size) through random sampling and eliminates under-performing hyperparameter settings by successive halving. Attempts have also been made to improve the epoch efﬁciency of other hyperparameter optimization algorithms in [5, 7, 18] which predict the ﬁnal learning outcome based on partially trained learning curves to identify hyperparameter settings that are expected to under-perform and early-stop them. In the context of DRL, however, these stopping criteria, including the exponential decay assumed in Freeze-thaw BO [42], may not be applicable, due to the unpredictable ﬂuctuations of DRL reward curves. In the supplement, we illustrate the noisiness of DRL training. The second category [16, 17, 23, 41, 48] aims to reduce the resource consumption of BO by utilizing low-ﬁdelity functions which can be obtained by using a subset of the training data or by training the ML model for a small number of iterations. Multi-task BO [41] requires the user to deﬁne a division of the dataset into pre-deﬁned and discrete subtasks. Multi-ﬁdelity BO with continuous approximation (BOCA) [16] and hierarchical partition [34] extend this idea to continuous settings. Speciﬁcally, BOCA ﬁrst selects the hyperparameter input and then the corresponding ﬁdelity to be evaluated at. The ﬁdelity in this context refers to the use of different number of learning iterations. Analogous to BOCA’s consideration of continuous ﬁdelities, Fabolas [17] proposes to model the combined space of input hyperparameter and dataset size and then select the optimal input and dataset size jointly. The above approaches typically identify performance of hyperparameters via the average (either training or validation) loss of the last learning iterations. Thereby, they do not account for potential noise in the learning process (e.g., they might select unstable settings that jumped to high perfor- mance in the last couple of iterations). 3 Bayesian Optimization for Iterative Learning (BOIL) Problem setting. We consider training a machine learning algorithm given a d-dimensional hy- perparameter x ∈X ⊂Rd for t iterations. This process has a training time costc(x,t) and produces 20 100 200 300 400 500 #Episode t 0.80 0.85 0.90 0.95 1.00x Tmin Tmax Augmented Obs Observation 0 100 200 300 400 500 0 50 100 150 200Score 4 18 34 8 45 5 14 26Reward Curve Sigmoid Func 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Cifar10 m* 0 =-4.0 g* 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for Reacher m* 0 =2.779 g* 0 =1.973 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . |m * 0 ,g* 0 ) for CartPole m* 0 =-3.266 g* 0 =3.0 Figure 1: Left: the score in pink box is a convolution of the reward curve r(·| x = 0.9,t = 500) and a Sigmoid function l(u |g0,m0) = 1 1+exp(−g0[u−m0]) up to time step t. Bottom: observations are selected to augment the dataset (red dots). The heatmap indicates the GP predictive mean µ for f across the number of episodest used to train an agent. Tmin and Tmax are two user-deﬁned thresholds for the number of training episodes. x is a hyperparameter to be tuned. Right: we learn the optimal parameter g∗ 0 and m∗ 0 for each experiment separately. training evaluations r(·| x,t) for t iterations, t ∈[Tmin,Tmax]. These could be episode rewards in DRL or training accuracies in DL. An important property of iterative training is that we know the whole curve at preceding steps r(t′|x,t), ∀t′≤t. Given the raw training curve r(·| x,t), we assume an underlying smoothed black-box function f , deﬁned in Sec. 3.2. Formally, we aim to ﬁnd x∗= argmaxx∈X f (x,Tmax); at the same time, we want to keep the overall training time, ∑N i=1 c(xi,ti), of evaluated settings [xi,ti] as low as possible. We summarize our variables in Table 1 in the supplement for ease of reading. 3.1 Selecting a next point using iteration-efﬁcient modeling We follow popular designs in [17, 19, 39, 41] and model the cost-sensitive black-box function as f (x,t) ∼GP(0,k([x,t],[x′,t′])), where k is an appropriate covariance functions and [x,t] ∈Rd+1. For simplicity and robustness, the cost function c(x,t) is approximated by a linear regressor. De- pending on the setting, it may be more appropriate to employ a second GP or different parametric model if the cost has a more complex dependence on hyperparameters x and iterations t. We regu- larly (re-)optimize both kernel and cost function parameters in between point acquisitions. More speciﬁcally, we choose the covariance function as a productk ([x,t],[x′,t′]) =k(x,x′)×k(t,t′) to induce joint similarities over parameter and iteration space. We estimate the predictive mean and uncertainty for a GP [31] at any input z∗= [x∗,t∗] as µ (z∗) =k∗ [ K +σ2 y I ]−1 y (1) σ2 (z∗) =k∗∗−k∗ [ K +σ2 y I ]−1 kT ∗ (2) where y = [yi]∀i, k∗= [k (z∗,zi)]∀i, K = [k (zi,zj)]∀i, j, k∗∗= k (z∗,z∗), and σ2 y is the noise variance of f . Cost predictions at any particular parameter x and time t are given by µc([x∗,t∗]) =βT [x,t], where β is directly computed from data {Z = [xi,ti],c = [ci]}∀i as β = (ZT Z)−1Zc [1]. Our goal is to select a point with high function value (exploitation), high uncertainty (exploration) and low cost (cheap). At each iteration n, we query the input parameter xn and the number of iteration tn [38, 48]: zn = [xn,tn] = argmax x∈X ,t∈[Tmin,Tmax] α(x,t)/µc(x,t). (3) 3Although our framework is available for any acquisition choices [13, 22, 47], to cope with output noise, we follow [45] and slight modify the expected improvement criterion using the maximum mean GP prediction µmax n . Let λ = µn(z)−µmaxn σn(z) , we then have a closed-form for the new expected improvement (EI) as αEI n (z) =σn (z)φ (λ) + [µn (z)−µmax n ]Φ(λ) where φ is the standard normal p.d.f., Φ is the c.d.f, µn and σn are the GP predictive mean and variance deﬁned in Eq. (1) and Eq. (2), respectively. 3.2 Training curve compression and estimating the transformation function Existing BO approaches [4, 23] typically deﬁne the objective function as an average loss over the ﬁnal learning episodes. However, this does not take into consideration how stable performance is or the training stage at which it has been achieved. We argue that averaging learning losses is likely misleading due to the noise and ﬂuctuations of our observations (learning curves) – particularly during the early stages of training. We propose to compress the whole learning curve into a numeric score via a preference function representing the user’s desired training curve. In the following, we use the Sigmoid function (speciﬁcally the Logistic function) to compute the utility score as y = ˆy(r,m0,g0) =r(·|x,t)•l(·|m0,g0) = t ∑ u=1 r(u |x,t) 1 +exp(−g0 [u −m0]) (4) where •is a dot product, a Logistic function l(·| m0,g0) is parameterized by a growth parameter g0 deﬁning a slope and the middle point of the curve m0. The optimal parameters g0 and m0 are estimated directly from the data. We illustrate different shapes of l parameterized by g0 and m0 in the appendix. The Sigmoid preference has a number of desirable properties. As early weights are small, less credit is given to ﬂuctuations at the initial stages, making it less likely for our surrogate to be biased towards randomly well performing settings. However, as weights monotonically increase, hyperparameters with improving performance are preferred. As weights saturate over time, stable, high performing conﬁgurations are preferred over short “performance spikes” characteristic of un- stable training. Lastly, this utility score assigns higher values to the same performance if it is being maintained over more episodes. Learning the transformation function from data. Different compression curves l(), parameter- ized by different choices of g0 and m0 in Eq. (4), may lead to different utilities y and thus affect the performance. The optimal values of g∗ 0 and m∗ 0 are unknown in advance. Therefore, we propose to learn these values g∗ 0 and m∗ 0 directly from the data. Our intuition is that the ‘optimal’ compression curve l(m∗ 0,g∗ 0) will lead to a better ﬁt of the GP. This better GP surrogate model, thus, will result in better prediction as well as optimization performance. We parameterize the GP log marginal likelihood L [31] as the function of m0 and g0: L(m0,g0) =1 2 ˆyT ( K +σ2 y I )−1 ˆy −1 2 ln ⏐⏐K +σ2 y I ⏐⏐ +const (5) where σ2 y is the output noise variance, ˆy is the function of m0 and g0 deﬁned in Eq. (4). We optimize m0 and g0 (jointly with other GP hyperparameters) using multi-start gradient descent. We derive the derivative ∂L ∂m0 = ∂L ∂ ˆy ∂ ˆy ∂m0 and ∂L ∂g0 = ∂L ∂ ˆy ∂ ˆy ∂g0 which can be computed analytically as: ∂L ∂ ˆy = ( K +σ2 y IN )−1 ˆy; ∂ ˆy ∂m0 = −g0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 ; ∂ ˆy ∂g0 = −m0 ×exp(−g0 [u −m0]) [1 +exp(−g0 [u −m0])]2 . The estimated compression curves are illustrated in Right Fig. 1 and in Sec. 4.1. 3.3 Augmenting the training data When evaluating a parameter x over t iterations, we obtain not only a ﬁnal score but also all reward sequences r(t′|x,t),∀t′= 1,..., t. The auxiliary information from the curve can be useful for BO. Therefore, we propose to augment the information from the curve into the sample set of our GP model. A naïve approach for augmentation is to add a full curve of points {[x, j],yj}t j=1 where yj is computed using Eq. (4). However, this approach can be redundant and may im- pose serious issues in the conditioning of the GP covariance matrix. As we cluster 40.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 x GP variance 2 0 2 4 6 8 10 12 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.80 0.85 0.90 0.95 1.00 200 300 400 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 GP variance 400 320 240 160 80 0 80 160 240 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040 x Figure 2: GP with different settings. Left: our augmentation. Right: using a full curve. If we add too many observations, the GP covariance matrix becomes ill-conditioned. On the right, the GP ﬁt is poor with a large mean estimate range of [−400,240] even though the output is standardized N (0,1). All x-axis are over x, a hyperparameter to be tuned. more evaluations closely, the conditioning of the GP covariance degrades further, as dis- cussed in [24]. This conditioning issue is especially serious in our noisy DRL settings. 0 10 20 30 40 50 60 Iterations 0 5 10 15 20 25Log of Condition Number Condition Number of GP Covariance Augmentation No Augmentation Full Curve Reasonable Threshold Figure 3: The condition number of GP covari- ance matrix deteriorates if we add the whole curve of points into a GP. The large condition number indicates the nearness to singularity. We highlight this effect on GP estimation in Fig. 2 wherein the GP mean varies erratically when the natural log of the condition number of the GP co- variance matrix goes above 25 (see Fig. 3) as we include the whole curve. Selecting subset of points from the curve. Dif- ferent solutions, such as the addition of artiﬁcial noise or altering the kernel’s length-scales, have been proposed. We decide to use an active learn- ing approach [10, 29] as sampled data points are expected to contain a lot of redundant informa- tion. As a consequence, the loss of information from sub-sampling the data should be minimal and information-eroding modiﬁcation of the ker- nel matrix itself can be avoided. As a side beneﬁt, the reduced number of sampled points speeds up inference in our GP models. In particular, we select samples at the maximum of the GP predictive uncertainty. Formally, we sequentially select a set Z = [z1,...zM], zm = [x,tm], by varying tm while keeping x ﬁxed as zm =argmax ∀t′≤t σ([x,t′] |D′),∀m ≤M s.t. lnof cond(K) ≤δ (6) where D′= D∪{zj = [x,tj]}m−1 j=1 . This sub-optimisation problem is done in a one-dimensional space of t′∈{Tmin,..., t}, thus it is cheap to optimize using (multi-start) gradient descent (the derivative of GP predictive variance is available [31]). Alternatively, a ﬁxed-size grid could be considered, but this could cause conditioning issues when a point in the grid [ x,tgrid ] is placed near another existing point [ x′,tgrid ] , i.e., ||x −x′||2 ≤ε for some small ε. These generated points Z are used to calculate the output r(zm) and augmented into the observation set (X,Y ) for ﬁtting the GP. The number of samplesM is adaptively chosen such that the natural log of the condition number of the covariance matrix is less than a threshold. This is to ensure that the GP covariance matrix condition number behaves well by reducing the number of unnecessary points added to the GP at later stages. We compute the utility score ym given zm for each augmented point using Eq. (4). In addition, we can estimate the running time cm using the predictive mean µc(zm). We illustrate the augmented observations and estimated scores in Fig. 1. We summarize the overall algorithm in Alg. 1. To enforce non-negativity and numerical stability, we make use of the transformations α ←log[1 +exp(α)] and µc ←log[1 +exp(µc)]. 4 Experiments We assess our model by tuning hyperparameters for two DRL agents on three environments and a CNN on two datasets. We provide additional illustrations and experiments in the appendix. 5Algorithm 1 Bayesian Optimization with Iterative Learning (BOIL) Input: #iter N, initial data D0, z = [x,t]. Output: optimal x∗and y∗= max∀y∈DN y 1: for n = 1....N do 2: Fit a GP to estimate µf (),σf () from Eqs. (1,2) and a LR for cost µc() 3: Select zn = argmaxx,t α(x,t)/µc(x,t) and observe a curve r and a cost c from f (zn) 4: Compressing the learning curve r(zn) into numeric score using Eq. (4). 5: Sample augmented points zn,m,yn,m,cn,m,∀m ≤M given the curve and Dn in Eq. (6) 6: Augment the data into Dn and estimate Logistic curve hyperparameters m0 and g0. 7: end for Experimental setup. All experimental results are averaged over 20 independent runs with differ- ent random seeds. Final performance is estimated by evaluating the chosen hyperparameter over the maximum number of iterations. All experiments are executed on a NVIDIA 1080 GTX GPU using the tensorﬂow-gpu Python package. The DRL environments are available through the OpenAI gym [3] and Mujoco [43]. Our DRL implementations are based on the open source from Open AI Baselines [6]. We release our implementation at https://github.com/ntienvu/BOIL. We use square-exponential kernels for the GP in our model and estimate their parameters by maxi- mizing the marginal likelihood [31]. We set the maximum number of augmented points to beM = 15 and a threshold for a natural log of GP condition numberδ = 20. We note that the optimization over- head is much less than the black-box function evaluation time. Baselines. We compare with Hyperband [23] which demonstrated empirical successes in tuning deep learning applications in an iteration-efﬁcient manner. We extend the discrete multi-task BO [41] to the continuous case – which can also be seen as continuous multi-ﬁdelity BO [16, 39] as in our setting, they both consider cost-sensitivity and iteration-efﬁciency. We, therefore, label the two baselines as continuous multi-task/ﬁdelity BO (CM-T/F-BO). We have ignored the minor difference in these settings, such as multi-task approaches jointly optimizes the ﬁdelity and input while BOCA [16] ﬁrst selects the input and then the ﬁdelity. Our focus is to demonstrate the effectiveness of optimizing the learning curve using compression and augmentation techniques. We therefore omit the comparison of various acquisition functions and kernel choices which can easily be used in our model. We also do not compare with Fabolas [17] which is designed to vary dataset sizes, not iteration numbers. We would expect the performance of Fabolas to be close to CM-T/F-BO. We are unable to compare with FreezeThaw as the code is not available. However, the curves in our setting are not exponential decays and thus ill-suited to their model (see last ﬁgure in the appendix). We have considered an ablation study in the appendix using a time kernel following the exponential decay proposed in Freeze-thaw method [42]. Task descriptions. We consider three DRL settings including a Dueling DQN (DDQN) [46] agent in the CartPole-v0 environment and Advantage Actor Critic (A2C) [25] agents in the InvertedPendulum-v2 and Reacher-v2 environments. In addition to the DRL applications, we tune 6 hyperparameters for training a convolutional neural network [21] on the SVHN dataset and CI- FAR10. Due to space considerations, we refer to the appendix for further details. 4.1 Model illustration /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018/uni00000006/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 /uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000024/uni00000058/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000047/uni00000003/uni00000032/uni00000045/uni00000056 Figure 4: DDQN on CartPole. The number of augmented observations reduces over time. We ﬁrst illustrate the estimated compression func- tion l(m∗ 0,g∗ 0) in Right Fig. 1 from different experi- ments. These Logistic parameters g∗ 0 and m∗ 0 are es- timated by maximizing the GP marginal likelihood and used for compressing the curve. We show that the estimated curve from CartPole tends to reach the highest performance much earlier than Reacher because CartPole is somewhat easier to train than Reacher. We next examine the count of augmented observa- tions generated per iteration in Fig. 4. Although this number is ﬂuctuating, it tends to reduce over 6/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni0000001a/uni00000013 /uni00000019/uni00000013 /uni00000018/uni00000013 /uni00000017/uni00000013 /uni00000016/uni00000013 /uni00000015/uni00000013 /uni00000014/uni00000013 /uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000014/uni00000017/uni00000013/uni00000013 /uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni0000001a/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000015/uni00000018 /uni00000014/uni00000018/uni00000013 /uni00000014/uni0000001a/uni00000018 /uni00000015/uni00000013/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000059/uni00000033/uni00000048/uni00000051/uni00000040/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000058/uni00000056/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 5: The learning curves of the best found parameters by different approaches. The curves show that BO-L and BOIL reliably identify parameters leading to stable training. BOIL takes only half total time to ﬁnd this optimal curve. time. BOIL does not add more augmented observations at the later stage when we have gained sufﬁcient information and GP covariance conditioning falls below our threshold δ = 20. 4.2 Ablation study of curve compression To demonstrate the impact of our training curve compression, we compare BOIL to vanilla Bayesian optimization (BO) and with compression (BO-L) given the same number of iterations at Tmax. We show that using the curve compression leads to stable performance, as opposed to the existing tech- nique of averaging the last iterations. We plot the learning curves of the best hyperparameters identiﬁed by BO, BO-L and BOIL. Fig. 5 shows the learning progress over Tmax episodes for each of these. The curves are smoothed by averaging over 100 consecutive episodes for increased clarity. We ﬁrst note that all three algorithms eventually obtain similar performance at the end of learning. However, since BO-L and BOIL take into account the preceding learning steps, they achieve higher performance more quickly. Furthermore, they achieve this more reliably as evidenced by the smaller error bars (shaded regions). 4.3 Tuning deep reinforcement learning and CNN We now optimize hyperparameters for deep reinforcement learning algorithms; in fact, this applica- tion motivated the development of BOIL. The combinations of hyperparameters to be tuned, target DRL algorithm and environment can be found in the appendix. Comparisons by iterations and real-time. Fig. 6 illustrates the performance of different algo- rithms against the number of iterations as well as real-time (the plots for CIFAR10 are in the ap- pendix). The performance is the utility score of the best hyperparameters identiﬁed by the baselines. Across all three tasks, BOIL identiﬁes optimal hyperparameters using signiﬁcantly less computation time than other approaches. The plots show that other approaches such as BO and BO-L can identify well-performing hyperpa- rameters in fewer iterations than BOIL. However, they do so only considering costly, high-ﬁdelity evaluations resulting in signiﬁcantly higher evaluation times. In contrast to this behavior, BOIL ac- counts for the evaluation costs and chooses to initially evaluate low-ﬁdelity settings consuming less time. This allows fast assessments of a multitude of hyperparameters. The information gathered here is then used to inform later point acquisitions. Hereby, the inclusion of augmented observations is crucial in offering useful information readily available from the data. In addition, this augmenta- tion is essential to prevent from the GP kernel issue instead of adding the full curve of points into our GP model. Hyperband [23] exhibits similar behavior in that it uses low ﬁdelity (small t) evaluations to reduce a pool of randomly sampled conﬁgurations before evaluating at high ﬁdelity (large t). To deal with noisy evaluations and other effects, this process is repeated several times. This puts Hyperband at a disadvantage particularly in the noisy DRL tasks. Since early performance ﬂuctuates hugely, Hyperband can be misled in where to allocate evaluation effort. It is then incapable of revising 7/uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000015/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000016/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000016/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000016/uni0000001a/uni00000011/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000027/uni00000027/uni00000034/uni00000031/uni00000010/uni00000026/uni00000044/uni00000055/uni00000057/uni00000033/uni00000052/uni0000004f/uni00000048/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000003/uni00000044/uni00000051/uni00000047/uni00000003 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000015/uni00000018 /uni00000016/uni00000013 /uni00000016/uni00000018 /uni00000017/uni00000013 /uni00000017/uni00000018 /uni00000018/uni00000013 /uni00000018/uni00000018 /uni00000019/uni00000013 /uni00000019/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000036/uni00000039/uni0000002b/uni00000031/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 6: Comparison over BO evaluations (Left) and real-time (Right). Given the same time bud- get, CM-T/F-BO, Hyperband and BOIL can take more evaluations than vanilla BO, BO-L and Rand. BOIL outperforms other competitors in ﬁnding the optimal parameters in an iteration-efﬁcient man- ner. CM-T/F-BO does not augment the observations from the curve and requires more evaluations. The results of InvertedPendulum and CNN-CIFAR10 are in the appendix. these choices until an entirely new pool of hyperparameters is sampled and evaluated from scratch. In contrast to this, BOIL is more ﬂexible than Hyperband in that it can freely explore-exploit the whole joint space. The GP surrogate hereby allows BOIL to generalize across hyperparameters and propagate information through the joint space. 5 Conclusion and Future work Our framework complements the existing BO toolbox for hyperparameter tuning with iterative learn- ing. We present a way of leveraging our understanding that later stages of the training process are informed by progress made in earlier ones. This results in a more iteration-efﬁcient hyperparame- ter tuning algorithm that is applicable to a broad range of machine learning systems. We evaluate its performance on a set of diverse benchmarks. The results demonstrate that our model surpasses the performance of well-established alternatives while consuming signiﬁcantly fewer resources. Fi- nally, we note that our approach is not necessarily speciﬁc to machine learning algorithms, but more generally applies to any process exhibiting an iterative structure to be exploited. 86 Broader Impact Our work aims at making the optimization of processes operating in a step-wise fashion more efﬁ- cient. As demonstrated this makes BOIL particularly well-suited to supporting supervised learning models and RL systems. By increasing training efﬁcience of these models, we hope to contribute to their widespread deployment whilst reducing the computational and therefore environmental cost their implementation has. Deep (reinforcement) learning systems ﬁnd application in a wide range of settings that directly contribute to real world decisions, e.g., natural language processing, visual task, autonomous driving and many more. As machine learning models building on our contributions are being deployed in the real world, we encourage practicioners to put in place necessary supervision and override mechanisms as precautions against potential failure. In a more general context, our algorithm may be seen as a step towards the construction of an automated pipeline for the training and deployment of machine learning models. A potential danger is that humans become further and further removed from the modelling process, making it harder to spot (potentially critical) failures. We do not see this as an argument against the construction of such a pipeline in principle, but instead encourage practicioners to reﬂect on potential biases indirectly encoded in the choice of data sets and models, they are feeding into said automated processes. The growing opacity of machine learning models is a concern of its own and which automated training procedures will only contribute to. Opposing this is a rapidly growing corpus of work addressing the interpretability of trained machine learning models and their decision making. These can and should be used to rigorously analyse ﬁnal training outcomes. Only then can we ensure that machine learning algorithm do indeed become a beneﬁcial source of information guiding real world policy making as opposed to opaque, unquestioned entities. While our main interest lies in the hyperparameter optimization of machine learning models, it should be noted that any iterative process depending on a set of parameters can make use of our con- tributions. Possible settings could, for instance, include the optimization of manufacturing pipelines in which factory setting are adjusted to increase productivity. 7 Acknowledgements S. Schulze is supported by an I-CASE studentship funded by the EPSRC and Dyson. References [1] Christopher M Bishop. Pattern recognition and machine learning. springer New York, 2006. [2] Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on Bayesian optimization of ex- pensive cost functions, with application to active user modeling and hierarchical reinforcement learning. arXiv preprint arXiv:1012.2599, 2010. [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [4] Yutian Chen, Aja Huang, Ziyu Wang, Ioannis Antonoglou, Julian Schrittwieser, David Silver, and Nando de Freitas. Bayesian optimization in AlphaGo. arXiv preprint arXiv:1812.06855, 2018. [5] Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimiza- tion meets Bayesian optimal stopping. In International Conference on Machine Learning , pages 1496–1506, 2019. [6] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. GitHub, GitHub repository, 2017. [7] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hy- perparameter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015. 9[8] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efﬁcient hyperparameter optimization at scale. In International Conference on Machine Learning , pages 1436–1445, 2018. [9] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811, 2018. [10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In Proceedings of the 34th International Conference on Machine Learning, pages 1183– 1192, 2017. [11] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [12] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti- mization. Journal of Machine Learning Research, 13:1809–1837, 2012. [13] José Miguel Hernández-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy search for efﬁcient global optimization of black-box functions. In Advances in Neural Information Processing Systems, pages 918–926, 2014. [14] Donald R Jones, Matthias Schonlau, and William J Welch. Efﬁcient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455–492, 1998. [15] M. I. Jordan and T. M. Mitchell. Machine learning: Trends, perspectives, and prospects. Science, 349(6245):255–260, 2015. [16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabás Póczos. Multi- ﬁdelity Bayesian optimisation with continuous approximations. In Proceedings of the 34th International Conference on Machine Learning, pages 1799–1808, 2017. [17] Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian optimization of machine learning hyperparameters on large datasets. In Artiﬁcial Intelligence and Statistics, pages 528–536, 2017. [18] Aaron Klein, Stefan Falkner, Jost Tobias Springenberg, and Frank Hutter. Learning curve pre- diction with Bayesian neural networks. International Conference on Learning Representations (ICLR), 2017. [19] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In Advances in Neural Information Processing Systems, pages 2447–2455, 2011. [20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105, 2012. [21] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [22] Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy, et al. Constrained Bayesian optimization with noisy experiments. Bayesian Analysis, 14(2):495–519, 2019. [23] Lisha Li and Kevin Jamieson. Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18:1–52, 2018. [24] Mark McLeod, Stephen Roberts, and Michael A Osborne. Optimization, fast and slow: Op- timally switching between local and Bayesian optimization. In International Conference on Machine Learning, pages 3440–3449, 2018. [25] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928–1937, 2016. [26] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. [27] Vu Nguyen, Sunil Gupta, Santu Rana, Cheng Li, and Svetha Venkatesh. Regret for expected improvement over the best-observed value and stopping condition. In Proceedings of The 9th Asian Conference on Machine Learning (ACML), pages 279–294, 2017. [28] Vu Nguyen and Michael A Osborne. Knowing the what but not the where in Bayesian opti- mization. In International Conference on Machine Learning, pages 7317–7326, 2020. 10[29] Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and Carl E Rasmussen. Active learning of model evidence using Bayesian quadrature. In Advances in Neural Information Processing Systems, pages 46–54, 2012. [30] Jack Parker-Holder, Vu Nguyen, and Stephen Roberts. Provably efﬁcient online hyperparame- ter optimization with population-based bandits. In Advances in Neural Information Processing Systems, 2020. [31] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006. [32] Binxin Ru, Mark McLeod, Diego Granziol, and Michael A Osborne. Fast information-theoretic Bayesian optimisation. In International Conference on Machine Learning, pages 4381–4389, 2018. [33] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. International Conference on Learning Representations, 2016. [34] Rajat Sen, Kirthevasan Kandasamy, and Sanjay Shakkottai. Multi-ﬁdelity black-box opti- mization with hierarchical partitions. In International conference on machine learning, pages 4538–4547, 2018. [35] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando de Freitas. Taking the human out of the loop: A review of Bayesian optimization. Proceedings of the IEEE , 104(1):148–175, 2016. [36] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc- tot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016. [37] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1–learning rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820, 2018. [38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical Bayesian optimization of ma- chine learning algorithms. In Advances in Neural Information Processing Systems , pages 2951–2959, 2012. [39] Jialin Song, Yuxin Chen, and Yisong Yue. A general framework for multi-ﬁdelity Bayesian optimization with Gaussian processes. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages 3158–3167, 2019. [40] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, pages 1015–1022, 2010. [41] Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task Bayesian optimization. In Advances in Neural Information Processing Systems, pages 2004–2012, 2013. [42] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896, 2014. [43] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012. [44] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization. In International Conference on Machine Learning, pages 3627–3635, 2017. [45] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown Gaussian process hyper-parameters. arXiv preprint arXiv:1406.7758, 2014. [46] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International Conference on Machine Learning, pages 1995–2003, 2016. [47] Jian Wu and Peter Frazier. The parallel knowledge gradient method for batch Bayesian opti- mization. In Advances In Neural Information Processing Systems, pages 3126–3134, 2016. [48] Jian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi- ﬁdelity Bayesian optimization for hyperparameter tuning. In 35th Conference on Uncertainty in Artiﬁcial Intelligence, 2019. 11The following sections are intended to give the reader further insights into our design choices and a deeper understanding of the algorithms properties. First, we give a brief overview of Bayesian optimization with Gaussian processes. We then illustrate our models behavior on a two dimensional problem. Last, we give further details of our experiments for reproducibility purposes. A Bayesian Optimization Preliminaries Bayesian optimization is a sequential approach to global optimization of black-box functions with- out making use of derivatives. It uses two components: a learned surrogate model of the objective function and an acquisition function derived from the surrogate for selecting new points to inform the surrogate with. In-depth discussions beyond our brief overview can be found in recent surveys [2, 9, 35]. Notation. We summarize all of the notations used in our model in Table 1 for ease of reading. A.1 Gaussian processes We present the GP surrogate model for the black-box function f [31]. A GP deﬁnes a probability distribution over functions f under the assumption that any subset of points {(xi, f (xi)}is normally distributed. Formally, this is denoted as: f (x) ∼GP ( m(x),k ( x,x′)) , where m(x) and k (x,x′) are the mean and covariance functions, given by m(x) =E[ f (x)] and k(x,x′) =E [ ( f (x)−m(x))( f (x′)−m(x′))T ] . Typically, the mean of the GP is assumed to be zero everywhere. The kernel k(x,x′) can be thought of as a similarity measure relating f (x) and f (x′). Numerous kernels encoding different prior be- liefs about f (x) have been proposed. A popular choice is given by the square exponential kernel k(x,x′) =σ2 f exp [ −(x −x′)2/2σ2 l ] . The length- and output-scales σ2 l and σ2 f regulate the maximal covariance between two points and can be estimated using maximum marginal likelihood. The SE kernel encodes the belief that nearby points are highly correlated as it is maximized at k(x,x) =σ2 f and decays the further x and x′are separated. For predicting f∗= f (x∗) at a new data point x∗, assuming a zero mean m(x) =0, we have: [ f f∗ ] ∼N ( 0, [ K kT ∗ k∗ k∗∗ ]) (7) where k∗∗= k (x∗,x∗), k∗= [k (x∗,xi)]∀i≤N and K = [k (xi,xj)]∀i, j≤N . The conditional probability of p( f∗|f ) follows a univariate Gaussian distribution as p( f∗|f ) ∼N ( µ (x∗),σ2 (x∗) ) . Its mean and variance are given by: µ (x∗) =k∗K−1y σ2 (x∗) =k∗∗−k∗K−1kT ∗. As GPs give full uncertainty information with any prediction, they provide a ﬂexible nonparametric prior for Bayesian optimization. We refer the interested readers to [31] for further details on GPs. A.2 Acquisition function Bayesian optimization is typically applied in settings in which the objective function is expensive to evaluate. To minimize interactions with that objective, an acquisition function is deﬁned to reason about the selection of the next evaluation point xt+1 = argmaxx∈X αt (x). The acquisition func- tion is constructed from the predictive mean and variance of the surrogate to be easy to evaluate and represents the trade-off between exploration (of points with high predictive uncertainty) and exploitation (of points with high predictive mean). Thus, by design the acquisition function can be maximized with standard global optimization toolboxes. Among the many acquisition functions [12, 13, 14, 32, 40, 44] available in the literature, the expected improvement [14, 27, 45] is one of the most popular. 12Table 1: Notation List Parameter Domain Meaning d integer, N dimension, no. of hyperparameters to be optimized x vector,Rd input hyperparameter N integer, N maximum number of BO iterations Tmin, Tmax integer, N the min/max no of iterations for training a ML algorithm t ∈[Tmin,...Tmax] index of training steps M integer, N the maximum number of augmentation. We set M = 15. δ scalar, R threshold for rejecting augmentation when ln of cond(K) > δ m ∈{1,...M} index of augmenting variables n ∈{1,..., N} index of BO iterations z = [x,t] vector, Rd+1 concatenation of the parameter x and iteration t cn,m scalar, R training cost (sec) yn scalar, R transformed score at the BO iteration n yn,m scalar, R transformed score at the BO iteration n, training step m α(x,t) function acquisition function for performance µc(x,t) function estimation of the cost by LR given x and t r(. |x,t) function a raw learning curve, r(x,t) = [r(1 |x,t),...r(t′|x,t),r(t |x,t)] f (x,t) function a black-box function which is compressed from the above f () l (. |m0,g0) function Logistic curve l(u |m0,g0) = 1 1+exp(−g0[u−m0]) g0, g∗ 0 scalar, R a growth parameter deﬁning a slope, g∗ 0 = argmaxg0 L m0, m∗ 0 scalar, R a middle point parameter, m∗ 0 = argmaxm0 L L scalar, R Gaussian process log marginal likelihood A.3 GP kernels and treatment of GP hyperparameters We present the GP kernels and treatment of GP hyperparameters for the black-box function f . Although the raw learning curve in DRL is noisy, the transformed version using our proposed curve compression is smooth. Therefore, we use two squared exponential kernels for input hyperparameter and training iteration, respectively. That iskx(x,x′) =exp ( −||x−x′||2 2σ2x ) and kt (t,t′) =exp ( −||t−t′||2 2σ2t ) where the observation x and t are normalized to [0,1]d and the outcome y is standardized y ∼ N (0,1) for robustness. As a result, our product kernel becomes k ( [x,t],[x′,t′] ) = k(x,x′)×k(t,t′) =exp ( −||x −x′||2 2σ2x −||t −t′||2 2σ2t ) . The length-scales σx and σt are learnable parameters indicating the variability of the function with regards to the hyperparameter input x and number of training iterations t. Estimating appropriate values for them is critical as this represents the GPs prior regarding the sensitivity of performance w.r.t. changes in the number of training iterations and hyperparameters. For extremely large σt we expect the objective function to change very little for different numbers of training iterations. For small σt by contrast we expect drastic changes even for small differences. We estimate these GP hyperparameters (including the length-scalesσx, σt and the output noise varianceσy) by maximizing their log marginal likelihood [31]. We optimize Eq. (5) with a gradient-based optimizer, providing the analytical gradient to the algo- rithm. We start the optimization from the previous hyperparameter values θprev. If the optimization fails due to numerical issues, we keep the previous value of the hyperparameters. We reﬁt the hy- perparameters every 3×d function evaluations where d is the dimension. B Algorithm Illustration and Further Experiments Fig. 7 and Fig. 8 illustrate the behavior of our proposed algorithm BOIL on the example of opti- mizing the discount factor γ of Dueling DQN [46] on the CartPole problem. The two settings differ in the inclusion augmented observations into BOIL in Fig. 7 and CM-T/F-BO (or BOIL without augmented observations) in Fig. 8. 130.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.06 0.12 0.18 0.24 0.30 0.36 0.42 0.48 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.4 1.6 0.8 0.0 0.8 1.6 2.4 3.2 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 1.36 0.0 0.6 1.2 1.8 2.4 3.0 3.6 4.2 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.64 0.72 0.80 0.88 0.96 1.04 1.12 1.20 1.28 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 3.2 3.6 Figure 7: Illustration of BOIL on a 2-dimensional optimization task of DDQN on CartPole. The augmented observations ﬁll the joint hyperparameter-iteration space quickly to inform our surrogate. Our decision balances utility α against cost τ for iteration-efﬁciency. Especially in situations of multiple locations sharing the same utility value, our algorithm prefers to select the cheapest option. Table 2: Dueling DQN algorithm on CartPole problem. Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .95586 learning rate model 1 e−6 0.01 0 .00589 #Episodes 300 800 - 140.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.0 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 1.80 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 1.6 0.00 0.08 0.16 0.24 0.32 0.40 0.48 0.56 0.64 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 1.65 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.80 0.85 0.90 0.95 1.00 200 250 300 350 400 450 500#Episode GP mean Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 gamma GP variance 0.80 0.85 0.90 0.95 1.00 Acquisition  Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Cost c Augmented Obs Obs 0.80 0.85 0.90 0.95 1.00 Decision  / c Selected 2.0 1.6 1.2 0.8 0.4 0.0 0.4 0.8 1.2 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 Figure 8: Illustration of the Continuous Multi task/ﬁdelity BO (CM-T/F-BO) -- this is the case of BOIL without using augmented observations (same setting as Fig. 7). This version leads to less efﬁcient optimization as the additional iteration dimension requires more evaluation than optimizing the hyperparameters on their own. 15Table 3: A2C algorithm on Reacher (left) and InvertedPendulum (right). Variables Min Max Best Found x∗ γ discount factor 0 .8 1 0 .8 learning rate actor 1 e−6 0.01 0 .00071 learning rate critic 1 e−6 0.01 0 .00042 #Episodes 200 500 - Min Max Best Found x∗ 0.8 1 0 .95586 1e−6 0.01 0 .00589 1e−6 0.01 0 .00037 700 1500 - Table 4: Convolutional Neural Network. Variables Min Max Best Found x∗ ﬁlter size 1 8 5 pool size 1 5 5 batch size 16 1000 8 learning rate 1 e−6 0.01 0 .000484 momentum 0 .8 0 .999 0 .82852 decay 0 .9 0 .999 0 .9746 number of epoch 30 150 - In both cases, we plot the GP predictive mean in Eq. (1), GP predictive variance in Eq. (2), the acquisition function in Eq. (3), the predicted function and the ﬁnal decision function in Eq. (8). These equations are deﬁned in the main manuscript. As shown in the respective ﬁgures the ﬁnal decision function balances between utility and cost of any pair (γ,t) to achieve iteration efﬁciency. Especially in situations where multiple locations share the same utility value, our decision will prefer to select the cheapest option. Using the augmented observations in Fig. 7, our joint space is ﬁlled quicker with points and the uncertainty (GP variance) across it reduces faster than in Fig. 8 – the case of vanilla CM-T/F-BO without augmenting obser- vations. A second advantage of having augmented observations is that the algorithm is discouraged to select the same hyperparameter setting at lower ﬁdelity than a previous evaluation. We do not add the full curve as it can be redundant while causing the conditioning problem of the GP covariance matrix. B.1 Experiment settings We summarize the hyperparameter search ranges for A2C on Reacher and InvertedPendulum in Table 3, CNN on SHVN in Table 4 and DDQN on CartPole in Table 2. Additionally, we present the best found parameter x∗for these problems. Further details of the DRL agents are listed in Table 5. B.2 Learning Logistic Function We ﬁrst present the Logistic curve l(u |x,t) = 1 1+exp(−g0[u−m0]) using different choices of g0 and m0 in Fig. 10. We then learn from the data to get the optimal choices g∗ 0 and m∗ 0 presented in Fig. 11. Table 5: Further speciﬁcation for DRL agents Hyperparameter Value A2C Critic-network architecture [32,32] Actor-network architecture [32,32] Entropy coefﬁcient 0 .01 Dueling DQN Q-network architecture [50,50] ε-greedy (start, ﬁnal, number of steps) (1.0,0.05,10000) Buffer size 10000 Batch size 64 PER-α [33] 1 .0 PER-β (start, ﬁnal, number of steps) (1.0,0.6,1000) 160 100 200 300 400 500 Episodes 70 60 50 40 30 20 Average Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 90 80 70 60 50 40 30 20 10 Reward Preference Curve as Sigmoid Best Found Reward Curve Sigmoid Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 10 Average Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 100 80 60 40 20 0 Reward Preference Curve as Log Best Found Reward Curve Log Curve 0 100 200 300 400 500 Episodes 60 50 40 30 20 Average Reward Preference Curve as Average Best Found Reward Curve Average Curve 0 100 200 300 400 500 Episodes 80 60 40 20 0 Reward Preference Curve as Average Best Found Reward Curve Average Curve Figure 9: To highlight the robustness, we examine the results using different preference functions such as Sigmoid curve, Log curve, and Average curve on Reacher experiments. The results include the best found reward curve with different preference choices that show the robustness of our model. Left column: the best found curve using averaged reward over 100 consecutive episodes. Right column: the best found curve using the original reward. 17/uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000016/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001a m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000015/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000010/uni00000014/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000013/uni00000003g0/uni00000020/uni00000016 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000019 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000013/uni00000011/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000014 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000015 /uni00000018/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018/uni00000018/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000018 /uni00000014/uni00000011/uni00000013/uni00000013 m0/uni00000020/uni00000014/uni00000003g0/uni00000020/uni00000016 Figure 10: Examples of Logistic function l(u) = 1 1+exp(−g0[u−m0]) with different values of middle parameter m0 and growth parameter g0. B.3 Robustness over Different Preference Functions We next study the learning effects with respect to different choices of the preference functions. We pick three preference functions including the Sigmoid, Log and Average to compute the utility score for each learning curve. Then, we report the best found reward curve under such choices. The experiments are tested using A2C on Reacher-v2. The results presented in Fig. 9 demonstrate the robustness of our model with the preference functions. B.4 Applying Freeze-Thaw BO in the settings considered While both the exponential decay in Freeze-Thaw BO [42] and our compression function encode preferences regarding training development, there is an important distinction between the two ap- proaches. Freeze-thaw BO utilises the exponential decay property to terminate the training curve, while BOIL only uses the sigmoid curve to guide the search. We refer to Fig. 13 for further illustra- tion of why Freeze-thaw BO struggles in DRL settings. B.5 Ablation Study using Freeze-Thraw Kernel for Time In the joint modeling framework of hyperparameter and time (iteration), we can replace the kernel either k(x,x) or k(t,t) with different choices. We, therefore, set up a new baseline of using the time- kernel k(t,t′) in Freeze-Thaw approach [42] which encodes the monotonously exponential decay from the curve. Particularly, we use the kernel deﬁned as k(t,t′) = βα (t +t′+β)α for parameters α,β > 0 which are optimized in the GP models. 186  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for CartPole m * 0 =-3.266   g * 0 =3.0 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for CNN_SHVN m * 0 =2.245   g * 0 =2.092 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l(m * 0 ,g * 0 ) for InvPendulum m * 0 =1.649   g * 0 =1.833 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Cifar10 m * 0 =-4.0   g * 0 =1.476 6  4  2  0 2 4 6 0.0 0.2 0.4 0.6 0.8 1.0 Estimated l( . | m * 0 ,g * 0 ) for Reacher m * 0 =2.779   g * 0 =1.973 Figure 11: We learn the suitable transformation curve directly from the data. We parameterized the Logistic curve as l (m0,g0) = 1 1+exp(−g0[1−m0]) then estimate g0 and m0. The estimated function l(m∗ 0,g∗ 0) is then used to compress our curve. The above plots are the estimated l() at different environments and datasets. /uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001b/uni00000013 /uni0000001c/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000015/uni00000013/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000013 /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f /uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013 /uni00000015/uni00000015/uni00000011/uni00000018 /uni00000015/uni00000018/uni00000011/uni00000013 /uni00000015/uni0000001a/uni00000011/uni00000018 /uni00000016/uni00000013/uni00000011/uni00000013 /uni00000016/uni00000015/uni00000011/uni00000018/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c /uni0000003e/uni00000026/uni00000031/uni00000031/uni00000010/uni00000026/uni0000004c/uni00000049/uni00000044/uni00000055/uni00000014/uni00000013/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000019/uni00000003/uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056 /uni00000035/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032 /uni00000025/uni00000032/uni00000010/uni0000002f /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032 /uni0000002b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000045/uni00000044/uni00000051/uni00000047 /uni00000025/uni00000032/uni0000002c/uni0000002f Figure 12: Tuning hyperparameters of a DRL on InvertedPendulum and a CNN model on CIFAR10. 190 250 500 750 1000 1250 1500 Epoch 0 20 40 60 80 100 120Reward Reward Curve Freeze-thaw 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 0 250 500 750 1000 1250 1500 Epoch 0 50 100 150 Reward Curves Examples using A2C on Inverted Pendulum Figure 13: Illustration of Freeze-thaw BO in DRL. Freeze-thaw BO will terminate training processes when training performance (in blue) signiﬁcantly drops (i.e. at the red locations) as the exponential decay model will predict low ﬁnal performance. In most RL enviroments noisy training curves are unavoidable. Thus, Freeze-thaw BO will dismiss all curves including good setting, never completing a single training run before the ﬁnal epoch. /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni00000014/uni00000015/uni0000001b /uni00000014/uni00000015/uni0000001c /uni00000014/uni00000016/uni00000013 /uni00000014/uni00000016/uni00000014 /uni00000014/uni00000016/uni00000015 /uni00000014/uni00000016/uni00000016 /uni00000014/uni00000016/uni00000017/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni00000035/uni00000048/uni00000044/uni00000046/uni0000004b/uni00000048/uni00000055/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013 /uni00000025/uni00000032/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000010/uni00000030/uni00000012/uni00000037/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000013 /uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000016/uni00000013/uni00000013 /uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000030/uni0000004c/uni00000051/uni00000058/uni00000057/uni00000048/uni00000056/uni0000000c /uni0000001c/uni00000013 /uni0000001c/uni00000018 /uni00000014/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000018 /uni00000014/uni00000014/uni00000013 /uni00000014/uni00000014/uni00000018 /uni00000014/uni00000015/uni00000013 /uni00000014/uni00000015/uni00000018/uni00000038/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048 /uni0000003e/uni00000024/uni00000015/uni00000026/uni00000010/uni0000002c/uni00000051/uni00000059/uni00000048/uni00000055/uni00000057/uni00000048/uni00000047/uni00000033/uni00000048/uni00000051/uni00000047/uni00000058/uni0000004f/uni00000058/uni00000050/uni00000040/uni00000003/uni00000037/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000000f/uni000000031/uni00000003/uni00000044/uni00000051/uni00000047/uni000000032 /uni00000026/uni00000030/uni00000010/uni00000037/uni00000012/uni00000029/uni00000010/uni00000025/uni00000032/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a /uni00000025/uni00000032/uni0000002c/uni0000002f/uni00000003/uni00000029/uni00000055/uni00000048/uni00000048/uni0000005d/uni00000048/uni00000037/uni0000004b/uni00000044/uni0000005a Figure 14: Comparison using freezethaw kernel for time component. We present the result in Fig. 14 that CM-T/F-BO is still less competitive to BOIL using this speciﬁc time kernel. The results again validate the robustness our approach cross different choices of kernel. B.6 Additional Experiments for Tuning DRL and CNN We present the additional experiments for tuning a DRL model using InvertedPendulum environ- ment and a CNN model using a subset of CIFAR10 in Fig. 12. Again, we show that the proposed model clearly gain advantages against the baselines in tuning hyperparameters for model with itera- tive learning information available. B.7 Examples of Deep Reinforcement Learning Training Curves Finally, we present examples of training curves produced by the deep reinforcement learning al- gorithm A2C in Fig. 15. These ﬂuctuate widely and it may not be trivial to deﬁne good stopping criteria as done for other applications in previous work [42]. 200 200 400 80 70 60 50 40 0 200 400 110 100 90 80 70 60 50 0 200 400 110 100 90 80 70 60 50 0 200 400 70 60 50 40 30 0 200 400 70 60 50 40 30 20 10 0 200 400 50 40 30 20 0 200 400 90 80 70 60 50 0 200 400 70 60 50 40 30 20 10 0 200 400 85 80 75 70 0 200 400 60 50 40 30 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 0 200 400 100 90 80 70 0 200 400 80 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 70 60 50 40 30 20 0 200 400 100 90 80 70 60 50 0 200 400 100 90 80 70 60 0 200 400 60 50 40 30 20 0 200 400 80 60 40 20 0 200 400 85 80 75 70 65 60 0 200 400 100 95 90 85 80 0 200 400 80 60 40 20 0 200 400 100 90 80 70 60 50 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0 500 1000 1500 0 50 100 150 200 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 15: Examples of reward curves using A2C on Reacher-v2 (rows 1 −3) and on InvertedPendulum-v2 (rows 4 −6). Y-axis is the reward averaged over 100 consecutive episodes. X-axis is the episode. The noisy performance illustrated is typical of DRL settings and complicates the design of early stopping criteria. Due to the property of DRL, it is not trivial to decide when to stop the training curve. In addition, it will be misleading if we only take average over the last 100 iterations. 21",
      "meta_data": {
        "arxiv_id": "1909.09593v5",
        "authors": [
          "Vu Nguyen",
          "Sebastian Schulze",
          "Michael A Osborne"
        ],
        "published_date": "2019-09-20T16:14:34Z",
        "pdf_url": "https://arxiv.org/pdf/1909.09593v5.pdf",
        "github_url": "https://github.com/ntienvu/BOIL"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper presents a Bayesian Optimization (BO) approach, termed BOIL, for efficient hyperparameter tuning of deep (reinforcement) learning systems that exploit their iterative training structure. It introduces a method to compress the entire learning curve into a single numeric score based on training success and stability, rather than relying solely on final performance. The framework balances the cost of additional training steps against their assessment benefit. It further enhances efficiency through a selective data augmentation technique that leverages intermediate information from the iterative process, demonstrating superior performance in identifying optimal hyperparameters in minimal wall-clock time for DRL agents and convolutional neural networks.",
        "methodology": "The BOIL algorithm models the cost-sensitive black-box function as a Gaussian Process (GP) over the joint space of input hyperparameters (x) and number of training iterations (t), using a product kernel. It compresses the entire learning curve into a numeric utility score using a Sigmoid (Logistic) preference function, whose growth (g0) and middle point (m0) parameters are learned by maximizing the GP's log marginal likelihood. To improve sample-efficiency and prevent GP covariance matrix ill-conditioning, it employs a selective data augmentation technique. This technique samples a subset of points from the observed learning curve at locations of maximum GP predictive uncertainty, dynamically controlling the number of augmented points based on a condition number threshold. The next hyperparameter and iteration count are chosen by maximizing an acquisition function (a modified Expected Improvement criterion) normalized by the predicted training cost (approximated by a linear regressor).",
        "experimental_setup": "All experiments were averaged over 20 independent runs using NVIDIA 1080 GTX GPUs and the TensorFlow-GPU Python package. The algorithm was evaluated on Deep Reinforcement Learning (DRL) tasks: a Dueling DQN (DDQN) agent in the CartPole-v0 environment, and Advantage Actor Critic (A2C) agents in the InvertedPendulum-v2 and Reacher-v2 environments (from OpenAI gym and Mujoco). It was also tested on tuning hyperparameters for a Convolutional Neural Network (CNN) on the SVHN and CIFAR10 datasets. Square-exponential kernels were used for the GP, with parameters estimated by maximizing marginal likelihood. Baselines included Hyperband and Continuous Multi-Task/Fidelity BO (CM-T/F-BO). Ablation studies were conducted with vanilla BO and BO with only Logistic curve compression (BO-L). Maximum augmented points were set to 15, and the natural log of the GP condition number threshold was 20.",
        "limitations": "A naive approach of augmenting training data by adding a full curve of points can lead to redundancy and severe ill-conditioning of the Gaussian Process covariance matrix, particularly in noisy DRL settings. Existing stopping criteria and models (like exponential decay in Freeze-thaw BO) are often not applicable to DRL due to the unpredictable fluctuations and noisiness of reward curves. From a broader impact perspective, the increasing automation facilitated by such algorithms could lead to humans becoming further removed from the modeling process, potentially making it harder to detect critical failures and contributing to the growing opacity of machine learning models.",
        "future_research_directions": "The proposed framework is not limited to machine learning algorithms and can be applied more generally to any iterative process where progress can be exploited, such as optimizing manufacturing pipelines. Future work can contribute to the widespread deployment of supervised learning and reinforcement learning systems by further enhancing training efficiency and reducing their computational and environmental costs. The algorithm represents a step towards constructing fully automated pipelines for machine learning model training and deployment. It is also suggested that such automated training procedures should be integrated with the growing body of work on machine learning interpretability to rigorously analyze final training outcomes and ensure transparent decision-making.",
        "experimental_code": "import numpy as npfrom bayes_opt.acquisition_functions import AcquisitionFunction, unique_rowsfrom bayes_opt import GaussianProcessfrom bayes_opt import ProductGaussianProcessfrom bayes_opt.acquisition_maximization import acq_max_with_name,acq_min_scipy_kwargsimport timefrom sklearn import linear_modelimport copyfrom bayes_opt.curve_compression import transform_logisticfrom sklearn.preprocessing import MinMaxScalercounter = 0class BOIL(object):    def __init__(self, func, SearchSpace,acq_name=\"ei_mu_max\",verbose=1):        self.method='boil'        self.verbose=verbose        if isinstance(SearchSpace,dict):            self.keys = list(SearchSpace.keys())            self.SearchSpace = []            for key in list(SearchSpace.keys()):                self.SearchSpace.append(SearchSpace[key])            self.SearchSpace = np.asarray(self.SearchSpace)        else:            self.SearchSpace=np.asarray(SearchSpace)            self.dim = len(SearchSpace)        scaler = MinMaxScaler()        scaler.fit(self.SearchSpace[:-1,:].T)        scalerT = MinMaxScaler()        SearchSpace_T=np.atleast_2d(self.SearchSpace[-1,:]).T        scalerT.fit(SearchSpace_T)        self.Xscaler=scaler        self.Tscaler=scalerT        self.scaleSearchSpace=np.array([np.zeros(self.dim), np.ones(self.dim)]).T                self.f = func        self.X_ori= None        self.X = None        self.Y = None               self.Y_ori = None        self.T=None        self.T_original=None        self.Y_cost_original=None        self.time_opt=0         self.max_min_gap=self.SearchSpace[:,1]-self.SearchSpace[:,0]        self.acq_name = acq_name        self.logmarginal=0        self.gp=ProductGaussianProcess(self.scaleSearchSpace,verbose=verbose)        self.Y_curves=[]        self.Y_cost_original=None        self.time_opt=0        self.acq_func = None           self.logmarginal=0        self.markVirtualObs=[]        self.countVirtual=[]        self.linear_regression = linear_model.LinearRegression()        self.condition_number=[]        self.max_n_augmentation=10        self.threshold_cond=15            def init(self, n_init_points=3, seed=1):        np.random.seed(seed)        SearchSpace=np.copy(self.SearchSpace)        SearchSpace[-1,0]=SearchSpace[-1,1]        l = [np.random.uniform(x[0], x[1]) for _ in range(n_init_points) for x in SearchSpace]         temp=np.asarray(l)        temp=temp.T        init_X=list(temp.reshape((n_init_points,-1)))        self.X_original = np.asarray(init_X)        self.T_original=self.X_original[:,-1]        self.T_original=np.reshape(self.T_original,(n_init_points,-1))        self.X_original=self.X_original[:,:-1]        self.X_original=np.reshape(self.X_original,(n_init_points,-1))           y_init_curves, y_init_cost=self.f(init_X)        y_init_cost=np.atleast_2d(np.asarray(y_init_cost))        self.Y_curves+=y_init_curves        y_init=transform_logistic(y_init_curves,self.gp.logistic_hyper['midpoint'],                                  self.gp.logistic_hyper['growth'], self.SearchSpace[-1,1])        y_init=np.reshape(y_init,(n_init_points,1))                self.Y_original = np.asarray(y_init)      self.Y_cost_original=np.reshape(y_init_cost,(-1,1))        self.X = self.Xscaler.transform(np.asarray(init_X)[:,:-1])        self.X=np.reshape(self.X,(n_init_points,-1))        self.T = self.Tscaler.transform(self.T_original)        self.markVirtualObs+=[0]*n_init_points        for ii in range(n_init_points):            self.generating_virtual_observations(self.X[ii,:],                         self.T[ii],[y_init_curves[ii]],y_init_cost[0][ii],IsRandom=False)        self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))        if np.std(self.Y_original)==0:            self.Y=(self.Y_original-np.mean(self.Y_original))        else:            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)       def utility_cost_evaluation(self,x,acq_func,isDebug=False):                def utility_cost_evaluation_single(x,acq_func,isDebug=False):            utility=acq_func.acq_kind(x,gp=self.gp)                        try:                mean_cost=self.linear_regression.predict(np.reshape(x,(1,-1)))                            except:                print(x)                print(\"bug\")                mean_cost=max(0,mean_cost)+0.1            if 'ei' in acq_func.acq_name:                acquisition_function_value= np.log(utility)-np.log(mean_cost)            else:                acquisition_function_value= np.log(1+np.exp(utility))/np.log(1+np.exp(mean_cost))            if isDebug==True:                print(\"acq_func at the selected point \t utility:\",np.round(utility,decimals=4),\"\\t cost:\",mean_cost)                if utility==0:                    print(\"utility =0===============================================================================\")               return acquisition_function_value*(-1)        if len(x)==self.dim:            temp=utility_cost_evaluation_single(x,acq_func,isDebug=isDebug)            if isDebug==True:                return temp            else:                utility=np.mean(temp)        else:            utility=[0]*len(x)            for idx,val in enumerate(x):                temp=utility_cost_evaluation_single(x=val,acq_func=acq_func,isDebug=isDebug)                                                     utility[idx]=np.mean(temp)                utility=np.asarray(utility)    return utility       def acq_utility_cost(self):        acq={}        acq['name']=self.acq_name        acq['dim']=self.scaleSearchSpace.shape[0]        acq['scaleSearchSpace']=self.scaleSearchSpace           if self.acq_name=='ei_mu_max':            x_mu_max,mu_max_val=acq_max_with_name(gp=self.gp,scaleSearchSpace=self.scaleSearchSpace,acq_name='mu',IsReturnY=True)            acq['mu_max']=  mu_max_val        myacq=AcquisitionFunction(acq)                x_min = acq_min_scipy_kwargs(myfunc=self.utility_cost_evaluation,SearchSpace=self.scaleSearchSpace,                        acq_func=myacq, isDebug=False)        if self.verbose==True:            acq_val=self.utility_cost_evaluation(x_min,myacq,isDebug=False)            print(\"selected point from acq func:\",np.round(x_min,decimals=4),\"acq val=log(Utility/Cost)=\",(-1)*np.round(acq_val,decimals=4))            if np.round(acq_val,decimals=4)==0:                print(\"acq value =0\")                return x_min        def select_informative_location_by_uncertainty(self,n_virtual_obs,x_max,t_max):                        SearchSpace=np.copy(self.scaleSearchSpace)        for dd in range(self.dim-1):            SearchSpace[dd,0],SearchSpace[dd,1]=x_max[dd],x_max[dd]            SearchSpace[-1,1]=t_max        temp_X,temp_T=self.X.copy(),self.T.copy()        temp_gp=copy.deepcopy(self.gp )                temp_Y=np.random.random(size=(len(temp_T),1))                temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)        new_batch_T=None        pred_var_value=[0]*n_virtual_obs        for ii in range(n_virtual_obs):            x_max_pred_variance, pred_var_value[ii]=acq_max_with_name(gp=temp_gp,                              scaleSearchSpace=SearchSpace,acq_name='pure_exploration',IsReturnY=True)            log_cond=np.log( temp_gp.compute_condition_number() )            if log_cond>self.threshold_cond or pred_var_value[ii]<(self.gp.noise_delta+1e-3):                break                      if x_max_pred_variance[-1] in temp_T[-ii:]:                break                        temp_X = np.vstack((temp_X, x_max.reshape((1, -1))))            temp_T = np.vstack((temp_T, x_max_pred_variance[-1].reshape((1, -1))))            temp_gp.X,temp_gp.T=temp_X,temp_T            temp_Y=np.random.random(size=(len(temp_T),1))                        temp_gp.fit(temp_X,temp_T,temp_Y,self.Y_curves)            if new_batch_T is None:                new_batch_T=x_max_pred_variance[-1].reshape((1, -1))            else:                new_batch_T= np.vstack((new_batch_T, x_max_pred_variance[-1].reshape((1, -1))))                if new_batch_T is None:            return [],0        else:            output=np.sort(new_batch_T.ravel()).tolist()            return output, len(output)        def generating_virtual_observations(self,x_max,t_max,y_original_curves,y_cost_original,IsRandom=False):                temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))        max_n_virtual_obs=np.int(t_max*self.max_n_augmentation)        if max_n_virtual_obs==0:            self.countVirtual.append(0)            return                if IsRandom==True:            l = [np.random.uniform(0, t_max) for _ in range(max_n_virtual_obs)]        else:            l,n_virtual_obs=self.select_informative_location_by_uncertainty(max_n_virtual_obs,x_max,t_max)                    self.countVirtual.append(n_virtual_obs)                if self.verbose:            np.set_printoptions(suppress=True)            print(\"Max #augmented points\",max_n_virtual_obs, \"\\t #augmented points \",len(l),                  \"\\t Augmented points: \",np.round(l,decimals=3))                    l_original=[self.SearchSpace[-1,0]+val*self.max_min_gap[-1] for val in l]                               virtual_obs_t_original=np.asarray(l_original).T        virtual_obs_t=np.asarray(l).T                y_virtual_original=[0]*n_virtual_obs        for ii in range(n_virtual_obs):                        idx=np.int(virtual_obs_t_original[ii])                        temp_curve=y_original_curves[0][:idx+1]            self.markVirtualObs.append(1)            y_virtual_original[ii]=transform_logistic([temp_curve],                      self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])                       self.X = np.vstack((self.X, x_max.reshape((1, -1))))            self.X_original=np.vstack((self.X_original, temp_X_new_original))                    self.T = np.vstack((self.T, virtual_obs_t[ii].reshape((1, -1))))            temp=np.asarray(virtual_obs_t_original[ii])            self.T_original=np.vstack((self.T_original, temp.reshape((1, -1))))            self.Y_original = np.append(self.Y_original,[y_virtual_original[ii]])            self.Y_curves.append(temp_curve)                        y_cost_estimate=y_cost_original*virtual_obs_t[ii]            self.Y_cost_original = np.append(self.Y_cost_original,[y_cost_estimate])                            def suggest_nextpoint(self):         self.gp=ProductGaussianProcess(self.scaleSearchSpace,self.gp.hyper,self.gp.logistic_hyper)        self.gp.fit(self.X, self.T,self.Y,self.Y_curves)                    self.condition_number.append(self.gp.cond_num)        if self.verbose:            print(\"ln of conditioning number of GP covariance matrix\", np.round(np.log(self.gp.cond_num),decimals=1))        count=len(self.markVirtualObs)-np.sum(self.markVirtualObs)        count=np.int(count)        if  len(self.Y)%(2*self.dim)==0:            hyper=[self.gp.hyper['lengthscale_x'],self.gp.hyper['lengthscale_t'],                     self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth']]            newlengthscale_x,newlengthscale_t,new_midpoint, new_growth = self.gp.optimize_lengthscale_logistic_hyper(hyper,self.gp.noise_delta)                        self.gp.hyper['lengthscale_x']=newlengthscale_x            self.gp.hyper['lengthscale_t']=self.gp.hyper['lengthscale_t']            self.gp.logistic_hyper['midpoint']=new_midpoint            self.gp.logistic_hyper['growth']=new_growth                      if self.verbose:                print(\"==estimated lengthscale_x={:.4f}   lengthscale_t={:.3f}   Logistic_m0={:.1f}   Logistic_g0={:.1f}\".format(                    newlengthscale_x,newlengthscale_t,new_midpoint,new_growth))                start_opt=time.time()        combine_input=np.hstack((self.X,self.T))        self.linear_regression.fit(combine_input,self.Y_cost)                x_max_temp=self.acq_utility_cost()        x_max=x_max_temp[:-1]        t_max=x_max_temp[-1]               finished_opt=time.time()        elapse_opt=finished_opt-start_opt        self.time_opt=np.hstack((self.time_opt,elapse_opt))                self.markVirtualObs.append(0)        self.X = np.vstack((self.X, x_max.reshape((1, -1))))        self.T = np.vstack((self.T, t_max.reshape((1, -1))))        temp_X_new_original=self.Xscaler.inverse_transform(np.reshape(x_max,(-1,self.dim-1)))        self.X_original=np.vstack((self.X_original, temp_X_new_original))                temp_T_new_original=self.Tscaler.inverse_transform(np.reshape(t_max,(-1,1)))        self.T_original=np.vstack((self.T_original, temp_T_new_original))        x_original_to_test=x_max_temp*self.max_min_gap+self.SearchSpace[:,0]        y_original_curves, y_cost_original= self.f(x_original_to_test)                y_original=transform_logistic(y_original_curves,              self.gp.logistic_hyper['midpoint'],self.gp.logistic_hyper['growth'],self.SearchSpace[-1,1])                if len(y_original_curves)==1:            self.Y_curves.append(y_original_curves[0])        else:            self.Y_curves.append(y_original_curves)                self.Y_original = np.append(self.Y_original,y_original)        self.Y_cost_original = np.append(self.Y_cost_original,y_cost_original)        self.generating_virtual_observations(x_max,t_max,y_original_curves,y_cost_original[0])                if np.std(self.Y_original)==0:            self.Y=(self.Y_original-np.mean(self.Y_original))        else:            self.Y=(self.Y_original-np.mean(self.Y_original))/np.std(self.Y_original)                    self.Y_cost=(self.Y_cost_original-np.min(self.Y_cost_original))/(np.max(self.Y_cost_original)-np.min(self.Y_cost_original))                            np.set_printoptions(suppress=True)        print(\"[original scale] x={} t={:.0f} current y={:.2f}, ybest={:.2f}\".format( np.round(self.X_original[-1],decimals=4),              np.asscalar(self.T_original[-1]),np.asscalar(self.Y_original[-1]), np.asscalar(self.Y_original.max())))import numpy as npfrom bayes_opt.acquisition_functions import unique_rowsfrom sklearn.preprocessing import MinMaxScalerfrom scipy.optimize import minimizefrom sklearn.metrics.pairwise import euclidean_distancesimport scipy.linalg as splafrom bayes_opt.curve_compression import apply_one_transform_logistic, transform_logisticclass ProductGaussianProcess(object):    def __init__ (self,SearchSpace,gp_hyper=None,logistic_hyper=None,verbose=0):        self.noise_delta=5e-4        self.noise_upperbound=1e-2        self.mycov=self.cov_RBF_time        self.SearchSpace=SearchSpace        scaler = MinMaxScaler()        scaler.fit(SearchSpace.T)        self.Xscaler=scaler        self.verbose=verbose        self.dim=SearchSpace.shape[0]                if gp_hyper is None:            self.hyper={}            self.hyper['var']=1            self.hyper['lengthscale_x']=0.02            self.hyper['lengthscale_t']=0.2        else:            self.hyper=gp_hyper                if logistic_hyper is None:            self.logistic_hyper={}            self.logistic_hyper['midpoint']=0.0            self.logistic_hyper['growth']=1.0        else:            self.logistic_hyper=logistic_hyper        self.X=[]        self.T=[]        self.Y=[]        self.Y_curves=None        self.alpha=[]        self.L=[]        self.MaxEpisode=0                return None               def cov_RBF_time(self, x1,t1,x2,t2,lengthscale,lengthscale_t):                Euc_dist=euclidean_distances(x1,x2)        exp_dist_x=np.exp(-np.square(Euc_dist)/lengthscale)                Euc_dist=euclidean_distances(t1,t2)        exp_dist_t=np.exp(-np.square(Euc_dist)/lengthscale_t)                return exp_dist_x*exp_dist_t                        def fit(self,X,T,Y,Y_curves):                temp=np.hstack((X,T))        ur = unique_rows(temp)                T=T[ur]        X=X[ur]        Y=Y[ur]                self.X=X        self.Y=Y        self.T=T        self.Y_curves=[val for idx,val in enumerate(Y_curves) if ur[idx]==True]                for curves in self.Y_curves:            self.MaxEpisode=max(len(curves),self.MaxEpisode)                        Euc_dist_x=euclidean_distances(X,X)            Euc_dist_t=euclidean_distances(T,T)                self.KK_x_x=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']                           -np.square(Euc_dist_t)/self.hyper['lengthscale_t'])+np.eye(len(X))*self.noise_delta                  if np.isnan(self.KK_x_x).any():            print(\"nan in KK_x_x\")                self.L=np.linalg.cholesky(self.KK_x_x)        temp=np.linalg.solve(self.L,self.Y)        self.alpha=np.linalg.solve(self.L.T,temp)        self.cond_num=self.compute_condition_number()            def compute_condition_number(self):        cond_num=np.linalg.cond(self.KK_x_x)        return cond_num            def log_marginal_lengthscale_logistic_hyper(self,hyper,noise_delta):                def compute_log_marginal_with_logistic_hyper(lengthscale, lengthscale_t,midpoint,growth,noise_delta):            temp=np.hstack((self.X,self.T))            ur = unique_rows(temp)            myX=self.X[ur]            myT=self.T[ur]                        Y_original=transform_logistic(self.Y_curves,midpoint,growth,self.MaxEpisode)            myY=(Y_original-np.mean(Y_original))/np.std(Y_original)                        myY=myY[ur]                      self.Euc_dist_x=euclidean_distances(myX,myX)            self.Euc_dist_t=euclidean_distances(myT,myT)                KK=np.exp(-np.square(self.Euc_dist_x)/lengthscale-np.square(self.Euc_dist_t)/lengthscale_t)                +np.eye(len(myX))*noise_delta                                    try:                temp_inv=np.linalg.solve(KK,myY)            except:                return -np.inf                        try:                first_term=-0.5*np.dot(myY.T,temp_inv)                                if KK.shape[0]>200:                    idx=np.random.permutation(KK.shape[0])                    idx=idx[:200]                    KK=KK[np.ix_(idx,idx)]                chol  = spla.cholesky(KK, lower=True)                W_logdet=np.sum(np.log(np.diag(chol)))                second_term=-W_logdet            except:                return -np.inf                        logmarginal=first_term+second_term-0.5*len(myY)*np.log(2*3.14)                        if np.isnan(np.asscalar(logmarginal))==True:                print(\"lengthscale_x={:f} lengthscale_t={:f} first term ={:.4f} second  term ={:.4f}\".format(                        lengthscale,lengthscale_t,np.asscalar(first_term),np.asscalar(second_term)))            return np.asscalar(logmarginal)                logmarginal=0        if not isinstance(hyper,list) and len(hyper.shape)==2:            logmarginal=[0]*hyper.shape[0]            growth=hyper[:,3]            midpoint=hyper[:,2]            lengthscale_t=hyper[:,1]            lengthscale_x=hyper[:,0]            for idx in range(hyper.shape[0]):                logmarginal[idx]=compute_log_marginal_with_logistic_hyper(lengthscale_x[idx],                           lengthscale_t[idx],midpoint[idx],growth[idx],noise_delta)        else:            lengthscale_x,lengthscale_t,midpoint,growth=hyper            logmarginal=compute_log_marginal_with_logistic_hyper(lengthscale_x,lengthscale_t,                                                                 midpoint,growth,noise_delta)        return logmarginal        def optimize_lengthscale_SE_logistic_hyper(self,previous_hyper,noise_delta):                SearchSpace_l_min=0.03        SearchSpace_l_max=0.3                SearchSpace_midpoint_min=-2        SearchSpace_midpoint_max=3                SearchSpace_growth_min=0.5        SearchSpace_growth_max=2                mySearchSpace=np.asarray([[SearchSpace_l_min,SearchSpace_l_max],[10*SearchSpace_l_min,2*SearchSpace_l_max],                             [SearchSpace_midpoint_min,SearchSpace_midpoint_max],[SearchSpace_growth_min,SearchSpace_growth_max]])                lengthscale_tries = np.random.uniform(mySearchSpace[:, 0], mySearchSpace[:, 1],size=(20, 4))        self.flagOptimizeHyperFirst=0        logmarginal_tries=self.log_marginal_lengthscale_logistic_hyper(lengthscale_tries,noise_delta)        idx_max=np.argmax(logmarginal_tries)        lengthscale_init_max=lengthscale_tries[idx_max]                myopts ={'maxiter':30*self.dim,'maxfun':30*self.dim}        x_max=[]        max_log_marginal=None                res = minimize(lambda x: -self.log_marginal_lengthscale_logistic_hyper(x,noise_delta),lengthscale_init_max,                       bounds=mySearchSpace,method=\"L-BFGS-B\",options=myopts)        if 'x' not in res:            val=self.log_marginal_lengthscale_logistic_hyper(res,noise_delta)        else:            val=self.log_marginal_lengthscale_logistic_hyper(res.x,noise_delta)          if max_log_marginal is None or val >= max_log_marginal:            if 'x' not in res:                x_max = res            else:                x_max = res.x            max_log_marginal = val        return x_max            def optimize_lengthscale_logistic_hyper(self,prev_hyper,noise_delta):                        newlengthscale,newlengthscale_t,newmidpoint,newgrowth=self.optimize_lengthscale_SE_logistic_hyper(prev_hyper,noise_delta)        self.hyper['lengthscale_x']=newlengthscale        self.hyper['lengthscale_t']=newlengthscale_t                temp=np.hstack((self.X,self.T))        ur = unique_rows(temp)        Y_original=transform_logistic(self.Y_curves,newmidpoint,newgrowth,self.SearchSpace[-1,1])        Y=(Y_original-np.mean(Y_original))/np.std(Y_original)        self.Y=Y                self.fit(self.X[ur],self.T[ur],self.Y[ur],self.Y_curves)                return newlengthscale,newlengthscale_t,newmidpoint,newgrowth            def compute_var(self,X,T,xTest,tTest):                xTest=np.asarray(xTest)        xTest=np.atleast_2d(xTest)                tTest=np.asarray(tTest)        tTest=np.atleast_2d(tTest)        tTest=np.reshape(tTest,(-1,1))                if self.kernel_name=='SE':            myX=X            myT=T                        Euc_dist_x=euclidean_distances(myX,myX)                    Euc_dist_t=euclidean_distances(myT,myT)                    KK=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])                +np.eye(len(myX))*self.noise_delta                                 Euc_dist_test_train_x=euclidean_distances(xTest,X)            Euc_dist_test_train_t=euclidean_distances(tTest,T)                        KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])                        try:            temp=np.linalg.solve(KK,KK_xTest_xTrain.T)        except:            temp=np.linalg.lstsq(KK,KK_xTest_xTrain.T, rcond=-1)            temp=temp[0]                var=np.eye(xTest.shape[0])-np.dot(temp.T,KK_xTest_xTrain.T)        var=np.diag(var)        var.flags['WRITEABLE']=True        var[var<1e-100]=0        return var             def predict(self,xTest, eval_MSE=True):                if len(xTest.shape)==1:            xTest=xTest.reshape((-1,self.X.shape[1]+1))                        tTest=xTest[:,-1]        tTest=np.atleast_2d(tTest)        tTest=np.reshape(tTest,(xTest.shape[0],-1))                xTest=xTest[:,:-1]                temp=np.hstack((self.X,self.T))        ur = unique_rows(temp)                X=self.X[ur]        T=self.T[ur]                        Euc_dist_x=euclidean_distances(xTest,xTest)        Euc_dist_t=euclidean_distances(tTest,tTest)        KK_xTest_xTest=np.exp(-np.square(Euc_dist_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_t)/self.hyper['lengthscale_t'])                +np.eye(xTest.shape[0])*self.noise_delta                Euc_dist_test_train_x=euclidean_distances(xTest,X)                Euc_dist_test_train_t=euclidean_distances(tTest,T)                KK_xTest_xTrain=np.exp(-np.square(Euc_dist_test_train_x)/self.hyper['lengthscale_x']-np.square(Euc_dist_test_train_t)/self.hyper['lengthscale_t'])                      mean=np.dot(KK_xTest_xTrain,self.alpha)        v=np.linalg.solve(self.L,KK_xTest_xTrain.T)        var=KK_xTest_xTest-np.dot(v.T,v)                return mean.ravel(),np.diag(var)            def posterior(self,x):        return self.predict(self,x)import numpy as npfrom scipy.stats import normcounter = 0class AcquisitionFunction(object):    def __init__(self, acq):        self.acq=acq        acq_name=acq['name']                if 'mu_max' in acq:            self.mu_max=acq['mu_max']        ListAcq=['bucb','ucb', 'ei','poi','random','ucb_pe',                 'pure_exploration','mu','lcb','ei_mu_max']                IsTrue=[val for idx,val in enumerate(ListAcq) if val in acq_name]        if  IsTrue == []:            err = \"The utility function \"                  \"{} has not been implemented, \"                  \"please choose one of ucb, ei, or poi.\".format(acq_name)            raise NotImplementedError(err)        else:            self.acq_name = acq_name                    self.dim=acq['dim']                if 'scalebounds' not in acq:            self.scalebounds=[0,1]*self.dim                    else:            self.scalebounds=acq['scalebounds']                           def acq_kind(self, x, gp):                y_max=np.max(gp.Y)        if np.any(np.isnan(x)):            return 0               if self.acq_name == 'ucb':            return self._ucb(x, gp)        if self.acq_name == 'lcb':            return self._lcb(x, gp)        if self.acq_name == 'ei':            return self._ei(x, gp, y_max)        if self.acq_name == 'ei_mu_max':            return self._ei(x, gp, self.mu_max)        if self.acq_name == 'poi':            return self._poi(x, gp, y_max)                if self.acq_name == 'pure_exploration':            return self._pure_exploration(x, gp)               if self.acq_name == 'mu':            return self._mu(x, gp)                if self.acq_name == 'ucb_pe':            return self._ucb_pe(x, gp,self.acq['kappa'],self.acq['maxlcb'])                   def utility_plot(self, x, gp, y_max):        if np.any(np.isnan(x)):            return 0        if self.acq_name == 'ei':            return self._ei_plot(x, gp, y_max)           @staticmethod    def _mu(x, gp):        mean, var = gp.predict(x, eval_MSE=True)        mean=np.atleast_2d(mean).T        return mean                        @staticmethod    def _lcb(x, gp):        mean, var = gp.predict(x, eval_MSE=True)        var.flags['WRITEABLE']=True        var[var<1e-10]=0        mean=np.atleast_2d(mean).T        var=np.atleast_2d(var).T        beta_t = 2 * np.log(len(gp.Y));        return mean - np.sqrt(beta_t) * np.sqrt(var)             @staticmethod    def _ucb(x, gp):        mean, var = gp.predict(x, eval_MSE=True)        var.flags['WRITEABLE']=True        var[var<1e-10]=0        mean=np.atleast_2d(mean).T        var=np.atleast_2d(var).T                        beta_t = 2 * np.log(len(gp.Y));          return mean + np.sqrt(beta_t) * np.sqrt(var)            @staticmethod    def _ucb_pe(x, gp, kappa, maxlcb):        mean, var = gp.predict_bucb(x, eval_MSE=True)        var.flags['WRITEABLE']=True        var[var<1e-10]=0        mean=np.atleast_2d(mean).T        var=np.atleast_2d(var).T        value=mean + kappa * np.sqrt(var)        myidx=[idx for idx,val in enumerate(value) if val<maxlcb]        var[myidx]=0        return var               @staticmethod    def _pure_exploration(x, gp):        mean, var = gp.predict(x, eval_MSE=True)        var.flags['WRITEABLE']=True        var[var<1e-10]=0        mean=np.atleast_2d(mean).T        var=np.atleast_2d(var).T        return np.sqrt(var)               @staticmethod    def _ei(x, gp, y_max):        y_max=np.asscalar(y_max)        mean, var = gp.predict(x, eval_MSE=True)        var2 = np.maximum(var, 1e-10 + 0 * var)        z = (mean - y_max)/np.sqrt(var2)        out=(mean - y_max) * norm.cdf(z) + np.sqrt(var2) * norm.pdf(z)        out[var2<1e-10]=0        return out           @staticmethod          def _poi(x, gp,y_max):        mean, var = gp.predict(x, eval_MSE=True)        var = np.maximum(var, 1e-9 + 0 * var)        z = (mean - y_max)/np.sqrt(var)        return norm.cdf(z)           def unique_rows(a):    order = np.lexsort(a.T)    reorder = np.argsort(order)    a = a[order]    diff = np.diff(a, axis=0)    ui = np.ones(len(a), 'bool')    ui[1:] = (diff != 0).any(axis=1)    return ui[reorder]class BColours(object):    BLUE = '\\033[94m'    CYAN = '\\033[36m'    GREEN = '\\033[32m'    MAGENTA = '\\033[35m'    RED = '\\033[31m'    ENDC = '\\033[0m'import numpy as npfrom scipy.optimize import minimizefrom bayes_opt.acquisition_functions import AcquisitionFunctionimport sobol_seq__author__ = 'Vu'def acq_max_with_name(gp,scaleSearchSpace,acq_name=\"ei\",IsReturnY=False,IsMax=True,fstar_scaled=None):    acq={}    acq['name']=acq_name    acq['dim']=scaleSearchSpace.shape[0]    acq['scaleSearchSpace']=scaleSearchSpace       if fstar_scaled:        acq['fstar_scaled']=fstar_scaled       myacq=AcquisitionFunction(acq)    if IsMax:        x_max = acq_max(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace,opt_toolbox='scipy')    else:        x_max = acq_min_scipy(ac=myacq.acq_kind,gp=gp,bounds=scaleSearchSpace)    if IsReturnY==True:        y_max=myacq.acq_kind(x_max,gp=gp)        return x_max,y_max    return x_maxdef acq_max(ac, gp, bounds, opt_toolbox='scipy',seeds=[],IsMax=True):    y_max=np.max(gp.Y)      x_max = acq_max_scipy(ac=ac,gp=gp,y_max=y_max,bounds=bounds)    return x_maxdef generate_sobol_seq(dim,nSobol):    mysobol_seq = sobol_seq.i4_sobol_generate(dim, nSobol)    return mysobol_seq        def acq_min_scipy_kwargs(myfunc, SearchSpace, **kwargs):    dim=SearchSpace.shape[0]    x_max = SearchSpace[:, 0]    min_acq = None    myopts ={'maxiter':10*dim,'maxfun':20*dim}    for i in range(3*dim):        x_tries = np.random.uniform(SearchSpace[:, 0], SearchSpace[:, 1],size=(100*dim, dim))            y_tries=myfunc(x_tries,**kwargs)                idx_min=np.argmin(y_tries)        x_init_min=x_tries[idx_min]        res = minimize(lambda x: myfunc(x.reshape(1, -1), **kwargs),x_init_min.reshape(1, -1),bounds=SearchSpace,                       method=\"L-BFGS-B\",options=myopts)        if 'x' not in res:            val=myfunc(res,**kwargs)        else:            val=myfunc(res.x,**kwargs)         if min_acq is None or val <= min_acq:            if 'x' not in res:                x_max = res            else:                x_max = res.x            min_acq = val    return np.clip(x_max, SearchSpace[:, 0], SearchSpace[:, 1])        def acq_min_scipy(ac, gp, bounds):    dim=bounds.shape[0]    x_max = bounds[:, 0]    min_acq = None    myopts ={'maxiter':10*dim,'maxfun':20*dim}    for i in range(3*dim):        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))            y_tries=ac(x_tries,gp=gp)                idx_max=np.argmin(y_tries)        x_init_max=x_tries[idx_max]            res = minimize(lambda x: ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,                       method=\"L-BFGS-B\",options=myopts)          if 'x' not in res:            val=ac(res,gp)        else:            val=ac(res.x,gp)         if min_acq is None or val <= min_acq:            if 'x' not in res:                x_max = res            else:                x_max = res.x            min_acq = val    return np.clip(x_max, bounds[:, 0], bounds[:, 1])        def acq_max_scipy(ac, gp, y_max, bounds):    dim=bounds.shape[0]    x_max = bounds[:, 0]    max_acq = None    myopts ={'maxiter':10*dim,'maxfun':20*dim}    for i in range(1*dim):        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(50*dim, dim))            y_tries=ac(x_tries,gp=gp)                idx_max=np.argmax(y_tries)        x_init_max=x_tries[idx_max]            res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,                       method=\"L-BFGS-B\",options=myopts)        if 'x' not in res:            val=ac(res,gp)        else:            val=ac(res.x,gp)         if max_acq is None or val >= max_acq:            if 'x' not in res:                x_max = res            else:                x_max = res.x            max_acq = val    return np.clip(x_max, bounds[:, 0], bounds[:, 1])        def acq_max_with_init(ac, gp, y_max, bounds, init_location=[]):    dim=bounds.shape[0]    x_max = bounds[:, 0]    max_acq = None    myopts ={'maxiter':5*dim,'maxfun':10*dim}    for i in range(2*dim):        x_tries = np.random.uniform(bounds[:, 0], bounds[:, 1],size=(20*dim, dim))                if init_location!=[]:            x_tries=np.vstack((x_tries,init_location))                y_tries=ac(x_tries,gp=gp)                idx_max=np.argmax(y_tries)        x_init_max=x_tries[idx_max]            res = minimize(lambda x: -ac(x.reshape(1, -1), gp=gp),x_init_max.reshape(1, -1),bounds=bounds,                       method=\"L-BFGS-B\",options=myopts)        if 'x' not in res:            val=ac(res,gp)        else:            val=ac(res.x,gp)         if max_acq is None or val >= max_acq:            if 'x' not in res:                x_max = res            else:                x_max = res.x            max_acq = val    return np.clip(x_max, bounds[:, 0], bounds[:, 1])import itertoolsimport numpy as npdef apply_one_transform_average(curve, midpoint=3, growth=1,MaxEpisode=1000):            if isinstance(curve, (list,)):        curve=curve[0]         def linear_func(x):        if len(x)==1:            return 1        else:            return [1 for u in x]    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)    my_logistic_value_scaled=linear_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    average=np.mean(curve)        return average,my_logistic_value_scaleddef return_logistic_curve(midpoint, growth, MaxEpoch=1000):        def logistic_func(x):        if len(x)==1:            return 1.0/(1+np.exp(-growth*(x-midpoint)))        else:            return [1.0/(1+np.exp(-growth*(u-midpoint))) for u in x]            my_xrange_scaled=np.linspace(-6,6, MaxEpoch)    my_logistic_value_scaled=logistic_func(my_xrange_scaled)        return my_logistic_value_scaleddef apply_one_transform_ln(curve, midpoint=3, growth=1,MaxEpisode=1000):    if isinstance(curve, (list,)):        curve=curve[0]         def ln_func(x):        if len(x)==1:            return 20+np.log(x)        else:            return [np.log(u) for u in x]    my_xrange_scaled=np.linspace(0.01,5, MaxEpisode)    my_logistic_value_scaled=ln_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    if np.max(curve)<=0 and np.min(curve)<=0:        curve=curve+500        threshold=(midpoint+6-2)*len(curve)/(12)    threshold=np.int(threshold)        prod_func=curve*my_logistic_value_scaled        average=[np.mean(prod_func[threshold:pos]) for pos in range(threshold,len(prod_func))]    if np.isnan(average[-1]):        print('bug [curve]')    return average[-1],my_logistic_value_scaleddef apply_one_transform_logistic(curve, midpoint=-2, growth=1,MaxEpisode=1000,IsReturnCurve=False):    if isinstance(curve, (list,)):        curve=curve[0]            def logistic_func(x):        return 1.0/(1+np.exp(-growth*(x-midpoint)))            my_xrange_scaled=np.linspace(-6,6, int(MaxEpisode))    my_logistic_value_scaled=logistic_func(my_xrange_scaled)    my_logistic_value_scaled=my_logistic_value_scaled[:len(curve)]    if np.max(curve)<=0 and np.min(curve)<=0:        curve=curve+500        threshold=(midpoint+6-2)*len(curve)/(12)    threshold=np.int(threshold)        prod_func=curve*my_logistic_value_scaled        average=[np.mean(prod_func[threshold:pos+1]) for pos in range(threshold,len(prod_func))]    if IsReturnCurve==True:        return average[-1],my_logistic_value_scaled    else:        return average[-1]def transform_logistic_marginal(curves,MaxEpisode=1000):    def transform_one_logistic_marginal(curves,MaxEpisode):            midpoint_list=[-3,-2,-1,0,1]        growth_list=[0.1,1,2,3]                temp_Y_value=[0]*(len(midpoint_list)*len(growth_list))        for idx, (val1, val2) in enumerate(itertools.product(midpoint_list,growth_list)):            temp_Y_value[idx]=apply_one_transform_logistic(curves,val1, val2,MaxEpisode)                        temp_Y_value=np.asarray(temp_Y_value)                Y=np.mean(temp_Y_value,axis=0)        return Y    if len(curves)==1:        output=transform_one_logistic_marginal(curves[0],MaxEpisode)    else:        output=[0]*len(curves)        for idx, curve in enumerate(curves):            output[idx]=transform_one_logistic_marginal(curve,MaxEpisode)    return outputdef transform_logistic(curves, midpoint=0, growth=1,MaxEpisode=1000):    if len(curves)==1:        output=apply_one_transform_logistic(curves[0], midpoint, growth,MaxEpisode)    else:        output=[0]*len(curves)        for idx, curve in enumerate(curves):            output[idx]=apply_one_transform_logistic(curve, midpoint, growth,MaxEpisode)    return output",
        "experimental_info": "The BOIL algorithm uses a Product Gaussian Process (GP) with a product kernel. Initial GP kernel hyperparameters are: `lengthscale_x = 0.02` and `lengthscale_t = 0.2`. The GP noise parameter `noise_delta` is set to `5e-4`, with an upper bound `noise_upperbound = 1e-2`. The utility score is compressed using a Sigmoid (Logistic) preference function, initialized with `midpoint = 0.0` and `growth = 1.0`. The `MaxEpisode` for utility calculation is dynamically set based on observed curves.\n\nThe initial phase of BOIL uses `n_init_points = 3` randomly sampled points. Hyperparameters for the GP (both lengthscales) and the Logistic preference function (midpoint, growth) are optimized together by maximizing the GP's log marginal likelihood every `2 * dim` iterations, where `dim` is the dimensionality of the search space. The optimization search bounds for these parameters are:\n- `lengthscale_x`: [0.03, 0.3]\n- `lengthscale_t`: [0.03, 0.6] (10*SearchSpace_l_min, 2*SearchSpace_l_max implicitly meaning 10*0.03 and 2*0.3)\n- `midpoint`: [-2, 3]\n- `growth`: [0.5, 2]\n\nFor selecting the next hyperparameter and iteration count, a modified Expected Improvement (`acq_name = \"ei_mu_max\"`) criterion is used, normalized by the predicted training cost. The training cost is modeled using a `sklearn.linear_model.LinearRegression()` model, fitted on the observed hyperparameter-iteration pairs and their corresponding costs.\n\nSelective data augmentation is employed to improve sample efficiency and prevent GP covariance matrix ill-conditioning. A maximum of `max_n_augmentation = 10` virtual points can be added per real observation. These points are strategically sampled at locations of maximum GP predictive uncertainty. The augmentation process is dynamically controlled by a condition number threshold, `threshold_cond = 15`. If the log of the GP covariance matrix's condition number exceeds this threshold, or if predictive uncertainty is too low, augmentation for that observation stops."
      }
    }
  ]
}